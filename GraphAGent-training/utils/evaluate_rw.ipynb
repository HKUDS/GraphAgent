{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/xzou428/Yuhao/HiGPT-tune-lightning\n"
     ]
    }
   ],
   "source": [
    "%cd /hpc2hdd/home/xzou428/Yuhao/HiGPT-tune-lightning/\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "res_dict = {}\n",
    "predict_dir = \"checkpoints/vanilla-llama3-nlp_rw-epoch30-8192-full_finetune/lightning_logs/version_1/predict\"\n",
    "ground_truth_file = \"dataset/NLP_rw_new_batch/instruction/test_data_list_1500_8192_ans_gt_256_llama3.pt\"\n",
    "\n",
    "predict_files = list(Path(predict_dir).glob(\"*.txt\"))\n",
    "for file in predict_files:\n",
    "    res_dict[int(file.stem)] = {\n",
    "        \"predict\": file.read_text()\n",
    "    }\n",
    "\n",
    "ground_truths = torch.load(ground_truth_file)\n",
    "for data_entry in ground_truths:\n",
    "    if data_entry[\"id\"] in res_dict:\n",
    "        res_dict[data_entry[\"id\"]][\"ground_truth\"] = data_entry[\"ground_truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(res_dict))\n",
    "\n",
    "res_dict = {k: v for k, v in res_dict.items() if \"ground_truth\" in v}\n",
    "print(len(res_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4964: {'predict': 'The Transformer model (@CITE[1]@) and its variants (@CITE[4]@; ; @CITE[6]@) have been widely adopted in NLP and CV tasks. In the Transformer model, the positional encoding is used to encode the position information of tokens. The positional encoding is usually a learnable embedding. In the original Transformer model, the learnable embedding is added to the token embedding to encode the position information. In the BERT model (@CITE[2]@), the absolute position embedding is used to encode the position information of tokens. The absolute position embedding is also used in the DeBERTa model (@CITE[3]@) and the Longformer model (@CITE[5]@). In the Swin Transformer model (@CITE[6]@), the shifted windowing scheme is used to encode the position information of tokens. The shifted windowing scheme is also used in the Swin V2 Transformer model (@CITE[5]@). In our model, we use the shifted windowing scheme to encode the position information of tokens. Our model is different from the Swin Transformer model and the Swin V2 Transformer model in that we use the shifted windowing scheme to encode the position information of tokens in the vision tasks, while the Swin Transformer model and the Swin V2 Transformer model use the shifted windowing scheme to encode the position information of tokens in the NLP tasks.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Self-Attention with Relative Position Representations\\nID: 6. Title: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
       "  'ground_truth': '\\n\\nThe attention mechanism is the primary component of transformer (@CITE[2]@). Since the attention mechanism is order-independent, information about the position should be injected for each token. In general, there are two mainstream encoding methods: absolute and relative position encoding. Absolute Position Encoding. The original transformer (@CITE[2]@) considers the absolute positions, and the encodings are generated based on the sinusoids of varying frequency. Recent 3D object detection studies also use absolute position encodings. In Group-free (Liu et al., 2021b), the encodings are learned by the center and size of the predicted bounding box, while the Fourier function is used in 3DETR (@CITE[5]@). Relative Position Encoding. Recently, some advanced works in natural language processing (@CITE[1]@; @CITE[3]@; @CITE[4]@) and image understanding (@CITE[6]@;, 2018) generate position encoding based on the relative distance between tokens. Relative relation representations are important for tasks where the relative ordering or distance matters. Our method extends relative position encoding to 3D Euclidean space and enhances relative relation reasoning ability in 3D visual grounding.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Self-Attention with Relative Position Representations\\nID: 6. Title: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows<|eot_id|>'},\n",
       " 3978: {'predict': 'Synthetic data augmentation is a promising technique for NLP tasks, which can be used to augment the training set and improve the generalization ability of models. For instance, @CITE[7]@; @CITE[2]@; @CITE[1]@; @CITE[4]@; @CITE[8]@; @CITE[3]@; @CITE[6]@; @CITE[5]@; @CITE[3]@; @CITE[8]@; @CITE[6]@; @CITE[3]@; @CITE[6]@; @CITE[3]@; @CITE[6]@ generate synthetic data for NER, sentiment analysis, and relation extraction. In this work, we focus on the relation extraction task and explore the effectiveness of synthetic data augmentation for improving the generalization ability of RE models.\\nReferences:ID: 1. Title: A Survey of Data Augmentation Approaches for NLP\\nID: 2. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 3. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 4. Title: Prototypical Representation Learning for Relation Extraction\\nID: 5. Title: RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\\nID: 6. Title: Prototypical Representation Learning for Relation Extraction\\nID: 7. Title: RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\\nID: 8. Title: STraTA: Self-Training with Task Augmentation for Better Few-shot Learning',\n",
       "  'ground_truth': '\\n\\nBuilt upon massive corpora, pre-trained language models are promising at producing texts of eligible quality, resulting in a surge of research interests in its usage for data augmentation (@CITE[8]@. One straightforward way is to introduce mask corruptions in the way language models are pre-trained, then collect predictions as augmented data (@CITE[5]@; @CITE[7]@. Later works further developed such technique into conditional augmentation (@CITE[3]@; Kumar et al., 2020). Nevertheless, these methods are mostly editing existing instances, which limits the diversity and scale of augmented data. With increasingly powerful LLMs, recent works turn to direct synthesis of new instances (Schick and Schütze, 2021; ; Meng et al., 2022; @CITE[4]@. Different from this work, most of them focus on zero-shot language understanding where no labeled data is available (Schick and Schütze, 2021; ; Meng et al., 2022; @CITE[4]@. They investigate ways to generate label-conditioned data by prompting LLMs, but these methods can hardly be applied to low-resource or full data scenarios while still preserving effectiveness. With the existence of labeled data, synthetic data needs to be of higher quality to bring further utility. Several works thus propose to finetune the generator (; @CITE[1]@; @CITE[6]@. There are also explorations for learning from synthetic and golden data together, including threshold-based confidence filtering (, classical semi-supervised learning (@CITE[6]@ or restricting the usage of synthetic data within a supplemental intermediate task (@CITE[1]@. For structured learning tasks, @CITE[7]@ similarly formulates NER task data as sequential language. Specifically for relational data synthesis, @CITE[2]@ explore the biomedical domain and @CITE[4]@ focus on zero-shot setting of triplet extraction. By contrast, Syn2RE distinguishes not only in applied scenario and synthesis strategy, but also in the two-stage learning framework, which is specially designed for improved synthetic data adaptation.\\nReferences:ID: 1. Title: A Survey of Data Augmentation Approaches for NLP\\nID: 2. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 3. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 4. Title: Prototypical Representation Learning for Relation Extraction\\nID: 5. Title: RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\\nID: 6. Title: Prototypical Representation Learning for Relation Extraction\\nID: 7. Title: RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\\nID: 8. Title: STraTA: Self-Training with Task Augmentation for Better Few-shot Learning<|eot_id|>'},\n",
       " 1028: {'predict': 'Knowledge graphs can be classified into different categories based on their size, coverage, and domain. Based on the size of the graph, we can classify KGCs into small-scale, medium-scale, and large-scale KGCs. Small-scale KGCs typically contain a few thousands of entities and relations, such as WordNet ( and YAGO (, while medium-scale KGCs contain tens of thousands of entities and relations, such as FB15k-237 (@CITE[3]@ and WN18RR (@CITE[3]@. Large-scale KGCs typically contain hundreds of thousands of entities and relations, such as Freebase ( and ConceptNet (). Based on the coverage of the graph, we can classify KGCs into domain-specific and domain-independent KGCs. Domain-specific KGCs are typically constructed from a specific domain, such as DBpedia (, YAGO (, and WordNet (@CITE[3]@. Domain-independent KGCs are typically constructed from multiple domains, such as Freebase (@CITE[1]@ and ConceptNet (). Based on the domain of the graph, we can classify KGCs into general KGCs and specialized KGCs. General KGCs are typically constructed from a general domain, such as Freebase ( and ConceptNet (. Specialized KGCs are typically constructed from a specialized domain, such as DBpedia (, YAGO (, and WordNet (@CITE[3]@. Based on the type of relations, we can classify KGCs into binary KGCs and multi-relational KGCs. Binary KGCs only contain binary relations, such as WordNet (@CITE[3]@. Multi-relational KGCs contain multi-relational data, such as FB15k-237 (@CITE[3]@.\\nReferences:ID: 1. Title: Holographic Embeddings of Knowledge Graphs\\nID: 2. Title: Convolutional 2D Knowledge Graph Embeddings\\nID: 3. Title: Convolutional 2D Knowledge Graph Embeddings',\n",
       "  'ground_truth': '\\n\\nWe can roughly classify current KGC datasets into two groups: inferential and non-inferential datasets. The ﬁrst group is usually manually cu- rated to ensure each testing sample can be inferred from training data through reasoning paths, while they only focus on speciﬁc relations, such as Families (, Kinship (@CITE[3]@, and Country (. The limited scale and inference patterns make them not challenging. HOLE (@CITE[3]@ achieves 99.7% ACU-PR on the dataset of Country. The second group of datasets are automatically derived from public KGs and randomly split posi- tive triples into train/test, leading to a risk of testing samples non-inferential from training data. Pop- ular datasets include FB15k-237 (, WN18RR (@CITE[1]@, and YAGO3-10 (@CITE[1]@. CoDEx ( argues the scope and difﬁculty of the above datasets, thus propose a comprehensive dataset with manually veriﬁed hard negatives.\\nReferences:ID: 1. Title: Holographic Embeddings of Knowledge Graphs\\nID: 2. Title: Convolutional 2D Knowledge Graph Embeddings\\nID: 3. Title: Convolutional 2D Knowledge Graph Embeddings<|eot_id|>'},\n",
       " 2073: {'predict': 'Recently, generative data augmentation has gained popularity in the NLP community. @CITE[5]@ proposed to utilize the language model (LM) to generate synthetic data for low-resource machine translation. @CITE[2]@ introduced a novel data augmentation method, DAGA, which generates synthetic data with the help of language models trained on the linearized labeled sentences. @CITE[1]@ proposed a generative data augmentation method, G-DAUGC, to generate synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. @CITE[3]@ proposed an unsupervised data generation method, UDG, which leverages few-shot prompts to synthesize high-quality training data without real human annotations. @CITE[4]@ proposed to use a combination of generative and adversarial data augmentation for text classification. In this paper, we propose a generative data augmentation method, which utilizes a pre-trained language model to generate synthetic data for zero-shot learning.\\nReferences:ID: 1. Title: Generalized Data Augmentation for Low-Resource Translation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 4. Title: Generative Data Augmentation for Commonsense Reasoning\\nID: 5. Title: Towards Zero-Label Language Learning',\n",
       "  'ground_truth': '\\n\\nHou et al. (2018) generates diverse utterances to improve dialogue understanding models. @CITE[2]@ uses a bilingual dictionary and an unsupervised machine translation model to expand low-resource machine translation training data. @CITE[3]@; Kumar et al. (2020) make use of the masking mechanism in many PLM pre-training objective functions (e.g., BERT (@CITE[4]@, BART (Lewis et al., 2020)) and produce new synthetic data by masking randomly chosen words in the original training instances. ; @CITE[3]@; apply LMs and PLMs to learn directly to generate new synthetic data for NLU tasks (i.e., sequence labeling and common- sense inference tasks after trained (fine-tuned) on the relatively large training data. These works often directly apply off-the-shelf LMs or PLMs to generate synthetic data. @CITE[5]@ proposes to use unlabelled data as hard prompt to generate synthetic data without any training, limiting its application in complicated NLP tasks. To best of our knowledge, PromDA is the first PLMs with Soft Prompt that are especially designed for the data augmentation task.\\nReferences:ID: 1. Title: Generalized Data Augmentation for Low-Resource Translation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 4. Title: Generative Data Augmentation for Commonsense Reasoning\\nID: 5. Title: Towards Zero-Label Language Learning<|eot_id|>'},\n",
       " 3093: {'predict': 'Causal knowledge acquisition has been studied in various domains, such as commonsense reasoning (, event reasoning (@CITE[2]@;, and cognitive science (@CITE[3]@. In this work, we focus on commonsense reasoning, which has been extensively studied in the past. The most common approach to commonsense reasoning is to train a model to generate a plausible consequence given a cause. For example, given the cause \"it is raining\", the model should generate \"take an umbrella\" as the consequence. To achieve this, a large corpus of if-then rules is required, such as ATOMIC (@CITE[1]@ and Event2Mind (@CITE[2]@. The if-then rules in these corpora are manually annotated by humans. However, these corpora are limited in scale and coverage, and annotating them is time-consuming and expensive. Our approach can be used to automatically generate causal rules from large-scale text corpora, and can potentially improve the performance of commonsense reasoning models. In this work, we focus on generating causal rules in the form of \"if X, then Y\", and we use a large-scale corpus of event phrases to train our model. Our model is similar to the causal reasoning model proposed by, but our model is trained on a large-scale corpus of event phrases, and our model is more general and can be used to generate causal rules in different domains.\\nReferences:ID: 1. Title: Revisiting Pre-Trained Models for Chinese Natural Language Processing\\nID: 2. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 3. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning',\n",
       "  'ground_truth': '\\n\\nCausal knowledge is crucial for various artificial intelligence applications. Many works extract large-scale and precise causal pairs through neural or symbolic ways. Hereafter, they connect causal pairs into causal chains or graphs based on the textual or semantic similarity between events. @CITE[3]@; ;. used linguistic patterns to construct CausalNet. Heindorf et al. (2020) built CauseNet from web resources. @CITE[1]@ constructed Event2mind and @CITE[2]@ built Atomic both through crowdsourcing. Zhang et al. (2020) proposed a large-scale eventuality knowledge graph called ASER. built CausalBank to improve the coverage of the causal knowledge base. Previous studies mainly focused on extracting high-precision causal pairs, while ignoring the transitive problems when connecting event pairs into causal chains. We are trying to solve the two transitive problems in generating reliable causal chains.\\nReferences:ID: 1. Title: Revisiting Pre-Trained Models for Chinese Natural Language Processing\\nID: 2. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 3. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning<|eot_id|>'},\n",
       " 5525: {'predict': 'Prompt tuning is a popular method for parameter-efficient fine-tuning of LLMs (@CITE[4]@; @CITE[1]@; ; @CITE[2]@. Recently, prompt tuning has been applied to the adaptation of VLMs, where the prompt is conditioned on the input image. @CITE[3]@ propose to learn a prompt by optimizing the output of a VLM on a set of examples from the target task. @CITE[5]@ introduce a lightweight prompt generator to generate prompts conditioned on the image. @CITE[1]@ propose a prompt generator to generate prompts conditioned on the image and the class label. @CITE[5]@ propose to learn a prompt by optimizing the output of a VLM on a set of examples from the target task. Bahng et al. (2022) propose a prompt generator to generate prompts conditioned on the image. @CITE[2]@ propose a prompt generator to generate prompts conditioned on the image and the class label. However, these methods are limited to a small number of classes and cannot be applied to the large-scale image classification task.\\nReferences:ID: 1. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 4. Title: Vision Transformer Adapter for Dense Predictions\\nID: 5. Title: Exploring Visual Prompts for Adapting Large-Scale Models',\n",
       "  'ground_truth': '\\n\\nPrompt tuning (@CITE[4]@; @CITE[3]@; @CITE[5]@) typically refers to prepending language instructions to the input text to facilitate a better understanding of the downstream tasks. For example, instead of feeding the system with a fixed template ‘a photo of a <CLASS>’, task-specific additional information could be more helpful for the model. Moreover, tuning-based methods achieve comparable performance as full finetuning but with ∼1000× fewer parameters. Context Optimization (CoOp) () proposes a replacement of the hand-crafted prompts with learnable soft prompts. However, their approach lacks generalizability and demonstrates suboptimal performance under a zero-shot setting. CoCoOp () generates an image-conditional context along with text conditioning through prompt tuning, while ProDA (@CITE[1]@) incorporates the prompt’s prior distribution learning. ProGrad (Zhu et al., 2022) performs prompt updates where the gradients are aligned with the original prompt’s knowledge. Recent works (@CITE[2]@; Khattak et al., 2023) leverage a multi-modal prompting (image + text) technique to exploit the natural association across modalities.\\nReferences:ID: 1. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 4. Title: Vision Transformer Adapter for Dense Predictions\\nID: 5. Title: Exploring Visual Prompts for Adapting Large-Scale Models<|eot_id|>'},\n",
       " 3548: {'predict': 'Task-oriented dialogue (TOD) aims to assist users in achieving a specific goal by understanding their request, generating relevant responses, and providing necessary information. Traditional TDO systems usually involve three components: Natural Language Understanding (NLU), Dialogue Policy, and Natural Language Generation (NLG) (@CITE[3]@; @CITE[5]@; @CITE[2]@; @CITE[8]@. In contrast, recent works (@CITE[6]@; @CITE[7]@; @CITE[1]@; @CITE[4]@; @CITE[1]@; @CITE[5]@; @CITE[5]@; @CITE[7]@; @CITE[8]@; @CITE[4]@; @CITE[5]@ focus on developing end-to-end TOD systems. They first pre-train the model on large-scale TOD datasets and then fine-tune it on downstream tasks, such as dialogue state tracking, action decision, and response generation. In this work, we aim to pre-train a TOD model on a large-scale dataset and fine-tune it on a small-scale dataset. We use a unified model for all TOD tasks, and utilize the pre-trained model to improve the performance of downstream tasks.\\nReferences:ID: 1. Title: Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\\nID: 2. Title: Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning\\nID: 3. Title: Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\\nID: 4. Title: Two Causal Principles for Improving Visual Dialog\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\\nID: 7. Title: SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine Teaching\\nID: 8. Title: A Simple Language Model for Task-Oriented Dialogue',\n",
       "  'ground_truth': '\\n\\nTraditionally, task-oriented dialogue systems are built via a pipeline-based approach where four independently designed and trained modules are connected together to generate the final system responses. These include natural language understanding (@CITE[7]@), dialogue state tracking (@CITE[4]@; Zhong et al., 2018), policy learning (), and natural language generation (@CITE[2]@). The pipeline-based approach can be very costly and time-consuming as each module needs module-specific training data and cannot be optimized in a unified way. To address this, many end-to-end approaches (Bordes et al., 2017; @CITE[3]@; Madotto et al., 2018) have been proposed to reduce human efforts in recent years. Lei et al. (2018) propose a two-stage sequence-to-sequence model to incorporate dialogue state tracking and response generation jointly in a single sequence-to-sequence architecture. propose a domain-aware multi-decoder network to combine belief state tracking, action prediction, and response generation in a single neural architecture. More recently, the field has shifted towards using large-scale pre-trained language models such as BERT (@CITE[8]@) and GPT-2 (@CITE[6]@) for task-oriented dialogue modeling due to their success on many NLP tasks (; @CITE[5]@; @CITE[1]@). Peng et al. (2020) and employed a GPT-2 based model jointly trained for belief state prediction and response generation in a multi-task fashion. pre-train BERT on multiple task-oriented dialogue datasets for response selection.\\nReferences:ID: 1. Title: Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\\nID: 2. Title: Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning\\nID: 3. Title: Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\\nID: 4. Title: Two Causal Principles for Improving Visual Dialog\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\\nID: 7. Title: SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine Teaching\\nID: 8. Title: A Simple Language Model for Task-Oriented Dialogue<|eot_id|>'},\n",
       " 3966: {'predict': 'Few-shot relation extraction has been studied for years. Most of the methods are based on prototype networks (@CITE[6]@; @CITE[8]@; @CITE[9]@; @CITE[10]@. Besides, there are also some works that utilize meta-learning to learn a better prototype (@CITE[3]@; @CITE[5]@; @CITE[7]@; @CITE[4]@. Another line of works focus on leveraging external knowledge to improve the few-shot performance. For example, @CITE[2]@ use a global relation graph to leverage the relationships between different relations. @CITE[1]@ and @CITE[4]@ use the type information of entities to improve the few-shot performance. @CITE[4]@ and @CITE[7]@ use distant supervision to collect more data for few-shot relation extraction. However, all of the above works are based on a fixed-size prototype, which may not be able to capture the complex semantics of relations.\\nReferences:ID: 1. Title: A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction\\nID: 2. Title: Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification\\nID: 3. Title: Matching the Blanks: Distributional Similarity for Relation Learning\\nID: 4. Title: DeepStruct: Pretraining of Language Models for Structure Prediction\\nID: 5. Title: Semantic Representation for Dialogue Modeling\\nID: 6. Title: Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs\\nID: 7. Title: Learning from Context or Names? An Empirical Study on Neural Relation Extraction\\nID: 8. Title: A Data Bootstrapping Recipe for Low Resource Multilingual Relation Classification\\nID: 9. Title: Effective Distant Supervision for Temporal Relation Extraction\\nID: 10. Title: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions',\n",
       "  'ground_truth': '\\n\\nThe goal of relation extraction (RE) is to detect and classify the relation between specified entities in a text according to some predefined schema. Current research in RE has mostly been carried out in a few-shot or a zero-shot setting to address the dearth of training data (@CITE[1]@) and the “long-tail” problem of skewness in relation classes (@CITE[6]@). Salient work in that direction includes designing RE-specific pretraining objectives for learning better representations (@CITE[4]@; Zhenzhen et al., 2022; @CITE[9]@), incorporating meta-information such as relation descriptions (Yang et al., 2020; @CITE[5]@), a global relation graph (@CITE[7]@), or entity types (@CITE[2]@), and leveraging additional information in the form of dependency parses (@CITE[3]@), translated texts for multilingual RE (), or distantly supervised instances (@CITE[8]@; @CITE[10]@). All of these techniques seek to alleviate the need of using expensive human-annotated training data. In this work, we question whether incorporating linguistic structure on existing models can aid learning robust representations which can be transferred to other domains.\\nReferences:ID: 1. Title: A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction\\nID: 2. Title: Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification\\nID: 3. Title: Matching the Blanks: Distributional Similarity for Relation Learning\\nID: 4. Title: DeepStruct: Pretraining of Language Models for Structure Prediction\\nID: 5. Title: Semantic Representation for Dialogue Modeling\\nID: 6. Title: Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs\\nID: 7. Title: Learning from Context or Names? An Empirical Study on Neural Relation Extraction\\nID: 8. Title: A Data Bootstrapping Recipe for Low Resource Multilingual Relation Classification\\nID: 9. Title: Effective Distant Supervision for Temporal Relation Extraction\\nID: 10. Title: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions<|eot_id|>'},\n",
       " 4406: {'predict': 'Paraphrase generation is a task of generating semantically similar sentences to the source sentence. Paraphrase generation can be used to improve the robustness of NLP models (@CITE[1]@; @CITE[6]@), enhance text retrieval systems (, and improve the performance of question answering systems (@CITE[2]@). There are two main approaches for paraphrase generation: supervised and unsupervised approaches. Supervised approaches use a large collection of annotated paraphrase pairs to train a model to generate paraphrases. The main challenge in supervised approaches is that collecting a large number of annotated paraphrase pairs is a costly and time-consuming process (@CITE[5]@; @CITE[7]@). On the other hand, unsupervised approaches do not require any annotated paraphrase pairs. However, the generated paraphrases are usually syntactically similar to the source sentence and are limited in diversity (@CITE[3]@; @CITE[4]@). In this paper, we propose a novel unsupervised approach to generate syntactically diverse paraphrases. Our approach uses the syntax of the source sentence to guide the generation of paraphrases. We use a large collection of unannotated texts to train a model to learn the syntax of the source sentence. We then use the learned syntax to guide the generation of paraphrases. Our approach is inspired by the work of @CITE[7]@, which uses the syntax of the source sentence to guide the generation of paraphrases. However, their approach requires annotated paraphrase pairs to train a model to learn the syntax of the source sentence. Our approach is different from their approach in that we do not require any annotated paraphrase pairs. We use a large collection of unannotated texts to train a model to learn the syntax of the source sentence. Our approach is also different from the work of @CITE[3]@, which uses a large collection of unannotated texts to train a model to generate paraphrases. However, their approach generates paraphrases based on the semantics of the source sentence, and the generated paraphrases are usually syntactically similar to the source sentence. Our approach is different from their approach in that we use the syntax of the source sentence to guide the generation of paraphrases, and the generated paraphrases are usually syntactically diverse.\\nReferences:ID: 1. Title: Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment\\nID: 2. Title: Factorising AMR generation through syntax\\nID: 3. Title: A Deep Generative Framework for Paraphrase Generation\\nID: 4. Title: Unsupervised Paraphrasing without Translation\\nID: 5. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks\\nID: 6. Title: Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs\\nID: 7. Title: Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs',\n",
       "  'ground_truth': '\\n\\nTraditional paraphrase generation models are usually based on hand-crafted rules, including rule-based methods (), thesaurus-based methods (@CITE[2]@; @CITE[1]@), and lattice matching methods (@CITE[6]@). In recent years, different neural models have been proposed for paraphrase generation (Prakash et al., 2016; Mallinson et al., 2017; Cao et al., 2017; )...Our proposed dataset is available at https://github.com/uclanlp/ParaAMR. (@CITE[3]@; ; ; @CITE[4]@; @CITE[5]@; @CITE[7]@). Some advanced techniques are proposed as well, such as multi-round generation (@CITE[7]@), reinforcement-learning-based paraphrasing (), and prompt-tuning (Chowdhury et al., 2022)...\\nReferences:ID: 1. Title: Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment\\nID: 2. Title: Factorising AMR generation through syntax\\nID: 3. Title: A Deep Generative Framework for Paraphrase Generation\\nID: 4. Title: Unsupervised Paraphrasing without Translation\\nID: 5. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks\\nID: 6. Title: Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs\\nID: 7. Title: Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs<|eot_id|>'},\n",
       " 5037: {'predict': 'Efficient model design is a long-standing research topic, which aims to reduce the computational cost and memory footprint of the model while preserving its performance. For BERT-based models, several approaches have been proposed to reduce the model size and inference latency. @CITE[4]@ proposed to compress the BERT model by knowledge distillation from a larger teacher model. @CITE[6]@ proposed to use a smaller model with the same architecture as the teacher model, but with a smaller hidden size. @CITE[3]@ proposed to use a dynamic BERT model with adaptive width and depth. In addition, @CITE[5]@ proposed to use a MobileBERT model with a thin bottleneck structure and a carefully designed balance between self-attentions and feed-forward networks. Recently, some works (@CITE[1]@; @CITE[2]@; @CITE[7]@) proposed to use a continuous representation of network architecture to search for efficient neural architecture. These works can be used to search for efficient architectures for BERT-based models. However, these works are mainly designed for computer vision tasks and are not directly applicable to NLP tasks. In this work, we propose to use a continuous representation of network architecture to search for efficient architectures for BERT-based models.\\nReferences:ID: 1. Title: PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination\\nID: 2. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 3. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 4. Title: MobileNetV2: Inverted Residuals and Linear Bottlenecks\\nID: 5. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 6. Title: FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\\nID: 7. Title: ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware',\n",
       "  'ground_truth': '\\n\\nThe desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input sequences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; ), and distillation has been used to reduce model size (@CITE[3]@; ). Other work has sought to design efficient model architectures by developing low-FLOP substitutes for standard, dense self-attention and convolution operations (Iandola et al., 2016; @CITE[6]@; @CITE[2]@; @CITE[5]@; Xiong et al., 2021; ). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage, have been incorporated into the objectives of neural architecture search (NAS) and AutoML methods (@CITE[7]@; Tan et al., 2019; @CITE[4]@). Manual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU devices (@CITE[1]@; Tan et al., 2019; @CITE[2]@). However, it is often impossible to control for the hardware systems used for collecting these metrics leading to conclusions that may not generalize across deployment settings.\\nReferences:ID: 1. Title: PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination\\nID: 2. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 3. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 4. Title: MobileNetV2: Inverted Residuals and Linear Bottlenecks\\nID: 5. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 6. Title: FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\\nID: 7. Title: ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware<|eot_id|>'},\n",
       " 211: {'predict': \"Peer reviews mining is a relatively new field of research, and it has received a lot of attention recently. The first publicly available dataset of peer reviews was introduced by @CITE[2]@. The dataset contains 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. This dataset has been used to predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline (@CITE[2]@). It has also been used to predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact' (@CITE[2]@). Recently, @CITE[1]@ have introduced a dataset of 14.2K reviews from major machine learning and natural language processing venues. The dataset is annotated with 10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference, or Quote. They have also trained state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain. The dataset has also been used to evaluate the impact of rebuttal in the peer review process (@CITE[3]@). The authors have quantitatively and qualitatively assessed the corpus, and proposed a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. They have also discussed the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research.\\nReferences:ID: 1. Title: A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications\\nID: 2. Title: Argument Mining for Understanding Peer Reviews\\nID: 3. Title: Does My Rebuttal Matter? Insights from a Major NLP Conference\",\n",
       "  'ground_truth': '\\n\\nPrevious works focus on studying peer reviews by introducing new datasets or understanding the structure and the content of peer reviews. @CITE[3]@ present a dataset named PeerRead collected from venues including ACL, NIPS and ICLR. It consists of 14.7K paper drafts with their corresponding accept/reject decisions and 10.7K peer reviews for a subset of papers. They also propose several tasks based on their dataset, such as predicting whether a paper is accepted. @CITE[1]@ then present a partially annotated dataset named AMPERE collected from ICLR, UAI, ACL and NeurIPS. They use their dataset to detect the propositions and to classify them into different types for understanding the contents of peer reviews. Compared to the two works described above, our work has two main differences. First, instead of only looking at peer reviews, we propose a task to study the reviews and rebuttals jointly. Another difference lies in dataset: ours is fully annotated with 4,764 pairs of reviews and rebuttals, while only 400 reviews are annotated in AMPERE. As mentioned, few works touch on the rebuttal part, which is the other important element of the peer review process. @CITE[2]@ propose a task to predict after-rebuttal scores from initial reviews and author responses. They collect their dataset from ACL, which includes around 4K reviews and 1.2K author responses. However, our argument pair extraction task focuses more on the internal structure and relations between reviews and rebuttals. In addition, the size of our dataset is much larger than theirs.\\nReferences:ID: 1. Title: A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications\\nID: 2. Title: Argument Mining for Understanding Peer Reviews\\nID: 3. Title: Does My Rebuttal Matter? Insights from a Major NLP Conference<|eot_id|>'},\n",
       " 6475: {'predict': 'Dense retrieval, which is also known as dense vector space model, is a retrieval method that represents queries and documents in a dense vector space. The query and document vectors are generated by a pre-trained language model, and the cosine similarity is used to measure the relevance between the query and documents. The retrieval results are the top-k documents with the highest cosine similarity to the query. The advantages of dense retrieval are as follows: (1) The retrieval results are not limited to the top-k documents, but can return all the documents in the corpus. (2) The retrieval results can be used as the input of the downstream task model, and the model can directly generate the answer from the input documents. (3) The retrieval results can be used as the knowledge base for the downstream task model, and the model can generate the answer by reasoning over the retrieved documents. Dense retrieval has been widely used in many natural language processing tasks, such as open-domain question answering (@CITE[4]@; @CITE[3]@;, document-level relation extraction (@CITE[1]@, and multi-hop question answering (@CITE[2]@.\\nReferences:ID: 1. Title: Dense Text Retrieval based on Pretrained Language Models: A Survey\\nID: 2. Title: Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval',\n",
       "  'ground_truth': '\\n\\nDense retrieval is a fast-developing research topic summarized adequately by the latest surveys, @CITE[1]@ and @CITE[2]@. @CITE[3]@ proposed a dense passage retriever (DPR) with bi-encoder encoding queries and contexts separately. Lee et al. (2021) extended DPR to phrase retrieval and conducts passage retrieval based on phrases. @CITE[4]@ proposed a multi-hop retriever with a shared encoder and query-augmented methods. Nevertheless, its experiments are constrained to two-hop reasoning, although it can theoretically be extended to more hops. Generative retrieval methods can potentially serve as retrievers in our method. We leave this direction as a feature study. However, dense retrievers designed for open-domain question answering (ODQA) cannot directly apply to evidence retrieval in cross-document RE because the semantics of queries in this setting are much sparser. In this work, we adapt representative dense retrieval methods and further develop a variant of DPR specifically for evidence retrieval in cross-document RE.\\nReferences:ID: 1. Title: Dense Text Retrieval based on Pretrained Language Models: A Survey\\nID: 2. Title: Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval<|eot_id|>'},\n",
       " 5711: {'predict': 'Soft prompts are continuous vectors that can be tuned and can be used to condition a frozen language model. The idea of using soft prompts to control language models has been explored in previous work (Schick and Schütze, 2021; @CITE[1]@; @CITE[2]@. The soft prompt is typically a vector that is concatenated with the input sequence to the language model, and is then used to compute the input embedding. The soft prompt can be tuned on a task by minimizing the task loss, or can be fixed to be the same for all tasks. Soft prompts can also be used to control language models in a more flexible way by being used to compute the input embedding. @CITE[3]@ use a soft prompt to compute the input embedding and then compute the attention weights based on the input embedding and the input sequence. This allows the language model to attend to different parts of the input sequence based on the task and the input sequence. In our work, we use a soft prompt to compute the input embedding, and then use the input embedding to compute the attention weights and the output embedding. This allows us to control the language model in a more flexible way and to use the language model to perform different tasks.\\nReferences:ID: 1. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 2. Title: Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\\nID: 3. Title: Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models',\n",
       "  'ground_truth': '\\n\\nSoft prompt tuning is an effective method to adapt pre-trained Transformers without updating existing parameters (@CITE[1]@; @CITE[2]@;. Newly initialized embeddings are prepended to the input sequence (the “soft prompt”), and optimization is performed with respect to these new parameters while the rest of the model is frozen. It is one of many parameter-efficient fine-tuning methods (@CITE[3]@ and is related to prefix tuning, where newly initialized parameters are prepended to the attention states instead (. Prompt compression propose to learn a soft prompt σ to compress the information contained in a context x. Given a pre-trained language model pLM, they draw continuations y ∼pLM(· | x) based on x and use a distillation objective to align the model’s predictions conditioned on the soft prompt pLM(y | σ) to the predictions conditioned on the context pLM(y | x). find that soft prompts retain high-level information and facilitate controllable generation. However, the approach requires running the optimization for every new context x, with no knowledge transfer between similar contexts. In contrast, our AutoCompressors learn to predict their own soft prompts σ as a function of x.\\nReferences:ID: 1. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 2. Title: Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\\nID: 3. Title: Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models<|eot_id|>'},\n",
       " 5451: {'predict': \"The majority of previous work has focused on compositional generalization in semantic parsing (@CITE[1]@, and it has been shown that data augmentation can improve compositional generalization (@CITE[2]@. In the context of morphological inflection, @CITE[4]@; @CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[1]@ have shown that training on data with different lemmata improves generalization to unseen lemmata, and that data augmentation can improve performance on unseen lemmata. @CITE[4]@; @CITE[4]@; @CITE[3]@; @CITE[3]@; @CITE[1]@ have also shown that models trained on data with different morphological patterns generalize better to unseen morphological patterns. However, most of this work has focused on morphological inflection in isolation, rather than as part of a larger compositional task. There has been some work on compositional morphological inflection, but it has focused on the morphological inflection of a fixed set of morphemes, rather than on generalizing to new morphemes. For example, @CITE[4]@; @CITE[3]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[1]@ have shown that models trained on a fixed set of morphemes generalize better to unseen morphemes. However, these models are not generalizing to new morphemes, but rather to new morphemes in the same domain, and therefore are not truly compositional. In this paper, we use data augmentation to improve compositional generalization in morphological inflection, and we show that it improves generalization to new morphemes and new morphological patterns.\\nReferences:ID: 1. Title: Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization\\nID: 2. Title: Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models\\nID: 3. Title: (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance\\nID: 4. Title: Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models\",\n",
       "  'ground_truth': \"\\n\\nAssessing the generalization capacity of morphological inflections has proven a challenging and multifaceted problem. Relying on standard “IID” (@CITE[2]@; @CITE[4]@ splits obfuscated (at least) two different manners in which inflection models fail to generalize compositionally. First, @CITE[1]@ uncovered that generalizing to novel lexemes was challenging for even state of the art inflection models. Experiments by @CITE[4]@, however, showed that the STEMCORRUPT method could significantly improve generalization to novel lexemes. Our work builds on theirs by contributing to understanding the relationship between STEMCORRUPT and lexical compositional generalization. Specifically, we studied the structure of the probability distribution that StemCorrupt promotes (Section 3), and the conditions under which it succeeds (Remark 1) and fails (Remark 2). Second, @CITE[3]@ showed that inflection models also fail to generalize compositionally to novel feature combinations, even with agglutinative languages that have typically a strong one-to-one alignment between morphological features and affixes. Discovering strategies to facilitate compositional generalization in terms of novel feature combinations remains an open area of research.\\nReferences:ID: 1. Title: Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization\\nID: 2. Title: Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models\\nID: 3. Title: (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance\\nID: 4. Title: Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models<|eot_id|>\"},\n",
       " 1823: {'predict': 'Another line of work has used external knowledge to enhance the model’s reasoning capabilities. @CITE[1]@ and @CITE[4]@ used a knowledge graph as a resource to retrieve relevant facts for visual question answering. They used the retrieved facts to generate a set of candidate answers and then used a neural network to predict the correct answer. @CITE[4]@ also used a graph convolutional network to perform joint reasoning over the retrieved facts. @CITE[3]@ and @CITE[2]@ used a large-scale knowledge base to retrieve relevant facts for visual question answering. They used the retrieved facts to generate a set of candidate answers and then used a neural network to predict the correct answer. @CITE[3]@ also provided a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual question answering. @CITE[2]@ used a graph convolutional network to perform joint reasoning over the retrieved facts. @CITE[1]@ and @CITE[4]@ used a knowledge graph as a resource to retrieve relevant facts for visual question answering. They used the retrieved facts to generate a set of candidate answers and then used a neural network to predict the correct answer. @CITE[4]@ also used a graph convolutional network to perform joint reasoning over the retrieved facts. @CITE[3]@ and @CITE[2]@ used a large-scale knowledge base to retrieve relevant facts for visual question answering. They used the retrieved facts to generate a set of candidate answers and then used a neural network to predict the correct answer. @CITE[3]@ also provided a dataset and a protocol by which to evaluate such methods, thus addressing one of the key issues in general visual question answering.\\nReferences:ID: 1. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 2. Title: Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering\\nID: 3. Title: Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering\\nID: 4. Title: Explicit Knowledge-based Reasoning for Visual Question Answering',\n",
       "  'ground_truth': '\\n\\nThe use of an external knowledge source (KS) has gained much attention in the ﬁeld of natural language pro- cessing (NLP), such as the work of Verga et al. (2020). Our work is inspired by that of Lewis et al. (2020), which introduced RAG, a generic approach for a variety of downstream NLP tasks using a learned retriever (DPR; @CITE[2]@) to augment the inputs by marginalizing across pas- sages retrieved from Wikipedia. In the multi-modal domain, previous efforts have focused on building different types of KS, such as the work of @CITE[1]@; Chen et al. (2013); Divvala et al. (2014); and Zhu et al. (2015), which use web information for the construction of the KS. Methods that use an external KS for a downstream task use a structured KS, such as the work of @CITE[4]@; @CITE[4]@;, 2018) and. introduced an iter- ative method for VQA tasks. @CITE[3]@ introduced OK-VQA, a novel VQA dataset that requires the use of an external KS. applied a KS to multi-modal dialogue. In our work, we focus on a more naturally aligned KS, in the form of images and captions, which better reﬂects the data generated in newspapers and social media.\\nReferences:ID: 1. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 2. Title: Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering\\nID: 3. Title: Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering\\nID: 4. Title: Explicit Knowledge-based Reasoning for Visual Question Answering<|eot_id|>'},\n",
       " 6794: {'predict': 'Contrastive learning is a popular technique in representation learning (@CITE[4]@; @CITE[5]@; @CITE[2]@). The key idea is to learn representations by contrasting positive and negative samples. In contrast to the traditional triplet loss (@CITE[6]@), which only considers the similarity between positive and negative samples, contrastive learning considers the similarity between samples in the same class and the dissimilarity between samples in different classes. This technique has been widely used in many fields, such as computer vision (@CITE[1]@; @CITE[3]@; @CITE[3]@; @CITE[3]@), natural language processing (@CITE[4]@; @CITE[4]@; ; ; @CITE[6]@), and reinforcement learning (@CITE[1]@). However, the contrastive learning technique is still under-explored in graph representation learning. In this work, we propose a contrastive learning framework for graph node representation learning. Our framework is inspired by the contrastive learning in computer vision and natural language processing. We propose a new contrastive loss function that can be used to learn node representations in a self-supervised way. We also design a new data augmentation method to generate negative samples for contrastive learning. Our framework can be easily extended to other graph representation learning tasks.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\\nID: 3. Title: Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling\\nID: 4. Title: Representation Learning with Contrastive Predictive Coding\\nID: 5. Title: Supervised Contrastive Learning\\nID: 6. Title: Supervised Contrastive Learning',\n",
       "  'ground_truth': '\\n\\nContrastive Learning (CL) is a specific form of self supervised learning, that focuses on using “positive\" and “negative\" samples to improve the latent embedding space of models for better classification results. The core idea of CL is finding a strong manifold representation via pulling \"positive” pairs closer together, and repelling negative samples under a dot product similarity metric. This idea was first applied in the field of Computer Vision and found great success in multiple tasks such as Data Augmentation (@CITE[4]@, @CITE[6]@, @CITE[3]@ and Instance Discrimination (, @CITE[2]@, Ye et al., 2019,. Although previous works in CL have achieved great performance, many of them do not consider the semantic structure of data, a fact which usually leads to samples being treated as either positive or negative pairs as long as they are from different instances without considering their semantic similarity. This results in samples with shared semantic similarity being pushed apart in the semantic space and prevents the model from improving. To address this issue, @CITE[5]@ propose Prototypical Contrastive Learning (PCL), which accounts for semantic structure. @CITE[5]@ propose Supervised Contrastive Learning which extends the self-supervised CL to fully-supervised CL and more effectively utilizes the label information in CL. @CITE[1]@ propose multi-label Supervised CL that allows the model to learn multiple representations of an image under the context of different given labels. In this paper, we apply the principles from these prior works to the multi-intent detection task by applying multi-label supervised CL, and PCL. Finally, we show our approach’s effectiveness in our experiments.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\\nID: 3. Title: Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling\\nID: 4. Title: Representation Learning with Contrastive Predictive Coding\\nID: 5. Title: Supervised Contrastive Learning\\nID: 6. Title: Supervised Contrastive Learning<|eot_id|>'},\n",
       " 2843: {'predict': 'In computer vision, video captioning is usually formulated as a video description task, where the goal is to generate a sentence to describe the content of the video. Recent work in this area has mainly focused on two tasks: video captioning (@CITE[1]@) and dense video captioning (@CITE[3]@). The main difference between the two tasks is that the former task aims to generate a single caption for the whole video, while the latter task aims to generate a sequence of captions, each of which is associated with a specific event in the video. As for dense video captioning, most existing methods adopt a two-stage framework, which first generates temporal proposals and then describes each proposal with a sentence. The generated proposals are usually obtained by temporal action proposal generation methods (@CITE[2]@; @CITE[3]@), which are designed for action detection. However, the two tasks have different goals. For video captioning, the generated captions are usually long and may contain multiple events. For action detection, the generated proposals are usually short and may contain only one event. Moreover, the events in video captioning are usually not necessarily actions, which are often short and may contain only one event. As a result, the existing temporal action proposal generation methods are not suitable for video captioning. To address this issue, we propose a new method to generate temporal proposals for video captioning. Different from the existing methods, our method is designed for video captioning and is able to generate temporal proposals which are suitable for video captioning.\\nReferences:ID: 1. Title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks\\nID: 2. Title: Dense-Captioning Events in Videos\\nID: 3. Title: Fast Learning of Temporal Action Proposal via Dense Boundary Generator',\n",
       "  'ground_truth': '\\n\\nDeveloping machine learning techniques that can utilize language to describe what happens in a video remains an open challenge, which lies at the core of both CV and NLP. In this context, seminal work by @CITE[2]@ was, to the best of our knowledge, the first to tackle the task of describing videos in an open-domain setting. In their proposed video captioning task (VC), a system had to generate a natural language description of the main activity on a given short video (<30 s). This video captioning task was later extended by who proposed the task of dense video captioning, where a model is required to detect multiple events that occur in a video, and then describe each one using natural language. Current approaches generally focus on two separate sub-tasks. First, there is a temporal event proposal step, where relevant segments in the input video are first identified, after which a captioning model generates the natural language descriptions of the identified zones (@CITE[3]@; @CITE[1]@.\\nReferences:ID: 1. Title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks\\nID: 2. Title: Dense-Captioning Events in Videos\\nID: 3. Title: Fast Learning of Temporal Action Proposal via Dense Boundary Generator<|eot_id|>'},\n",
       " 5612: {'predict': 'Synthetic question generation has been explored for reading comprehension (Rajpurkar et al., 2016; @CITE[5]@; @CITE[1]@; @CITE[4]@; @CITE[2]@), adversarial QA (, and QA retrieval (Bhakthavatsalam et al., 2020; @CITE[3]@). However, to the best of our knowledge, we are the first to explore the use of synthetic questions for fact verification. In this work, we generate questions from Wikipedia evidence for fact verification. This is different from generating questions for reading comprehension, as the questions are not designed to be answerable, but rather to be refutable. We also generate questions from evidence with a specific focus on fact verification, by generating questions that are answerable with a clear yes/no label.\\nReferences:ID: 1. Title: Learning to Ask: Neural Question Generation for Reading Comprehension\\nID: 2. Title: Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia\\nID: 3. Title: Synthetic QA Corpora Generation with Roundtrip Consistency\\nID: 4. Title: Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation\\nID: 5. Title: PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\\nID: 6. Title: Zero-shot Fact Verification by Claim Generation',\n",
       "  'ground_truth': '\\n\\nMany approaches have been proposed to synthesize questions given a passage and an answer within it (@CITE[4]@; ; @CITE[6]@; @CITE[3]@; @CITE[5]@; @CITE[1]@. These methods are designed to generate answerable question-answer pairs and do not directly apply to our setting where we must synthesize challenging unanswerable questions. @CITE[2]@ generate (topically relevant) unanswerable questions by conditioning a question generator on phrases from the surrounding context of passages used in the QA dataset. SCENE does not assume access to such external text sources (e.g. source documents of QA datasets).\\nReferences:ID: 1. Title: Learning to Ask: Neural Question Generation for Reading Comprehension\\nID: 2. Title: Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia\\nID: 3. Title: Synthetic QA Corpora Generation with Roundtrip Consistency\\nID: 4. Title: Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation\\nID: 5. Title: PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\\nID: 6. Title: Zero-shot Fact Verification by Claim Generation<|eot_id|>'},\n",
       " 5257: {'predict': 'As NLG has become more prevalent, so too have concerns about the potential for NLG systems to generate deceptive text. The impact of NLG on society has been discussed in a number of recent surveys (@CITE[1]@; @CITE[3]@. In this paper, we focus on the issue of interactivity in NLG, which is a topic that has received less attention. Recent work has shown that interactive NLG systems can be more effective than non-interactive systems, and that they can generate more realistic text (@CITE[2]@. However, the impact of interactivity on the generation of deceptive text is still an open question. In this paper, we investigate this topic through a series of experiments that evaluate the impact of interactivity on the generation of deceptive text.\\nReferences:ID: 1. Title: Survey of Hallucination in Natural Language Generation\\nID: 2. Title: A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception\\nID: 3. Title: Parachute: Evaluating Interactive Human-LM Co-writing Systems',\n",
       "  'ground_truth': '\\n\\nSeveral works have explored interactivity between humans and models to complete tasks collaboratively. @CITE[3]@ presented a dataset to reveal large language models’ capabilities in assisting creative writing. Zhang et al. (2022); explored pre-trained language models for code editing based on human-written comments. Closer to our work, Lahiri et al. (2022) created an interactive framework to refine user intents through test case generations and user feedback, and Kiseleva et al. (2022) studied an interactive agent that can interact with humans 5https://chat.openai.com/chat Example 1 Example 2 target: Progress 59 spacecraft will re-enter Earth’s atmosphere in a week, Russia space agency says. target: The Large Hadron Collider (LHC) begins again after a two-year shutdown. oracle: Progress 59 re-enter oracle: (LHC) agent: Progress in Europe means he has a 59 per cent chance to re-enter the EU. agent: Curators at the Mars Space Telescope (LHC) will launch in space the next month. oracle: Progress in Europe means he has a 59 per cent spacecraft will re-enter the EU. oracle: The Large at the Mars Space Telescope (LHC) will launch in space the next month. agent: Progress in will help the Europe means he has a 59 million per cent spacecraft will re-enter the Earth EU. agent: The Large X-ray Telescope at the Mars Space Telescope (LHC) is located will launch in space the Atlantic. month. oracle: Progress will help the 59 million spacecraft will re-enter Earth’s says. oracle: The Large X-ray Telescope (LHC) is lo- cated in two-year shutdown. agent: Progress comes after Boeing will help the 59 spacecraft will re-enter NASA Earth’s spacecraft, it says. agent: The Large Mars Telescope-ray Telescope (LHC) will end a two-year shutdown. oracle: Progress comes after Boeing 59 spacecraft will re-enter NASA’s spacecraft, Russia space agency says. oracle: The Large Hadron Collider (LHC) will end a two-year shutdown. agent: Progress comes as Boeing’s after Boeing 59 spacecraft will re-enter Earth NASA spacecraft, Russia’s space agency says. agent: The Large Hadron Collider (LHC) will end a end a two-year shutdown. Table 4: Example interactions between the oracle and agent. The oracle changes are shown red. Deleted words are crossed out. Note that these are randomly selected examples (i.e., without cherry-picking), with the only curation being the rejection of potentially offensive examples. and follow natural language instructions to achieve goals in Minecraft. Finally, interactivity has also been studied from a Human-Computer Interaction viewpoint (@CITE[1]@; @CITE[2]@.\\nReferences:ID: 1. Title: Survey of Hallucination in Natural Language Generation\\nID: 2. Title: A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception\\nID: 3. Title: Parachute: Evaluating Interactive Human-LM Co-writing Systems<|eot_id|>'},\n",
       " 3730: {'predict': 'Calibration methods have been widely studied in the context of neural networks (@CITE[5]@; @CITE[1]@; ; @CITE[3]@. These methods aim to correct the issue of overconfidence that neural networks have, and can be roughly divided into two categories: temperature scaling and post-hoc calibration. Temperature scaling (@CITE[4]@; @CITE[2]@; @CITE[3]@ is a simple calibration method that adjusts the output probabilities of the model by scaling them with a temperature factor. This method has been widely used in the context of natural language processing, and has been shown to be effective in many scenarios (@CITE[2]@; @CITE[3]@; @CITE[5]@. Post-hoc calibration methods, on the other hand, aim to modify the output probabilities of the model in a more sophisticated way. These methods can be further divided into two categories: model-based calibration and data-based calibration. Model-based calibration methods (@CITE[1]@; @CITE[5]@ modify the model parameters to make the output probabilities of the model better calibrated. Data-based calibration methods (@CITE[2]@; @CITE[3]@; @CITE[5]@, on the other hand, modify the output probabilities of the model based on the training data, and can be further divided into two categories: model-based calibration and data-based calibration. Model-based calibration methods (@CITE[1]@; @CITE[5]@ modify the model parameters to make the output probabilities of the model better calibrated. Data-based calibration methods (@CITE[2]@; @CITE[3]@; @CITE[5]@, on the other hand, modify the output probabilities of the model based on the training data, and can be further divided into two categories: label smoothing and calibration networks. Label smoothing methods (@CITE[2]@; @CITE[3]@; @CITE[5]@ aim to modify the output probabilities of the model by smoothing the label distribution, and have been shown to be effective in many scenarios (@CITE[2]@; @CITE[3]@; @CITE[5]@. Calibration networks (@CITE[5]@, on the other hand, aim to modify the output probabilities of the model by training a calibration network, and have been shown to be effective in many scenarios (@CITE[5]@. In this paper, we propose a novel calibration method based on label smoothing and calibration networks, and show that it is effective in many scenarios.\\nReferences:ID: 1. Title: On Calibration of Modern Neural Networks\\nID: 2. Title: Calibration of Pre-trained Transformers\\nID: 3. Title: Posterior calibration and exploratory analysis for natural language processing models\\nID: 4. Title: Selective Question Answering under Domain Shift\\nID: 5. Title: How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering',\n",
       "  'ground_truth': '\\n\\n@CITE[2]@ first experiment various post-hoc calibration methods including temperature scaling, and Thulasidasan et al. (2019) find that mixup training improves the calibration of image classifiers evaluated by the ECE metric. Within NLP, previous works explore calibration on multi-class classification (@CITE[4]@ and sequence tagging (Nguyen and O’@CITE[3]@. In question answering, @CITE[5]@ propose the selective question answering setting that aims to abstain on as few questions as possible while maintaining high accuracy. Towards this goal, later approaches (Zhang et al., 2021; extract features to train a binary classifier to decide on which questions to abstain. While selective question answering offers a measurement of calibration, the scale of confidence values is not considered since abstention can be effective as long as correct predictions have higher confidence than wrong ones, regardless of the absolute scales. Concurrent work (Dhuliawala et al., 2022) explores calibration for retriever-reader ODQA, focusing on combining information from the retriever and reader. In addition to span-extraction QA, @CITE[1]@ and Si et al. (2022) also explore calibration for generative QA. However, these works use ECE for evaluation.\\nReferences:ID: 1. Title: On Calibration of Modern Neural Networks\\nID: 2. Title: Calibration of Pre-trained Transformers\\nID: 3. Title: Posterior calibration and exploratory analysis for natural language processing models\\nID: 4. Title: Selective Question Answering under Domain Shift\\nID: 5. Title: How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering<|eot_id|>'},\n",
       " 7135: {'predict': 'Sentence ordering is the task of arranging the sentences of a given text in the correct order. Previous works have mainly focused on improving the performance of the task by using pair-wise strategy. @CITE[6]@; @CITE[4]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE',\n",
       "  'ground_truth': '\\n\\nEarly works on sentence ordering attempt to utilize probabilistic transition method based on linguistic features (), content model () and entity-based approaches (Barzilay and @CITE[2]@; @CITE[7]@). Recent neural models for the sentence ordering task are built on generation or ranking structure (@CITE[3]@; @CITE[4]@; @CITE[1]@; ). For ranking-based strategy, @CITE[3]@ introduced a pairwise model which ranks relative order for sentence pairs. Kumar et al. (2020) employed BERT-improved feedforward neural networks to generate scores for each sentence and optimized the scores with several ranking losses, e.g., pairwise ranking loss. @CITE[7]@ proposed to constrain the relative order between paired sentences and introduced the classic topological sort algorithm to find the gold order, which achieves considerable improvement. For generation-based strategy, @CITE[3]@ firstly utilized an RNN to encode sentences and Logeswaran et al. (2018) explored LSTM-based attention encoder, both works employed a similar recurrent pointer net to generate the coherent paragraph sentence-by-sentence. However, since the input of sentence ordering is a set of unordered sentences, it is inappropriate to model the unordered sentences in an autoregressive manner. To address this issue, @CITE[5]@ proposed to use Transformer but without positional embedding to encode the unordered sentences. Most of the follow-up methods also applied the order invariant Transformer as paragraph encoder (@CITE[6]@, 2020; Kumar et al., 2020; ). Besides, other attempts also have been explored for order invariant encoding. @CITE[6]@ devised a graph structure to encode the input order-free sentence set. proposed a hierarchical attention network composed of a word encoder and a sentence encoder-decoder. and further took advantage of BERT to explore the deep semantic connection and relative order between sentences. Basu Roy Chowdhury et al. (2021) employed the BART as backbone, which benefited from the sentence permutation task during pre-training and set a strong SotA.\\nReferences:ID: 1. Title: Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization\\nID: 2. Title: Topological Sort for Sentence Ordering\\nID: 3. Title: End-to-End Neural Sentence Ordering Using Pointer Network\\nID: 4. Title: Sentence Ordering and Coherence Modeling using Recurrent Neural Networks\\nID: 5. Title: End-to-End Neural Sentence Ordering Using Pointer Network\\nID: 6. Title: Topological Sort for Sentence Ordering\\nID: 7. Title: End-to-End Neural Sentence Ordering Using Pointer Network<|eot_id|>'},\n",
       " 7553: {'predict': 'The idea of using tool-augmented language models to solve complex tasks is not new. The first work to leverage tools with LLMs is @CITE[5]@, which proposes to use verifiers to solve math word problems. The verifiers are trained to rank the solutions generated by LLMs. Recently, several works (@CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[5]@) have also explored the idea of tool-augmented LLMs for solving complex tasks. For example, @CITE[3]@ uses classical planners to solve long-horizon planning problems, and @CITE[2]@ uses computer programs to solve math word problems. In contrast, our work is the first to explore the idea of using LLMs to solve complex tasks in a more general and systematic way. We also propose a new training method to train tool-augmented LLMs. Moreover, we demonstrate the effectiveness of tool-augmented LLMs in solving complex tasks such as code generation and reasoning.\\nReferences:ID: 1. Title: Training Verifiers to Solve Math Word Problems\\nID: 2. Title: LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\\nID: 3. Title: Training Verifiers to Solve Math Word Problems\\nID: 4. Title: Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\\nID: 5. Title: LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'ground_truth': '\\n\\nLanguage models have inherent limitations such as the inability to access up-to-date information, take actions, or perform precise mathematical reasoning. Recent work has begun to augment language models with access to external tools and resources, such as the information retriever (@CITE[4]@; @CITE[5]@; @CITE[3]@), calculator (@CITE[4]@), code interpreter (@CITE[1]@), planner (@CITE[5]@), and other pre-trained models (@CITE[2]@. Recent works (; @CITE[1]@) have achieved improved performance on arithmetic reasoning tasks by generating Python programs that specify the reasoning procedure as chained commands in the order of execution. However, this idea has not been extended to logical reasoning problems, primarily due to the challenge of representing their highly \"non-linear\" reasoning procedure (e.g., hypothesizing, case-by-case analysis, and the process of elimination) with functional programming. Our work provides a novel way to solve this within the framework of augmented LLMs. Instead of parsing the problem-solving procedure as programs, we only describe the problem with symbolic language using LLMs and then offload the reasoning to external symbolic solvers.\\nReferences:ID: 1. Title: Training Verifiers to Solve Math Word Problems\\nID: 2. Title: LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\\nID: 3. Title: Training Verifiers to Solve Math Word Problems\\nID: 4. Title: Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\\nID: 5. Title: LLM+P: Empowering Large Language Models with Optimal Planning Proficiency<|eot_id|>'},\n",
       " 3925: {'predict': 'Multitask NLP benchmarks are designed to evaluate the generalization capabilities of language models on a variety of tasks, including natural language understanding (NLU) and generation (NLG) tasks. The General Language Understanding Evaluation (GLUE) benchmark (@CITE[1]@) is one of the most widely used NLU benchmarks, which consists of nine tasks, including sentiment analysis, question answering, and natural language inference. SuperGLUE (@CITE[6]@) is a more challenging benchmark than GLUE, which includes more complex tasks, such as question answering with reasoning and commonsense inference. In addition, there are also some benchmarks focusing on NLG tasks, such as GEM (@CITE[3]@), GLGE (@CITE[4]@), and DialoGLUE (@CITE[5]@). Recently, there are also some benchmarks focusing on long text modeling, such as LOT (@CITE[7]@) and LongT5 (@CITE[2]@). In this work, we propose a benchmark for evaluating the generalization capabilities of language models on various tasks, including text classification, sentiment analysis, question answering, and text generation.\\nReferences:ID: 1. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 2. Title: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\nID: 3. Title: The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\\nID: 4. Title: GLGE: A New General Language Generation Evaluation Benchmark\\nID: 5. Title: DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue\\nID: 6. Title: LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation\\nID: 7. Title: Fine-tuned Language Models are Continual Learners',\n",
       "  'ground_truth': '\\n\\nExisting multitask NLP benchmarks tend to focus on evaluating either understanding (@CITE[2]@, @CITE[3]@) or generation (@CITE[6]@, Khashabi et al., 2021, @CITE[1]@) capabilities of NLP models. There are task-specific benchmarks that address both, such as those for dialog (@CITE[5]@) and code (). For the task of storytelling, the LOT benchmark (@CITE[7]@) focuses on both aspects but is limited to Chinese and has fewer tasks than our proposed STORYWARS dataset. BIG-bench (@CITE[4]@), which includes 204 tasks for understanding and generation, only tests zero-shot and few-shot abilities without finetuning. STORYWARS provides a benchmark for story understanding and generation with 101 tasks spanning all zero-shot, few-shot, and full-supervised scenarios for various applications.\\nReferences:ID: 1. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 2. Title: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\nID: 3. Title: The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\\nID: 4. Title: GLGE: A New General Language Generation Evaluation Benchmark\\nID: 5. Title: DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue\\nID: 6. Title: LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation\\nID: 7. Title: Fine-tuned Language Models are Continual Learners<|eot_id|>'},\n",
       " 2256: {'predict': 'Sentence simplification is the task of converting complex sentences to simpler ones, which has been studied in NLP for several decades. Early approaches include rule-based systems (@CITE[1]@; @CITE[4]@; @CITE[5]@; ; @CITE[3]@) and statistical machine translation (@CITE[2]@;. Recently, neural networks have been applied to sentence simplification. @CITE[3]@ proposed a sequence-to-sequence model with a reinforcement learning framework, while @CITE[5]@ used a memory-augmented neural network. @CITE[2]@ introduced a multi-task learning framework with a latent variable model for sentence simplification. @CITE[1]@ proposed a controllable sequence-to-sequence model with a discrete parameterization mechanism. In this paper, we propose a novel multi-task learning framework for sentence simplification, which is based on the multi-task learning framework proposed by @CITE[2]@. However, we propose a new latent variable model for sentence simplification, which is based on the latent variable model for word definition proposed by @CITE[2]@. The latent variable model for sentence simplification can be used to model the ambiguity of sentence simplification.\\nReferences:ID: 1. Title: Sequence to Sequence Learning with Neural Networks\\nID: 2. Title: Sentence Simplification with Deep Reinforcement Learning\\nID: 3. Title: Sentence Simplification with Memory-Augmented Neural Networks\\nID: 4. Title: Conditional Generators of Words Definitions\\nID: 5. Title: Controllable Sentence Simplification',\n",
       "  'ground_truth': '\\n\\nResearchers usually regard the sentence simplification task as a monolingual variant of machine translation (MT) (@CITE[3]@. Benefiting from the advancement of neural machine translation, this task has also made great progress in recent years. Lately, many works built upon the Seq2Seq MT model ( performed well. First attempted by @CITE[5]@, the Seq2Seq models for this task are able to perform lexical simplification and content reduction simultaneously by training on complex-simple sentence pairs. This method was inherited and improved by many subsequent works, such as combining with the reinforcement learning method by setting a simplification reward (, augmenting memory capacities (@CITE[4]@ or training with multitasking on entailment and paraphrase generation (@CITE[1]@. proposed to prepend additional prompt tokens to source sentences at train time, which enables the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity. This controllable simplification system (called ACCESS) and its improved version MUSS ( achieved SOTA results on the Turk corpus in terms of the SARI metric (@CITE[2]@. The generation-simplification pipeline methods are used as baselines of the SDG task, and we use both ACCESS and MUSS models for the simplification. Unlike the baseline, the SimpDefiner can generate simple definitions directly, alleviating the accumulated errors.\\nReferences:ID: 1. Title: Sequence to Sequence Learning with Neural Networks\\nID: 2. Title: Sentence Simplification with Deep Reinforcement Learning\\nID: 3. Title: Sentence Simplification with Memory-Augmented Neural Networks\\nID: 4. Title: Conditional Generators of Words Definitions\\nID: 5. Title: Controllable Sentence Simplification<|eot_id|>'},\n",
       " 6596: {'predict': 'Zero-shot commonsense QA aims to answer questions without any annotated data, relying on the implicit knowledge embedded in pre-trained language models (PLMs). The most common approaches for zero-shot commonsense QA can be categorized into two types: (1) using external knowledge graphs (KGs) (@CITE[1]@; @CITE[2]@; ; @CITE[5]@; @CITE[7]@; @CITE[8]@; @CITE[4]@; @CITE[6]@; @CITE[3]@, and (2) using PLMs directly (@CITE[1]@; @CITE[5]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[3]@; @CITE[3]@. In this work, we follow the second line of work and focus on improving the performance of PLMs on zero-shot commonsense QA. Recent works have shown that PLMs can be improved by prompting (@CITE[3]@; @CITE[7]@; @CITE[8]@; @CITE[4]@; @CITE[3]@. However, it remains unclear what kind of prompts can improve the performance of PLMs on zero-shot commonsense QA. In this work, we propose a novel prompt generation method to improve the performance of PLMs on zero-shot commonsense QA. Our approach is based on the observation that negative commonsense knowledge is rarely mentioned in the text (@CITE[3]@, and we propose to generate prompts by selecting negative commonsense knowledge to improve the performance of PLMs.\\nReferences:ID: 1. Title: A Simple Method for Commonsense Reasoning\\nID: 2. Title: Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning\\nID: 3. Title: Unsupervised Commonsense Question Answering with Self-Talk\\nID: 4. Title: Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization\\nID: 5. Title: Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering\\nID: 6. Title: MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation\\nID: 7. Title: Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning\\nID: 8. Title: Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge',\n",
       "  'ground_truth': '\\n\\nZero-shot commonsense QA evaluates a model’s reasoning gener- alizability on unseen QA entries without any super- vision signals from the corresponding annotated training data. To tackle this task, two primary pipelines have emerged in existing works. The first paradigm employs off-the-shelf language mod- els without changing the parameters, either using vanilla language modeling with prompts (@CITE[1]@;, or with some inference- time mechanisms specifically designed for reason- ing, such as self-talk (@CITE[4]@, cloze translation (@CITE[8]@, and dynamic generation of reasoning sub-graphs and graph rea- soning (@CITE[5]@. The second pipeline leverages external CSKBs as knowledge sources to provide PLMs with additional supervision sig- nals for further fine-tuning (; @CITE[2]@; @CITE[3]@. A common strategy involves converting knowledge triples in CSKBs to synthetic QA pairs by transforming the head and relation to a question, the tail to a gold answer, and (randomly) sample tails from other heads as distractors. Such fine-tuning paradigm benefits from incorporating CSKBs within differ- ent domains (; @CITE[6]@ and exploiting multi-hop graph structures with graph neural networks (@CITE[7]@, and heightens the model’s commonsense sensitivity in a QA con- text, which leads to state-of-the-art performances.\\nReferences:ID: 1. Title: A Simple Method for Commonsense Reasoning\\nID: 2. Title: Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning\\nID: 3. Title: Unsupervised Commonsense Question Answering with Self-Talk\\nID: 4. Title: Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization\\nID: 5. Title: Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering\\nID: 6. Title: MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation\\nID: 7. Title: Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning\\nID: 8. Title: Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge<|eot_id|>'},\n",
       " 4812: {'predict': 'The cross-modal representation learning for speech-to-text translation has been a hot topic recently. @CITE[5]@ proposed the Listen-Understand-Translate (LUT) framework to decouple the end-to-end speech-to-text translation task. LUT consists of three modules: acoustic encoder, semantic encoder and decoder. The acoustic encoder is used to extract the acoustic features from the input speech signal. The semantic encoder is used to encode the input text and audio features into a unified representation. The decoder is used to generate the target translation. @CITE[3]@ proposed the COSTT framework to generate the source transcript and target translation text with a single decoder. @CITE[6]@ proposed the Chimera framework to bridge the modality gap by projecting audio and text features to a common semantic representation. @CITE[4]@ proposed the ConST framework to learn similar representations for semantically similar speech and text. @CITE[1]@ proposed the MuST-2 framework to bridge the modality gap by learning a shared representation for speech and text. @CITE[2]@ proposed the MuST-3 framework to bridge the modality gap by learning a shared representation for speech and text with a contrastive learning framework. @CITE[2]@ proposed the MuST-4 framework to bridge the modality gap by learning a shared representation for speech and text with a multi-task learning framework. The above methods are all based on the Transformer architecture and are trained with the masked language model (MLM) objective. However, they are not suitable for low-resource languages, because they require a large amount of parallel data and are sensitive to the quality of the parallel data.\\nReferences:ID: 1. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 2. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 3. Title: Learning Shared Semantic Space for Speech-to-Text Translation\\nID: 4. Title: Cross-modal Contrastive Learning for Speech Translation\\nID: 5. Title: No Language Left Behind: Scaling Human-Centered Machine Translation\\nID: 6. Title: Consecutive Decoding for Speech-to-text Translation',\n",
       "  'ground_truth': '\\n\\nSearchers realized recently that the misalignment between speech and text representation hinders the knowledge transfer from external data (; @CITE[4]@; @CITE[3]@; @CITE[6]@; ; @CITE[5]@;. shrank the speech representation to match the length of text representation and also closed the representational gap by minimizing their L2 distance. @CITE[3]@ mapped speech representation to text representation through both the Connectionist Temporal Clas- siﬁcation (CTC) (@CITE[1]@ distribution and a mapping layer. @CITE[4]@ proposed a cross attention layer to force the speech-text alignment. @CITE[6]@ developed a novel architecture enabling ﬁxed-length shared semantic space for both modalities. Saxon et al. (2021) proposed a hierarchical speech understanding system leveraging both ASR and text understanding data. employed sentence-level contrastive loss to reduce the modality gap and achieved state-of-the-art results on MuST-C. Our method, however, works on word-level instead of sentence-level and empirically provides both better performance and higher data efﬁciency. @CITE[2]@ also proposes to align the word-level representations between speech and text, but their method heavily relies on target translation while our method only requires ASR data for modality gap reduction.\\nReferences:ID: 1. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 2. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 3. Title: Learning Shared Semantic Space for Speech-to-Text Translation\\nID: 4. Title: Cross-modal Contrastive Learning for Speech Translation\\nID: 5. Title: No Language Left Behind: Scaling Human-Centered Machine Translation\\nID: 6. Title: Consecutive Decoding for Speech-to-text Translation<|eot_id|>'},\n",
       " 774: {'predict': 'Model-based approaches interpret the predictions of a target model by training a second model that mimics the behavior of the target model. This approach is attractive as it can be used to interpret any target model. However, it requires large amounts of data for training the second model. To achieve interpretability, the second model is typically trained to predict the target model’s outputs and/or to generate rationales for the predictions. The second model can be trained in a supervised manner with human-annotated rationales (; @CITE[6]@; @CITE[4]@; @CITE[6]@; @CITE[4]@) or in an unsupervised manner with attention weights of the target model (@CITE[6]@; @CITE[4]@). The second model can also be trained to mimic the target model by minimizing the difference between the two models’ outputs ( or to optimize the information bottleneck principle ().\\nReferences:ID: 1. Title: CXPlain: Causal Explanations for Model Interpretation under Uncertainty\\nID: 2. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 3. Title: Rationalizing Neural Predictions\\nID: 4. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 5. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 6. Title: Explaining a black-box using Deep Variational Information Bottleneck Approach',\n",
       "  'ground_truth': '\\n\\nThese approaches train the explainer with an objective function to improve efficiency at test time. The closest work to ours is by @CITE[1]@, who train an explainer using a causality-based explanation algorithm. However, these approaches do not learn from arbitrary algorithms or discretize feature weights — the high variation of continuous weights may impair the ability to capture the commonalities in an explanation algorithm. @CITE[2]@ discretize the weights produced by an existing method, but they use these weights to build a faithful classifier for an underlying black-box model, rather than using them to explain the model directly. Other works train a classifier and an explainer jointly in order to incorporate explainability directly into the classifier (@CITE[3]@; Camburu et al., 2018). Unlike these approaches, we do not change the classifier or require an expensive process to collect human rationales, as done in (@CITE[4]@. Lastly, a few works use information-theoretic objectives to train an explainer directly from the underlying classifier (@CITE[4]@; @CITE[6]@. These explainers require careful training to select a low number of important features (@CITE[5]@; hence, some input features do not have attributions.\\nReferences:ID: 1. Title: CXPlain: Causal Explanations for Model Interpretation under Uncertainty\\nID: 2. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 3. Title: Rationalizing Neural Predictions\\nID: 4. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 5. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 6. Title: Explaining a black-box using Deep Variational Information Bottleneck Approach<|eot_id|>'},\n",
       " 5990: {'predict': 'Recent work has explored the book summarization task, including both extractive and abstractive approaches. @CITE[1]@; @CITE[4]@; @CITE[4]@; @CITE[2]@; @CITE[3]@ have shown that extractive summarization methods, such as BERTSUM (@CITE[1]@) and SummaRuNNer (@CITE[3]@), can produce summaries of books that are on par with human-written summaries. Abstractive summarization methods, such as T5 (@CITE[4]@), have also been shown to be effective at generating book summaries. However, abstractive methods often struggle to produce summaries that are coherent and well-organized, as they do not have access to the source text while generating the summary. Our work explores the use of a divide-and-conquer approach to abstractive summarization, where the task of summarizing an entire book is broken down into the task of summarizing a series of short chapters. We find that this approach can lead to summaries that are both coherent and well-organized, and that are comparable in quality to those produced by abstractive summarization methods.\\nReferences:ID: 1. Title: Recursively Summarizing Books with Human Feedback\\nID: 2. Title: Recursively Summarizing Books with Human Feedback\\nID: 3. Title: A Divide-and-Conquer Approach to the Summarization of Long Documents\\nID: 4. Title: SummEval: Re-evaluating Summarization Evaluation',\n",
       "  'ground_truth': '\\n\\nKryscin- ski et al. (2021) conducted experiments on full- book summarization using a generate&rank strat-. This approach involves training a system to generate paragraph-level summaries, which are then sorted by perplexity and concatenated to form a full-book summary. More recently, Wu et al. (2021) proposed an approach where passages are recursively summarized and concatenated to form a full summary. However, generated summaries are affected by the errors accumulated from pre- vious stages (@CITE[2]@. Recursively gener- ating a summary is a paradigm that has also been used by other works for long-document summariza- tion (@CITE[2]@; @CITE[4]@. Another family of approaches is that of extractive-then-abstractive approaches. This fam- ily of approaches first extracts key sentences from the input document and then uses such sentences as input to an abstractive model, which is tasked with generating a summary that captures the main ideas and themes of the source. While it was suc- cessfully employed in previous works for short (@CITE[3]@ and long-form summarization (@CITE[1]@, this paradigm has never been explored for summarizing books. In this paper, we aim to fill this gap by presenting a new, simple extractive-then-abstractive model and showing its effectiveness for book summarization.\\nReferences:ID: 1. Title: Recursively Summarizing Books with Human Feedback\\nID: 2. Title: Recursively Summarizing Books with Human Feedback\\nID: 3. Title: A Divide-and-Conquer Approach to the Summarization of Long Documents\\nID: 4. Title: SummEval: Re-evaluating Summarization Evaluation<|eot_id|>'},\n",
       " 742: {'predict': \"Adversarial training is a common approach for improving the robustness of models. It has been used to improve the robustness of models in various NLP tasks, including machine translation (@CITE[1]@;, semantic parsing (, question answering (@CITE[3]@;, and natural language inference (). Adversarial training is typically done by perturbing the training data using some form of noise, such as word substitution, deletion, or permutation, and training the model on the perturbed data. The perturbations are typically designed to be imperceptible to humans, while causing the model to misclassify the example. The goal of adversarial training is to increase the model’s robustness to such perturbations. In addition to adversarial training, there has also been work on evaluating model robustness to perturbations. Some of this work has focused on evaluating model robustness to adversarial perturbations, including adversarial attacks (@CITE[2]@; @CITE[4]@ and adversarial training ().\\nReferences:ID: 1. Title: Synthetic and Natural Noise Both Break Neural Machine Translation\\nID: 2. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks\\nID: 3. Title: Evaluating Models' Local Decision Boundaries via Contrast Sets\\nID: 4. Title: CLEAR: Contrastive Learning for Sentence Representation\",\n",
       "  'ground_truth': \"\\n\\nTo make neural networks more robust to adversarial examples, many defense strategies have been proposed, and adversarial training is widely considered to be the most effective. Different from the image domain, it is more challenging to deal with text data due to its discrete property, which is hard to optimize. Previous works focus on heuristics for creating adversarial examples in the black-box setting. @CITE[3]@ manipulate every word in a sentence with synthetic or natural noise in machine translation systems. @CITE[4]@ leverage back-translated to produce paraphrases that have different sentence structures. Recently, extend adversarial and virtual adversarial training ( to text classification tasks by applying perturbations to word embeddings rather than discrete input symbols. Following this, many adversarial training methods in the text domain have been proposed and have been applied to the state-of-the-art PLMs. @CITE[2]@ introduce a token-level perturbation to improves the robustness of PLMs. @CITE[1]@ use the gradients obtained in adversarial training to boost the performance of PLMs. Although many studies seem to achieve a robust representation, our pilot experiments (Section 2) show that there is still a long way to go.\\nReferences:ID: 1. Title: Synthetic and Natural Noise Both Break Neural Machine Translation\\nID: 2. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks\\nID: 3. Title: Evaluating Models' Local Decision Boundaries via Contrast Sets\\nID: 4. Title: CLEAR: Contrastive Learning for Sentence Representation<|eot_id|>\"},\n",
       " 6236: {'predict': 'Language model pre-training and task-specific fine-tuning have been widely used in NLP tasks (@CITE[6]@; @CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[1]@. However, pre-trained language models are not directly applicable to IE tasks. The main challenge is that IE tasks are not text classification tasks. The output of IE tasks is structured, which is different from the sequence output of text classification tasks. Therefore, pre-trained language models need to be adapted to IE tasks. Existing works on IE tasks can be roughly divided into two categories: (1) task-specific fine-tuning, which fine-tunes the pre-trained language models on IE tasks (@CITE[3]@; @CITE[1]@; @CITE[1]@. However, these methods require a large amount of annotated data, which is hard to obtain. (2) task-agnostic pre-training, which pre-trains the language models on IE tasks to learn task-specific knowledge. However, these methods require a large amount of unannotated data, which is hard to obtain. In this paper, we propose a novel framework, which can be used in both task-specific fine-tuning and task-agnostic pre-training. It is also applicable to both supervised and unsupervised settings.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 3. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 4. Title: Revisiting Pre-Trained Models for Chinese Natural Language Processing\\nID: 5. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 6. Title: Unified Structure Generation for Universal Information Extraction',\n",
       "  'ground_truth': '\\n\\nLanguage model pre-training (@CITE[5]@; @CITE[1]@ and task-specific fine-tuning achieve significant improvement on many NLP tasks. Recently, some work (@CITE[3]@; @CITE[4]@; ; @CITE[6]@ further modified the MLM to achieve better results. In information extraction tasks, UIE (@CITE[2]@ is proposed as a universal pre-training model for several extraction tasks by generation, it is generic but lacks further fitting for different extraction tasks. Currently, there is no task-specific pre-training model for attribute value extraction.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 3. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 4. Title: Revisiting Pre-Trained Models for Chinese Natural Language Processing\\nID: 5. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 6. Title: Unified Structure Generation for Universal Information Extraction<|eot_id|>'},\n",
       " 2608: {'predict': 'Pre-trained language models (PrLMs) have achieved impressive results in various NLP tasks by learning general language representations from large-scale unlabeled corpora (@CITE[6]@; @CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[1]@; @CITE[2]@. These models can be fine-tuned on specific downstream tasks with only a few labeled samples. The most popular PrLMs are BERT (@CITE[4]@, RoBERTa (@CITE[1]@, and T5 (@CITE[2]@. In this work, we use T5 as our PrLM, which is a text-to-text transformer (T5) with a sequence-to-sequence architecture and pre-trained on a large corpus of web pages.\\nReferences:ID: 1. Title: Semantic-Preserving Adversarial Code Comprehension\\nID: 2. Title: Semantic-Preserving Adversarial Code Comprehension\\nID: 3. Title: Attention Is All You Need\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 6. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 7. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',\n",
       "  'ground_truth': '\\n\\nOur model is implemented based on pre-trained language models (PrLMs), which have achieved remarkable results on many natural language understanding (NLU) tasks and are widely used as a text encoder by many researchers (; ). Based on the self-attention mechanism and Transformer (@CITE[4]@), together with pre-training on large corpora, PrLMs have a strong capability of encoding natural language texts into contextualized representations. To name a few, BERT (@CITE[3]@), ALBERT (@CITE[1]@), and ELECTRA (@CITE[2]@) are the most prominent ones for NLU; GPT (@CITE[5]@), T5 (@CITE[7]@), and BART (@CITE[6]@) are the most representative ones for natural language generation. In our work, we select BERT, ELECTRA, and BART as the encoder backbones of our model.\\nReferences:ID: 1. Title: Semantic-Preserving Adversarial Code Comprehension\\nID: 2. Title: Semantic-Preserving Adversarial Code Comprehension\\nID: 3. Title: Attention Is All You Need\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 6. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 7. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<|eot_id|>'},\n",
       " 7380: {'predict': 'Most existing dialogue datasets are collected using Wizard of Oz (WOZ) (, 2016) or self-dialog (@CITE[7]@) approaches. WOZ (, 2016) is a widely used method to collect task-oriented dialogue datasets. It involves a crowd-sourced worker interacting with a system written by another worker. The system worker can access a set of APIs to fulfill the user’s requests. The user worker is unaware of the system worker’s access to the APIs. This approach is widely used in collecting task-oriented datasets such as MultiWOZ (@CITE[8]@), WoZ (@CITE[7]@), Schema-Guided Dialogue (@CITE[9]@), Taskmaster-1 (@CITE[10]@), and CrossWOZ (@CITE[11]@). Self-dialog (@CITE[7]@) is another widely used method to collect dialogue datasets. It involves a worker writing both the user and system sides of a dialogue. This approach is widely used in collecting open-domain dialogue datasets such as DailyDialog (@CITE[7]@), Persona-Chat (@CITE[6]@), and TopicalChat (@CITE[12]@). Recently, some researchers have proposed new approaches to collect dialogue datasets. @CITE[1]@ proposed a dialogue generation framework that uses a pre-trained language model to generate a dialogue. @CITE[5]@ proposed a data augmentation approach that uses a pre-trained language model to generate new dialogues based on the existing ones. @CITE[4]@ proposed a dialogue generation framework that uses a hierarchical model to generate a dialogue. @CITE[2]@ proposed a dialogue generation framework that uses a generative model to generate a dialogue. @CITE[3]@ proposed a dialogue generation framework that uses a pre-trained language model to generate a dialogue. In this paper, we propose a novel approach to collect dialogue datasets. It involves a crowd-sourced worker writing a dialogue based on a given template. This approach is widely used in collecting task-oriented datasets such as COD (@CITE[11]@), WoD (@CITE[7]@), and FaithDial (@CITE[1]@).\\nReferences:ID: 1. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 2. Title: MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling\\nID: 3. Title: Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access\\nID: 4. Title: CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\\nID: 5. Title: Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset\\nID: 6. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 7. Title: Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset\\nID: 8. Title: Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation\\nID: 9. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 10. Title: Augmenting Neural Response Generation with Context-Aware Topical Attention\\nID: 11. Title: FaithDial: A Faithful Benchmark for Information-Seeking Dialogue',\n",
       "  'ground_truth': '\\n\\nResearch on task-oriented dialogue systems has mainly focused on customer service, for instance, restaurant reservations (; Gašic et al., 2014). Notably, @CITE[10]@ col- lect such dialogues with the Wizard-of-Oz (WoZ) paradigm (@CITE[3]@, where crowdworkers are connected to roleplay interlocutors. One plays the user who interacts with the system, and the other roleplays the system and is often exclusively given access to domain knowledge. WoZ has been used to collect many popular datasets, such as Multi- WoZ (@CITE[4]@ and extensions (@CITE[9]@; @CITE[1]@, Taskmaster (@CITE[7]@, and open-domain datasets like Wizard-of-Wikipedia (@CITE[8]@. Other collection methods include crowdworkers filling dialogue outlines (Shah et al., 2018; @CITE[11]@; @CITE[6]@, or scraping from the web (@CITE[10]@;. Multiple works have shown shortcomings in using non-expert crowdworkers. For instance, document-grounded corpora often contain hallu- cinations in ground-truth data (@CITE[2]@, and task-oriented corpora tend to suffer from anno- tation errors and low lexical diversity (@CITE[5]@. More closely related to this work, cur- rent tutoring corpora lack sufficient tutoring quality\\nReferences:ID: 1. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 2. Title: MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling\\nID: 3. Title: Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access\\nID: 4. Title: CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\\nID: 5. Title: Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset\\nID: 6. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 7. Title: Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset\\nID: 8. Title: Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation\\nID: 9. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 10. Title: Augmenting Neural Response Generation with Context-Aware Topical Attention\\nID: 11. Title: FaithDial: A Faithful Benchmark for Information-Seeking Dialogue<|eot_id|>'},\n",
       " 5279: {'predict': 'Recently, there is a growing interest in building multi-party conversation systems (; @CITE[5]@; @CITE[4]@; @CITE[3]@). @CITE[5]@ proposed a sequential matching network (SMN) to model the utterance level dependencies in the conversation. @CITE[4]@ proposed a speaker interaction recurrent neural network (SI-RNN) to model the speaker role dependencies. @CITE[3]@ proposed a heterogeneous graph transformer (HGT) to model the complex conversation structure. @CITE[2]@ proposed a heterogeneous graph neural network (HeterMPC) to model the semantics of utterances and interlocutors. @CITE[1]@ proposed a graph-based neural network to model the speaker role dependencies. Our work is most similar to @CITE[3]@, but they only focus on the response selection task. In this paper, we focus on the response generation task and propose a heterogeneous graph-based neural network to model the semantics of utterances and interlocutors simultaneously.\\nReferences:ID: 1. Title: Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\\nID: 2. Title: Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs\\nID: 3. Title: Heterogeneous Graph Transformer\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations',\n",
       "  'ground_truth': '\\n\\nExisting methods on building dialogue systems can be generally categorized into generation-based (Serban et al., 2016; Wen et al., 2017; ; ) or retrieval-based approaches (; ; @CITE[5]@; Gu et al., 2020). In this paper, we study MPC generation, where in addition to utterances, interlocutors are also important components who play the roles of speakers or addressees. Previous methods have explored retrieval-based approaches for MPCs. For example, proposed the dynamic model which updated speaker embeddings with conversation streams. @CITE[2]@ proposed speaker interaction RNN which updated speaker embeddings role-sensitively. proposed to track the dynamic topic in a conversation. Gu et al. (2021) proposed jointly learning “who says what to whom\" in a unified framework by designing self-supervised tasks during pre-training. On the other hand, @CITE[1]@ explored generation-based approaches by proposing a graph-structured network (GSN). At its core was an utterance-level graph-structured encoder that encoded the conversation context using homogeneous GNNs. @CITE[3]@ proposed HeterMPC to model the complicated interactions between utterances and interlocutors with a heterogeneous graph (@CITE[4]@, where two types of nodes and six types of edges were designed to model heterogeneity. Contemporaneous to our work, focus on predicting the missing addressee labels in pre-training instead, but still only target at the ideal setting during fine-tuning where each utterance must be tagged with the addressee label.\\nReferences:ID: 1. Title: Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\\nID: 2. Title: Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs\\nID: 3. Title: Heterogeneous Graph Transformer\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations<|eot_id|>'},\n",
       " 889: {'predict': 'The generation of synthetic parallel data has been widely explored for MT tasks. @CITE[4]@; @CITE[5]@; @CITE[1]@ use back-translation to create pseudo-parallel data for supervised training. In contrast, our approach focuses on unsupervised MT, which is a more challenging task. @CITE[3]@; @CITE[2]@; @CITE[2]@; @CITE[3]@; @CITE[3]@; @CITE[2]@ use back-translation to create synthetic parallel data for unsupervised MT. The key difference between our work and the above methods is that we use a single model for both source-to-target and target-to-source translation, which is more efficient and effective than using two separate models. Our method also improves the quality of synthetic data, which is essential for unsupervised MT. In addition, we use a new method to select the most reliable synthetic data for unsupervised MT, which is more effective than previous methods (@CITE[3]@; @CITE[2]@). Our method can also be used to improve the performance of supervised MT systems. In this paper, we use compare-mt (@CITE[2]@ to evaluate the performance of our method.\\nReferences:ID: 1. Title: Revisiting Self-Training for Neural Sequence Generation\\nID: 2. Title: Improving Neural Machine Translation Models with Monolingual Data\\nID: 3. Title: Understanding Back-Translation at Scale\\nID: 4. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 5. Title: compare-mt: A Tool for Holistic Comparison of Language Generation Systems',\n",
       "  'ground_truth': '\\n\\nData augmentation by synthetic parallel data has been the most simple and effective way to utilize monolingual data for NMT, which can be achieved by self-training (@CITE[5]@) and back-translation (). While back-translation has dominated the NMT area for years (@CITE[5]@; ; Caswell et al., 2019), recent works on translationese (; ) suggest that NMT models trained with back-translation may lead to distortions in automatic and human evaluation. To address the problem, starting from WMT2019 (), the test sets only include naturally occurring text at the source-side, which is a more realistic scenario for practical translation usage. In this new testing setup, the forward-translation (@CITE[5]@), i.e., self-training in NMT, becomes a more promising method as it also introduces naturally occurring text at the source-side. Therefore, we focus on the data sampling strategy in the self-training scenario, which is different from these prior studies.\\nReferences:ID: 1. Title: Revisiting Self-Training for Neural Sequence Generation\\nID: 2. Title: Improving Neural Machine Translation Models with Monolingual Data\\nID: 3. Title: Understanding Back-Translation at Scale\\nID: 4. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 5. Title: compare-mt: A Tool for Holistic Comparison of Language Generation Systems<|eot_id|>'},\n",
       " 5241: {'predict': 'AAL feature extraction has been studied extensively in the past, with a focus on morphological features. For example, @CITE[1]@ and @CITE[3]@ used rule-based systems to identify morphological features in AAL. These systems were trained on manually annotated data, and their performance was evaluated using human annotated data. @CITE[2]@ used a combination of rule-based systems and data augmentation to identify morphological features in AAL. They used a corpus of AAL texts, and their performance was evaluated using a corpus of AAL texts. In contrast, we use a corpus of AAL speech, and we evaluate our performance using a corpus of AAL speech. The most similar work to ours is that of, who used a combination of acoustic and language modeling features to identify morphological features in AAL. They used a corpus of AAL speech, and their performance was evaluated using a corpus of AAL speech. However, their work was limited to identifying morphological features, and they did not evaluate their system on a corpus of non-AAL speech. In contrast, we use a combination of acoustic and language modeling features to identify morphological and phonological features in AAL, and we evaluate our system on both AAL and non-AAL speech.\\nReferences:ID: 1. Title: Corpus-Guided Contrast Sets for Morphosyntactic Feature Detection in Low-Resource English Varieties\\nID: 2. Title: Disambiguation of morpho-syntactic features of African American English -- the case of habitual be\\nID: 3. Title: Automatic Dialect Density Estimation for African American English',\n",
       "  'ground_truth': '\\n\\nPast work makes progress in lowering performance gaps between AAL and WME by focusing on linguistic feature extraction tasks. Given that some features of AAL such as the aspectual verbs (i.e., habitual be, remote past been) do not have equivalent meanings and functions in WME (, standard part-of-speech (POS) taggers and dependency parsers cannot maintain performance for AAL text. Studies have attempted to lessen this gap by creating a POS tagger specifically for AAL through domain adaptation (Jørgensen et al., 2016) and a dependency parser for AAL in Tweets (Blodgett et al., 2018). Beyond these tasks, considerable attention has been given to developing tools for features specific to AAL and other language varieties, such as detecting dialect-specific constructions (@CITE[2]@; Demszky et al., 2021; @CITE[3]@; @CITE[1]@ to aid in bias mitigation strategies.\\nReferences:ID: 1. Title: Corpus-Guided Contrast Sets for Morphosyntactic Feature Detection in Low-Resource English Varieties\\nID: 2. Title: Disambiguation of morpho-syntactic features of African American English -- the case of habitual be\\nID: 3. Title: Automatic Dialect Density Estimation for African American English<|eot_id|>'},\n",
       " 3913: {'predict': 'To better leverage the knowledge from KGs, various knowledge-enhanced representation methods are proposed. ERNIE (@CITE[6]@) and ERNIE-M (@CITE[6]@) mask entities and phrases in the text and predict them, respectively. KEPLER (@CITE[2]@) and KEPLER-2 (@CITE[2]@) encode entity descriptions into embeddings and jointly optimize the KE and language modeling objectives. In addition, there are some other knowledge-enhanced representation methods, such as ERNIE-M (@CITE[6]@), which masks phrases in the text and predicts them, and SpanBERT (@CITE[3]@), which masks spans in the text and predicts them. However, all these methods mask text and predict the masked tokens, which is not suitable for the task of knowledge graph completion, as the predicted tokens are not necessarily in the KG. Therefore, we propose to use a novel method of masking entities in the text and predicting entity relations in the KG, which is suitable for the task of knowledge graph completion.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 3. Title: UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training\\nID: 4. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 5. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 6. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans',\n",
       "  'ground_truth': '\\n\\nFollowing the BERT (@CITE[5]@ architecture, many efforts are devoted to pre-trained language models for learning informative representations. There are some studies show that extra knowledge, such as facts in WikiData and WordNet, can further benefit the pre-trained models (@CITE[1]@; ; ; @CITE[2]@, but the embeddings of words in the text and entities in the knowledge graphs are not in the same vector space, so a cumbersome adaptation module is required (; @CITE[3]@. Another research line is to excavate the potential human cognitive laws of the text itself: ERNIE (@CITE[1]@ creativity proposes entity-level mask in pre-training to incorporate the human knowledge into language models. Similarly, SpanBERT (@CITE[6]@ modifies the making schema and training objectives to better represent and predict text spans. BERT-wwm ( introduces a whole word masking strategy for Chinese language models. Outside the field of plain text, ERNIE- ViL (@CITE[4]@ incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. Inspired by the above work, we leverage the implicit knowledge related to layout, e.g., reading order, for the understanding of visually rich documents.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 3. Title: UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training\\nID: 4. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 5. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 6. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans<|eot_id|>'},\n",
       " 7318: {'predict': 'Our work is related to communicative agents, which have been studied in many domains. In the context of language models, there has been a growing interest in using LMs to generate reasoning traces and task-specific actions in an interleaved manner (@CITE[1]@; @CITE[5]@. In the context of dialogue systems, there has been a growing interest in using LMs to generate dialogue policies (; @CITE[6]@; @CITE[2]@. In the context of game playing, there has been a growing interest in using LMs to generate strategies for cooperative games such as Hanabi (@CITE[3]@. In the context of human-AI collaboration, there has been a growing interest in using LMs to generate instructions for LMs to assist humans (@CITE[4]@. Our work is related to the work of @CITE[4]@, which studies how to use LMs to generate instructions for LMs to assist humans. However, our work is different from theirs in that we focus on generating instructions to communicate with humans, and we also focus on evaluating the quality of generated instructions.\\nReferences:ID: 1. Title: ReAct: Synergizing Reasoning and Acting in Language Models\\nID: 2. Title: Cooperative Inverse Reinforcement Learning\\nID: 3. Title: The Hanabi Challenge: A New Frontier for AI Research\\nID: 4. Title: Open Problems in Cooperative AI\\nID: 5. Title: A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-Oriented Dialogue Policy Learning\\nID: 6. Title: GPTScore: Evaluate as You Desire',\n",
       "  'ground_truth': '\\n\\nLarge language models have exhibited remarkable multi-dimensional capabilities in tackling complex tasks. However, their effectiveness greatly hinges on human guidance to steer the conversation, a process that can present challenges and consume significant time (; @CITE[5]@; @CITE[2]@. It is important to consider how to autonomously guide the communicative agents toward task completion while maintaining consistency with human intentions. Communication between AI agents can occur in a competitive setting or a cooperative setting (@CITE[1]@; @CITE[4]@; ; @CITE[6]@; @CITE[3]@;. Cooperative AI systems consider the requirements and capacities of other agents within the system, actively striving to collaborate and synchronize their actions. This approach offers numerous advantages, such as enhanced efficiency, improved decision-making, and the ability to address intricate problems that surpass the capabilities of individual agents. enables two communicative agents to engage in a conversation and cooperate with each other to solve assigned tasks. However, designing effective cooperative AI systems is still an active area of research, as it requires addressing a range of technical, ethical, and social challenges.\\nReferences:ID: 1. Title: ReAct: Synergizing Reasoning and Acting in Language Models\\nID: 2. Title: Cooperative Inverse Reinforcement Learning\\nID: 3. Title: The Hanabi Challenge: A New Frontier for AI Research\\nID: 4. Title: Open Problems in Cooperative AI\\nID: 5. Title: A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-Oriented Dialogue Policy Learning\\nID: 6. Title: GPTScore: Evaluate as You Desire<|eot_id|>'},\n",
       " 1258: {'predict': 'Program synthesis and repair are long-standing problems in computer science. Traditional approaches to program synthesis are based on logic, which can be either inductive (@CITE[3]@; ; ; @CITE[4]@ or deductive (Kearns et al., 2016; @CITE[1]@. However, recent advances in neural networks have led to a variety of neural program synthesis methods (@CITE[5]@; ; @CITE[2]@;. Some of these methods use reinforcement learning to synthesize programs that can solve tasks such as sorting lists (@CITE[2]@ or translating natural language to SQL (@CITE[5]@. Other methods use generative language models to synthesize programs that can solve mathematical problems (@CITE[1]@;. Our method is based on the observation that neural program synthesis methods can be used to generate programs that can solve tasks, and then repair those programs to make them more efficient. In this way, we can combine the strengths of different methods to achieve better performance.\\nReferences:ID: 1. Title: Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis\\nID: 2. Title: Language to Logical Form with Neural Attention\\nID: 3. Title: DeepCoder: Learning to Write Programs\\nID: 4. Title: Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning\\nID: 5. Title: Generative Language Modeling for Automated Theorem Proving',\n",
       "  'ground_truth': '\\n\\nRepairing / revising generated programs has been studied in the field of program synthesis. For example, prior work repairs a program based on the syntax of the underlying language (@CITE[3]@), by generating sketches (@CITE[1]@). More recently, @CITE[5]@ proposes a framework in which they use a program debugger to revise the program generated by a neural program synthesizer. However, none of these works take advantage of the similarity between semantic relations present in the knowledge base, and hence, unlike us, they do not use embeddings of similar relation to align relations. More generally, many prior efforts have employed neural models to generate SPARQL-like code for semantic parsing (@CITE[2]@; @CITE[4]@; ), SQL queries over relational databases (Zhong et al., 2017b), program-structured neural network layouts (Andreas et al., 2016), or even proofs for mathematical theorems (). Our work differs in our use of the programs of multiple retrieved similar queries to generate the target program.\\nReferences:ID: 1. Title: Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis\\nID: 2. Title: Language to Logical Form with Neural Attention\\nID: 3. Title: DeepCoder: Learning to Write Programs\\nID: 4. Title: Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning\\nID: 5. Title: Generative Language Modeling for Automated Theorem Proving<|eot_id|>'},\n",
       " 3298: {'predict': 'Several works have attempted to improve the quality of simplification outputs by leveraging external resources and/or training a simplification model with a specific objective. For instance, @CITE[5]@ used a reinforcement learning framework to encourage the model to generate fluent outputs. @CITE[6]@ and @CITE[4]@ used the Transformer (@CITE[1]@) and its variants to learn simplification rules from parallel sentence pairs. @CITE[3]@ and @CITE[2]@ used complexity weights to guide the training of simplification models. @CITE[6]@ and @CITE[2]@ used paraphrase rules to improve the diversity of generated outputs. However, the above works only focused on the generation of simpler texts, without explicitly controlling the complexity of the generated texts. To address this limitation, @CITE[6]@ proposed to use a discrete parameterization mechanism to control the degree of simplification. @CITE[7]@ used a similar mechanism to control the amount of paraphrasing. However, these works only focused on controlling the degree of simplification and paraphrasing, without considering the overall complexity of the generated texts. In contrast, our work proposes to control the complexity of generated texts by directly using the complexity of the input texts as a constraint.\\nReferences:ID: 1. Title: Sentence Simplification with Deep Reinforcement Learning\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: Integrating Transformer and Paraphrase Rules for Sentence Simplification\\nID: 4. Title: Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification\\nID: 5. Title: Controllable Text Simplification with Explicit Paraphrasing\\nID: 6. Title: Controllable Sentence Simplification',\n",
       "  'ground_truth': '\\n\\nRecent work, reviewed below, leverages neural models in a generative and revision-based manner. Supervised Generative Methods employ Seq2Seq models to learn simplification operations from aligned complex-simple sentence pairs (Nisioi et al., 2017). Building on a Seq2Seq model, @CITE[4]@ used reinforcement learning to optimize a reward based on simplicity, fluency and relevance. Recent methods build on transformer (@CITE[2]@ models, by integrating external databases containing simplification rules (@CITE[1]@, using an additional loss function to generate diverse outputs (@CITE[5]@, combining syntactic rules (@CITE[6]@, and conditioning on length and syntactic and lexical complexity features (@CITE[3]@).\\nReferences:ID: 1. Title: Sentence Simplification with Deep Reinforcement Learning\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: Integrating Transformer and Paraphrase Rules for Sentence Simplification\\nID: 4. Title: Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification\\nID: 5. Title: Controllable Text Simplification with Explicit Paraphrasing\\nID: 6. Title: Controllable Sentence Simplification<|eot_id|>'},\n",
       " 1771: {'predict': 'There has been a lot of work on compressing word embeddings. One of the most common techniques is quantization, which maps a vector to a lower-precision representation (e.g., @CITE[1]@; @CITE[3]@). Another common approach is to use a hash code to represent a word (@CITE[2]@; @CITE[4]@). Recently, there has been a lot of work on compressing the Transformer architecture (@CITE[5]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[5]@; @CITE[6]@). In this work, we focus on compressing the embedding layer, which is the most expensive part of the Transformer architecture. We propose a new method for compressing the embedding layer, which is based on the idea of low-rank matrix approximation. We show that our method achieves better compression than previous methods, while preserving the performance of the model.\\nReferences:ID: 1. Title: Compressing Word Embeddings\\nID: 2. Title: Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\\nID: 3. Title: Compressing Word Embeddings via Deep Compositional Code Learning\\nID: 4. Title: A Tensorized Transformer for Language Modeling\\nID: 5. Title: A Tensorized Transformer for Language Modeling\\nID: 6. Title: GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking',\n",
       "  'ground_truth': '\\n\\nWord embedding is a crucial part for natural language processing, and it requires considerable size. Thus, many approaches have been proposed for compressing it. Several works were proposed for compact representation of word embedding. ( proposed a way of exploiting Lloyd’s algorithm to get low-bit representations of embedding vectors. ( studied 8-bit representations for word embedding with training. (@CITE[3]@ proposed low-bit quantized neural networks for convolutional neural networks and recurrent neural networks. More recent works focus on devising a better structure of word embedding or an optimized way of computing encodings. ( proposed a method compressing word embedding through compositional discrete codes, which can be trained by gradient descent. (@CITE[2]@ proposed a product quantization-based compression method, which divides an embedding matrix into sub-matrices via k-means clustering. In aspect of word clustering, it is similar to our clustering method, but we do not use embedding vectors as the targets of clustering. In a slightly different line of research, (@CITE[5]@ proposed an evaluation score for the downstream performance of compressed word embeddings, which is named the eigenspace overlap score. In addition, (@CITE[5]@ showed a lower bound of the eigenspace overlap score for a uniform simple quantization-based compression method to explain its empirical effectiveness. We do not use the eigenspace overlap score in this work, but the quantization method will be used in the experiments. (Kim et al., 2020b) proposed a codebook-based compression method supporting word-level adaptive code length. The adaptive code length of a word can be considered as a word importance measure, but the code length should be predefined on domain of very limited size. Since word embedding is generally represented by a matrix, decomposition-based compression techniques and efficient embedding structures were proposed. ( proposed the blockwise low-rank approximation method for word embedding. (@CITE[6]@ devised a way of interpreting an embedding matrix into a 3-dimensional tensor and proposed an embedding structure by decomposing it with tensor-train decomposition. (@CITE[1]@ proposed a small-size word embedding structure inspired by quantum entanglement. (@CITE[4]@ proposed a recent study for word embedding factorization based on distillation. As (@CITE[4]@ conducted experiments for combining their approach with GroupReduce, it can be also applied to our algorithm. There are lots of existing approaches for word embedding compression, but none of existing approaches deeply study word weights in sense of compressing word embedding.\\nReferences:ID: 1. Title: Compressing Word Embeddings\\nID: 2. Title: Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\\nID: 3. Title: Compressing Word Embeddings via Deep Compositional Code Learning\\nID: 4. Title: A Tensorized Transformer for Language Modeling\\nID: 5. Title: A Tensorized Transformer for Language Modeling\\nID: 6. Title: GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking<|eot_id|>'},\n",
       " 4834: {'predict': 'Information Extraction (IE) is a long-standing research area in NLP. The tasks of IE can be divided into four main tasks: named entity recognition (NER), relation extraction (RE), event extraction (EE), and sentiment analysis (SA). Many previous works have been conducted on these tasks, and the performance of the tasks is constantly improved. For example, @CITE[11]@ proposed a neural network model based on BiLSTM-CRF for NER, which is a baseline model for many subsequent works. @CITE[9]@ proposed a biaffine model for NER, which has been widely used in many IE tasks. For RE, many works have been conducted on the task (@CITE[8]@; @CITE[10]@; @CITE[2]@; @CITE[5]@; @CITE[4]@; @CITE[7]@; @CITE[6]@; @CITE[3]@; @CITE[1]@; @CITE[1]@; @CITE[3]@; @CITE[1]@. Recently, some works have been conducted on the joint extraction of entities and relations (@CITE[11]@; @CITE[5]@; @CITE[4]@; @CITE[6]@; @CITE[7]@; @CITE[3]@; @CITE[1]@. However, these works are based on a pipeline model, which is prone to error propagation. In this paper, we propose a unified framework for joint extraction of entities and relations, which can avoid error propagation and achieve better performance.\\nReferences:ID: 1. Title: Neural Architectures for Named Entity Recognition\\nID: 2. Title: Named Entity Recognition as Dependency Parsing\\nID: 3. Title: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\\nID: 4. Title: One for All: Neural Joint Modeling of Entities and Events\\nID: 5. Title: Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\\nID: 6. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 7. Title: Unified Structure Generation for Universal Information Extraction\\nID: 8. Title: Universal Information Extraction as Unified Semantic Matching\\nID: 9. Title: A General Framework for Information Extraction using Dynamic Span Graphs\\nID: 10. Title: Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks\\nID: 11. Title: Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks',\n",
       "  'ground_truth': '\\n\\nClassical IE models are typically task-specific (@CITE[1]@; @CITE[6]@; @CITE[4]@; Wang et al., 2019a). Recent efforts develop joint methods for multiple IE tasks (; @CITE[5]@; @CITE[7]@; @CITE[2]@; or general architectures for universal IE (; @CITE[10]@; @CITE[11]@. Graph-based joint IE methods formulate multiple IE tasks as a graph prediction task and aim to capture dependencies between different instances or tasks. Lots of previous works leverage encoder sharing or graph convolutional networks (GCNs) on instance dependency graphs to enrich instance representations (@CITE[8]@; ;, 2022a,b). This work is more relevant to some recent works that take efforts on type interactions and global inference. @CITE[9]@ manually designs global features as constraints and leverages beam search to find approximated global optima. Based on the method of @CITE[9]@, further incorporates AMR graphs as external dependencies. The work of is more similar to ours in that they adopt a CRF to model type dependencies, but they learn a transition matrix that only scores binary dependencies. Besides, they employ Noise Contrastive Estimation (NCE) (@CITE[3]@ to perform approximate training and Simulated An- Different from their work, we model both binary and ternary dependencies and leverage MFVI to achieve consistent training and inference.\\nReferences:ID: 1. Title: Neural Architectures for Named Entity Recognition\\nID: 2. Title: Named Entity Recognition as Dependency Parsing\\nID: 3. Title: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\\nID: 4. Title: One for All: Neural Joint Modeling of Entities and Events\\nID: 5. Title: Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\\nID: 6. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 7. Title: Unified Structure Generation for Universal Information Extraction\\nID: 8. Title: Universal Information Extraction as Unified Semantic Matching\\nID: 9. Title: A General Framework for Information Extraction using Dynamic Span Graphs\\nID: 10. Title: Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks\\nID: 11. Title: Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks<|eot_id|>'},\n",
       " 5854: {'predict': 'There is a growing interest in studying bias in historical documents. Previous work has explored the issue of gender bias in historical documents, and found that the gender bias in word embeddings has persisted over time (, 2020; @CITE[4]@; @CITE[3]@. @CITE[2]@ study the issue of bias in word embeddings in relation to the use of derogatory terms, and find that such terms are more likely to be associated with women. This work is related to our own, as we also find that gender bias in word embeddings has persisted over time. However, we use a different approach to study this bias, as we focus on the way that bias is reflected in word embeddings trained on texts from different eras. We also study bias in relation to other social groups, and find that the bias in word embeddings is not limited to gender. Previous work has also studied the issue of bias in relation to ethnicity and race, and found that the bias in word embeddings is also reflected in these areas (@CITE[1]@; @CITE[3]@.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Unsupervised Discovery of Gendered Language through Latent-Variable Modeling\\nID: 3. Title: Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes\\nID: 4. Title: A Large-Scale Comparison of Historical Text Normalization Systems',\n",
       "  'ground_truth': '\\n\\nHistorical corpora have been employed to study societal phenomena such as language change (@CITE[4]@; Hamilton et al., 2016) and societal biases. Gender bias has been analysed in biomedical research over a span of 60 years (, in English-language books published between 1520 and 2008 (@CITE[3]@, and in Dutch newspapers from the second half of the 20th century (Wevers, 2019). @CITE[1]@ investigate the evolution of the discourse on Jews in France during the 19th century. @CITE[2]@ study the temporal change in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the US. However, they neglect the emergent intersectionality bias. When analysing the transformations of biases in historical texts, researchers rely on conventional tools developed for modern language. However, historical texts can be viewed as a separate domain due to their unique challenges of small and idiosyncratic corpora and noisy, archaic text (Piotrowski, 2012). Prior work has attempted to overcome the challenges such documents pose for modern tools, including recognition of spelling variations ( and misspelt words (, and ensuring the stability of the applied methods (. We study the dynamics of intersectional biases and their manifestations in language while addressing the challenges of historical data.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Unsupervised Discovery of Gendered Language through Latent-Variable Modeling\\nID: 3. Title: Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes\\nID: 4. Title: A Large-Scale Comparison of Historical Text Normalization Systems<|eot_id|>'},\n",
       " 2534: {'predict': 'Event argument extraction (EAE) is a subtask of information extraction, which aims to extract arguments of an event. Many approaches have been proposed to extract event arguments, including template-based methods (@CITE[7]@; @CITE[8]@), rule-based methods (@CITE[9]@; @CITE[6]@), and neural network-based methods (@CITE[9]@; @CITE[7]@; @CITE[6]@; @CITE[4]@; @CITE[1]@; @CITE[5]@; @CITE[2]@; @CITE[3]@). Recently, neural network-based methods have achieved state-of-the-art performance. However, most neural network-based methods are designed for sentence-level EAE, which only extract arguments within a sentence. To extract event arguments from the whole document, some methods (@CITE[6]@; @CITE[1]@; @CITE[5]@) adopt a document-level EAE framework, which extracts arguments from the whole document instead of a sentence. However, these methods are usually designed based on the same neural network architecture as sentence-level EAE methods, which are not able to effectively model the relationships between events and arguments. To address this problem, we propose a novel document-level EAE framework based on a hierarchical graph network, which is able to effectively model the relationships between events and arguments.\\nReferences:ID: 1. Title: Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction\\nID: 2. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 3. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 4. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 5. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 6. Title: Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction\\nID: 7. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 8. Title: Document-Level Event Argument Extraction by Conditional Generation\\nID: 9. Title: Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction',\n",
       "  'ground_truth': '\\n\\nFor the EAE task, most previous works adopt a classification-based strategy where each role is assigned static querying parameters (@CITE[8]@; @CITE[4]@; ; @CITE[5]@; ;. Recently, two interesting alternative strategies have been explored to enable extraction in more flexible ways: MRC-based methods cast the problem as answering role questions (; ; ; @CITE[2]@; @CITE[6]@;, while generation-based methods adopt sequence-to-sequence generation schemes (; ; @CITE[9]@; ; Du et al., 2021; @CITE[9]@. We cover all these strategies within a unified role querying framework and further explore a template-based role querying strategy. This strategy is also related with prompt-based learning (@CITE[3]@; Schick and Schütze, 2021; @CITE[1]@; @CITE[7]@, but differs in the extraction-targeted paradigm.\\nReferences:ID: 1. Title: Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction\\nID: 2. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 3. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 4. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 5. Title: Reading the Manual: Event Extraction as Definition Comprehension\\nID: 6. Title: Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction\\nID: 7. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 8. Title: Document-Level Event Argument Extraction by Conditional Generation\\nID: 9. Title: Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction<|eot_id|>'},\n",
       " 3554: {'predict': '@CITE[3]@ and @CITE[1]@ proposed adapter-based methods for NMT. @CITE[3]@ focused on domain adaptation and multilingual NMT. They showed that adapters can be used to adapt a model to a new domain and also to make a multilingual model more language-specific. @CITE[1]@ studied how to fine-tune pre-trained NMT models for a given language pair. They showed that freezing the majority of the model parameters and fine-tuning only the adapters leads to the best results. @CITE[2]@ studied how to combine domain and language adapters in the context of multilingual NMT. They showed that a naive combination of adapters can result in catastrophic forgetting of missing languages. They proposed different ways to combine the adapters to alleviate this issue. In this work, we use adapters to make a model more language-specific for a given language pair. We study how to combine language and domain adapters and how to fine-tune them.\\nReferences:ID: 1. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 2. Title: Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation\\nID: 3. Title: Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters',\n",
       "  'ground_truth': '\\n\\n@CITE[2]@ train language-pair adapters on top of a pre-trained generic MNMT model, in order to recover lost performance on high-resource language pairs compared to bilingual NMT models. @CITE[1]@ train adapters for each language and show that adding them to a trained model improves the performance of zero-shot translation. train adapters for each language family and show promising results on multilingual machine translation. train language-agnostic adapters to efficiently fine-tune a pre-trained model for many language pairs. More recently, @CITE[3]@ stack language adapters and domain adapters on top of an MNMT model and they conclude that it is not possible to transfer domain knowledge across languages, except by employing back-translation which requires significant in-domain resources. In this work, we introduce adapters into the meta-learning algorithm and show that this approach permits transfer between domains and languages. Our work is mostly related to @CITE[3]@, however we note several differences: i) we study a more realistic scenario: the corpus of each domain and language pair is low-resource (i.e., the meta-training corpus in each domain for each language pair is limited to 5000 sentences and the fine-tuning corpus to 500 sentences), which is easier to obtain; ii) our approach can simultaneously adapt to new domains and new language pairs without using back-translation. iii) we also show that m4Adapter can transfer domain information across different languages and language knowledge across different domains through a detailed ablation analysis.\\nReferences:ID: 1. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 2. Title: Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation\\nID: 3. Title: Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters<|eot_id|>'},\n",
       " 1932: {'predict': 'In order to generate text with a specific style, various methods have been proposed. One line of work focuses on generating text with a specific attribute (@CITE[1]@; ; @CITE[3]@; @CITE[2]@; @CITE[5]@). Another line of work focuses on generating text with a specific style, e.g. formal, informal, polite, and so on (@CITE[4]@; ; ; @CITE[2]@; @CITE[5]@). Different from these works, our work focuses on generating text with a specific topic. In addition, the existing work usually generates text from scratch, while our work generates text from a given text.\\nReferences:ID: 1. Title: Toward Controlled Generation of Text\\nID: 2. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 3. Title: Constituency Parsing with a Self-Attentive Encoder\\nID: 4. Title: Towards Controlled Transformation of Sentiment in Sentences\\nID: 5. Title: Politeness Transfer: A Tag and Generate Approach',\n",
       "  'ground_truth': '\\n\\nGenerating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (; @CITE[2]@; @CITE[3]@; Xu et al., 2015; @CITE[1]@. More recently, pipeline-based or stepwise approaches (@CITE[5]@; ; @CITE[4]@ focuses on ﬁrst localiz- ing the style to a ﬁxed portion of the word, then gen- erating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incor- porating the localized style attribute into a joint-embedding and enforces the text generation model to pay attention to the modiﬁcations.\\nReferences:ID: 1. Title: Toward Controlled Generation of Text\\nID: 2. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 3. Title: Constituency Parsing with a Self-Attentive Encoder\\nID: 4. Title: Towards Controlled Transformation of Sentiment in Sentences\\nID: 5. Title: Politeness Transfer: A Tag and Generate Approach<|eot_id|>'},\n",
       " 1415: {'predict': 'Contrastive learning has been widely used in self-supervised learning to learn discriminative representations. In computer vision, contrastive learning is used for image representation learning (Oordonez-Feliú et al., 2021; @CITE[2]@; @CITE[4]@). In NLP, contrastive learning is used for text representation learning (@CITE[5]@; @CITE[3]@). For multimodal learning, contrastive learning is used for learning discriminative multimodal representations (Chen et al., 2019; @CITE[1]@; @CITE[1]@).\\nReferences:ID: 1. Title: A Survey on Contrastive Self-supervised Learning\\nID: 2. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning\\nID: 5. Title: Interventional Video Grounding with Dual Contrastive Learning',\n",
       "  'ground_truth': '\\n\\nContrastive learning (@CITE[3]@) has gained popularity recently in the CV community. The core idea is to contrast the similarities and dissimilarities between data instances, which pulls the positives closer and pushes negatives away simultaneously. CPC (van den Oord et al., 2018) proposes a universal unsupervised learning approach. MoCo () presents a mechanism for building dynamic dictionaries for contrastive learning. Sim-CLR (@CITE[4]@) improves contrastive learning by using a larger batch size and data augmentation. extend the self-supervised contrastive approach to the supervised setting. @CITE[1]@ propose a dual contrastive learning approach for video grounding. There are also some applications of contrastive learning in the field of NLP. @CITE[5]@ employ back translation and MoCo to learn sentence-level representations. @CITE[2]@ design supervised contrastive learning for pre-trained language model fine-tuning. Inspired by these works, we propose a heterogeneous relation-prototype contrastive learning in a supervised way to obtain more discriminative representations.\\nReferences:ID: 1. Title: A Survey on Contrastive Self-supervised Learning\\nID: 2. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning\\nID: 5. Title: Interventional Video Grounding with Dual Contrastive Learning<|eot_id|>'},\n",
       " 6481: {'predict': 'Automatic report generation is a challenging task, which has been extensively studied in the medical domain. @CITE[6]@ and @CITE[5]@ proposed to utilize the attention mechanism to model the relationship between images and reports. @CITE[8]@ proposed a multi-task learning framework to generate medical reports. @CITE[9]@ proposed a knowledge-driven encode, retrieve, paraphrase (KERP) framework, which transforms visual features into a structured abnormality graph by incorporating prior medical knowledge and generates reports based on the detected abnormalities. @CITE[3]@ proposed a multi-modal transformer to model the relationship between images and reports. @CITE[7]@ proposed a memory-driven Transformer to incorporate the memory into the decoder. @CITE[4]@ proposed a multi-modal transformer to model the relationship between images and reports. @CITE[1]@ proposed a multi-modal graph transformer to model the relationship between images and reports. @CITE[2]@ proposed a multi-modal transformer to model the relationship between images and reports. However, these methods are not designed to handle the medical image report generation task in a real-world scenario, where images are collected from different hospitals and reports are written by different radiologists.\\nReferences:ID: 1. Title: On the Automatic Generation of Medical Imaging Reports\\nID: 2. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 3. Title: ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\\nID: 4. Title: Show and Tell: A Neural Image Caption Generator\\nID: 5. Title: Attention Is All You Need\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 8. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 9. Title: Generating Radiology Reports via Memory-driven Transformer',\n",
       "  'ground_truth': '\\n\\nThe topic of automatic report generation was researched by @CITE[1]@; Zhang et al. (2017); @CITE[5]@. Several attempts have been made in the medical field to create medical reports from the corresponding images. Most researchers use multilabel image captioning to produce X-ray reports, and they subsequently use those captions as textual features. The IU X-ray dataset was created by @CITE[3]@ to generate radiology reports automatically. The IU X-RAY dataset’s chest X-ray images were used to generate the first structured report using tags predicted by a CNN-RNN model (Shin et al., 2016). A system for generating natural reports for the Chest-Xray14 dataset, employing private reports, was presented by @CITE[9]@. This framework used a non-hierarchical CNN-LSTM architecture and focused on semantic and visual aspects. Visual attention given to recurrent decoders and convolution-recurrent architectures (CNN-RNN) was first introduced by @CITE[4]@ on image captioning. Radiology report generation has recently shifted to transformer-based models (@CITE[6]@; @CITE[2]@. Knowledge-driven Encode, Retrieve, and Paraphrase (KERP) (@CITE[5]@ is used for accurate and reliable medical report generation. @CITE[5]@ suggests pretraining the encoder with several chest X-ray images to properly recognize 14 typical radiographic observations. According to proposals, radiology reports can be generated using a memory-driven transformer, while @CITE[7]@ suggests a template-based X-ray report generation approach. @CITE[7]@ clinically defines the templates for each abnormality to indicate its presence or absence. If the generated tags indicate any disease, then the system will choose the corresponding abnormal sentence template. This method cannot generate patient-specific data like anatomical location, size, etc. @CITE[8]@ proposed a template-based multi-attention report generation model (TMRGM) for normal and abnormal studies. This work uses template-based techniques exclusively for normal reports. The differences between our work and previous work are as follows: i) Instead of generating the whole report at once, we generate smaller sentences for only abnormal findings and then replace them in the normal report template. ii) Unlike other state-of-the-art models, our methodology does not put excessive emphasis on normal sentences. iii) If the report’s findings are normal, we use a standard template for normal reports.\\nReferences:ID: 1. Title: On the Automatic Generation of Medical Imaging Reports\\nID: 2. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 3. Title: ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\\nID: 4. Title: Show and Tell: A Neural Image Caption Generator\\nID: 5. Title: Attention Is All You Need\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 8. Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\\nID: 9. Title: Generating Radiology Reports via Memory-driven Transformer<|eot_id|>'},\n",
       " 671: {'predict': 'The conventional neural summarization models have been mainly designed for short documents, and they are usually not effective for summarizing long documents. To address this problem, some works have explored hierarchical structures to encode long documents (; @CITE[11]@; @CITE[5]@; @CITE[4]@; @CITE[7]@; @CITE[8]@; @CITE[6]@. However, these models are mainly designed for extractive summarization, which requires the model to extract relevant sentences from the document. For abstractive summarization, which requires the model to generate the summary from scratch, it is still challenging to design an effective model for long documents. To address this problem, some works have explored the divide-and-conquer approach (@CITE[9]@; @CITE[3]@; @CITE[1]@; @CITE[10]@, which splits the long document into multiple short parts and then generates a summary for each part separately. @CITE[2]@ proposed a novel neural abstractive summarization model for long documents, which utilizes the discourse structure of the document to split the long document into multiple parts and then generates a summary for each part separately. However, these models are mainly designed for scientific papers, and they are not suitable for summarizing general long documents. To address this problem, we propose a novel neural abstractive summarization model for long documents, which utilizes the discourse structure of the document to split the long document into multiple parts and then generates a summary for each part separately.\\nReferences:ID: 1. Title: Big Bird: Transformers for Longer Sequences\\nID: 2. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\\nID: 5. Title: Stepwise Extractive Summarization and Planning with Structured Transformers\\nID: 6. Title: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\\nID: 7. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\\nID: 8. Title: Automatic Summarization of Open-Domain Podcast Episodes\\nID: 9. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 10. Title: A Divide-and-Conquer Approach to the Summarization of Long Documents\\nID: 11. Title: Extractive Summarization of Long Documents by Combining Global and Local Context',\n",
       "  'ground_truth': '\\n\\nEfﬁcient transformer architectures have been applied to summarize long documents such as BigBird (@CITE[10]@, and Longformer-Encoder-Decoder (LED) (@CITE[3]@, which has recently been revised parallel to this work. Hierarchical transformer architectures have been applied to multi-document summarization (@CITE[7]@, and extractive news and table-to-text summarization (@CITE[1]@; @CITE[6]@). Hierarchical attention RNN system has been applied to summarize long articles (@CITE[2]@. Alternatively, earlier methods show that good content selection helps abstractive news summarization systems (@CITE[4]@; ; @CITE[5]@. Hybrid systems that select sentences and generate an abstractive summary have been proposed such as extractive system + TLM for scientific articles (Pilault et al., 2020), simple selection + BART for podcasts (;, and guided summarization by BERT-based keyword/sentence extraction + BART for news and scientific articles (@CITE[10]@; @CITE[8]@). Other work includes dividing the source and target into multiple smaller pairs to train abstractive summarizers (@CITE[11]@. Extractive methods with and without redundancy reduction techniques for long-span summarization have been studied (, 2020).\\nReferences:ID: 1. Title: Big Bird: Transformers for Longer Sequences\\nID: 2. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\\nID: 5. Title: Stepwise Extractive Summarization and Planning with Structured Transformers\\nID: 6. Title: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\\nID: 7. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\\nID: 8. Title: Automatic Summarization of Open-Domain Podcast Episodes\\nID: 9. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 10. Title: A Divide-and-Conquer Approach to the Summarization of Long Documents\\nID: 11. Title: Extractive Summarization of Long Documents by Combining Global and Local Context<|eot_id|>'},\n",
       " 1991: {'predict': 'Factuality in text summarization is the degree to which the generated summary is consistent with the source document (@CITE[3]@. Existing work on factuality can be divided into two categories. The first is to verify the factuality of a generated summary, which can be done by checking whether the summary is consistent with the source document (@CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[2]@. The second is to assess the factuality of a model, which can be done by evaluating the consistency of multiple summaries generated by a model with the source documents. @CITE[2]@ proposed a framework to evaluate faithfulness of a generated summary given its source document. They first collected human annotations of faithfulness for outputs from numerous models on two datasets. Next, they proposed an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. @CITE[2]@ found that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. @CITE[2]@ proposed a framework to evaluate faithfulness of a generated summary given its source document. They first collected human annotations of faithfulness for outputs from numerous models on two datasets. Next, they proposed an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. @CITE[2]@ found that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. @CITE[2]@ found that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful.\\nReferences:ID: 1. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 2. Title: Evaluating Factuality in Generation with Dependency-level Entailment\\nID: 3. Title: Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\\nID: 4. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 5. Title: CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems',\n",
       "  'ground_truth': '\\n\\nRecent studies on factuality of text generation focus on metric design and system optimization. Automated metrics are designed with respect to factuality as different problems: text entailment over sequential (; @CITE[3]@) or tree (@CITE[2]@, 2021a) structures; question answering (@CITE[1]@; ) and sequence labeling (Zhao et al., 2020a). Pagnoni et al. (2021a) constructs human annotated test sets for factuality metrics with a different typology, which is challenging for automatic data augmentation. Some works aim to develop factuality-aware summarization systems by utilizing open information extraction and dependency parsing (Cao et al., 2018b; @CITE[4]@). @CITE[5]@ investigate the influence of domain shift on factuality metrics, suggesting that out-of-domain systems may outperform in-domain systems in terms of factuality. They also note limitations in the predictive power of positive samples from factuality checkers like FactCC.\\nReferences:ID: 1. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 2. Title: Evaluating Factuality in Generation with Dependency-level Entailment\\nID: 3. Title: Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\\nID: 4. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 5. Title: CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems<|eot_id|>'},\n",
       " 3062: {'predict': 'Neural topic models, which are also known as neural variational inference models (, have been widely used to perform topic modeling on text corpora (@CITE[3]@. These models are based on the variational autoencoder (VAE) (Kingma and Welling, 2014) and are able to model the text and topics jointly. These models have been used in various applications such as text generation (@CITE[1]@; @CITE[4]@; ; @CITE[2]@, question answering (, and text classification (@CITE[5]@. The main idea of these models is to learn a probabilistic mapping between the text and topics by maximizing the evidence lower bound (ELBO) of the text. The main advantage of these models is that they can capture the topics of the text in an unsupervised manner and can be used in any application. However, the main disadvantage of these models is that they are not interpretable and the topics they learn are not easily interpretable.\\nReferences:ID: 1. Title: Topic Modelling Meets Deep Neural Networks: A Survey\\nID: 2. Title: Neural Variational Inference for Text Processing\\nID: 3. Title: PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\\nID: 4. Title: Topic-Aware Neural Keyphrase Generation for Social Media Language\\nID: 5. Title: Topic Memory Networks for Short Text Classification',\n",
       "  'ground_truth': '\\n\\nThe success of Variational AutoEncoder (VAE) (@CITE[4]@) has led to the development of a series of neural topic models (NTM) (, 2021b), (Refer to @CITE[2]@ for a comprehensive survey on NTMs), where the posterior distribution of document topics is approximated by a neural network during the variational inference process, known as the neural variational inference (NVI) (). The NVI also enables NTMs to be easily combined with various language models for capturing the global semantics information. These hybrid models are known as the topic-aware language models. Their combination strategies can be largely characterized by either concatenation of (document) topic distributions with the local word embeddings, or (multi-head) attention mechanisms towards the inferred topics. These models have been shown to achieve higher performance than the plain language models on various tasks, such as text summarization (@CITE[5]@), keyphrase generation (@CITE[1]@) and document classification (@CITE[3]@).\\nReferences:ID: 1. Title: Topic Modelling Meets Deep Neural Networks: A Survey\\nID: 2. Title: Neural Variational Inference for Text Processing\\nID: 3. Title: PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\\nID: 4. Title: Topic-Aware Neural Keyphrase Generation for Social Media Language\\nID: 5. Title: Topic Memory Networks for Short Text Classification<|eot_id|>'},\n",
       " 6419: {'predict': 'Existing language models (@CITE[8]@; @CITE[4]@; @CITE[2]@; @CITE[3]@; @CITE[5]@) and knowledge graph embedding models (@CITE[6]@; @CITE[1]@; @CITE[7]@; @CITE[1]@) are two main categories of methods for knowledge graph completion. The language model based methods (@CITE[8]@; @CITE[4]@; @CITE[2]@; @CITE[3]@; @CITE[5]@) treat the knowledge graph as a text corpus and utilize the pre-trained language models to encode the knowledge graph, which is then used to predict the missing links. The knowledge graph embedding models (@CITE[6]@; @CITE[1]@; @CITE[7]@) embed the knowledge graph into a continuous vector space and use the semantic distance between the entities and relations to measure the probability of a triple being true. Existing methods in both categories have achieved promising results, but they are not designed for large-scale knowledge graphs and may suffer from the scalability problem.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: Knowledge Enhanced Contextual Word Representations\\nID: 3. Title: CoLAKE: Contextualized Language and Knowledge Embedding\\nID: 4. Title: KG-BERT: BERT for Knowledge Graph Completion\\nID: 5. Title: Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion\\nID: 6. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 7. Title: Language Models as Knowledge Embeddings\\nID: 8. Title: SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models',\n",
       "  'ground_truth': '\\n\\nJoint pre-trained language models (PLMs) and knowledge graph approaches can be broadly classified into two categories: one that introduces knowledge from KGs to enhance PLMs on NLP downstream tasks (@CITE[1]@; ; @CITE[2]@). However, they freeze the knowledge embedding during training PLMs, which are not real models for learning knowledge representations. To solve the above problem, K-BERT (@CITE[2]@) and CoLAKE () combine entities and relations in the form of tokens with text, which are jointly fed into the PLMs. The other is to learn the knowledge embeddings from natural language texts through PLMs, called text-based methods, which are orthogonal to our work. Recent methods learn to generate entity embeddings with PLMs from entity text descriptions (@CITE[8]@; @CITE[7]@; @CITE[4]@). Specially, KEPLER (@CITE[3]@) encodes textual entity descriptions with jointly optimizing KE and language modeling objectives. LMKE (@CITE[6]@) and SimKGC (@CITE[5]@) introduce efficient contrastive learning to improve the performance of text-based methods. But none of these models consider the temporal aspect of knowledge graphs. ECOLA () uses PLMs to enhance temporal knowledge graph embeddings with temporally relevant texts. Nevertheless, not all facts have summary texts in the practical application. Besides, above approaches focus on the combination of structured knowledge and unstructured text, which is fundamentally different from our method focusing on the contextual information of the TKG subgraph.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: Knowledge Enhanced Contextual Word Representations\\nID: 3. Title: CoLAKE: Contextualized Language and Knowledge Embedding\\nID: 4. Title: KG-BERT: BERT for Knowledge Graph Completion\\nID: 5. Title: Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion\\nID: 6. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 7. Title: Language Models as Knowledge Embeddings\\nID: 8. Title: SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models<|eot_id|>'},\n",
       " 7439: {'predict': 'In recent years, LLMs have been increasingly used as a tool for decision-making in various scenarios. For example, @CITE[1]@; @CITE[1]@; @CITE[4]@; @CITE[5]@; @CITE[2]@ use LLMs to generate action plans for robotic agents. @CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[2]@ use LLMs to generate dialogue plans for conversational agents. @CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@ use LLMs to generate action plans for embodied agents. While these works focus on using LLMs for decision-making, they are limited to a narrow domain and do not explore how LLMs can be used for decision-making in a variety of domains.\\nReferences:ID: 1. Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\\nID: 2. Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\\nID: 3. Title: GPT-4 Technical Report\\nID: 4. Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\\nID: 5. Title: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances',\n",
       "  'ground_truth': '\\n\\nPowerful LLMs are able to act as policy models to make decisions in interactive environments. constructed a general framework for decision-making that uses LLMs to encode observations, goals, and history and then generate actions. Demonstration of example prompts and utilization of high-level function libraries are employed to explore innovative strategies (; ; @CITE[2]@; @CITE[5]@; @CITE[3]@. Prompting structure with pre-defined functions, behaviors, and examples are leveraged to ground LLMs to generate robotic actions (@CITE[4]@; Huang et al., 2022b; @CITE[1]@. However, these methods use manually crafted examples to prompt the LLMs which results in decision-making in a fixed direction. Our LDM2 leverages dynamic state-action tuples as prompt to improve the effectiveness of decisions.\\nReferences:ID: 1. Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\\nID: 2. Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\\nID: 3. Title: GPT-4 Technical Report\\nID: 4. Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\\nID: 5. Title: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances<|eot_id|>'},\n",
       " 1180: {'predict': 'There are several shortcuts that models can exploit in NLP. For example, @CITE[1]@ show that models can exploit excessive invariance in the input distribution, such as ignoring the visual features of images in image classification. @CITE[2]@ show that models can exploit spurious correlations between features, such as using the length of a passage in reading comprehension. @CITE[3]@ show that models can exploit shortcuts in the form of spurious words, such as the word \"not\" in NLI. @CITE[4]@ show that models can exploit shortcuts in the form of annotator bias, such as using the identity of the annotator to predict the correct answer. @CITE[6]@ show that models can exploit shortcuts in the form of spurious statistical cues, such as using the number of words in an argument to predict the correct answer. @CITE[5]@ show that models can exploit shortcuts in the form of spurious cues in the form of a single word, such as the word \"therefore\" in argumentation.\\nReferences:ID: 1. Title: Excessive Invariance Causes Adversarial Vulnerability\\nID: 2. Title: Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes\\nID: 3. Title: How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks\\nID: 4. Title: Hypothesis Only Baselines in Natural Language Inference\\nID: 5. Title: Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments',\n",
       "  'ground_truth': '\\n\\nDeep neural networks, such as Transformers, as well as loss functions (cross-entropy) and optimizers, tend to favor simple functions (@CITE[5]@; @CITE[3]@; ; Valle-Pérez et al., 2018; @CITE[4]@, which can be susceptible to wrong generalizations as the eventual models “hook” onto spurious artifacts in the data. This has been shown to happen in NLP, as illustrated with these examples, among others: \\n• Machine reading comprehension (MRC) models appear not to do much “reading” (@CITE[6]@, as models can perform reasonably well when given only a passage or a passage with a randomly assigned question, instead of a whole input with which this task was conceived. \\n• Large-scale natural language inference (NLI) datasets exhibit linguistic phenomena that correlate well with certain classes. Simple classifier models can perform well by looking only at the hypothesis, instead of the rest of the argument (Gururangan et al., 2018;. \\n• A study (@CITE[1]@ on the annotator bias on natural language understanding datasets has found that annotator bias is evident and models did not generalize well across annotators. Moreover, a model can exploit those biases to inflate its performance if data from the same annotator is present in the training and test sets. Even when using novel pre-trained language models, such as BERT, which have more knowledgeable priors due to their pre-training regime, such phenomena persists. (@CITE[2]@ studied the results from the SemEval 2018 Task 12 – Argument Reasoning Comprehension Task (ARCT) (...\\nReferences:ID: 1. Title: Excessive Invariance Causes Adversarial Vulnerability\\nID: 2. Title: Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes\\nID: 3. Title: How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks\\nID: 4. Title: Hypothesis Only Baselines in Natural Language Inference\\nID: 5. Title: Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments<|eot_id|>'},\n",
       " 5557: {'predict': 'Visual Document Understanding (VDU) aims to extract key information from documents, which is a fundamental task in various domains. Existing VDU methods can be divided into two categories: (1) End-to-End (E2E) methods (; @CITE[3]@; @CITE[4]@; @CITE[5]@, which directly generate the output from document images. (2) Two-Stage (2S) methods (@CITE[6]@; @CITE[1]@; @CITE[2]@; @CITE[6]@, which first recognize the text from document images and then extract key information from the recognized text. Our proposed method is an E2E method, which is more efficient than 2S methods. In addition, the proposed method can generate the output directly from document images, which is more practical than the 2S methods.\\nReferences:ID: 1. Title: LayoutLM: Pre-training of Text and Layout for Document Image Understanding\\nID: 2. Title: Scaling Instruction-Finetuned Language Models\\nID: 3. Title: BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents\\nID: 4. Title: End-to-end Document Recognition and Understanding with Dessurt\\nID: 5. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 6. Title: Unifying Vision, Text, and Layout for Universal Document Processing',\n",
       "  'ground_truth': '\\n\\nVisually-situated NLU blends computer vision and NLU techniques to accurately analyze visual data through language. Early approaches emphasized OCR and leveraged extracted text for contextual analysis (; @CITE[4]@). Some contemporary methods directly processed document images, circumventing the need for external OCR models (@CITE[5]@; @CITE[6]@; @CITE[2]@; ), while others leveraged both image and OCR-extracted text for improved performances (@CITE[3]@; ; @CITE[1]@; Xu et al., 2021; @CITE[4]@). LayoutLMv3 (@CITE[4]@) introduces the word-patch alignment technique, which classifies the relationship between text and corresponding image patches. UDOP () employs a unified encoder that handles both image and text features, transforming the information from both modalities into vision-text embeddings by summing the image patch and text features. This strategy ensures alignment between modalities, such as image patches, tokens, and layout information, by fusing multimodal features within a single encoder. In contrast, Cream extracts fine-grained, aligned multimodal features through contrastive learning (CL), eliminating the need for the fusion encoder. The CL approach prevents over-fusion of information from each modality, allowing the decoder to effectively utilize the multimodal semantics inherent in visually-rich documents. Moreover, due to this architectural design, Cream exhibits enhanced robustness to OCR errors.\\nReferences:ID: 1. Title: LayoutLM: Pre-training of Text and Layout for Document Image Understanding\\nID: 2. Title: Scaling Instruction-Finetuned Language Models\\nID: 3. Title: BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents\\nID: 4. Title: End-to-end Document Recognition and Understanding with Dessurt\\nID: 5. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 6. Title: Unifying Vision, Text, and Layout for Universal Document Processing<|eot_id|>'},\n",
       " 6188: {'predict': 'Adversarial detection is a popular method for detecting adversarial examples. Existing works can be categorized into three types based on the detection granularity: input-level, model-level, and intermediate-level. Input-level methods focus on identifying adversarial examples by analyzing the text input. @CITE[4]@ propose a frequency-guided word substitution algorithm to detect adversarial examples. @CITE[5]@ propose a method to construct input-specific attention subnetworks and extract three features to discriminate between authentic and adversarial inputs. @CITE[1]@ propose a residue-based detector to identify adversarial examples. @CITE[3]@ propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. Model-level methods focus on detecting adversarial examples by analyzing the model predictions. @CITE[2]@ propose a method to detect adversarial examples based on the prediction confidence of a model. @CITE[1]@ propose a method to detect adversarial examples based on the model’s prediction probability of each word. Intermediate-level methods focus on detecting adversarial examples by analyzing the intermediate features of the model. @CITE[2]@ propose a method to detect adversarial examples based on the intermediate features of a model. @CITE[3]@ propose a method to detect adversarial examples based on the intermediate features of a model. However, these methods are not applicable to the PrLM-based dialogue generation task, as they only consider the prediction of a single label.\\nReferences:ID: 1. Title: Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples\\nID: 2. Title: Input-specific Attention Subnetworks for Adversarial Detection\\nID: 3. Title: Residue-Based Natural Language Adversarial Attack Detection\\nID: 4. Title: Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model\\nID: 5. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment',\n",
       "  'ground_truth': '\\n\\nZhou et al. (2019) trained a BERT model to identify the perturbed words. @CITE[5]@ focused on low-frequency words that were likely changed by an attacker. @CITE[2]@ detected the adversarial text via its synonyms. Yoo et al. (2022) and @CITE[3]@ distinguished adversarial text with original text based on density estimation and attention input, respectively. showed that adversarial text induces residues (larger components than original text) in PCA eigenvector. @CITE[4]@ finetuned a transformer model on compliant and adversarial text, which is not required to fool a target model. The change in the requirement efficiently defends against a wide range of adversarial attacks. @CITE[1]@ extracted the word importance as a feature to classify original and adversarial text. Compared with VoteTRANS, previous detectors need adversarial or/and original data to train their models or optimize their models on training data to satisfy some requirements (such as FPR as in Yoo et al. (2022)). These methods are often more suitable for word-based than character-based attacks. On the other hand, VoteTRANS can be applied to various kinds of attacks at both word-level as in PWWS and character-level as in TextBugger as well as other modifications, such as deletion as in Input-Reduction or CLARE and insertion as in BAE. For example, CLARE deletes a word by merging it with a nearby word. When applying the same merging operator on each input word, VoteTRANS will change the attacked words and observe the change in target predictions to detect the attacked text. Voting on the Hard Labels of Transformations (VoteTRANS) Problem statement: We follow notations from TextFooler paper (. Given N texts, X = {X1, X2,..., XN} corresponding to …project to tryout radiocommunication… adv/org … plan to tryout radiocommunication… … task to tryout radiocommunication… compare Target prediction Support prediction Transformation set …project to tryout radio …project to tryout wireless … Business Business Sci/Tech Sci/Tech Business Business Sci/Tech Sci/Tech Sci/Tech Sci/Tech Sci/Tech Sci/Tech Figure 2: Given input text, we replace each word (e.g., “project” and “radiocommunication”) with the corresponding transformation set. The modified text is then predicted by a target and support models as hard labels. These labels are voted and compared with the original label to determine whether the input text is adversarial or original. N labels Y = {Y1, Y2,..., YN}, a target model F : X →Y maps the input space X to the label space Y. Adversarial text Xadv generated from input X ∈X should comply with the following constraints: F(Xadv) ̸= F(X), and Sim(Xadv, X) ≥ϵ, (1) where Sim(Xadv, X) measures the similarity between adversarial text Xadv and original text X; ϵ is the minimum similarity. Our objective is to determine whether input text X is adversarial or original text. The process of VoteTRANS is depicted in Figure 2, and the overall algorithm is summarized in Algorithm 1. Model detail: To process input text X, we use a target model F and an auxiliary attack A (e.g., PWWS). An optional word ratio α can be used to speed up the detection process. Moreover, a support model list F sup is used to improve the performance. The support models and the target model should solve the same task such as sentiment analysis. First, we create a list W including input words that are sorted by their importance using word importance score estimation in the auxiliary attack A (lines 2-5). For example, PWWS calculates the change in the predictions before and after replacing a certain word with an unknown word to estimate word importance. The impact of word importance is shown in Appendix B. Second, we select the top words W ∗(line 6) with the word ratio threshold α (100% by default). Since VoteTRANS maintains the performance with α = 100%, it remarkably improves the run time with a smaller α, with a slight change in performance, as shown in Figures 4a and 4b in the experimental results. Third, we obtain a transformed set W trans for each word wj in W ∗(line 9) using word transformation in A. For example, PWWS uses synonyms from WordNet to form the transformed set. Fourth, we replace each transformed word in W trans for the corresponding wj (line 11) in X and check the second constraint in Equation 1 (line 12). The Sim(·) and ϵ are provided by A. This step is mainly used to preserve the original meaning of the input in the transformed text. For example, PWWS uses stop word checking for this step. Fifth, we construct Y trans containing the label predictions for the valid X′ from target model F and support model list F sup (lines 13-16). We then use the majority vote on Y trans to obtain the top majority classes Y ′ with the highest occurrence in Y trans. Finally, we check whether the input text X is adversarial or original based on the input label Y and majority set Y ′. If Y does not belong to Y ′ or Y belongs to Y ′, which contains more than one majority class, we decide that X is adversarial text (lines 20-22). If we cannot decide if X is adversarial text after checking all words in W ∗, X is considered original text. Evaluation Comparison We follow recent work (@CITE[1]@ to conduct the same experiments including attacks, datasets, models, parameter settings, evaluation metrics, number of training/testing samples, etc. In particular, evaluation was performed with adversarial text generated by four attacks2 including PWWS, TextFooler, IGA, and BAE. These attacks targeted four common models3 (LSTM, CNN, DistilBERT, and BERT) on IMDB (235.72 words/text), Yelp 2All attacks ran with the default settings from the TextAttack framework. 3All pretrained models were reused from the TextAttack framework.\\nReferences:ID: 1. Title: Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples\\nID: 2. Title: Input-specific Attention Subnetworks for Adversarial Detection\\nID: 3. Title: Residue-Based Natural Language Adversarial Attack Detection\\nID: 4. Title: Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model\\nID: 5. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment<|eot_id|>'},\n",
       " 700: {'predict': 'Context-aware textual MT is a challenging task, which has attracted much attention in recent years. A few approaches have been proposed to incorporate context information from the source side (; @CITE[7]@; @CITE[5]@; @CITE[4]@; @CITE[6]@; @CITE[8]@; @CITE[9]@. Some works have also explored to incorporate target-side context information (Zhao et al., 2018; @CITE[1]@; @CITE[3]@; @CITE[2]@. However, most of these studies focus on document-level MT, where the context is typically represented as a sequence of sentences. For ST, the context is typically represented as a sequence of words, which is more challenging to model. To the best of our knowledge, this work is the first attempt to explore context-aware ST, which we call context-aware ST. In this paper, we propose a novel context-aware ST framework, which is based on a hierarchical attention network (HAN) and incorporates context information from both source and target sides.\\nReferences:ID: 1. Title: Neural Machine Translation with Extended Context\\nID: 2. Title: Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches\\nID: 3. Title: Improving the Transformer Translation Model with Document-Level Context\\nID: 4. Title: Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning\\nID: 5. Title: Modeling Discourse Structure for Document-level Neural Machine Translation\\nID: 6. Title: Re-translation versus Streaming for Simultaneous Translation\\nID: 7. Title: Curriculum Pre-training for End-to-End Speech Translation\\nID: 8. Title: Evaluating Discourse Phenomena in Neural Machine Translation\\nID: 9. Title: When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion',\n",
       "  'ground_truth': '\\n\\nOur work is inspired by pioneer studies on context-aware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (@CITE[8]@; ). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and @CITE[7]@; @CITE[8]@; Miculicich et al., 2018;, 2020c; @CITE[5]@; @CITE[8]@; ; @CITE[9]@) and/or improved analysis and evaluation targeted at specific discourse phenomena (; Läubli et al., 2018; @CITE[7]@; ; ; @CITE[6]@).\\nReferences:ID: 1. Title: Neural Machine Translation with Extended Context\\nID: 2. Title: Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches\\nID: 3. Title: Improving the Transformer Translation Model with Document-Level Context\\nID: 4. Title: Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning\\nID: 5. Title: Modeling Discourse Structure for Document-level Neural Machine Translation\\nID: 6. Title: Re-translation versus Streaming for Simultaneous Translation\\nID: 7. Title: Curriculum Pre-training for End-to-End Speech Translation\\nID: 8. Title: Evaluating Discourse Phenomena in Neural Machine Translation\\nID: 9. Title: When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion<|eot_id|>'},\n",
       " 2138: {'predict': 'Dialogue disentanglement is a task that aims to separate intermingled messages into detached sessions. Existing work on this task can be roughly divided into two categories. The first category is to utilize the hand-crafted features to train a classifier to identify the session of a given message. For example, @CITE[6]@ proposed a pointer network to capture the relations between messages and a joint-learning objective to better capture the contextual information. @CITE[5]@ proposed a dependency parser to extract the dependency relations between messages, and then used a graph neural network to learn the representation of a message. @CITE[7]@ proposed a hierarchical pre-trained model to capture the matching information between utterances and the context information. @CITE[3]@ proposed a co-training algorithm to train a model without referencing any human annotations. The second category is to use the pre-trained language models to learn the representation of a message and directly predict the session of the message. For example, @CITE[4]@ proposed to use the pre-trained BERT to capture the matching information between utterances. @CITE[1]@ proposed to use the pre-trained ELECTRA to capture the matching information between utterances. @CITE[2]@ proposed to use the pre-trained BERT to capture the matching information between utterances and the context information. However, these methods only focus on the disentanglement of single session, and ignore the information of the other sessions.\\nReferences:ID: 1. Title: Online Conversation Disentanglement with Pointer Networks\\nID: 2. Title: Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 5. Title: Multi-turn Response Selection using Dialogue Dependency Relations\\nID: 6. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 7. Title: Unsupervised Conversation Disentanglement through Co-Training',\n",
       "  'ground_truth': '\\n\\nDialogue disentanglement (@CITE[7]@, which is also referred to as conversation management (Traum et al., 2004), thread detection (@CITE[1]@ or thread extraction (@CITE[5]@, has been studied for decades, since understanding long multi-party dialogues remains to be non-trivial. Thus, dialogue disentanglement methods have been proposed to cluster utterances. Early works can be summarized as feature encoder and clustering algorithms. Well-designed handcraft features are constructed as input of simple networks that predict whether a pair of utterances are alike or different, and clustering methods are then borrowed for partitioning (@CITE[7]@; @CITE[6]@. Researches are facilitated by a large-scale, high-quality public dataset, Ubuntu IRC, created by Kummerfeld et al. (2019). And then the application of FeedForward network and pointer network (Vinyals et al., 2015) leads to significant progress, but the improvement still partially relies on handcraft-related features (; @CITE[3]@. Then the end-to-end strategy is proposed and fills the gap between the match and clustering (@CITE[6]@, where dialogue disentanglement is modeled as a dialogue state transition process. The utterances are clustered by mapping with the states of each dialogue thread. Inspired by achievements of pre-trained language models (; ; an, 2019), latest work use BERT to contextually encode the dialogue context (; @CITE[2]@. investigates disentanglement from a different perspective. Their end-to-end co-training approach provides a novel unsupervised baseline. However, attention paid to the characteristics of dialogues seems to be inadequate. Feature engineering-based works represent properties of individual utterances such as time, speakers, and topics with naive handcraft methods, thus ignoring dialogue contexts (@CITE[7]@;. PrLM-based Masked Hierarchical Transformer ( utilizes the golden conversation structures to operate attentions on related utterances when training models, which results in exposure bias. DialBERT (@CITE[2]@, a recent architecture including a BERT ( and an LSTM (, models contextual clues but no dialogue-specific features, and claims a state-of-the-art performance. Our approach draws inspiration from these works and further models structural features for better dialogue understanding. Unlike the above studies, our work incorporates dialogue-specific characters. We propose a new model considering structural characteristics of dialogues, based on the fact that dialogues are developed according to the behavior of speakers. In detail, we model dialogue structures with two highlights: 1) speaker properties of each utterance and 2) reference of speakers between utterances, which both help with modeling inherent interactions among a dialogue passage.\\nReferences:ID: 1. Title: Online Conversation Disentanglement with Pointer Networks\\nID: 2. Title: Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 5. Title: Multi-turn Response Selection using Dialogue Dependency Relations\\nID: 6. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 7. Title: Unsupervised Conversation Disentanglement through Co-Training<|eot_id|>'},\n",
       " 1954: {'predict': 'Reinforcement learning has been widely used in NLP tasks, such as machine translation (@CITE[4]@; @CITE[3]@, text summarization (@CITE[1]@;, dialogue generation (, and text classification (@CITE[2]@. In this work, we focus on the task of text summarization. Previous work has applied reinforcement learning to summarization by using a policy network to select important sentences from the source document, and then a decoder to generate a summary from the selected sentences (@CITE[1]@;. Our work differs in that we use a policy network to select important sentences, and then a generator to generate the final summary from the selected sentences. We also propose a novel reward function that takes into account both the quality of the generated summary and the importance of the selected sentences.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: An Actor-Critic Algorithm for Sequence Prediction\\nID: 3. Title: Deep Reinforcement Learning for Dialogue Generation\\nID: 4. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting',\n",
       "  'ground_truth': '\\n\\nReinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (@CITE[4]@; Henß et al., 2015; @CITE[1]@; Paulus et al., 2017; Fedus et al., 2018). In addition, RL has been explored for a range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (@CITE[3]@, text summarization (@CITE[2]@, knowledge-graph reasoning (@CITE[2]@ and relation extraction (Qin et al., 2018). To the best of our knowledge, we are the first to explore RL techniques for exposing underlying content organization structures in news articles as well as using a linguistically motivated critic to reduce the variance of the reinforcement algorithm.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: An Actor-Critic Algorithm for Sequence Prediction\\nID: 3. Title: Deep Reinforcement Learning for Dialogue Generation\\nID: 4. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting<|eot_id|>'},\n",
       " 7365: {'predict': 'The majority of embedding-based entity alignment methods are based on TransE (Bordes et al., 2013), which encodes entities and relations as vectors in a unified space. To align entities from different KGs, these methods utilize cross-lingual transitions to transform the embedding space of each KG into the same space as the others. Existing methods can be categorized into three types: (1) translation-based methods (@CITE[3]@; @CITE[6]@; @CITE[3]@; @CITE[7]@; ; @CITE[8]@), which learn translation vectors to transform the embedding space of each KG; (2) linear transformation-based methods (@CITE[6]@; ; @CITE[4]@), which learn linear transformations to transform the embedding space of each KG; and (3) graph neural network-based methods (@CITE[1]@; @CITE[5]@; @CITE[2]@; @CITE[3]@), which use graph neural networks to learn the structural information of KGs. However, these methods usually rely on the assumption that the KGs are aligned to some extent, which is not always true. Besides, they usually learn the alignment score of each entity pair independently, which ignores the interaction among entity pairs. To address these limitations, we propose to learn the alignment score of each entity pair based on the global interaction among entity pairs, which can be achieved by the graph attention network. We also utilize the cross-lingual transitions to transform the embedding space of each KG into the same space as the others.\\nReferences:ID: 1. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 2. Title: Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding\\nID: 3. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 4. Title: Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation\\nID: 5. Title: Knowledge Embedding Based Graph Convolutional Network\\nID: 6. Title: Informed Multi-context Entity Alignment\\nID: 7. Title: Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment\\nID: 8. Title: Aligning Cross-Lingual Entities with Multi-Aspect Information',\n",
       "  'ground_truth': '\\n\\nWhile the recent years have witnessed the rapid development of deep learning techniques, embedding-based EA approaches obtain promising results. Among them, some early studies (@CITE[1]@; @CITE[4]@ are based on the knowledge embedding methods, in which entities are embedded by exploring the fine-grained relational semantics. For example, MTransE (@CITE[1]@ applies TransE ( as the KG encoder to embed different KGs into independent vector spaces and then conducts transitions via designed alignment modules. However, they need to carefully balance the weight between the encoder and alignment module in one unified optimization problem. Due to the powerful structure learning capability, Graph Neural Networks (GNNs) like GCN (@CITE[8]@ and GAT (Veliˇ ckovi´ et al., 2018) have been employed as the encoder with Siamese architecture (i.e., shared-parameter) for many embedding-based models. GCN-Align ( applies Graph Convolution Network (GCN) for the first time to capture neighborhood information and embed entities into a unified vector space, but it suffers from the structural heterogeneity of different KGs. To mitigate this issue and improve the structure learning, AliNet (@CITE[5]@ adopts multi-hop aggregation with a gating mechanism to expand neighborhood ranges for better structure modeling, and KE-GCN (@CITE[2]@ combines GCN and knowledge embedding methods to jointly capture the rich structural features and relation semantics of entities. More recently, IMEA ( designs a Transformer-like architecture to encode multiple structural contexts in a KG while capturing alignment interactions across different KGs. In addition, some works further improve the EA performance by introducing the side information about entities, such as entity names (, attributes (@CITE[7]@, and literal descriptions (. Afterward, a series of methods were proposed to integrate knowledge from different modalities (e.g., relational, visual, and numerical) to obtain joint entity representation for EA (@CITE[6]@; Liu et al., 2021; @CITE[3]@. However, these discriminative features are usually hard to collect, noise polluted, and privacy sensitive (Pei et al., 2022).\\nReferences:ID: 1. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 2. Title: Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding\\nID: 3. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 4. Title: Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation\\nID: 5. Title: Knowledge Embedding Based Graph Convolutional Network\\nID: 6. Title: Informed Multi-context Entity Alignment\\nID: 7. Title: Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment\\nID: 8. Title: Aligning Cross-Lingual Entities with Multi-Aspect Information<|eot_id|>'},\n",
       " 67: {'predict': 'In the last few years, controllable text generation has gained popularity in the field of natural language processing. This line of work aims to generate texts with a specific theme, style, sentiment, or topic, while controlling the content of the generated text. @CITE[5]@ and @CITE[3]@ introduced a theme rewriting approach to generate math word problems with a new theme, while keeping the original math concepts. @CITE[4]@ proposed a conditional language model to generate sentences with a specific style, content, and task-specific behavior. @CITE[6]@ proposed a Plug and Play Language Model to control the text generation by adding attribute classifiers on top of a pretrained language model. @CITE[7]@ proposed a conditional transformer language model to generate texts with a specific style, content, and task-specific behavior. @CITE[8]@ proposed a text generation framework to control the generation of texts in different styles. @CITE[1]@ proposed a controlled language generation model based on BERT. Our work is closely related to the above work, but we propose a novel approach to control the generation of texts by using a set of control codes, and demonstrate its effectiveness on generating a large number of texts in different styles. @CITE[2]@ proposed a large-scale pretrained language model, BERT, which is pretrained on a large corpus of text, and can be fine-tuned on a specific downstream task. We use BERT as our base model to generate texts, and propose a novel approach to control the generation of texts.\\nReferences:ID: 1. Title: A Theme-Rewriting Approach for Generating Algebra Word Problems\\nID: 2. Title: Toward Controlled Generation of Text\\nID: 3. Title: Controlling Linguistic Style Aspects in Neural Language Generation\\nID: 4. Title: Style Transfer from Non-Parallel Text by Cross-Alignment\\nID: 5. Title: Defending Against Neural Fake News\\nID: 6. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'ground_truth': '\\n\\nEarlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as theme, sentiment, or style (; @CITE[4]@; @CITE[8]@; @CITE[5]@; ). Recent works such as Grover (@CITE[6]@) and CTRL model () used these ideas to augment transformer language models that can condition on structured metadata such as source, domain, etc. The Plug & Play model (PPLM; @CITE[1]@) controls topic and sentiment in an approach similar to ours that involves forward and backward passes to update token distributions. However, PPLM relies on trained attribute discriminators for supervision, while our method is unsupervised. While these models are restricted to specific dimensions, often with predefined values, our model can adjust to any open-ended textual constraint. Perhaps the most similar work in that aspect is the “text infilling” models, which, however, are in a more narrow setting by filling only a relatively short text span (@CITE[2]@; Zhu et al., 2019; ), and more restrictive due to the reliance on an extra right-to-left language model (@CITE[3]@) or a pre-specified generation length (@CITE[7]@, which is not publicly available).\\nReferences:ID: 1. Title: A Theme-Rewriting Approach for Generating Algebra Word Problems\\nID: 2. Title: Toward Controlled Generation of Text\\nID: 3. Title: Controlling Linguistic Style Aspects in Neural Language Generation\\nID: 4. Title: Style Transfer from Non-Parallel Text by Cross-Alignment\\nID: 5. Title: Defending Against Neural Fake News\\nID: 6. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<|eot_id|>'},\n",
       " 4881: {'predict': 'Knowledge injection aims to enrich the knowledge representation of PLMs by incorporating external knowledge sources, such as knowledge graphs, dictionaries, or encyclopedias. ERNIE (@CITE[1]@) injects knowledge into BERT by masking entities and phrases in the text, and pre-training the model with a knowledge masking objective. K-Adapter (@CITE[2]@) introduces adapters into PLMs to inject different types of knowledge. KEPLER (@CITE[3]@) combines knowledge embedding and pre-trained language representation by encoding textual entity descriptions as embeddings. KI-BERT (@CITE[4]@) enhances BERT with knowledge context from ConceptNet and WordNet by introducing new token types for entities and aligning entity position ids. K-BERT (@CITE[5]@) injects knowledge into BERT by learning entity representations from knowledge graphs. KnowBERT (@CITE[6]@) updates contextual word representations via word-to-entity attention. SenseBERT (@CITE[7]@) pre-trains BERT to predict not only the masked words but also their WordNet supersenses. However, these methods are all based on the pre-training paradigm and are not suitable for PLMs with fixed parameters. Our proposed method can be used to inject knowledge into PLMs with fixed parameters, which is more practical. Other knowledge injection methods include knowledge-aware graph networks (@CITE[8]@), knowledge-enhanced attention (@CITE[9]@), and knowledge-enhanced graph attention (@CITE[10]@). These methods are mostly designed for knowledge-aware graph networks and are not suitable for PLMs with fixed parameters. In contrast, our method can be used to inject knowledge into PLMs with fixed parameters. @CITE[11]@ shows that a transformer can be viewed as a key-value memory network. We show that the knowledge injection method can be viewed as a key-value memory network. @CITE[12]@ shows that the feed-forward layer can be viewed as a key-value memory network. We show that the knowledge injection method can be viewed as a key-value memory network. @CITE[13]@ shows that the knowledge injection method can be viewed as a key-value memory network.\\nReferences:ID: 1. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 2. Title: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model\\nID: 3. Title: ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations\\nID: 4. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 5. Title: SenseBERT: Driving Some Sense into BERT\\nID: 6. Title: Knowledge Enhanced Contextual Word Representations\\nID: 7. Title: KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding\\nID: 8. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 9. Title: KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning\\nID: 10. Title: Editable Neural Networks\\nID: 11. Title: Transformer Feed-Forward Layers Are Key-Value Memories',\n",
       "  'ground_truth': '\\n\\nKnowledge can be injected into PLMs by pre-training or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs (@CITE[1]@; @CITE[5]@), entities (@CITE[3]@; @CITE[10]@), n-grams (), knowledge embedding (@CITE[6]@), synonym and hyponym-hypernym relations in WordNet (@CITE[3]@), word-supersense knowledge (), and knowledge bases (@CITE[2]@ can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs (@CITE[5]@) and knowledge context (@CITE[8]@), while other studies explored specific model and training designs, like knowledge adapter networks (@CITE[9]@), graph convolutional networks and LSTMs (@CITE[11]@), and meta-learning (. Zhu et al. (2020) formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies (; @CITE[4]@; @CITE[7]@) reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.\\nReferences:ID: 1. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 2. Title: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model\\nID: 3. Title: ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations\\nID: 4. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 5. Title: SenseBERT: Driving Some Sense into BERT\\nID: 6. Title: Knowledge Enhanced Contextual Word Representations\\nID: 7. Title: KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding\\nID: 8. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 9. Title: KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning\\nID: 10. Title: Editable Neural Networks\\nID: 11. Title: Transformer Feed-Forward Layers Are Key-Value Memories<|eot_id|>'},\n",
       " 464: {'predict': 'Neural ranking models have been widely studied and applied in IR. These models can be divided into two categories: shallow neural models and deep neural models. Shallow neural models are based on feed-forward neural networks (FFNNs) and can be trained from scratch. They are widely used for ad-hoc retrieval (@CITE[4]@; @CITE[7]@; @CITE[3]@; @CITE[6]@; ; @CITE[2]@; @CITE[5]@; @CITE[9]@). However, they are not suitable for long-tail retrieval because they require large-scale training data. Deep neural models are pre-trained on large-scale corpora and can be fine-tuned on a small-scale dataset. They are widely used for long-tail retrieval (@CITE[1]@; @CITE[8]@; @CITE[1]@; @CITE[9]@). The pre-trained models are usually based on transformer architecture (@CITE[8]@; @CITE[1]@; @CITE[1]@).\\nReferences:ID: 1. Title: A Deep Look into Neural Ranking Models for Information Retrieval\\nID: 2. Title: A Deep Relevance Matching Model for Ad-hoc Retrieval\\nID: 3. Title: End-to-End Neural Ad-hoc Ranking with Kernel Pooling\\nID: 4. Title: PACRR: A Position-Aware Neural IR Model for Relevance Matching\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Passage Re-ranking with BERT\\nID: 7. Title: Passage Re-ranking with BERT\\nID: 8. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 9. Title: Overview of the TREC 2019 deep learning track',\n",
       "  'ground_truth': '\\n\\nNeural models have demonstrated significant advances across various ranking tasks (). Early approaches investigated diverse ways to capture relevance between queries and documents (@CITE[5]@; @CITE[4]@; @CITE[3]@; ). And recently the state-of-the-art in many text ranking tasks has been taken by BERT or other pretrained language models (@CITE[7]@; @CITE[9]@; @CITE[9]@; @CITE[8]@; @CITE[1]@; @CITE[6]@, when sufficient relevance labels are available for fine-tuning (e.g., on MS MARCO (@CITE[2]@).\\nReferences:ID: 1. Title: A Deep Look into Neural Ranking Models for Information Retrieval\\nID: 2. Title: A Deep Relevance Matching Model for Ad-hoc Retrieval\\nID: 3. Title: End-to-End Neural Ad-hoc Ranking with Kernel Pooling\\nID: 4. Title: PACRR: A Position-Aware Neural IR Model for Relevance Matching\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Passage Re-ranking with BERT\\nID: 7. Title: Passage Re-ranking with BERT\\nID: 8. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 9. Title: Overview of the TREC 2019 deep learning track<|eot_id|>'},\n",
       " 7009: {'predict': 'LLMs Benchmarks. The recent development of LLMs has led to a surge of interest in the evaluation of their knowledge and reasoning abilities. Several benchmarks have been proposed to evaluate the knowledge and reasoning abilities of LLMs. One of the most widely used benchmarks is GLUE (Wu et al., 2019), which is a collection of NLP tasks. The GLUE benchmark has been used to evaluate the performance of LLMs such as GPT-3 (@CITE[2]@, GPT-4 (@CITE[3]@ and ChatGPT (@CITE[1]@. Another widely used benchmark is SuperGLUE (@CITE[2]@, which consists of 5 challenging tasks, including two text-to-text tasks and three visual reasoning tasks. The GLUE and SuperGLUE benchmarks are not suitable for evaluating the knowledge and reasoning abilities of LLMs in Chinese. Therefore, several Chinese benchmarks have been proposed to evaluate the knowledge and reasoning abilities of LLMs. For example, C-Eval () is the first comprehensive Chinese evaluation suite for LLMs, which consists of multiple-choice questions across four difficulty levels. The C-Eval benchmark has been used to evaluate the performance of GPT-4 (@CITE[3]@ and ChatGPT (@CITE[1]@. Another Chinese benchmark is ECO-QA (), which is a large-scale question answering dataset for evaluating the knowledge and reasoning abilities of LLMs. The ECO-QA dataset has been used to evaluate the performance of ChatGPT ().\\nReferences:ID: 1. Title: Measuring Massive Multitask Language Understanding\\nID: 2. Title: C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\\nID: 3. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4',\n",
       "  'ground_truth': '\\n\\nNew NLP benchmarks are urgently needed to align with the rapid development of LLMs. MMLU(@CITE[2]@ is a collection of English-language materials that encompasses knowledge from 57 different disciplines including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem-solving ability. Another significant contribution to this field is the C-EVAL(@CITE[3]@, which represents the first comprehensive effort to evaluate foundational models’ knowledge and reasoning capabilities within a Chinese context. C-EVAL consists of multiple-choice questions designed to assess performance across four difficulty levels: middle school, high school, college, and professional. These questions cover 52 diverse disciplines, spanning from humanities to science and engineering, thereby providing a holistic evaluation of the model’s capabilities. @CITE[1]@ introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating LLMs. Most benchmarks focus on evaluating the performance of LLMs in answering or answering questions, but few focus on the ability of LLMs to explain the answers given.\\nReferences:ID: 1. Title: Measuring Massive Multitask Language Understanding\\nID: 2. Title: C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\\nID: 3. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4<|eot_id|>'},\n",
       " 4070: {'predict': \"Mixed-type medical dialogue datasets have been proposed in recent years. Some of them are collected from medical platforms, such as MedDG (@CITE[6]@, MedDG (@CITE[6]@, and IMCS (@CITE[6]@. Some of them are collected from medical books, such as Medical Dialogue (@CITE[7]@ and MedDialogue (@CITE[7]@. Some of them are collected from medical forums, such as DodecaDialogue (@CITE[3]@ and BlendedSkillTalk (@CITE[4]@. Some of them are collected from medical chatbots, such as KR-DS (@CITE[1]@ and MedDialogue (@CITE[7]@. Some of them are collected from medical consultations, such as Medical Dialogue (@CITE[5]@. Some of them are collected from medical question-answering systems, such as Medical Dialogue (@CITE[2]@. These datasets can be classified into two categories: one is medical dialogue with mixed types of information, such as symptoms, diseases, tests, and medicines; the other is medical dialogue with mixed types of conversation, such as diagnosis, treatment, and consultation.\\nReferences:ID: 1. Title: End-to-End Knowledge-Routed Relational Dialogue System for Automatic Diagnosis\\nID: 2. Title: MedDG: An Entity-Centric Medical Consultation Dataset for Entity-Aware Medical Dialogue Generation\\nID: 3. Title: A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets\\nID: 4. Title: The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents\\nID: 5. Title: Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills\\nID: 6. Title: Semi-Supervised Variational Reasoning for Medical Dialogue Generation\\nID: 7. Title: Semi-Supervised Variational Reasoning for Medical Dialogue Generation\",\n",
       "  'ground_truth': \"\\n\\nTask-oriented dialogue for diagnosis DX (@CITE[3]@), Task-oriented dialogue for diagnosis CMDD (@CITE[2]@), Task-oriented dialogue for diagnosis MedDG (@CITE[7]@), Task-oriented dialogue for diagnosis MedDialog (), Task-oriented dialogue for diagnosis DialoAMC (), Task-oriented dialogue for diagnosis DuRecDial (@CITE[1]@), Rec., chitchat, QA, task-oriented dialogue DodecaDialogue(), Know., chitchat, QA, empathetic dialogue, image chat BlendedSkillTalk(@CITE[5]@), Know., empathetic dialogue, chitchat ACCENTOR(@CITE[6]@), Chitchat, task-oriented dialogue DuRecDial 2.0 (@CITE[6]@), Rec., chitchat, QA, task-oriented dialogue SalesBot(), Chitchat, task-oriented dialogue DuClarifyDial(@CITE[4]@), Rec., know. chitchat, QA, task-oriented dialogue MidMed (Ours) Rec., chitchat, know., QA, diagnosis-oriented dialogue\\nReferences:ID: 1. Title: End-to-End Knowledge-Routed Relational Dialogue System for Automatic Diagnosis\\nID: 2. Title: MedDG: An Entity-Centric Medical Consultation Dataset for Entity-Aware Medical Dialogue Generation\\nID: 3. Title: A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets\\nID: 4. Title: The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents\\nID: 5. Title: Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills\\nID: 6. Title: Semi-Supervised Variational Reasoning for Medical Dialogue Generation\\nID: 7. Title: Semi-Supervised Variational Reasoning for Medical Dialogue Generation<|eot_id|>\"},\n",
       " 5127: {'predict': 'Dense retrieval has been widely used in various NLP tasks, such as open-domain question answering (@CITE[4]@; @CITE[1]@; @CITE[6]@, text classification (@CITE[5]@;, and information retrieval (Zhu et al., 2022). Recently, many efforts have been made to improve dense retrieval. @CITE[2]@ propose to use cross-batch negatives to train the retriever. @CITE[3]@ propose to use denoised hard negatives to train the retriever. @CITE[2]@ propose to use data augmentation to train the retriever. @CITE[3]@ propose to use metric-guided distillation to distill knowledge from the metric to the ranker and retriever. @CITE[2]@ propose to use contrastive learning to pre-train the retriever. @CITE[1]@ propose to use contrastive learning to pre-train the retriever and ranker. In this paper, we propose to use contrastive learning to pre-train the retriever and ranker, and use the fine-tuned ranker to filter out the irrelevant passages. Our method does not require any additional data augmentation, denoised hard negatives, cross-batch negatives, or metric-guided distillation.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 3. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 5. Title: Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning\\nID: 6. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval',\n",
       "  'ground_truth': '\\n\\nIn recent years, with the development of large- scale pre-trained language models, such as BERT (@CITE[1]@ and RoBERTa (@CITE[3]@, we witness the research interest in infor- mation retrieval shifting from the traditional sparse retrieval to neural dense retrieval. Karpukhin et al. (2020); proposed dense pas- sage retriever (DPR), which uses two neural en- coders initialized with BERT to model the query and document, independently. The subsequent works follow this dual-encoder framework. One line of work improves dense retrieval by mining hard negatives, where they select the top-ranked documents retrieved by the recent retriever as hard negatives and then re-train the retriever with the newly mined hard negatives (; @CITE[6]@. However, the mined hard nega- tives are highly likely to contain false negatives, harming the performance. To mitigate this, follow- ing studies denoise the mined hard negatives with re-rankers (@CITE[6]@; @CITE[6]@; @CITE[2]@. Another kind of work focuses on pre-training to make the pre-trained models more suitable for dense retrieval, such as Condenser (@CITE[4]@, coCondenser ( and SIMLM (. Dense retrieval models typically depend on ex- tensive supervised data, comprising pairs of queries and positive documents. To address the challenge of limited training data, @CITE[5]@; Sun et al. (2021) proposed to train a query generator on high- resource information retrieval data, and then used the query generator to generate synthetic queries for low-source target domains. Additionally, harnessed large language models in zero/few settings to produce synthetic queries for documents in target domains, eliminating the need for training a general query generator. In contrast to these approaches, our work leverages synthetic queries for document expansion.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 3. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 5. Title: Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning\\nID: 6. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval<|eot_id|>'},\n",
       " 2482: {'predict': 'Vision-and-language foiling is a long-standing challenge in vision-and-language research. Recent work has shown that models trained on VQA datasets often fail to generalize to out-of-distribution test sets (@CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[1]@; @CITE[1]@; @CITE[2]@; @CITE[2]@. To better understand and improve the generalization of VQA models, several recent works have proposed various evaluation protocols and metrics. For example, @CITE[4]@ propose a contrast set generation method to evaluate models’ performance on different aspects of VQA. @CITE[5]@ propose to use a probing task to evaluate models’ ability to count objects in an image. @CITE[1]@ propose a robustness measure, Robustness to Augmented Data (RAD), to evaluate models’ ability to generalize to unseen augmentations. @CITE[2]@ propose to use a set of evaluation protocols to evaluate models’ generalization to out-of-distribution test sets. Our work is inspired by these efforts, but we focus on the task of question answering. We propose a set of evaluation protocols and metrics to evaluate models’ ability to generalize to unseen questions, and we demonstrate that our evaluation protocols can help to identify models that are better at generalizing to unseen questions.\\nReferences:ID: 1. Title: MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering\\nID: 2. Title: Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA\\nID: 3. Title: Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks\\nID: 4. Title: Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions\\nID: 5. Title: Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions',\n",
       "  'ground_truth': '\\n\\nA trend within the latter research line that is speciﬁc to V&L mod- els is vision-and-language foiling (@CITE[1]@; @CITE[2]@; ; ; @CITE[4]@, where the idea is to create counterfactual (i.e., foiled) and/or minimally edited examples by per- forming data augmentation on captions (@CITE[1]@,a) or images (@CITE[4]@. Since most V&L models are pretrained on some version of the image-text alignment task, it is pos- sible to test their ability to distinguish correct from foiled captions (in relation to an image) in a zero- shot setting. The construction of foils can serve many investigation purposes. With VALSE, we target the linguistic grounding capabilities of V&L models, focusing on pervasive linguistic phenom- ena that span multiple tokens, described in §3.1– §3.6. At the same time, we ensure that our data is robust to perturbations and artefacts by i) con- trolling for word frequency biases between cap- tions and foils, and ii) testing against unimodal collapse, a known issue of V&L models (@CITE[3]@; @CITE[5]@, thereby pre- venting models from solving the task using a single input modality.\\nReferences:ID: 1. Title: MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering\\nID: 2. Title: Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA\\nID: 3. Title: Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks\\nID: 4. Title: Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions\\nID: 5. Title: Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions<|eot_id|>'},\n",
       " 2937: {'predict': 'To make the Transformer architecture suitable for graph-structured data, many studies have been conducted to design structure-aware Transformer models. Existing studies can be categorized into two groups: (1) Graph Transformer (GT) and (2) Graph Neural Network (GNN) Transformer. (1) Graph Transformer (GT): GT is a Transformer-based architecture that is designed for graph-structured data, which has been widely used in computer vision (@CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[2]@. GT models the graph as a set of nodes and their corresponding sequences, and uses the Transformer architecture to encode the graph information. The GT architecture is designed for graph classification tasks, which is different from our code summarization task. (2) GNN Transformer: GNN is a graph neural network that is designed for graph-structured data, which is widely used in graph classification (@CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[5]@ and code summarization (@CITE[4]@ tasks. GNN Transformer is a Transformer-based architecture that incorporates GNN into the Transformer architecture, which is designed to encode the graph information. GNN Transformer is widely used in code summarization tasks, which is similar to our code summarization task.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Code Structure Guided Transformer for Source Code Summarization\\nID: 3. Title: Code Summarization with Structure-induced Transformer\\nID: 4. Title: On the Bottleneck of Graph Neural Networks and its Practical Implications\\nID: 5. Title: Rethinking Graph Transformers with Spectral Attention',\n",
       "  'ground_truth': '\\n\\nTransformer model (@CITE[1]@ is the most widely used architecture in language representation learning. Several works have recently explored extending the Transformer from plain text to structural data such as graphs and trees. Technically, two approaches exist to integrate inductive bias in Transformer: the hard or soft methods. The hard-coded methods usually use the mask to restrict the attention only to adjacent nodes in graphs or trees (; @CITE[2]@), that is, the GNN-like message-passing paradigm exists therein. However, there is a growing recognition that inherent limitations exist in message passing, such as over-smoothing and over-squashing (@CITE[5]@; @CITE[3]@; @CITE[4]@. More recently, a growing interest in deep learning is to encode a structure as soft inductive bias toward more flexible architectures, such as positional encodings in Transformer.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Code Structure Guided Transformer for Source Code Summarization\\nID: 3. Title: Code Summarization with Structure-induced Transformer\\nID: 4. Title: On the Bottleneck of Graph Neural Networks and its Practical Implications\\nID: 5. Title: Rethinking Graph Transformers with Spectral Attention<|eot_id|>'},\n",
       " 5801: {'predict': 'Prompt learning is a new paradigm for natural language processing (NLP) (@CITE[5]@; @CITE[1]@; @CITE[4]@; @CITE[2]@. It uses a template (or prompt) to guide a pre-trained language model (PLM) to generate the desired output. For example, in the masked language model (MLM) pre-training task, the prompt is a text with a masked token, and the output is the corresponding text with the masked token filled in (@CITE[8]@. The prompt can be discrete (e.g., a natural language template) or continuous (e.g., a soft prompt) (@CITE[6]@. The prompt can also be learned during fine-tuning (@CITE[7]@; @CITE[3]@; @CITE[1]@; @CITE[3]@. In this work, we use the discrete prompt and learn it during fine-tuning.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation\\nID: 4. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 5. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 6. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 7. Title: ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\\nID: 8. Title: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer',\n",
       "  'ground_truth': '\\n\\nPrompt learning is a thriving method in the field of NLP. Prompt learning converts fine-tuning text into a format similar to pre-training to leverage implicit pre-training knowledge and alleviate the discrepancy between pre-training and fine-tuning (@CITE[7]@. GPT- 2 (@CITE[6]@ and T5 ( add human-written task prompts to the input text. For instance, T5 prepends “Summarize:” to the input document for summarization tasks. Some researchers also design elaborate prompts for each task and dataset and investigate their effectiveness and robustness (; @CITE[3]@. To overcome the constraints of manually constructed prompts, researchers develop continuous (soft) prompts that can be optimized in continuous space (@CITE[2]@; @CITE[1]@;. Considering the random initialization of soft prompts, @CITE[5]@ propose PPT to pre-train continuous prompts using unlabeled data. SPoT (@CITE[8]@, Unified-SKG (, and PTG (@CITE[4]@ further learn the prompts on related tasks and transfer the prompts to new tasks.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation\\nID: 4. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 5. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 6. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 7. Title: ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\\nID: 8. Title: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer<|eot_id|>'},\n",
       " 869: {'predict': 'Several works have studied bias in pretrained language models. For instance, @CITE[3]@ proposed a debiasing method for word embeddings. @CITE[1]@ quantified bias in BERT using templates. @CITE[2]@ introduced a benchmark for measuring social bias in masked language models. @CITE[3]@ proposed a debiasing method for contextualized word embeddings. @CITE[1]@ proposed a debiasing method for pre-trained language models. @CITE[2]@ proposed a debiasing method for pre-trained language models that reduces bias in the model’s predictions. @CITE[3]@ proposed a debiasing method for pre-trained language models that reduces bias in the model’s predictions. @CITE[1]@ proposed a debiasing method for pre-trained language models that reduces bias in the model’s predictions. @CITE[2]@ proposed a debiasing method for pre-trained language models that reduces bias in the model’s predictions.\\nReferences:ID: 1. Title: Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings\\nID: 2. Title: Measuring Bias in Contextualized Word Representations\\nID: 3. Title: CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models',\n",
       "  'ground_truth': '\\n\\n@CITE[2]@ extend WEAT to sentence encoders, calling it the Sentence Encoder Association Test (SEAT). For a target term and its attribute, they create artificial sentences using generic context of the form \"This is [target].\" and \"They are [attribute].\" and obtain contextual word embeddings of the target and the attribute terms. They repeat Caliskan et al. (2017)’s study using these embeddings and cosine similarity as the association metric but their study was inconclusive. Later, @CITE[3]@ show that cosine similarity is not the best association metric and define a new association metric based on the probability of predicting an attribute given the target in generic sentential context, e.g., [target] is [mask], where [mask] is the attribute. They show that similar observations of Caliskan et al. (2017) are observed on contextual word embeddings too. Our intrasentence CAT is similar to their setting but with natural context. We also go beyond intrasentence to propose intersentence CATs, since language modeling is not limited at sentence level. Concurrent to our work, @CITE[1]@ introduced CrowS-Pairs, which examines stereotypical bias via minimal pairs. However, CrowS-Pairs only studies bias within a single sentence (intrasentence) and ignores discourse-level (intersentence) measurements. Furthermore, StereoSet contains an order of magnitude of data that contains greater variety, and hence, has the potential to detect a wider range of biases that may be otherwise overlooked. Lastly, StereoSet measures bias across both masked and autoregressive language models, while CrowS-Pairs only measures bias in masked language models.\\nReferences:ID: 1. Title: Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings\\nID: 2. Title: Measuring Bias in Contextualized Word Representations\\nID: 3. Title: CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models<|eot_id|>'},\n",
       " 6884: {'predict': 'To reduce hallucination, existing work has explored several approaches. @CITE[4]@ propose to fine-tune LLMs to align outputs with the preferences of a prototypical user. @CITE[3]@ introduce a reward model to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group. @CITE[1]@ train a model to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. @CITE[2]@ propose to over-learn the original task and then retrain the model on the new task. @CITE[1]@ introduce a method for accurately answering yes-no questions given only unlabeled model activations. @CITE[2]@ use a prompt-based approach to improve the model’s ability to generate factual and accurate responses.\\nReferences:ID: 1. Title: Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\\nID: 2. Title: Fine-tuning language models to find agreement among humans with diverse preferences\\nID: 3. Title: Overcoming catastrophic forgetting in neural networks\\nID: 4. Title: Discovering Latent Knowledge in Language Models Without Supervision',\n",
       "  'ground_truth': '\\n\\nSome methods for reducing hallucination assume that the LLM is a black box (@CITE[2]@). This approach uses different methods for prompting the LLM, possibly by posting multiple queries for achieving better performance. Some methods that can be used for detecting false statements may include repeated queries and measuring the discrepancy among them. We note that instead of asking the LLM to answer the same query multiple times, it is possible to request the LLM to rephrase the query (without changing its meaning or any possible answer) and then asking it to answer each rephrased question. Other methods fine-tune the LLM, using human feedback, reinforcement learning, or both (@CITE[4]@; Ouyang et al., 2022). Ouyang et al. propose a method to improve LLM-generated content using reinforcement learning from human feedback. Their approach focuses on fine-tuning the LLM with a reward model based on human judgments, aiming to encourage the generation of better content. However, fine-tuning, in general, may cause a model to not perform as well on other tasks (@CITE[3]@). In this paper, we take an intermediate approach, that is, we assume access to the model parameters, but do not fine-tune or modify them. Another approach that can be applied to our settings is presented by (@CITE[1]@), named Contrast-Consistent Search (CCS). However, CCS requires rephrasing a statement into a question, evaluating the LLM on two different versions of the prompt, and requires training data from the same dataset (topic) as the test set. These limitations render it unsuitable for running in practice on statements generated by an LLM. In addition, CCS increases the accuracy by only approximately 4% over the 0-shot LLM query, while our approach demonstrates a nearly 20% increase over the 0-shot.\\nReferences:ID: 1. Title: Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\\nID: 2. Title: Fine-tuning language models to find agreement among humans with diverse preferences\\nID: 3. Title: Overcoming catastrophic forgetting in neural networks\\nID: 4. Title: Discovering Latent Knowledge in Language Models Without Supervision<|eot_id|>'},\n",
       " 1976: {'predict': 'Traditional NER methods are based on a sequence tagging framework. They can be categorized into two groups: sequence labeling and sequence segmentation. For sequence labeling, the model predicts a tag for each token in the input sequence. For example, @CITE[2]@ and @CITE[5]@ proposed to use a bidirectional LSTM-CRF model for sequence labeling. For sequence segmentation, the model predicts a span for each token in the input sequence. For example, @CITE[3]@ proposed a transition-based model to construct and label spans. However, these methods are based on a sequence tagging framework, which cannot effectively handle the sequence segmentation problem. Recently, some works (@CITE[1]@; @CITE[4]@) propose to use a span-based framework for sequence segmentation. However, these methods are based on a sequence tagging framework, which cannot effectively handle the sequence segmentation problem.\\nReferences:ID: 1. Title: Bidirectional LSTM-CRF Models for Sequence Tagging\\nID: 2. Title: Neural Architectures for Named Entity Recognition\\nID: 3. Title: Hierarchical Contextualized Representation for Named Entity Recognition\\nID: 4. Title: Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition\\nID: 5. Title: A Minimal Span-Based Neural Constituency Parser',\n",
       "  'ground_truth': '\\n\\nThe traditional method to sequence segmentation converts it into a sequence labeling tasks with IOB tagging scheme. This method is simple and effective, which has inspired a lot of well-performed models (@CITE[3]@; ; @CITE[2]@). For example, present Flair Embeddings that pre-trains character embedding in a large corpus and directly use it, instead of word representation, to encode a sentence. @CITE[4]@ use hierarchical contextualized representations to incorporate both sentence-level and document-level information. Recently, span-based models have received much attention. They treat a span, instead of a token, as the basic unit for labeling. For instance, @CITE[5]@; rank all the spans in terms of the scores predicted by a bi-affine model (@CITE[1]@). Span-based models also emerge in other fields. integrate LSTM-minus feature into constituent parsing models.\\nReferences:ID: 1. Title: Bidirectional LSTM-CRF Models for Sequence Tagging\\nID: 2. Title: Neural Architectures for Named Entity Recognition\\nID: 3. Title: Hierarchical Contextualized Representation for Named Entity Recognition\\nID: 4. Title: Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition\\nID: 5. Title: A Minimal Span-Based Neural Constituency Parser<|eot_id|>'},\n",
       " 3820: {'predict': 'The AI development community is increasingly concerned about the ethical issues arising from the development of AI systems. As a result, many researchers are now focusing on developing AI systems that are ethical (@CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[6]@; @CITE[5]@; @CITE[7]@. The majority of these works have focused on developing systems that are aligned with human values. However, the development of such systems requires a clear understanding of what values are and how they can be incorporated into AI systems. In this paper, we focus on the incorporation of values into language models, which are the most widely used AI systems. In order to incorporate values into language models, we first need to understand what values are. In this paper, we focus on the incorporation of values that are based on social norms. We show that social norms can be effectively incorporated into language models using a combination of reinforcement learning and inverse reinforcement learning. Our approach is based on the assumption that social norms are based on the values of society. We show that social norms can be effectively used to align language models with the values of society.\\nReferences:ID: 1. Title: Case Study: Deontological Ethics in NLP\\nID: 2. Title: Artificial Intelligence, Values and Alignment\\nID: 3. Title: Deep reinforcement learning from human preferences\\nID: 4. Title: Generative Adversarial Imitation Learning\\nID: 5. Title: Learning Norms from Stories: A Prior for Value Aligned Agents\\nID: 6. Title: Cooperative Inverse Reinforcement Learning\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation',\n",
       "  'ground_truth': '\\n\\nThere is a growing body of work concerning the development of AI/machines that behave ethically and/or are aligned with human values. For example, @CITE[7]@ investigate potential applications of deontological ethics in the context of NLP and, similarly to @CITE[3]@, study the first-principles of generalization and autonomy. Other works have prioritized aligning artificial agents with shared human values (@CITE[2]@. Value alignment has been approached from numerous angles, including preference learning (;, imitation (@CITE[5]@ and inverse reinforcement learning (; @CITE[6]@. Additionally, several approaches concerning controllable text generation have been proposed to steer model generation towards specific attributes (@CITE[1]@; @CITE[4]@. However, our experimental setup focuses on classification tasks instead of generation.\\nReferences:ID: 1. Title: Case Study: Deontological Ethics in NLP\\nID: 2. Title: Artificial Intelligence, Values and Alignment\\nID: 3. Title: Deep reinforcement learning from human preferences\\nID: 4. Title: Generative Adversarial Imitation Learning\\nID: 5. Title: Learning Norms from Stories: A Prior for Value Aligned Agents\\nID: 6. Title: Cooperative Inverse Reinforcement Learning\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation<|eot_id|>'},\n",
       " 1459: {'predict': 'To accelerate the training of common architectures for NLP, we can either reduce the computational cost of the model or reduce the memory required for the model. Reducing the computational cost of the model is done by using more lightweight recurrent units (@CITE[5]@; @CITE[3]@; @CITE[4]@, 2019; @CITE[6]@) or by using a more efficient attention mechanism (@CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[5]@). However, these models are still not efficient enough for long sequence tasks and are not directly applicable to the large language models.\\nReferences:ID: 1. Title: Simple Recurrent Units for Highly Parallelizable Recurrence\\nID: 2. Title: Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth\\nID: 3. Title: Simple Recurrent Units for Highly Parallelizable Recurrence\\nID: 4. Title: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\\nID: 5. Title: Big Bird: Transformers for Longer Sequences\\nID: 6. Title: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention',\n",
       "  'ground_truth': '\\n\\nOur work is closely related to two lines of research under this topic. Notably, the Quasi-RNN (@CITE[6]@) and SRU (@CITE[4]@ have invented highly-parallelizable recurrence and combined them with convolutions or highway networks respectively. The resulting architectures achieve equivalent parallelism as convolutional and attention models. This advancement eliminates the need of avoiding recurrence computation to trade model training efficiency, a design choice made by the Transformer architecture. Our model builds on top of SRU. Several recent works have argued that using attention alone is not the best architecture in terms of model expressiveness. For example, demonstrate theoretically and empirically that using pure attention results in performance degeneration. have combined convolution and attention and obtained new state-of-the-art results for speech recognition. Moreover, RNNs have been incorporated into Transformer architectures, resulting in improved results in machine translation and language understanding tasks (@CITE[4]@; @CITE[1]@). Our work is built upon a similar hypothesis that recurrence and attention are complementary at sequence modeling. We demonstrate that jointly leveraging fast recurrence and attention not only achieves state-of-the-art modeling results but also obtains significant computation reduction. Borthogonal to our work, many recent works improve the efficiency of Transformer models by accelerating attention computation (@CITE[5]@; @CITE[2]@; @CITE[2]@; @CITE[3]@). Examples include Longformer (Beltagy et al., 2020), Reformer (Kitaev et al., 2020), Linformer () and Routing Transformer (@CITE[3]@. In contrast, our work optimizes computational efficiency using recurrence combined with minimal attention and our model can incorporate these attention variants for additional speed improvement.\\nReferences:ID: 1. Title: Simple Recurrent Units for Highly Parallelizable Recurrence\\nID: 2. Title: Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth\\nID: 3. Title: Simple Recurrent Units for Highly Parallelizable Recurrence\\nID: 4. Title: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\\nID: 5. Title: Big Bird: Transformers for Longer Sequences\\nID: 6. Title: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention<|eot_id|>'},\n",
       " 4386: {'predict': 'In order to utilize the feedback from the users, several approaches have been proposed to incorporate feedback into the model (@CITE[1]@; @CITE[4]@; @CITE[2]@; @CITE[7]@; @CITE[6]@; @CITE[3]@). For example, @CITE[1]@ propose to use natural language patches to fix systematic problems in NLP models. @CITE[4]@ introduce a method called “Constitutional AI” that trains an AI assistant with a list of rules or principles, and the only human oversight is provided through a list of rules or principles. @CITE[2]@ introduce a method called “RL from AI Feedback” (RLAIF) that trains a harmless AI assistant that engages with harmful queries by explaining its objections to them. @CITE[7]@ propose to pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user’s intents, along with user feedback for clarification. @CITE[6]@ introduce a collaborative language model, PEER, that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. @CITE[8]@ propose to fine-tune large language models to write natural language critiques using behavioral cloning. @CITE[5]@ propose to use natural language patches to fix systematic problems in NLP models. @CITE[3]@ propose to use a user’s feedback to improve the model’s performance. Our approach differs from these works in that we use the user’s feedback to improve the model’s interpretability.\\nReferences:ID: 1. Title: Fixing Model Bugs with Natural Language Patches\\nID: 2. Title: Constitutional AI: Harmlessness from AI Feedback\\nID: 3. Title: Language Model Cascades\\nID: 4. Title: Memory-assisted prompt editing to improve GPT-3 after deployment\\nID: 5. Title: PEER: A Collaborative Language Model\\nID: 6. Title: Memory-assisted prompt editing to improve GPT-3 after deployment\\nID: 7. Title: Self-critiquing models for assisting human evaluators\\nID: 8. Title: Fixing Model Bugs with Natural Language Patches',\n",
       "  'ground_truth': '\\n\\nHuman feedback on model predictions come in different flavors. The most notable ones include (1) binary feedback, e.g. thumbs up/down and pairwise comparisons (@CITE[1]@; @CITE[6]@; @CITE[4]@), (2) natural language critiques (@CITE[8]@; @CITE[5]@; @CITE[8]@; @CITE[7]@; @CITE[1]@; @CITE[2]@; @CITE[3]@), and (3) direct textual refinements to outcomes (@CITE[7]@; @CITE[1]@; @CITE[6]@).\\nReferences:ID: 1. Title: Fixing Model Bugs with Natural Language Patches\\nID: 2. Title: Constitutional AI: Harmlessness from AI Feedback\\nID: 3. Title: Language Model Cascades\\nID: 4. Title: Memory-assisted prompt editing to improve GPT-3 after deployment\\nID: 5. Title: PEER: A Collaborative Language Model\\nID: 6. Title: Memory-assisted prompt editing to improve GPT-3 after deployment\\nID: 7. Title: Self-critiquing models for assisting human evaluators\\nID: 8. Title: Fixing Model Bugs with Natural Language Patches<|eot_id|>'},\n",
       " 5504: {'predict': 'The factors determining social biases in MLMs have been extensively studied in the literature. Most of the existing work has focused on the factors that contribute to social biases in MLMs, including the data used for pre-training (@CITE[1]@; ; ), the model architecture (@CITE[3]@), and the model size (). In addition, there has been a growing interest in the factors that affect the degree of social biases in MLMs, such as the pre-training objective (@CITE[2]@), the pre-training data distribution (Nadeem et al., 2020), the pre-training corpus (Bender et al., 2021), and the downstream task (Kurita et al., 2020). In this work, we focus on the factors that affect the degree of social biases in MLMs, and examine whether the factors identified in previous work are consistent across different types of social biases.\\nReferences:ID: 1. Title: Which *BERT? A Survey Organizing Contextualized Encoders\\nID: 2. Title: Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\\nID: 3. Title: Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer',\n",
       "  'ground_truth': '\\n\\nAs MLMs have been successfully applied to diverse NLP tasks, it is important to study the factors that determine their social biases. Rogers et al. (2020) reviewed the current state of knowledge regarding how BERT works, what kind of information it learns and how it is encoded, typically alterations to its training objectives and architecture, the overparameterization issue and approaches to compression. @CITE[1]@ studied contextualised encoders in various aspects and discussed the trade-off between task performance and the potential harms contained in the pretraining data. Later, Pérez-@CITE[3]@ investigated the effect of pretraining data size on the syntactic capabilities of RoBERTa and they showed that models pretrained with more data tend to encode better syntactic information and provide more syntactic generalisation across different syntactic structures. However, these studies focus on MLMs in downstream tasks, while none consider the social biases in MLMs. On the other hand, models trained for different downstream tasks have been found to exhibit social biases. evaluated gender and racial biases across 219 automatic sentiment analysis systems and discovered statistically significant biases occurring in several systems. Díaz et al. (2018) studied age-related biases in sentiment classification and discovered that significant age bias is encoded in the output of many sentiment analysis systems as well as word embeddings. focused on gender bias in multilingual embeddings and its influence on the process of transfer learning for NLP applications. They showed that the level of bias in multilingual representations varies depending on how the embeddings are aligned to different target spaces, and that the alignment direction can also affect the bias in transfer learning. @CITE[2]@ investigated the types of stereotypical information that are captured by pretrained language models and showed the variability of attitudes towards various social groups among models and the rapid changes in emotions and stereotypes that occur during the fine-tuning phase.\\nReferences:ID: 1. Title: Which *BERT? A Survey Organizing Contextualized Encoders\\nID: 2. Title: Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\\nID: 3. Title: Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer<|eot_id|>'},\n",
       " 6429: {'predict': 'Zero-shot text classification is a challenging NLU task that has been studied by a few recent works. The main challenge of this task is to classify texts into different labels without any training data. @CITE[1]@ and @CITE[1]@ propose a textual entailment formulation for zero-shot text classification. They propose two datasets for zero-shot text classification: ZEST and ZEST-entail. ZEST is a dataset of 12K labeled texts from Reddit. ZEST-entail is a dataset of 8K labeled texts from Reddit, where the labels are derived from the textual entailment task. They propose two evaluation setups for zero-shot text classification: label-partially-unseen and label-fully-unseen. Label-partially-unseen is a setup where the model is trained on some labels and evaluated on the remaining labels. Label-fully-unseen is a setup where the model is trained on no labels and evaluated on all labels. They also propose a baseline model based on a textual entailment approach. @CITE[2]@ propose a zero-shot text classification model based on a multi-task learning approach. They use a textual entailment task and a topic classification task as auxiliary tasks to help the model learn to classify texts into different labels. They propose a dataset of 15K labeled texts from Reddit for zero-shot text classification. They evaluate their model on two evaluation setups: label-partially-unseen and label-fully-unseen. They also propose a baseline model based on a textual entailment approach. @CITE[1]@ propose a zero-shot text classification model based on a generative approach. They propose a dataset of 5K labeled texts from Reddit for zero-shot text classification. They evaluate their model on two evaluation setups: label-partially-unseen and label-fully-unseen. They also propose a baseline model based on a textual entailment approach. They also propose a dataset of 10K labeled texts from Reddit for zero-shot text classification. They evaluate their model on two evaluation setups: label-partially-unseen and label-fully-unseen. They also propose a baseline model based on a textual entailment approach. They propose a zero-shot text classification model based on a multi-task learning approach. They propose a dataset of 5K labeled texts from Reddit for zero-shot text classification. They evaluate their model on two evaluation setups: label-partially-unseen and label-fully-unseen. They also propose a baseline model based on a textual entailment approach. They also propose a dataset of 10K labeled texts from Reddit for zero-shot text classification. They evaluate their model on two evaluation setups: label-partially-unseen and label-fully-unseen. They also propose a baseline model based on a textual entailment approach.\\nReferences:ID: 1. Title: Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach\\nID: 2. Title: Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach',\n",
       "  'ground_truth': '\\n\\nZero-shot text classification is the task of classifying text into novel categories unseen during training. Early zero-shot classification studies frame the problem as binary classification on whether a given label describes the text (Pushp and Srivastava, 2017; @CITE[2]@). With the advancement of PLMs, subsequent works (@CITE[2]@; @CITE[1]@) rely on transformer architectures to learn representations from descriptive labels passed in. In particular, @CITE[1]@ fine-tune an autoregressive language model to generate titles based on a prompt template containing Tweet articles and a list of title options. Though the model is trained on a great variety of title options, the approach limits the learning to topic classification only, as the authors only analyze performance on topic datasets, unlike our approach which considers a wide array of aspects, each requiring focus on different sections of a given text. @CITE[2]@ similarly categorize zero-shot text classification by aspects and implicitly introduce aspects during training with a dedicated template for each aspect. They further propose the classification of a text, label pair as a logic entailment problem. However, the authors analyze a less challenging zero-shot case where a model is trained on a subset of text, label pairs, and evaluated on the remaining text with unseen labels in the same domain. Additionally, the authors introduce WordNet definition of the labels as the labels are all single words. This process requires manual intervention and is not applicable for multiple-word label sequences common in intent classification, such as \"Check Balance\". Our work evaluates a more diverse set of datasets for each aspect and a more comprehensive set of zero-shot architectures.\\nReferences:ID: 1. Title: Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach\\nID: 2. Title: Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach<|eot_id|>'},\n",
       " 651: {'predict': 'Parallel modeling is a popular approach for knowledge-grounded dialogue systems. The primary idea is to model the dialogue generation and knowledge grounding processes in parallel. For example, @CITE[4]@ proposed a novel Transformer-based architecture for multi-turn document grounded conversations, which encodes multi-turn utterances along with knowledge in related documents. @CITE[2]@ presented a document-grounded matching network (DGMN) for response selection, which fuses information in a document and a context into representations of each other. @CITE[3]@ introduced Topical-Chat, a knowledge-grounded human-human conversation dataset, and proposed a parallel knowledge-grounded dialogue model that jointly learns to generate and select the best response from a set of candidates. @CITE[1]@ proposed a novel Transformer-based architecture for multi-turn document grounded conversations, which encodes multi-turn utterances along with knowledge in related documents and employs a deliberation decoder to improve context coherence and knowledge correctness. @CITE[5]@ presented a parallel dialogue model that uses a generative and a retrieval decoder to generate and select the best response from a set of candidates, respectively. However, these parallel models suffer from the same limitations as the sequential models: they are prone to overfitting the dataset and cannot generalize well to unseen knowledge. Moreover, they require a large amount of annotated data and a large number of parameters, which makes them computationally expensive.\\nReferences:ID: 1. Title: Towards Exploiting Background Knowledge for Building Conversation Systems\\nID: 2. Title: Incremental Transformer with Deliberation Decoder for Document Grounded Conversations\\nID: 3. Title: A Document-grounded Matching Network for Response Selection in Retrieval-based Chatbots\\nID: 4. Title: Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations\\nID: 5. Title: Incremental Transformer with Deliberation Decoder for Document Grounded Conversations',\n",
       "  'ground_truth': '\\n\\nFor the first category, @CITE[4]@ used a generation-based model that learns to copy information from the background knowledge and a span prediction model that predicts the appropriate response span in the background knowledge. claimed the first to unify knowledge triples and long texts as a graph. Then employed a reinforce learning process in the flexible multi-hop knowledge graph reasoning process. To improve the process of using background knowledge, (Zhang et al., 2019) firstly adopted the encoder state of the utterance history context as a query to select the most relevant knowledge, then employed a modified version of BiDAF (@CITE[1]@) to point out the most relevant token positions of the background sequence. used a decoding switcher to predict the probabilities of executing the reference decoding or generation decoding. Some other researchers (@CITE[3]@; @CITE[5]@; @CITE[2]@; ; also followed this parallel encoding method.\\nReferences:ID: 1. Title: Towards Exploiting Background Knowledge for Building Conversation Systems\\nID: 2. Title: Incremental Transformer with Deliberation Decoder for Document Grounded Conversations\\nID: 3. Title: A Document-grounded Matching Network for Response Selection in Retrieval-based Chatbots\\nID: 4. Title: Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations\\nID: 5. Title: Incremental Transformer with Deliberation Decoder for Document Grounded Conversations<|eot_id|>'},\n",
       " 1190: {'predict': 'The decoder-based methods for text style transfer can be divided into two categories: one is to generate text in the target style based on the text in the source style, and the other is to generate text in the target style based on the text in the target style. The former category includes a wide range of methods, such as adversarial networks (@CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[7]@; @CITE[8]@), reinforcement learning (@CITE[5]@; @CITE[3]@; @CITE[6]@; @CITE[6]@), and back-translation (@CITE[3]@). The latter category includes dual learning (@CITE[3]@; @CITE[6]@) and one-to-many translation (@CITE[3]@). The former category is the most widely used, which is because it is more challenging to generate text in the target style based on the text in the target style. Moreover, the former category is more challenging in terms of the semantic preservation and fluency of the generated text.\\nReferences:ID: 1. Title: Style Transfer in Text: Exploration and Evaluation\\nID: 2. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 3. Title: Style Transfer Through Back-Translation\\nID: 4. Title: Reformulating Unsupervised Style Transfer as Paraphrase Generation\\nID: 5. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 6. Title: Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus\\nID: 7. Title: A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer\\nID: 8. Title: Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus',\n",
       "  'ground_truth': '\\n\\nThe decoder based methods build a decoder for each style or transfer direction, where the style is implicitly represented as the parameters in the corresponding decoder. The former schema built an independent decoder for each style, which first disentangled the style-irrelevant content from the text and then applied the corresponding decoder to generate sentences with the target style (@CITE[8]@; @CITE[7]@; @CITE[5]@; @CITE[6]@). The latter built for each transfer direction, which often regarded style transfer as a translation task (@CITE[7]@; @CITE[1]@; @CITE[2]@; @CITE[1]@; Wu et al., 2019a; @CITE[3]@). This paradigm reduced the complexity of the learning of style to a certain extent but consumed more resources. It is worth mentioning that the boundaries of embedding controlled and decoder controlled methods are sometimes not very clear, and many studies (@CITE[8]@; @CITE[4]@) consider that they are alternative.\\nReferences:ID: 1. Title: Style Transfer in Text: Exploration and Evaluation\\nID: 2. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 3. Title: Style Transfer Through Back-Translation\\nID: 4. Title: Reformulating Unsupervised Style Transfer as Paraphrase Generation\\nID: 5. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 6. Title: Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus\\nID: 7. Title: A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer\\nID: 8. Title: Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus<|eot_id|>'},\n",
       " 712: {'predict': 'Evaluation of pre-trained models is a hot topic in NLP, which has been widely studied in recent years. The typical methods include the masked language modeling (MLM) task, next sentence prediction (NSP) task, and sentence-level cloze test. For example, BERT (@CITE[3]@) uses MLM and NSP to evaluate the model. GPT (@CITE[4]@) uses the masked language modeling task. OPTUS (@CITE[2]@) uses the VAE reconstruction task. However, these tasks are not enough to evaluate the model comprehensively. Because the tasks are only based on the word-level information, which is not enough to capture the sentence-level semantics. To this end, we propose the task of generating the semantic summary of a sentence. The semantic summary is a concise and essential part of a sentence, which is more helpful to evaluate the model comprehensively. Our task is different from the previous works in two aspects. First, the task is not based on the word-level information, but the sentence-level semantics. Second, the task is not a classification task, but a generation task. It is more challenging to generate a summary than to classify a sentence.\\nReferences:ID: 1. Title: Deep contextualized word representations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Robust Conversational AI with Grounded Text Generation\\nID: 4. Title: Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space',\n",
       "  'ground_truth': '\\n\\nPre-trained language models (PLMs) have substantially advanced the state of the art across a variety of language understanding and generation tasks (; @CITE[2]@; ; @CITE[4]@). PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to quickly adapt to various downstream tasks, exhibiting strong generalization capacity even with just a few in-domain training examples. Building task bots at scale requires the model to deal with the limited data problem for each domain, which can be used as a testbed to evaluate the generalization ability of PLMs. To this end, we limit the number of task-specific training examples in RADDLE to evaluate the sample-efficiency of models. Meanwhile, task-oriented dialogs pose a unique set of challenges for PLMs (@CITE[1]@): a dialog is intrinsically goal-driven, multi-turn and often informal/noisy. Indeed, dialog-specific PLMs are proposed (; @CITE[3]@). However, the robustness of PLMs to linguistic perturbations often occurring in dialog settings (See Section 4 for details) is largely unexplored. Note that our notion of robustness emphasizes natural language variations, which is different from adversarial examples/training that aim to fool a trained model (Nie et al., 2019). From this perspective, RADDLE provides a unique benchmark for assessing PLMs with a robustness orientation.\\nReferences:ID: 1. Title: Deep contextualized word representations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Robust Conversational AI with Grounded Text Generation\\nID: 4. Title: Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space<|eot_id|>'},\n",
       " 515: {'predict': 'Recently, the success of self-supervised learning (SSL) has led to a new wave of interest in learned representations for speech recognition. SSL methods such as contrastive learning (@CITE[1]@; @CITE[3]@; @CITE[2]@ and masked language modeling (Devlin et al., 2019) have shown promising results in speech recognition. In particular, contrastive learning has been used to learn representations from raw audio and then fine-tuned for speech recognition tasks. For example, in the Zero Resource Speech Challenge 2017, 17 models were evaluated on the task of learning subword units from raw speech, and contrastive learning was used by most of the participants. Most of these works use a contrastive loss function to learn representations, which encourages the model to distinguish between different views of the same data. However, these methods rely on a pre-defined view function and are limited to learning representations that are invariant to the transformations defined by the view function. In this paper, we use a different approach to learn representations that are invariant to different views of the same data. We use a mutual information maximization approach, which has been shown to be effective in learning representations in computer vision (@CITE[1]@; @CITE[3]@; @CITE[2]@. In particular, we use a multi-view approach to learn representations from multiple views of the same audio data, and use a mutual information maximization loss to encourage the model to learn representations that are invariant to the different views of the data.\\nReferences:ID: 1. Title: Learning Representations by Maximizing Mutual Information Across Views\\nID: 2. Title: Learning Representations by Maximizing Mutual Information Across Views\\nID: 3. Title: The Zero Resource Speech Challenge 2017',\n",
       "  'ground_truth': '\\n\\nThe last two years have produced a large amount of work on unsupervised speech representation learning. Some of this work has been evaluated in terms of its ability to perform phone recognition and similar audio classification tasks (van den Oord et al., 2018). Like us, @CITE[2]@; @CITE[2]@ applied learned representations to speech recognition tasks and evaluated on how well in-domain WER was improved. However, as we argued in the paper, such an evaluation misses the opportunity to assess whether these systems become more robust to domain shift and to what extent the learned representations are appropriate for different languages. Finally, the ZeroSpeech challenges have explicitly looked at correlations between learned representations and phonetic structures that generalize across many languages and adapt to new speakers (@CITE[3]@, 2019). @CITE[1]@; Rivi` ere et al. (2020) learned representations with contrastive predictive coding on 60,000 hours of English speech and could show that their representations are correlated well with the phonetic structure of English and other languages; however, they did not evaluate these representations in a supervised speech recognizer.\\nReferences:ID: 1. Title: Learning Representations by Maximizing Mutual Information Across Views\\nID: 2. Title: Learning Representations by Maximizing Mutual Information Across Views\\nID: 3. Title: The Zero Resource Speech Challenge 2017<|eot_id|>'},\n",
       " 4287: {'predict': 'The idea of generative relational reasoning has been explored in previous studies. @CITE[5]@ propose to generate a definition-like sentence to describe the relation between two entities. @CITE[3]@ propose to generate a sentence to describe the relation between two entities by incorporating reasoning paths in KGs. @CITE[4]@ propose to generate a sentence to describe the relation between two entities by incorporating reasoning paths in KGs and including a reasoning path selection mechanism. In the field of story generation, @CITE[5]@ propose to generate a story by first generating a premise and then transforming it into a passage of text. @CITE[2]@ propose to generate a story by first generating a topic and then generating a passage of text based on the topic. In the field of text generation, @CITE[1]@ propose to generate a sentence by first generating a topic and then generating a passage of text based on the topic. In this paper, we propose to generate a sentence by first generating a topic and then generating a passage of text based on the topic.\\nReferences:ID: 1. Title: Open Relation Modeling: Learning to Define Relations between Entities\\nID: 2. Title: Open Relation Modeling: Learning to Define Relations between Entities\\nID: 3. Title: Beam Search Strategies for Neural Machine Translation\\nID: 4. Title: Hierarchical Neural Story Generation\\nID: 5. Title: The Curious Case of Neural Text Degeneration',\n",
       "  'ground_truth': '\\n\\nGenerative relational reasoning attempts to generate a coherent sentence involving a pair or a set of concepts/entities (; @CITE[3]@,c; ). For instance, introduce CommonGen, which aims to generate a coherent sentence that describes an everyday scenario involving a given set of common concepts. @CITE[3]@ propose Open Relation Modeling, which aims to generate an informative sentence describing relationships between concepts. However, these methods do not consider the diversity of possible relationships that can exist between concepts, leading to a limited understanding of relationships between the concepts. Incorporating diversity at the inference phrase is achieved by sampling methods. Instead of selecting the next token based on maximum likelihood (Freitag and @CITE[5]@), tokens are sampled from a probability distribution of the vocabulary. For example, @CITE[2]@ reduces the sampling pool by keeping only the top-k candidates for each token in the generation. @CITE[4]@ limits the next-token sampling pool by a threshold of the probability mass. @CITE[1]@ restrict the generated words to expected information content by shifting the truncation set with a conditional entropy of prior content. While these methods reduce the training effort for neural models, they are criticized for the low quality of generations ().\\nReferences:ID: 1. Title: Open Relation Modeling: Learning to Define Relations between Entities\\nID: 2. Title: Open Relation Modeling: Learning to Define Relations between Entities\\nID: 3. Title: Beam Search Strategies for Neural Machine Translation\\nID: 4. Title: Hierarchical Neural Story Generation\\nID: 5. Title: The Curious Case of Neural Text Degeneration<|eot_id|>'},\n",
       " 2665: {'predict': 'Pre-trained language models (PLMs) have been shown to be effective in many NLP tasks. BERT (@CITE[3]@) is a representative model, which has been pre-trained on a large corpus of monolingual data. To further improve the cross-lingual transfer ability of PLMs, multilingual PLMs (mPLMs) have been proposed, such as mBERT (@CITE[3]@), XLM (@CITE[6]@), and mBART (). In this work, we focus on zero-shot cross-lingual transfer, which aims to transfer knowledge from a source language to a target language without any training data in the target language. Many works have shown that mPLMs can achieve impressive results in zero-shot cross-lingual transfer (@CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[2]@; @CITE[5]@). However, existing works mainly focus on general NLP tasks, such as classification, NLI, and dependency parsing, which are not directly applicable to MEL. In this work, we show that mBERT can also be applied to MEL, and we explore how to improve the performance of mBERT in zero-shot cross-lingual transfer for MEL.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Multilingual Autoregressive Entity Linking\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Multilingual Autoregressive Entity Linking',\n",
       "  'ground_truth': '\\n\\nLeveraging cross-lingual transfer has the potential to drastically improve model performance without requiring large amounts of data in the target language (Conneau et al., 2020) but it is not always clear from which language we must transfer from (@CITE[2]@; de @CITE[3]@). To this end, recent work investigates methods for selecting good transfer languages and informative features. For instance, token overlap between the source and target language is a useful predictor of transfer performance for some tasks (@CITE[2]@; @CITE[4]@). Linguistic distance (@CITE[2]@; de @CITE[3]@), word order (K et al., 2020; @CITE[6]@), and script differences (de @CITE[3]@, and syntactic similarity (@CITE[5]@) have also been shown to impact performance. Another research direction attempts to build models of transfer performance that predicts the best transfer language for a target language by using some linguistic and data-dependent features (@CITE[2]@; @CITE[1]@).\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Multilingual Autoregressive Entity Linking\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Multilingual Autoregressive Entity Linking<|eot_id|>'},\n",
       " 6131: {'predict': 'Recent work has explored the use of large language models for pre-training conversational agents. @CITE[2]@ pre-train a multilingual T5 model on the Alexa corpus and find that it outperforms XLM-R and mT5 on XNLI and SNIPS. @CITE[1]@ use a 5B-parameter seq2seq model and fine-tune it on instructions to generate annotated data for intent classification and slot tagging. @CITE[1]@ use a 5B-parameter seq2seq model to generate annotated data for intent classification and slot tagging. They fine-tune the model on instructions and use it to generate data for intent classification and slot tagging. @CITE[1]@ propose a method for generating annotated data for intent classification and slot tagging. They fine-tune a 5B-parameter seq2seq model on instructions and use it to generate data for intent classification and slot tagging. @CITE[1]@ propose a method for generating annotated data for intent classification and slot tagging. They fine-tune a 5B-parameter seq2seq model on instructions and use it to generate data for intent classification and slot tagging. They find that the generated data is better than the back-translated data.\\nReferences:ID: 1. Title: Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems\\nID: 2. Title: LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging',\n",
       "  'ground_truth': '\\n\\nMotivated by the need for both an encoder model for efficient sequence labeling and a seq2seq model for generative tasks like semantic parsing and summarization, we explore recipes to pre-train both models. Compared to training each model from scratch, we propose two sequential training recipes which reduce the total compute cost (Section 2.1.6). The first recipe is to extract the encoder of a seq2seq model as proposed in @CITE[1]@. Although it performs well on classification tasks, we show that the encoder from seq2seq under-performs a from-scratch encoder on sequence labeling tasks. Variations of masking during seq2seq training and reducing the decoder size do not provide a consistent benefit to the encoder. We also explore continuing training the extracted encoder on MLM for a small number of updates. However, we show it cannot consistently close the gap in performance across different datasets. The second recipe is to warm-start seq2seq pre-training with an encoder pre-trained via MLM (Figure 1). Rothe et al. (2020) proposed a similar idea for fine-tuning. AlexaTM 20B and AlexaTM 5B applied this recipe for pre-training, by warm-starting with Alexa Teacher Model encoders (@CITE[2]@; ; @CITE[2]@.\\nReferences:ID: 1. Title: Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems\\nID: 2. Title: LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging<|eot_id|>'},\n",
       " 3979: {'predict': 'Existing automatic metrics for NLG can be broadly classified into two categories: heuristic-based metrics (@CITE[1]@; ; @CITE[4]@) and trained metrics (@CITE[3]@; ; @CITE[7]@; ; @CITE[5]@; @CITE[2]@). Heuristic-based metrics rely on predefined rules or templates to score generated texts. However, they are not effective in capturing the nuances of NLG tasks, such as summarization and dialog generation. Trained metrics, on the other hand, are trained to predict human ratings of generated texts. They are more effective in capturing the nuances of NLG tasks, but they are also prone to biases and artifacts in the training data (@CITE[6]@).\\nReferences:ID: 1. Title: A Survey of Evaluation Metrics Used for NLG Systems\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 4. Title: InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation\\nID: 5. Title: A Comprehensive Assessment of Dialog Evaluation Metrics\\nID: 6. Title: Why We Need New Evaluation Metrics for NLG\\nID: 7. Title: Perturbation CheckLists for Evaluating NLG Evaluation Metrics',\n",
       "  'ground_truth': '\\n\\nExisting automatic metrics in NLG are mainly dominated by task-agnostic metrics - metrics that assess the quality of generation outputs without considering human evaluation aspects as context or objective of the evaluation task (@CITE[1]@). Task-agnostic metrics are highly adaptable across NLG tasks because the adaptation does not require task-specific design. For example, BLEU (@CITE[2]@) and ROUGE (), which represent string-based metrics, are largely adopted in Neural Machine Translation (NMT) and Text Summarization. Perplexity (@CITE[6]@; ), a reference-less metric, is a standard evaluation metric in Language Modeling-based NLG tasks, including Controlled Generation (@CITE[3]@; ). BERTScore (Zhang* et al., 2020) has been largely adopted in diverse NLG tasks, including NMT (), Text Summarization (), and Dialogue System (@CITE[5]@). Nevertheless, currently available task-agnostic metrics are weakly correlated to human judgment (@CITE[7]@; @CITE[4]@, 2022). A low correlation score introduces a criticism on the capability of automatic metrics at identifying the different quality of system outputs and their potential usage to substitute a costly human evaluation study.\\nReferences:ID: 1. Title: A Survey of Evaluation Metrics Used for NLG Systems\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 4. Title: InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation\\nID: 5. Title: A Comprehensive Assessment of Dialog Evaluation Metrics\\nID: 6. Title: Why We Need New Evaluation Metrics for NLG\\nID: 7. Title: Perturbation CheckLists for Evaluating NLG Evaluation Metrics<|eot_id|>'},\n",
       " 995: {'predict': 'Fully non-autoregressive models generate all tokens in parallel, which is more efficient than partially non-autoregressive models. @CITE[4]@ proposed the mask-predict model, which predicts the target token by predicting the probability distribution of the token. @CITE[5]@ proposed the ReorderNAT, which explicitly models the reordering information in the decoding procedure. @CITE[6]@ proposed the CNAT, which learns implicit categorical codes to improve the model capacity. @CITE[2]@ proposed the curriculum learning for NAT, which progressively switches the training from autoregressive generation to non-autoregressive generation. @CITE[1]@ proposed the EM approach to NAT, which jointly optimizes both AR and NAR models in a unified EM framework. @CITE[3]@ proposed the bag-of-ngrams training objective, which encourages NAT to capture the target-side sequential dependency. However, these models still suffer from the multi-modality in the output sequences.\\nReferences:ID: 1. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 2. Title: Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information\\nID: 3. Title: Non-Autoregressive Translation by Learning Target Categorical Codes\\nID: 4. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 5. Title: An EM Approach to Non-autoregressive Conditional Sequence Generation\\nID: 6. Title: Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nA line of work introduces various forms of latent variables to reduce the model’s burden of dealing with dependencies among output words (@CITE[5]@; @CITE[5]@; ; @CITE[4]@; @CITE[5]@). Another branch of work considers transferring the knowledge from autoregressive models to non-autoregressive models (Wei et al., 2019; ; ; ). Besides, there are also some works that apply different training objectives to train non-autoregressive models (Libovick` y and ; ; @CITE[6]@).\\nReferences:ID: 1. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 2. Title: Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information\\nID: 3. Title: Non-Autoregressive Translation by Learning Target Categorical Codes\\nID: 4. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 5. Title: An EM Approach to Non-autoregressive Conditional Sequence Generation\\nID: 6. Title: Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation<|eot_id|>'},\n",
       " 4154: {'predict': 'Modular reasoning aims to break down a complex task into a series of simpler sub-tasks and solve them in a pipeline manner. The first work of modular reasoning is the Chain-of-Thought (@CITE[6]@), which solves a task by iteratively generating the intermediate results and using them as input for the next step. Inspired by this, several works (@CITE[3]@; @CITE[4]@; @CITE[7]@; @CITE[5]@; @CITE[1]@; @CITE[2]@) have been proposed to further improve the performance of modular reasoning. For example, @CITE[3]@ proposed a least-to-most prompting strategy to solve complex problems by solving a series of simpler subproblems. @CITE[4]@ proposed a successive prompting method to decompose a complex task into a series of simpler sub-tasks and solve them in a pipeline manner. @CITE[7]@ proposed a modular prompting strategy to solve a complex task by solving a series of simpler sub-tasks and then combining them to get the final results. @CITE[1]@ proposed a modular reasoning method based on the modular prompting strategy, which can solve a complex task by solving a series of simpler sub-tasks. @CITE[2]@ proposed a modular reasoning method based on the LLM program, which can solve a complex task by solving a series of simpler sub-tasks and combining them to get the final results. In this paper, we propose a modular reasoning method based on the modular prompting strategy, which can solve a complex task by solving a series of simpler sub-tasks and combining them to get the final results. Compared with previous works, our method can solve complex tasks in a more flexible and efficient way.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Decomposed Prompting: A Modular Approach for Solving Complex Tasks\\nID: 3. Title: Natural Language Deduction with Incomplete Information\\nID: 4. Title: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\\nID: 5. Title: Successive Prompting for Decomposing Complex Questions\\nID: 6. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 7. Title: Large Language Model Programs',\n",
       "  'ground_truth': '\\n\\nThese approaches break the problem into smaller modules and use separate LMs to solve each module (@CITE[5]@; @CITE[3]@; @CITE[4]@; @CITE[2]@; @CITE[7]@; @CITE[5]@; @CITE[1]@. LM-based approaches to logical reasoning typically make use of a single LM module; for example, in @CITE[6]@, a single LM module iteratively and exhaustively infers all conclusions based on the facts and rules, and then the goal statement is compared against the final set of conclusions to confirm if it can be proved from the theory. Since exhaustively deriving all conclusions is computationally expensive, Creswell et al. (2023) consider a more scalable approach where the conclusions that are derived are informed by the goal; they iteratively apply two LLM modules one selecting a subset of the facts and rules informed by the goal and the other making new inferences based on the selected facts and rules and adding it back to the theory. In this paper, we compare against the second approach.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Decomposed Prompting: A Modular Approach for Solving Complex Tasks\\nID: 3. Title: Natural Language Deduction with Incomplete Information\\nID: 4. Title: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\\nID: 5. Title: Successive Prompting for Decomposing Complex Questions\\nID: 6. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 7. Title: Large Language Model Programs<|eot_id|>'},\n",
       " 1901: {'predict': 'Low-resource speech recognition is a challenging task, which requires a large amount of data to train a model. In recent years, many researchers have proposed various methods to improve the performance of ASR in low-resource scenarios. For example, @CITE[4]@ proposed to use a single end-to-end model to recognize multiple languages. @CITE[3]@ proposed to use meta-learning to improve the performance of ASR in low-resource scenarios. @CITE[1]@ proposed to use adversarial meta sampling to improve the performance of ASR in low-resource scenarios. @CITE[2]@ proposed to use a generative pre-training approach to improve the performance of ASR in low-resource scenarios. @CITE[5]@ proposed to use a non-autoregressive ASR model to improve the performance of ASR in low-resource scenarios.\\nReferences:ID: 1. Title: Multilingual Speech Recognition With A Single End-To-End Model\\nID: 2. Title: Meta Learning for End-to-End Low-Resource Speech Recognition\\nID: 3. Title: Adversarial Meta Sampling for Multilingual Low-Resource Speech Recognition\\nID: 4. Title: Generative Pre-Training for Speech with Autoregressive Predictive Coding\\nID: 5. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models',\n",
       "  'ground_truth': '\\n\\nTo tackle the low-resource ASR task, transfer learning ASR (Kunze et al., 2017) and multilingual transfer learning ASR (@CITE[5]@; Watanabe et al., 2017a; @CITE[3]@ are explored via using different source languages to improve the performance of low-resource languages. Meta-learning approaches (@CITE[2]@; Nichol et al., 2018) are also adopted for low-resource ASR (@CITE[1]@; to obtain fast adaptation ability to new tasks with only a few data through meta-learning a model initialization from training tasks. In addition, recent works utilize unsupervised pre-training (@CITE[4]@; and semi-supervised learning (; to exploit a large amount of unlabeled data to learn general representations for low-resource adaptation. Among them, Wav2vec 2.0 ( achieved excellent results through self-supervised learning, which learns powerful and contextual acoustic representations of a large speech audio corpus by solving contrastive tasks that require identifying the true quantized latent speech representations for masked time steps. Then it shows strong feasibility of ultra-low resource speech recognition with even only 10 minutes of labeled data.\\nReferences:ID: 1. Title: Multilingual Speech Recognition With A Single End-To-End Model\\nID: 2. Title: Meta Learning for End-to-End Low-Resource Speech Recognition\\nID: 3. Title: Adversarial Meta Sampling for Multilingual Low-Resource Speech Recognition\\nID: 4. Title: Generative Pre-Training for Speech with Autoregressive Predictive Coding\\nID: 5. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models<|eot_id|>'},\n",
       " 7312: {'predict': 'There is a growing body of work that explores the use of public resources for summarization. This includes the use of cross-lingual resources to train cross-lingual summarization models (@CITE[1]@; @CITE[3]@; ;, and the use of parallel corpora to train machine translation models for summarization (Zhu et al., 2019; @CITE[2]@. In this work, we use a combination of monolingual and cross-lingual resources to train a model for multilingual summarization. Additionally, we focus on summarization in the context of a specific domain, which is a key aspect of our work.\\nReferences:ID: 1. Title: CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs\\nID: 2. Title: WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization\\nID: 3. Title: PMIndia -- A Collection of Parallel Corpora of Languages of India',\n",
       "  'ground_truth': '\\n\\nExploiting public resources usually results in same-language document-summary pairs as introduced above. Zhu et al. (2019) translated summaries in monolingual datasets into another language while ensuring that the round-trip translation of the summary did not deviate from the original one. The CrossSum corpus (@CITE[3]@) was derived from XL-Sum by thresholding similarity between cross-lingual summaries, and pairing a summary with the document of a parallel summary. The naturally arising cross-lingual datasets include WikiLingua which aligned summaries and documents using image pivots (@CITE[1]@ and EUR-Lex-Sum from a platform hosting legal documents (Aumiller et al., 2022). Our PMIndiaSum falls in the category of using public article-headline pairs. The data itself is massively cross-lingual due to the multilingualism of the published news. We greatly benefit from the materials released by the PMIndia parallel corpus (@CITE[2]@.\\nReferences:ID: 1. Title: CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs\\nID: 2. Title: WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization\\nID: 3. Title: PMIndia -- A Collection of Parallel Corpora of Languages of India<|eot_id|>'},\n",
       " 963: {'predict': 'Backdoor attacks on NLP models have been studied in recent years. @CITE[8]@ and @CITE[3]@ inject backdoors into language models by inserting trigger words or phrases into the input. @CITE[5]@ introduce the concept of \"Universal Litmus Patterns\" (ULPs) for detecting backdoor attacks in CNNs. @CITE[4]@ propose a universal adversarial trigger as the backdoor trigger to attack video recognition models. @CITE[1]@ introduce a backdoor attack on video recognition models by adding invisible perturbations to the input images. @CITE[2]@ propose a Trojan attack on language models by injecting trigger words into the input. @CITE[6]@ introduce a backdoor attack on language models by injecting trigger words into the input. @CITE[9]@ propose a backdoor attack on language models by injecting trigger words into the input. @CITE[7]@ propose a backdoor attack on language models by injecting trigger words into the input. However, these attacks are mostly focused on image and video data. The backdoor attack on NLP models is still in its infancy. In this paper, we propose a backdoor attack on PLMs by injecting trigger words into the input. Our backdoor attack is more stealthy and effective than existing backdoor attacks on NLP models.\\nReferences:ID: 1. Title: Categorical Reparameterization with Gumbel-Softmax\\nID: 2. Title: Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation\\nID: 3. Title: Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs\\nID: 4. Title: Clean-Label Backdoor Attacks on Video Recognition Models\\nID: 5. Title: Neural Trojans\\nID: 6. Title: Neural Lattice Language Models\\nID: 7. Title: Modeling Semantic Compositionality with Sememe Knowledge\\nID: 8. Title: Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs\\nID: 9. Title: Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy',\n",
       "  'ground_truth': '\\n\\nRecently, backdoor attacks (@CITE[6]@, also known as trojan attacks (, have drawn considerable attention because of their serious security threat to deep neural networks. Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (@CITE[6]@; ; @CITE[1]@; @CITE[8]@. Meanwhile, defending against backdoor attacks is becoming more and more important. Researchers also have proposed diverse backdoor defense methods (@CITE[9]@; @CITE[3]@; @CITE[7]@; @CITE[1]@; @CITE[2]@. Considering that the manifest triggers like a patch can be easily detected and removed by defenses, @CITE[5]@ further impose the invisibility requirement on triggers, aiming to make the trigger-embedded poisoned examples indistinguishable from benign examples. Some invisible triggers such as random noise (@CITE[5]@ and reflection (@CITE[6]@ are presented. The research on backdoor attacks in NLP is still in its infancy. \\nFailed. Too much noise.\\nReferences:ID: 1. Title: Categorical Reparameterization with Gumbel-Softmax\\nID: 2. Title: Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation\\nID: 3. Title: Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs\\nID: 4. Title: Clean-Label Backdoor Attacks on Video Recognition Models\\nID: 5. Title: Neural Trojans\\nID: 6. Title: Neural Lattice Language Models\\nID: 7. Title: Modeling Semantic Compositionality with Sememe Knowledge\\nID: 8. Title: Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs\\nID: 9. Title: Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy<|eot_id|>'},\n",
       " 5625: {'predict': 'Multilingual and multi-task training is a popular paradigm for training NLP models (@CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[4]@; @CITE[5]@). In this setting, the model is trained on a set of tasks and languages, which enables it to leverage cross-lingual transfer and knowledge sharing between tasks (@CITE[6]@; @CITE[5]@). This has been shown to be particularly beneficial for low-resource languages, where the availability of annotated data is limited. In the context of emotion detection, a multilingual and multi-task approach has been explored by (@CITE[5]@) and (, who train a single model on multiple languages and tasks. In this work, we explore the benefits of training a separate model for each language, and examine the benefits of sharing knowledge between languages via a multi-task setting.\\nReferences:ID: 1. Title: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\\nID: 2. Title: Adversarial Alignment of Multilingual Models for Extracting Temporal Expressions from Text\\nID: 3. Title: A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios\\nID: 4. Title: NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis\\nID: 5. Title: An Overview of Multi-Task Learning in Deep Neural Networks\\nID: 6. Title: Gradient Surgery for Multi-Task Learning',\n",
       "  'ground_truth': '\\n\\nA growing number of research projects investigates multilingual training to cover a variety of languages, including low-resource languages (@CITE[1]@; ; @CITE[4]@; FitzGerald et al., 2022). In the context of low-resource sentiment analysis, @CITE[6]@ recently used the cross-lingual transfer score between pairs of languages to select source languages for multilingual training. Our approach differs from these works in that we investigate language interactions from a global optimization perspective. Considering each language as a separate task, multilingual training can be treated as a multi-task learning (MTL) problem (@CITE[5]@. A line of existing work utilizes gradient-based techniques to improve multi-task learning (; @CITE[2]@; @CITE[3]@; Wang et al., 2020). They show that negative cosine similarity between gradients leads to negative interference for MTL optimization, and projecting out the conflicting gradients can improve the optimization dynamics. Our work follows this insightful observation. However, in contrast to their work, we propose to leverage multilingual gradients for language grouping to ensure that gradients are aligned in each language group.\\nReferences:ID: 1. Title: XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\\nID: 2. Title: Adversarial Alignment of Multilingual Models for Extracting Temporal Expressions from Text\\nID: 3. Title: A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios\\nID: 4. Title: NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis\\nID: 5. Title: An Overview of Multi-Task Learning in Deep Neural Networks\\nID: 6. Title: Gradient Surgery for Multi-Task Learning<|eot_id|>'},\n",
       " 1054: {'predict': 'Prior work has shown that zero-shot LMs can be competitive with few-shot LMs. For example, @CITE[4]@ showed that zero-shot prompts can outperform few-shot prompts on some tasks. @CITE[6]@ and @CITE[2]@ showed that zero-shot LMs can be competitive with few-shot LMs on many tasks. More recently, @CITE[5]@ showed that instruction-tuned LMs can surpass few-shot GPT-3 on many tasks. However, @CITE[1]@ showed that the performance of few-shot LMs is highly dependent on prompt engineering and hyperparameter tuning. @CITE[3]@ showed that the performance of few-shot LMs can be highly variable. These findings suggest that few-shot LMs may not be as robust as zero-shot LMs.\\nReferences:ID: 1. Title: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: WARP: Word-level Adversarial ReProgramming\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: Finetuned Language Models Are Zero-Shot Learners\\nID: 6. Title: True Few-Shot Learning with Language Models',\n",
       "  'ground_truth': '\\n\\nZero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (@CITE[5]@; ; @CITE[1]@. However, popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (; ; @CITE[6]@;,b). demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (; ; Schick and Schütze, 2020a,b,c; @CITE[6]@, an intermediate learning phase (Ye et al., 2021), or calibration (@CITE[4]@, though most assume access to a validation set (@CITE[2]@. Recent work suggests it may be possible to finetune language models in order to improve their zero-shot and few-shot capabilities on a large swathe of tasks (@CITE[4]@; @CITE[3]@.\\nReferences:ID: 1. Title: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: WARP: Word-level Adversarial ReProgramming\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: Finetuned Language Models Are Zero-Shot Learners\\nID: 6. Title: True Few-Shot Learning with Language Models<|eot_id|>'},\n",
       " 734: {'predict': 'Pretrained language models, such as BERT (@CITE[1]@, RoBERTa (@CITE[5]@, and XLM-R (@CITE[4]@, are trained on large amounts of unlabeled data and have achieved state-of-the-art results on various downstream tasks. However, the large size of these models makes it difficult to deploy them to low-resource devices. Therefore, many researchers have proposed methods to fine-tune the pretrained language models on downstream tasks. For example, Hu et al. (2019) proposed a method to fine-tune BERT on downstream tasks by inserting task-specific adapter layers. @CITE[6]@ proposed a method to fine-tune BERT on downstream tasks by adding a lightweight head module. @CITE[7]@ proposed a method to fine-tune BERT on downstream tasks by injecting task-specific knowledge into the pretrained language model. @CITE[2]@ proposed a method to fine-tune BERT on downstream tasks by using a parallel sequence decoding algorithm. @CITE[3]@ proposed a method to fine-tune BERT on downstream tasks by inserting task-specific adapter layers into the pretrained language model. @CITE[2]@ proposed a method to fine-tune BERT on downstream tasks by using a multi-task learning algorithm. However, the above methods require the pretrained language model to be fine-tuned on downstream tasks, which requires a large amount of labeled data and is difficult to be deployed to low-resource devices.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nID: 3. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 4. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 5. Title: AdapterHub: A Framework for Adapting Transformers\\nID: 6. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 7. Title: Incorporating BERT into Parallel Sequence Decoding with Adapters',\n",
       "  'ground_truth': '\\n\\nFine-tuning pretrained large scale language models has proven its effectiveness on a wide range of NLP tasks (@CITE[5]@; @CITE[7]@; @CITE[5]@; Brown et al., 2020). However, fine-tuning requires a new set of weights for each task, which is parameter inefficient. Adapter-based tuning is proposed to deal with this problem (Houlsby et al., 2019). Most previous work has demonstrated that it achieves comparable performance to fine-tuning (@CITE[6]@; @CITE[5]@,a,c; Ruckle et al., 2020; @CITE[6]@; @CITE[7]@.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nID: 3. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 4. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 5. Title: AdapterHub: A Framework for Adapting Transformers\\nID: 6. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 7. Title: Incorporating BERT into Parallel Sequence Decoding with Adapters<|eot_id|>'},\n",
       " 537: {'predict': 'Context-aware text generation has been explored in a variety of domains. @CITE[4]@; @CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[1]@; ; ; @CITE[1]@; ; @CITE[1]@; @CITE[5]@; ; ; ; propose to integrate context information into the language model by using topic modeling, attention mechanism, or hierarchical recurrent neural network. In the dialogue generation domain, @CITE[4]@; ; @CITE[5]@; @CITE[1]@; propose to incorporate context information into the language model by using the dynamic context, attention mechanism, or hierarchical recurrent neural network. In the query suggestion domain, @CITE[2]@; propose to incorporate context information into the language model by using topic modeling and attention mechanism. In the text summarization domain, @CITE[5]@; @CITE[1]@ propose to incorporate context information into the language model by using the attention mechanism. In the machine translation domain, @CITE[3]@; propose to incorporate context information into the language model by using the dynamic context and hierarchical recurrent neural network. In the question answering domain, @CITE[5]@ propose to incorporate context information into the language model by using the attention mechanism. In the information retrieval domain, @CITE[3]@ propose to incorporate context information into the language model by using the dynamic context.\\nReferences:ID: 1. Title: Topic Compositional Neural Language Model\\nID: 2. Title: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses\\nID: 3. Title: Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking\\nID: 4. Title: A Neural Conversational Model\\nID: 5. Title: A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion',\n",
       "  'ground_truth': '\\n\\nOur work is related to context-aware text generation (; ), which can be applied to many NLP tasks (Mangrulkar et al., 2018). For example, previous work has investigated language modeling with context information (@CITE[5]@; @CITE[2]@; @CITE[4]@), treating the preceding sentences as context. There are also studies on response generation for conversational systems (; ), where dialogue history is treated as a context. @CITE[1]@ introduced a neural model to generate long reviews from aspect-sentiment scores given the topics. proposed a model to predict the next sentence given the previous sentences in a dialogue session. @CITE[3]@ presented a hierarchical recurrent encoder-decoder model to encode dialogue context. Our work is the first to explore context information in the text style transfer task.\\nReferences:ID: 1. Title: Topic Compositional Neural Language Model\\nID: 2. Title: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses\\nID: 3. Title: Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking\\nID: 4. Title: A Neural Conversational Model\\nID: 5. Title: A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware Query Suggestion<|eot_id|>'},\n",
       " 3549: {'predict': 'Spurious cues are known to be a major issue in NLP (@CITE[8]@; @CITE[6]@; @CITE[5]@; @CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[4]@. They are often exploited by models to achieve high performance, which makes the model less robust and hard to generalize to real-world scenarios. In this paper, we focus on spurious cues in multi-turn dialogue datasets, including dialogue context, speaker identity, and dialogue history. Spurious cues in dialogue context include words, phrases, and entities that appear frequently in the context. For example, in a movie-related dialogue dataset, the word \"movie\" may appear frequently in the context. Speaker identity is another spurious cue in dialogue datasets. Models may perform well on speaker identification tasks, but this does not mean that models can generalize to other speaker identities. Dialogue history is another spurious cue in dialogue datasets. It is often used as a cue for the next utterance in a dialogue. However, the next utterance may not always follow the previous utterance, which makes it a spurious cue.\\nReferences:ID: 1. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 2. Title: Hierarchical Recurrent Attention Network for Response Generation\\nID: 3. Title: Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments\\nID: 7. Title: Natural Adversarial Examples\\nID: 8. Title: Don\\'t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference',\n",
       "  'ground_truth': \"\\n\\nDeep neural networks have achieved tremendous progress on many NLP tasks (@CITE[6]@; ; ; Veliˇ ckovi´ c et al., 2017; ) with the emergence of large language models like BERT (@CITE[1]@) and GPT-2 (@CITE[8]@). However, many recent NLP studies (@CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[6]@; ) have found that deep neural networks are prone to exploit spurious artifacts present in the data rather than learning the underlying task. In natural language inference (NLI), (@CITE[7]@) found that certain linguistic phenomena in the NLI benchmark datasets correlate well with certain classes. For example, by only looking at the hypothesis, simple classifier models can perform as well as the model using the full inputs (both hypothesis and premise). (@CITE[5]@) found that BERT achieves a performance close to human on the Argument Reasoning Comprehension Task (ARCT) with 77% accuracy (3% below human performance). However, they discover that the impressive performance is attributed to the exploitation of shortcuts in the dataset. (@CITE[3]@) analyzed annotator bias on NLP datasets and found that a model that uses only annotator identifiers can achieve a similar performance to one that uses the full data. In commonsense reasoning, (@CITE[4]@) has performed a systematic investigation over four commonsense-related tasks and found that most datasets experimented with are problematic, with models being prone to leveraging the non-robust features in the inputs to make decisions and do not generalize well to the overall tasks intended to be conveyed by the commonsense reasoning tasks and datasets. Inspired by these studies, our paper focuses on eliminating spurious correlations in task-oriented dialogue systems.\\nReferences:ID: 1. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 2. Title: Hierarchical Recurrent Attention Network for Response Generation\\nID: 3. Title: Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments\\nID: 7. Title: Natural Adversarial Examples\\nID: 8. Title: Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference<|eot_id|>\"},\n",
       " 4531: {'predict': 'Neuro-Symbolic computing (NeSy) aims at bridging the gap between symbolic and sub-symbolic representations by integrating logical rules and neural networks. In this field, various works have been proposed to combine neural networks with symbolic rules for improving the performance of neural networks and enhancing their interpretability. The most popular approaches are rule-based learning methods that utilize logical rules to enhance the learning ability of neural networks. For example, @CITE[7]@ propose an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. @CITE[8]@ propose a logic tensor network that integrates neural networks with first-order fuzzy logic to allow efficient learning from noisy data and reasoning with logical formulas. @CITE[4]@ propose a semi-supervised semantic role labeling method that explicitly enforces syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component. @CITE[1]@ propose a semantic loss function that bridges between neural output vectors and logical constraints. In the field of neuro-symbolic computing, various approaches have been proposed to integrate neural networks with symbolic rules for improving the performance of neural networks and enhancing their interpretability. For example, @CITE[2]@ propose a probabilistic soft logic that integrates neural networks with probabilistic soft logic to model the boundary between neural and symbolic representations. @CITE[5]@ propose a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. @CITE[3]@ propose a logic tensor network that integrates neural networks with first-order fuzzy logic to allow efficient learning from noisy data and reasoning with logical formulas. @CITE[6]@ propose a neural logic network that integrates neural networks with first-order fuzzy logic to allow efficient learning from noisy data and reasoning with logical formulas. In this paper, we propose a novel approach that combines neural networks with logical rules to enhance the interpretability of neural networks and improve the performance of neural networks. Our approach can be used to improve the performance of neural networks and enhance their interpretability in various applications, such as image classification, semantic role labeling, and machine translation.\\nReferences:ID: 1. Title: Harnessing Deep Neural Networks with Logic Rules\\nID: 2. Title: Logic Tensor Networks for Semantic Image Interpretation\\nID: 3. Title: Towards Semi-Supervised Learning for Deep Semantic Role Labeling\\nID: 4. Title: A Semantic Loss Function for Deep Learning with Symbolic Knowledge\\nID: 5. Title: From Statistical Relational to Neuro-Symbolic Artificial Intelligence\\nID: 6. Title: NeuPSL: Neural Probabilistic Soft Logic\\nID: 7. Title: Neural Probabilistic Logic Programming in DeepProbLog\\nID: 8. Title: Logic Tensor Networks',\n",
       "  'ground_truth': '\\n\\nNeuro-Symbolic computing (NeSy) is an active area of research that aims to incorporate logic-based reasoning with neural computation. This field contains a plethora of different neural symbolic methods and techniques. The methods that closely relate to our line of work seek to enforce constraints on the output of a neural network (@CITE[6]@; @CITE[8]@; ; @CITE[1]@; @CITE[3]@; Nandwani et al., 2019). For a more in-depth introduction, we refer the reader to these excellent recent surveys: @CITE[4]@ and @CITE[7]@. These methods, although powerful, are either specific to the domain they work in, do not use the same soft logic formulation, have not been designed for unsupervised systems, or have not been used for dialog structure induction. Finally, our method is most closely related to the novel NeSy approaches of Neural Probabilistic Soft Logic (NeuPSL) (@CITE[2]@), DeepProbLog (DPL) (@CITE[5]@), and Logic Tensor Networks (LTNs) (). LTNs instantiate a model which forwards neural network predictions into functions representing symbolic relations with real-valued or fuzzy logic semantics, and DeepProbLog uses the output of a neural network to specify probabilities of events. The mathematical formulation of LTNs and DPL differs from our underlying soft logic distribution. NeuPSL unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks through Probabilistic Soft Logic (PSL). Our method uses a NeuPSL formulation; however, we introduce a novel variation to the soft logic formulation, develop theory for unsupervised tasks, introduce the whole system in Tensorflow, and apply it to dialog structure induction.\\nReferences:ID: 1. Title: Harnessing Deep Neural Networks with Logic Rules\\nID: 2. Title: Logic Tensor Networks for Semantic Image Interpretation\\nID: 3. Title: Towards Semi-Supervised Learning for Deep Semantic Role Labeling\\nID: 4. Title: A Semantic Loss Function for Deep Learning with Symbolic Knowledge\\nID: 5. Title: From Statistical Relational to Neuro-Symbolic Artificial Intelligence\\nID: 6. Title: NeuPSL: Neural Probabilistic Soft Logic\\nID: 7. Title: Neural Probabilistic Logic Programming in DeepProbLog\\nID: 8. Title: Logic Tensor Networks<|eot_id|>'},\n",
       " 5551: {'predict': 'RNNs can be related to finite automata, which are a fundamental model of computation in computer science. The connection between RNNs and finite automata has been studied in several works, mostly from a theoretical perspective. For example, @CITE[3]@ show that finite precision RNNs with ReLU activation can learn to recognize regular languages. @CITE[2]@ show that RNNs can be seen as a type of finite automata, and characterize the types of computation that different RNN architectures can perform. @CITE[1]@ show that RNNs can be used to recognize regular languages, and propose a formal hierarchy of RNN architectures based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. @CITE[3]@ show that QRNNs, which are a type of RNNs that use a QRNN cell instead of the standard RNN cell, are equivalent to deterministic finite automata. In our work, we use the connection between RNNs and finite automata to study the capacity of QRNNs in recognizing regular languages.\\nReferences:ID: 1. Title: On the Practical Computational Power of Finite Precision RNNs for Language Recognition\\nID: 2. Title: Sequential Neural Networks as Automata\\nID: 3. Title: A Formal Hierarchy of RNN Architectures',\n",
       "  'ground_truth': '\\n\\nTo the best of our knowledge, the only existing connection between RNNs and weighted automata was made by Peng et al. (2018), where the authors connect the recurrences analogous to Eq. (2) of different RNN variants to the process of computing the probability of a string under a general PFSA. With this, they are able to show that the hidden states of an RNN can be used to store the probability of the input string, which can be used to upper-bound the representational capacity of specific RNN variants. Importantly, the interpretation of the hidden state is different to ours: Rather than tracking the current state of the PFSA, Peng et al. (2018)’s construction stores the distribution over all possible states. While this suggests a way of simulating PFSAs, the translation of the probabilities captured in the hidden state to the probability under an RNN LM is not straightforward. @CITE[3]@, @CITE[1]@ and @CITE[2]@ consider the representational capacity of saturated RNNs, whose parameters take their limiting values ˘8 to make the updates to the hidden states discrete. In this sense, their formal model is similar to ours. However, rather than considering the probabilistic representational capacity, they consider the flexibility of the update mechanisms of the variants in the sense of their long-term dependencies and the number of values the hidden states can take as a function of the string length. Connecting the assumptions of saturated activations with the results of Peng et al. (2018), they establish a hierarchy of different RNN architectures based on whether their update step is finite-state and whether the hidden state can be used to store arbitrary amounts of information. Analogous to our results, they show that Elman RNNs are finite-state while some other variants such as LSTMs are provably more expressive.\\nReferences:ID: 1. Title: On the Practical Computational Power of Finite Precision RNNs for Language Recognition\\nID: 2. Title: Sequential Neural Networks as Automata\\nID: 3. Title: A Formal Hierarchy of RNN Architectures<|eot_id|>'},\n",
       " 154: {'predict': 'We focus on the ability of models to generalize to novel compositional structures, i.e., new combinations of known components. This is a fundamental aspect of human cognition, and has been studied extensively in cognitive science (@CITE[6]@). In NLP, we have seen a recent surge of interest in compositional generalization. @CITE[5]@ propose SCAN, a dataset of navigation instructions for which a model must learn to generalize to novel combinations of known components. Several follow-up studies have explored the capabilities of different architectures on this dataset (@CITE[3]@; @CITE[4]@; @CITE[2]@; @CITE[1]@). Another line of work has explored the use of tree-based models to improve generalization. These models are trained on a dataset of examples with syntactic structure, and then evaluated on a test set with novel structures. Tree-based models have been shown to improve performance on a variety of tasks, including machine translation ( and semantic parsing (@CITE[2]@). Our work is most closely related to the study of compositional generalization in language models. We explore the use of tree-based models to improve generalization in LMs, and we show that these models can be used to improve performance on a variety of tasks.\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Recurrent Neural Network Grammars\\nID: 3. Title: Neural Module Networks\\nID: 4. Title: Inferring and Executing Programs for Visual Reasoning\\nID: 5. Title: On Tree-Based Neural Sentence Modeling\\nID: 6. Title: Good-Enough Compositional Data Augmentation',\n",
       "  'ground_truth': '\\n\\nWhile neural networks trained on large datasets have led to signiﬁcant improvements across a wide range of NLP tasks, training them to generalize by learning the compositional structure of language re- mains a challenging open problem. Notably, @CITE[6]@ propose an inﬂuential dataset (SCAN) to evaluate the systematic compositional- ity of neural models and ﬁnd that they often fail to generalize compositionally. One approach to encouraging compositional be- havior in neural models is by incorporating compo- sitional structures such as parse trees or programs directly into a network’s computational graph (@CITE[3]@; ; ; @CITE[2]@; @CITE[5]@. While effective on certain domains such as visual question answering, these approaches usu- ally rely on intermediate structures predicted from pipelined models, which limits their applicability in general. Further, it is an open question as to whether such putatively compositional models re- sult in signiﬁcant empirical improvements on many NLP tasks (. Expressive parameterizations over high dimen- sional input afforded by neural networks contribute to their excellent performance in high resource set- tings; however, such ﬂexible parameterizations can easily lead to a model’s memorizing—i.e., over- ﬁtting to—long segments of text, instead of rely- ing on the appropriate subparts of segments. An- other approach to encouraging compositionality in richly-parameterized neural models, then, is to augment the training data with more examples. Ex- isting work in this vein include SwitchOut (, which replaces a word in a sentence with a random word from the vocabulary, GECA (@CITE[1]@, which creates new examples by switching subparts that occur in similar contexts, and TMix (@CITE[4]@, which interpolates between hidden states of neural models for text classiﬁcation. We compare to these approaches to our proposed approach in this paper.\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Recurrent Neural Network Grammars\\nID: 3. Title: Neural Module Networks\\nID: 4. Title: Inferring and Executing Programs for Visual Reasoning\\nID: 5. Title: On Tree-Based Neural Sentence Modeling\\nID: 6. Title: Good-Enough Compositional Data Augmentation<|eot_id|>'},\n",
       " 6894: {'predict': 'To assess the factual consistency of the generated text, @CITE[3]@ introduced the Q2 metric, which uses question generation and question answering to evaluate the factual consistency of the generated text. @CITE[2]@ proposed the BEGIN benchmark, which evaluates the attribution of the generated text to the knowledge source. @CITE[4]@ proposed a human evaluation method to assess the factual consistency of the generated text. @CITE[5]@ proposed a method to evaluate the factual consistency of the generated text based on the retrieval results. @CITE[1]@ proposed a method to evaluate the factual consistency of the generated text based on the knowledge graph. However, these methods do not consider the factuality of the knowledge base. In this paper, we propose a novel method to evaluate the factuality of the generated text based on the knowledge base and the retrieval results.\\nReferences:ID: 1. Title: $Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering\\nID: 2. Title: Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark\\nID: 3. Title: AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators\\nID: 4. Title: Active Retrieval Augmented Generation\\nID: 5. Title: Assessing The Factual Accuracy of Generated Text',\n",
       "  'ground_truth': '\\n\\nFEVER (@CITE[4]@ is a popular crowdsourced dataset that compares claims against evidence retrieved from Wikipedia, and was extended to dialogues by. The state-of-the-art system on this dataset (@CITE[2]@ has an accuracy of 81% when compared against human labels. Q2 (@CITE[3]@ uses question answering and natural language inference models to evaluate the factuality of dialogue agents. @CITE[5]@ compare this and several other automatic metrics to human evaluation and find that automatic metrics fall significantly behind human performance, and rely on spurious correlations. Concurrently and similarly to our work, @CITE[1]@ break down long system outputs to claims and evaluate their factuality using retrieval. Their best model has a significant 13% error rate on individual claims. Given the high error rates for these automatic metrics, our evaluation methodology (Section 5) incorporates human judgment where needed. Approaches like and TripleScore ( only consider simple entity-relations between retrieved documents and system outputs, while we generalize that notion to all forms of facts.\\nReferences:ID: 1. Title: $Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering\\nID: 2. Title: Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark\\nID: 3. Title: AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators\\nID: 4. Title: Active Retrieval Augmented Generation\\nID: 5. Title: Assessing The Factual Accuracy of Generated Text<|eot_id|>'},\n",
       " 3868: {'predict': 'Analyzing semantic change of text over time is an important research area. Previous work in this area can be roughly divided into two categories: one is about the change of word meanings over time (@CITE[4]@; @CITE[5]@; @CITE[6]@; @CITE[3]@; @CITE[1]@, and the other is about the performance degradation of language models over time (@CITE[7]@; @CITE[2]@; @CITE[2]@. The former studies the change of word meanings over time, and the latter studies how language models perform over time. However, the two are closely related: the performance degradation of language models is due to the change of word meanings over time. In this paper, we focus on the latter and propose a method to alleviate the performance degradation of language models over time.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora\\nID: 3. Title: Mind the Gap: Assessing Temporal Generalization in Neural Language Models\\nID: 4. Title: Dynamic Contextualized Word Embeddings\\nID: 5. Title: Back to the Future -- Sequential Alignment of Text Representations\\nID: 6. Title: Dynamic Contextualized Word Embeddings\\nID: 7. Title: Temporal Effects on Pre-trained Models for Language Processing Tasks',\n",
       "  'ground_truth': '\\n\\nAnalyzing semantic change of text over time has been of great interest since the pioneering work by Hamilton et al. (2016b) and others (; Rudolph and Blei, 2018; Martinc et al., 2020; @CITE[5]@. However, its influence on downstream task performance has only recently gained attention. Most importantly, the advent of contextualized word embeddings and large pretrained language models has led researchers to re-evaluate the role of temporality in language modeling (@CITE[4]@; ; @CITE[3]@; Kulkarni et al., 2021) and text classification (@CITE[6]@; ; Röttger and @CITE[3]@; Agarwal and @CITE[7]@. The performance degradation due to temporal factors has been confirmed in several studies and across multiple domains. @CITE[1]@ analyzed the temporal performance degradation of age and gender classification models based on user’s social media posts. Based on features derived from Latent Dirichlet Allocation and word embeddings, they find that models perform best if test and training data come from the same time span. investigated temporal effects on Hate Speech detection in Italian social media over the period of five months. Their results suggest that models trained on data temporally closer to the test data perform better with transformer-based models. studied semantic shifts in social media and proposed a dataset annotated with words that have undergone a semantic shift over the past two years. Loureiro et al. (2022a) focus on Twitter as a text domain and contribute pretrained language models which have been further trained on time-specific data from Twitter. @CITE[6]@ proposed to use sequential subspace alignment (SSA) to adapt contextualized word embeddings for language change over time. Their results suggest that SSA applied on past data is able to outperform baselines that have access to data from all time steps. Röttger and Pierre- humbert (2021) compared time-agnostic domain adaptation with temporal domain adaptation which considers the temporal order of the data. They found that, while temporal adaptation clearly outperforms domain adaptation in language modeling, this does not necessarily translate onto downstream classification performance due to updated tokens not being relevant for the task. @CITE[7]@ found the temporal model performance deterioration to be less significant when using language representations which have been pretrained on temporally closer data. Finally, @CITE[2]@ have made the effort of conducting a large-scale study of temporal mis- alignment, the generalized scenario where training and evaluation data are drawn from different periods of time. Across multiple NLP classification tasks and domains, they identify performance degradation with varying degrees but with social media and news being the most affected domains.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora\\nID: 3. Title: Mind the Gap: Assessing Temporal Generalization in Neural Language Models\\nID: 4. Title: Dynamic Contextualized Word Embeddings\\nID: 5. Title: Back to the Future -- Sequential Alignment of Text Representations\\nID: 6. Title: Dynamic Contextualized Word Embeddings\\nID: 7. Title: Temporal Effects on Pre-trained Models for Language Processing Tasks<|eot_id|>'},\n",
       " 7040: {'predict': 'The online safety problem is a long-standing issue in the online community. Early works focus on detecting toxic comments, such as personal attacks (@CITE[4]@), constructive comments (@CITE[2]@), and hate speech (@CITE[3]@). However, the existing methods mainly focus on the content of comments, and ignore the context in which the comments are posted. Recently, the online safety problem has been extensively studied in the field of NLP, with a focus on the safety of the models themselves, such as detecting toxic language (@CITE[1]@) and mitigating toxic responses (@CITE[1]@). In this work, we focus on the online safety problem in the context of online dialogue, and study the safety of the responses in the context of online dialogue. We propose a new task, safe response detection, which aims to detect whether a response is safe or not, and a new dataset, SafeResponse, which is a large-scale dataset for safe response detection. We also propose a new framework, SafeDialogue, which can detect the safety of responses in online dialogue. Our experiments show that SafeDialogue can achieve better performance than the state-of-the-art methods on the SafeResponse dataset.\\nReferences:ID: 1. Title: Ex Machina: Personal Attacks Seen at Scale\\nID: 2. Title: Classifying Constructive Comments\\nID: 3. Title: HateBERT: Retraining BERT for Abusive Language Detection in English\\nID: 4. Title: Challenges in Detoxifying Language Models',\n",
       "  'ground_truth': '\\n\\nOver the past decade, a significant branch of research has been motivated by the goal of improving the safety of online conversations. A popular approach is developing classifiers to detect whether individual comments contain toxicity, personal attacks, hate speech, and other unhealthy attributes of online conversations (@CITE[1]@; Davidson et al., 2017). Classifiers for positive attributes such as constructiveness ( have also been explored. The underlying models for detecting these attributes have also grown in sophistication, from linear models such as support vector machines ( to more modern approaches that rely on deep learning architectures like CNNs (Gambäck and @CITE[4]@ and, more recently, transformers (@CITE[3]@; Zhou et al., 2021). With the rapid progress of chatbots, online chat support, and digital assistants, there has been a growing focus on multi-turn dialogue, with the goal of improving conversational agents and making them more robust to adversarial users (; @CITE[2]@. Another recent area of research has focused on reducing the toxicity generated by large language models, although this risks diminished performance for the language of marginalized groups (.\\nReferences:ID: 1. Title: Ex Machina: Personal Attacks Seen at Scale\\nID: 2. Title: Classifying Constructive Comments\\nID: 3. Title: HateBERT: Retraining BERT for Abusive Language Detection in English\\nID: 4. Title: Challenges in Detoxifying Language Models<|eot_id|>'},\n",
       " 4308: {'predict': 'Multi-task learning (MTL) with pre-trained language models (PLMs) has shown promising results in many areas (@CITE[3]@; @CITE[5]@; @CITE[4]@; @CITE[2]@; @CITE[1]@;. In this work, we focus on MTL for PLMs in the context of dialogue generation. We propose a simple yet effective approach to enhance the dialogue generation performance of PLMs by learning to generate a variety of dialogue types. We conduct experiments on three dialogue generation tasks and show that our approach can improve the performance of PLMs in dialogue generation.\\nReferences:ID: 1. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 2. Title: Finetuned Language Models Are Zero-Shot Learners\\nID: 3. Title: Cross-Task Generalization via Natural Language Crowdsourcing Instructions\\nID: 4. Title: DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation\\nID: 5. Title: Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks',\n",
       "  'ground_truth': '\\n\\nMulti-task learning is a common paradigm to transfer knowledge from multiple related tasks to enhance generalization capacity and has shown promising results in a variety of NLP tasks (; @CITE[4]@). Large-scale pre-trained language models (PLMs) have presented brand-new difficulties for multi-task learning. @CITE[2]@ propose pre-finetuning which refines the pre-trained representations through massively multi-task learning. In spite of its efficiency, pre-finetuning may result in catastrophic forgetting of the pre-training task. To alleviate this issue, propose multi-task pre-training which bridges the gap between pre-training and finetuning data distributions. T0 () is an early attempt to induce the zero-shot generalization capability of PLMs through explicit multi-task learning, which converts NLP tasks into a manually-collected prompted form. Another prevalent paradigm in multi-task learning using PLMs is instruction tuning, in which the PLMs encode task-specific instructions together with input and produce task output (@CITE[3]@; ; ). Despite promising results, these methods may suffer from the negative transfer problem. Researchers have resorted to parameter-efficient methods which allocate separate adapters for each task (@CITE[1]@), and compositional modules which only activate relevant parts of the models (@CITE[5]@). Our method is orthogonal to earlier efforts in that it attempts to mitigate the unexplored negative transfer problem in auto-regressive decoding.\\nReferences:ID: 1. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 2. Title: Finetuned Language Models Are Zero-Shot Learners\\nID: 3. Title: Cross-Task Generalization via Natural Language Crowdsourcing Instructions\\nID: 4. Title: DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation\\nID: 5. Title: Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks<|eot_id|>'},\n",
       " 5034: {'predict': 'Advances in social and emotional commonsense reasoning have been made in the areas of story understanding (@CITE[1]@; @CITE[3]@), event reasoning (@CITE[4]@; @CITE[5]@; ), social conversation analysis (@CITE[2]@; ), and social reasoning (@CITE[6]@). However, the above-mentioned tasks are limited in terms of their scope, as they either focus on specific aspects of commonsense reasoning (e.g., story understanding) or target a specific domain of commonsense knowledge (e.g., social reasoning). In contrast, our work focuses on developing a general-purpose commonsense knowledge graph for reasoning about a wide range of everyday situations.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 3. Title: GLUCOSE: GeneraLized and COntextualized Story Explanations\\nID: 4. Title: Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations\\nID: 5. Title: SocialIQA: Commonsense Reasoning about Social Interactions\\nID: 6. Title: COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs',\n",
       "  'ground_truth': '\\n\\nMost relevant to our work are recent advances in social and emotional commonsense reasoning using language models. Specifically, prior methods have used finetuning of language models such as BERT (; @CITE[4]@ and GPT-2 (Radford et al.) to model events and the emotional reactions caused by everyday events (@CITE[6]@, 2018; Sap et al., 2019b; Bosselut et al., 2019; @CITE[3]@; @CITE[3]@; @CITE[1]@ as well as predicting empathy, condolence, or prosocial outcomes (; Kumano et al.; ; Zhou and Jurgens, 2020; @CITE[5]@. Understanding the emotional reactions elicited by events is a challenging task for many NLP systems, as it requires commonsense knowledge and extrapolation of meanings beyond the text alone. Prior works use commonsense knowledge graphs to infer and automatically generate commonsense knowledge of emotional reactions and reasoning about social interactions (,b; Bosselut et al., 2019;. However, there are still many under-explored challenges in developing systems that have social intelligence and the ability to infer states between people (@CITE[2]@. In contrast to previous works, we present a task for reasoning between pairs of stories, beyond predicting social commonsense features of texts alone. Our work builds on top of prior work by developing a framework around empathic resonance in personal narratives in addition to assessing the human effect of AI-retrieved stories on empathic response beyond automatic metrics. Unlike previous works, our human evaluation is a full user study to see how the model performs given a story that the users told themselves, which is much more aligned with real-world impact.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 3. Title: GLUCOSE: GeneraLized and COntextualized Story Explanations\\nID: 4. Title: Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations\\nID: 5. Title: SocialIQA: Commonsense Reasoning about Social Interactions\\nID: 6. Title: COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs<|eot_id|>'},\n",
       " 6509: {'predict': 'Multilingual PL pre-training has been gaining attention recently. The main focus of the previous work is on pre-training on a large corpus of multilingual code in various programming languages and translating it to different languages. For example, PLBART (@CITE[4]@ is a multilingual pre-trained model that is pre-trained on a large corpus of Java and Python functions and associated NL text via denoising autoencoding. PLBART is then fine-tuned on a set of multilingual code translation tasks. In a similar way, CodeT5 (@CITE[5]@ is pre-trained on a large corpus of multilingual code snippets and associated NL comments via denoising autoencoding. CodeT5 is then fine-tuned on a set of multilingual code generation tasks. Other work has focused on pre-training on multilingual NL code pairs. For example, MCoNaLa (@CITE[7]@ is a multilingual dataset of 896 NL-code pairs in three languages: Spanish, Japanese, and Russian. MCoNaLa is then used to evaluate the performance of a multilingual code generation system. Another work has focused on multilingual PL understanding and generation. For example, CodeGPT ( is a multilingual PL pre-trained model that is pre-trained on a large corpus of multilingual code in 10 programming languages and associated NL text via denoising autoencoding. CodeGPT is then fine-tuned on a set of multilingual code generation tasks. In a similar way, GPT-Neo (@CITE[6]@ is a multilingual PL pre-trained model that is pre-trained on a large corpus of multilingual code in 10 programming languages and associated NL text via denoising autoencoding. GPT-Neo is then fine-tuned on a set of multilingual code generation tasks. In this work, we also focus on multilingual PL pre-training, but we do not use any pre-training corpus of multilingual code. Instead, we focus on the task of code translation from multilingual NL to multilingual PL. We use a parallel corpus of multilingual NL and PL pairs to pre-train a multilingual PL pre-trained model that can translate NL to PL in different languages. We use a set of multilingual code generation tasks to evaluate the performance of our model. Our work is similar to that of @CITE[3]@, which uses a parallel corpus of multilingual NL and PL pairs to pre-train a multilingual PL pre-trained model that can translate NL to PL in different languages. However, @CITE[3]@ uses a different parallel corpus of multilingual NL and PL pairs and a different multilingual PL pre-trained model. For example, the parallel corpus used by @CITE[3]@ consists of multilingual NL and PL pairs from the Wikipedia corpus, while the parallel corpus used in our work consists of multilingual NL and PL pairs from the XGLUE corpus. In addition, the multilingual PL pre-trained model used by @CITE[3]@ is a multilingual encoder-only pre-trained model, while the multilingual PL pre-trained model used in our work is a multilingual encoder-decoder pre-trained model. Furthermore, we evaluate the performance of our model on a set of multilingual code generation tasks, while @CITE[3]@ evaluates the performance of their model on a set of multilingual code generation tasks.\\nReferences:ID: 1. Title: Unified Pre-training for Program Understanding and Generation\\nID: 2. Title: Evaluating Large Language Models Trained on Code\\nID: 3. Title: Program Synthesis with Large Language Models\\nID: 4. Title: MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages\\nID: 5. Title: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation\\nID: 6. Title: Unified Pre-training for Program Understanding and Generation\\nID: 7. Title: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation',\n",
       "  'ground_truth': '\\n\\nThe success of large-scale pre-training has led to impressive advances in computer programs. This line of research involves pre-training on multilingual PLs using bidirectional transformer encoders (; @CITE[4]@, casual transformer decoders (@CITE[3]@; @CITE[7]@; Fried et al., 2022; ; @CITE[5]@, and transformer encoder-decoder architectures (@CITE[1]@; @CITE[4]@; @CITE[2]@. Those with bidirectional encoder focus on program understanding tasks, such as code search (@CITE[6]@, while the encoder-decoder ones target at building unified LLMs for both program understanding and generation. We observe that a large body of pre-trained models for PL tend to scale up their parameters under the framework of causal language modeling, mainly focusing on program synthesis (@CITE[3]@; @CITE[7]@; Fried et al., 2022; ; @CITE[5]@. Nevertheless, all of these works are almost English-centric, posing significant challenges to coping with PL end-tasks in non-English scenarios.\\nReferences:ID: 1. Title: Unified Pre-training for Program Understanding and Generation\\nID: 2. Title: Evaluating Large Language Models Trained on Code\\nID: 3. Title: Program Synthesis with Large Language Models\\nID: 4. Title: MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages\\nID: 5. Title: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation\\nID: 6. Title: Unified Pre-training for Program Understanding and Generation\\nID: 7. Title: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation<|eot_id|>'},\n",
       " 6215: {'predict': 'Machine reading comprehension (MRC) has been a popular research topic in natural language processing. Early MRC datasets such as SQuAD (@CITE[1]@), HotPotQA (@CITE[3]@), and WikiReading (@CITE[4]@) focus on factual knowledge. Recently, new datasets have been proposed to evaluate the ability of models to perform commonsense reasoning, such as DREAM (@CITE[2]@), QuaRTz (@CITE[5]@), and Cosmos QA (@CITE[5]@). The latter two datasets require models to perform more open-ended reasoning tasks. However, these datasets are still limited to a single-turn setting. Our work is the first to explore the open-domain MRC in a multi-turn setting, which is more challenging than the single-turn setting. Moreover, we also explore the multi-turn setting for the first time in the context of commonsense reasoning.\\nReferences:ID: 1. Title: A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension\\nID: 2. Title: WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia\\nID: 3. Title: DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension\\nID: 4. Title: QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions\\nID: 5. Title: Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning',\n",
       "  'ground_truth': '\\n\\nDevising intelligent systems to answer questions on knowledge in text form has long been a challenge in Natural Language Understanding (NLU) (Welbl et al., 2018), and the MRC task plays an important part in evaluating NLU (@CITE[1]@). Abundant datasets have been proposed to advance research in MRC. One of the earliest work is MCTest (Richardson et al., 2013), a multiple-choice reading comprehension dataset. Following works have surged to advance more challenging text understanding with more complicated answer formats. Based on the answer format, MRC datasets can be grouped into four types: span extraction (@CITE[5]@; Welbl et al., 2018; @CITE[3]@), multiple-choice (; @CITE[4]@; ; @CITE[3]@), cloze style (@CITE[2]@, and free-form () answer.\\nReferences:ID: 1. Title: A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension\\nID: 2. Title: WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia\\nID: 3. Title: DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension\\nID: 4. Title: QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions\\nID: 5. Title: Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning<|eot_id|>'},\n",
       " 5613: {'predict': 'Visual commonsense reasoning (VCR) is a task that requires a model to make inferences based on the visual content of an image. The task is first introduced by @CITE[4]@, which requires the model to answer questions based on the visual content of an image and common sense knowledge. To make the task more challenging, several new datasets have been proposed to address different aspects of visual commonsense reasoning. For example, @CITE[5]@ propose the task of visual abductive reasoning, which requires the model to answer questions based on the visual content of an image and the commonsense knowledge. @CITE[3]@ propose the task of causal video QA, which requires the model to answer questions based on the causal relationship of two video frames. @CITE[1]@ propose the task of physical audio-visual commonsense reasoning, which requires the model to answer questions based on the physical properties and affordances of available objects, how they can be manipulated, and how they interact with other objects. @CITE[2]@ propose the task of visual commonsense story generation, which requires the model to generate a story based on the visual content of a still image. In this work, we propose a new task of visual commonsense story generation, which requires the model to generate a story based on the visual content of a video frame. To the best of our knowledge, we are the first to propose this task.\\nReferences:ID: 1. Title: Decoupled Weight Decay Regularization\\nID: 2. Title: The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning\\nID: 3. Title: From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering\\nID: 4. Title: PACS: A Dataset for Physical Audiovisual CommonSense Reasoning\\nID: 5. Title: VisualCOMET: Reasoning about the Dynamic Context of a Still Image',\n",
       "  'ground_truth': \"\\n\\nWith the goal of reasoning beyond visual recognition, the community has actively explored several visual commonsense reasoning tasks. @CITE[1]@ have proposed a visual commonsense reasoning benchmark to test if a model can identify an answer with rationale, given a question that requires a thorough understanding of images based on commonsense knowledge. @CITE[4]@ have proposed an abductive reasoning benchmark beyond literal image contents to evaluate the capacity of models to retrieve relevant inferences, localize evidence, and compare plausible inferences. @CITE[2]@ have introduced a video QA benchmark that requires understanding of evidences and commonsense reasoning over time. have proposed an audiovisual commonsense reasoning benchmark focusing on physical knowledge, which requires an understanding of multi-sensory inputs. However, these benchmarks evaluate models in a question answering format, limiting the evaluation of the models' capability to generate commonsense inferences. To address this, @CITE[3]@ have introduced a visual commonsense generation task with VCG, which asks models to generate inferences about potential antecedents, consequences, and the intents of people based on images and their corresponding textual descriptions. This requires models to generate free-form textual descriptions of inference results, which is more challenging and in line with the process of human reasoning (@CITE[5]@.\\nReferences:ID: 1. Title: Decoupled Weight Decay Regularization\\nID: 2. Title: The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning\\nID: 3. Title: From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering\\nID: 4. Title: PACS: A Dataset for Physical Audiovisual CommonSense Reasoning\\nID: 5. Title: VisualCOMET: Reasoning about the Dynamic Context of a Still Image<|eot_id|>\"},\n",
       " 2195: {'predict': 'Recent studies have shown that pretrained encoders are able to transfer knowledge across different languages, domains, and even different modalities (; @CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[7]@; @CITE[8]@; @CITE[1]@. In particular, @CITE[3]@ show that the pretrained encoders encode linguistic structures such as subjecthood, which are independent of the surface form of the input. @CITE[6]@ also show that pretrained encoders encode grammatical features such as morphosyntactic alignment. In this paper, we show that pretrained encoders encode a wide range of linguistic structures, including syntactic, semantic, and phonological features, which are not limited to the surface form of the input.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 3. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 4. Title: Cross-Lingual Ability of Multilingual BERT: An Empirical Study\\nID: 5. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 6. Title: Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models\\nID: 7. Title: What you can cram into a single vector: Probing sentence embeddings for linguistic properties\\nID: 8. Title: Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT',\n",
       "  'ground_truth': '\\n\\nMultilingual language models trained with masked language modeling objective (; Doddapaneni et al., 2021) have demonstrated a surprisingly strong cross-lingual transfer capability (, given the model is only trained with a mix of monolingual corpora. This leads to several studies investigating the source of the cross-lingual capability of multilingual models. An early common hypothesis was that the models take advantage of a common word-piece vocabulary across languages (@CITE[5]@; @CITE[6]@), which provides cross-lingual alignment signals to learn useful multilingual representations. However, this hypothesis has been questioned by recent studies (; @CITE[6]@) which show that shared word-pieces only play a minor role in the performance. These studies suggest that the model can exploit abstract structures of languages to learn shared multilingual representations. Another line of research suggests that the learning of transferable knowledge happens even in monolingual pretraining. Artetxe et al. (2020) showed that a Transformer encoder pretrained only on L1 exhibits strong cross-lingual transfer performance simply by aligning the L2 embeddings to the encoder. @CITE[5]@ pretrained LSTM encoders with natural languages and non-linguistic data (e.g., code, music, and artificial data) to demonstrate that the encoders achieve reasonable performance in Spanish language modeling. These studies provide additional evidence for the existence of transferable linguistic knowledge learned in the model. Then what is such knowledge? Probing studies (; @CITE[7]@) have revealed that the model captures language-independent structures such as universal dependency relations (Chi et al., 2020) and subjecthood (@CITE[8]@). However, the probing methodology does not answer whether such linguistic knowledge contributes to the performance in cross-lingual transfer. In this study, we shed light on this question by studying transfer learning from artificial language with the Test for Inductive Bias via Language Model Transfer (TILT) (@CITE[5]@). This framework enables us to assess if abstract features generalizable to L2 (natural language) are encoded in L1. Here we explicitly design artificial languages with some structural properties as L1 to investigate their transferability.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 3. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 4. Title: Cross-Lingual Ability of Multilingual BERT: An Empirical Study\\nID: 5. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 6. Title: Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models\\nID: 7. Title: What you can cram into a single vector: Probing sentence embeddings for linguistic properties\\nID: 8. Title: Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT<|eot_id|>'},\n",
       " 5258: {'predict': 'Fine-tuning is the most widely used method for adapting PLMs to downstream tasks. It fine-tunes the entire PLM on downstream tasks by updating the parameters of the PLM. To reduce the cost of fine-tuning, parameter-efficient tuning methods have been proposed. Adapter-tuning (@CITE[6]@; @CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[3]@) is one of the most widely used parameter-efficient tuning methods. It inserts adapters into the layers of the PLM, and the parameters of the adapters are updated during fine-tuning. Prompt-tuning (@CITE[2]@; @CITE[2]@) is another widely used parameter-efficient tuning method. It updates the parameters of the prompts instead of the PLM. However, prompt-tuning usually requires a large number of prompts, which is difficult to generate for downstream tasks with few examples. In this paper, we propose a new parameter-efficient tuning method, which combines the advantages of adapter-tuning and prompt-tuning. It inserts adapters into the layers of the PLM and updates the parameters of the adapters and prompts. This method can reduce the cost of fine-tuning and can be applied to downstream tasks with few examples.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: LLaMA: Open and Efficient Foundation Language Models\\nID: 3. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 4. Title: Instance-aware Prompt Learning for Language Understanding and Generation\\nID: 5. Title: Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks\\nID: 6. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks',\n",
       "  'ground_truth': '\\n\\nLarge language models leverage parameterized Transformers as a foundational framework and train them on extensive unsupervised language corpora (; ; @CITE[3]@; @CITE[6]@. Subsequently, specific task objectives are introduced for downstream tasks to perform full fine-tuning on the pretrained language models (@CITE[4]@, 2023; Muennighoff et al., 2022). As the primary method for optimizing pretrained language models, full fine-tuning involves initializing the model with pretrained weights, updating all model parameters, and storing a separate fully optimized model for each downstream task. However, as the model parameter grows, conducting full fine-tuning on existing computational devices becomes increasingly challenging. In light of this, researchers have started exploring efficient methods for effectively harnessing the power of large language models. As an efficient alternative, parameter-efficient tuning is a promising way of stimulating large language models. Compared to vanilla fine-tuning, parameter-efficient tuning methods only tune a small portion of the model parameters while keeping the rest frozen. Adapter tuning methods, such as those proposed by @CITE[5]@ and @CITE[1]@,a), aim to learn task-specific information by incorporating small-scale task-specific modules into the layers of the Transformer. Prefix tuning methods, as introduced by @CITE[2]@ and discussed in the work by, also introduce additional parameters within the Transformer layers. LoRA, proposed by @CITE[2]@, merges low-rank and trainable matrices with the frozen weights at each layer of the Transformer. BitFit (Ben Zaken et al., 2022) is a simple yet effective method that optimizes the bias terms within a model while keeping the other parameters frozen. However, these existing parameter-efficient tuning methods typically rely on gradient-based optimization and still involve substantial memory usage. Our proposed method, on the other hand, enables substantial memory savings while maintaining comparable performance.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: LLaMA: Open and Efficient Foundation Language Models\\nID: 3. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 4. Title: Instance-aware Prompt Learning for Language Understanding and Generation\\nID: 5. Title: Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks\\nID: 6. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks<|eot_id|>'},\n",
       " 5676: {'predict': 'Most DST models use a closed vocabulary, which can be represented as a dictionary of predefined slot values (@CITE[8]@; @CITE[5]@; @CITE[4]@). However, using a closed vocabulary can lead to a mismatch between the vocabulary and the actual slot values in the test set, especially for the slot values that are not seen in the training set. To address this issue, some models (@CITE[7]@; @CITE[1]@; @CITE[2]@; @CITE[6]@) have been proposed to track the open-vocabulary DST. These models usually rely on a large language model (LM) to generate the slot values and the DST model is trained to correct the LM’s predictions. However, these models are not end-to-end, which may lead to error propagation (@CITE[3]@). In this paper, we propose an end-to-end open-vocabulary DST model by leveraging the masked LM pre-training objective. The model can generate slot values with a large vocabulary and correct its predictions without error propagation.\\nReferences:ID: 1. Title: Dialog State Tracking: A Neural Reading Comprehension Approach\\nID: 2. Title: Parallel Interactive Networks for Multi-Domain Dialogue State Generation\\nID: 3. Title: Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking\\nID: 4. Title: LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking\\nID: 5. Title: Amendable Generation for Dialogue State Tracking\\nID: 6. Title: Effective Sequence-to-Sequence Dialogue State Tracking\\nID: 7. Title: Dialogue State Distillation Network with Inter-slot Contrastive Learning for Dialogue State Tracking\\nID: 8. Title: Amendable Generation for Dialogue State Tracking',\n",
       "  'ground_truth': '\\n\\nTherefore, most researchers focus on developing open-vocabulary DST models (@CITE[3]@; @CITE[7]@; ; @CITE[5]@; @CITE[6]@; @CITE[8]@, which extract dialogue states directly from the complete dialogue history, ignoring the issue of incomplete context information caused by truncation of dialogue history. To address this challenge, (@CITE[4]@; @CITE[1]@; Zhou et al., 2021; @CITE[2]@ attempts to track users’ goals from the context of the dialogue and the previous dialogue state. However, error propagation has become the reason for limiting model performance. To address this problem, the idea of correcting dialogue state is proposed (@CITE[4]@;, but the methods heavily rely on specific DST models, i.e., lacking scalability.\\nReferences:ID: 1. Title: Dialog State Tracking: A Neural Reading Comprehension Approach\\nID: 2. Title: Parallel Interactive Networks for Multi-Domain Dialogue State Generation\\nID: 3. Title: Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking\\nID: 4. Title: LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking\\nID: 5. Title: Amendable Generation for Dialogue State Tracking\\nID: 6. Title: Effective Sequence-to-Sequence Dialogue State Tracking\\nID: 7. Title: Dialogue State Distillation Network with Inter-slot Contrastive Learning for Dialogue State Tracking\\nID: 8. Title: Amendable Generation for Dialogue State Tracking<|eot_id|>'},\n",
       " 7136: {'predict': 'Non-Autoregressive Transformer (NAT) is a Transformer model that generates all tokens in parallel. @CITE[1]@ introduced the NAT model for machine translation and proposed a mask-predict method to predict missing tokens iteratively. @CITE[5]@ proposed a unified tree structure to present a solution expression in MWP and a path-based metric to evaluate the partial accuracy of expressions of a unified tree. @CITE[2]@ proposed a NAT model to order a given set of sentences and a novel task of ordering abstracts from scientific articles. @CITE[3]@ proposed a NAT model for the problem of sentence ordering and coherence modeling. @CITE[4]@ proposed a NAT model to solve the problem of sentence ordering and coherence modeling using recurrent neural networks. @CITE[9]@ proposed a NAT model for machine translation with enhanced decoder inputs. @CITE[6]@ proposed a partially autoregressive model for more flexible and amenable sequence generation. @CITE[8]@ proposed a NAT model to solve the problem of text summarization. @CITE[1]@ proposed a NAT model to solve the problem of machine translation. @CITE[7]@ proposed a NAT model to solve the problem of text-to-speech synthesis. @CITE[1]@ proposed a NAT model to solve the problem of machine translation with curriculum learning. @CITE[8]@ proposed a NAT model to solve the problem of machine translation with curriculum learning. @CITE[7]@ proposed a NAT model to solve the problem of text-to-speech synthesis with curriculum learning.\\nReferences:ID: 1. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 2. Title: Non-Autoregressive Math Word Problem Solver with Unified Tree Structure\\nID: 3. Title: Sentence Ordering and Coherence Modeling using Recurrent Neural Networks\\nID: 4. Title: Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input\\nID: 5. Title: Levenshtein Transformer\\nID: 6. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 7. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 8. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 9. Title: FastSpeech: Fast, Robust and Controllable Text to Speech',\n",
       "  'ground_truth': '\\n\\nNon-autoregressive transformer (NAT) (Gu et al., 2018) has been proposed as an effective non-autoregressive decoding method in machine translation. NAT predicts the output sequence based on source sequence and latent representation via parallel fashion. In order to narrow the performance gap between NAT and autoregressive transformer (AT), various approaches have been proposed in many research areas (@CITE[8]@; Yang et al., 2021; @CITE[5]@). Several methods (; @CITE[6]@; @CITE[9]@; @CITE[1]@; @CITE[8]@) proposed to modify the latent presentation by making use of extra refinement process, designing auxiliary modules and latent forms. Another line of research (@CITE[1]@; ; ) tended to achieve a trade-off between performance and inference efficiency by using semi-NAT architecture. Besides, many researchers had made attempts to introduce NAT into different sequence generation tasks like speech recognition (@CITE[3]@) and dialog state tracking (@CITE[7]@). Moreover, it is worth noting that many NAT related pre-trained generation models (; @CITE[8]@) had been proposed to improve the generation results of pre-trained models. Our work makes the first attempt to accomplish sentence ordering task by a NAT style model. In order to perform parallel decoding, sequence length should be determined firstly in NAT. The pioneer work in NAT (Gu et al., 2018) designed fertility predictor to predict the copy times of each token in the source sequence. By summarizing the copy times of all the tokens, model can obtain the target length of output sequence. Later, () proposed to use a separate length predictor to obtain the target length for the inference process. Some semi-NAT models (; ) even proposed to implement insertion or deletion-based methods to control the length of generated sequences. @CITE[4]@ introduced an intermediate pseudo-translation to align the order between source and target languages, which could help model the conditional dependencies of the target words and encourage the decoder to choose right words. claimed the penalty for word order errors should not be included, because the semantic alignment between source and target words could be order-agnostic. In the ordering problem (@CITE[2]@), as the number of sentences is fixed in the input, our NAON model does not need to consider this issue and can fully explore the potential of NAT in this task. NAT models usually suffer from repetitive generation issue due to conditional independence between output representations. For machine translation, Gu et al. (2018) proposed to use fertility i.e., copy times of each source token, to bridge the encoding and decoding processes, copying input tokens for several times as inputs of the decoder. Due to the conditional independence between output tokens, NAT model tends to generate repeated tokens in the output sequence. To alleviate this issue, @CITE[9]@ and @CITE[1]@ introduced auxiliary loss functions to deal with the problem of repetition. Recently, semi-NAT style models (; ) proposed new generation approaches to alleviate repetition issue. In our NAON model, we introduce an exclusive loss to tackle this problem.\\nReferences:ID: 1. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 2. Title: Non-Autoregressive Math Word Problem Solver with Unified Tree Structure\\nID: 3. Title: Sentence Ordering and Coherence Modeling using Recurrent Neural Networks\\nID: 4. Title: Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input\\nID: 5. Title: Levenshtein Transformer\\nID: 6. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 7. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation\\nID: 8. Title: Mask-Predict: Parallel Decoding of Conditional Masked Language Models\\nID: 9. Title: FastSpeech: Fast, Robust and Controllable Text to Speech<|eot_id|>'},\n",
       " 5220: {'predict': 'Recent works have shown that large language models can store factual knowledge in their parameters (@CITE[7]@; @CITE[6]@; @CITE[4]@; @CITE[9]@; @CITE[1]@; @CITE[2]@; @CITE[8]@; @CITE[3]@; @CITE[5]@; @CITE[1]@; @CITE[5]@; @CITE[1]@. Many approaches have been proposed to inject new knowledge into LLMs. @CITE[7]@ and @CITE[6]@ proposed to inject knowledge into the model by fine-tuning the model on a new dataset. @CITE[4]@ proposed to inject knowledge by prepending entity definitions in the input. @CITE[9]@ proposed to inject knowledge by updating knowledge neurons. @CITE[1]@ proposed to inject knowledge by updating the model’s parameters via knowledge injection in Transformer FFN layers. @CITE[5]@ proposed to inject knowledge by modifying feed-forward weights. @CITE[3]@ proposed to inject knowledge by modifying the model’s parameters via a causal intervention. @CITE[2]@ proposed to inject knowledge by modifying the model’s parameters via a causal intervention and then updating the model’s parameters via fine-tuning.\\nReferences:ID: 1. Title: Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark\\nID: 2. Title: MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions\\nID: 3. Title: Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge\\nID: 4. Title: Transformer Feed-Forward Layers Are Key-Value Memories\\nID: 5. Title: Rewriting a Deep Generative Model\\nID: 6. Title: Locating and Editing Factual Associations in GPT\\nID: 7. Title: Knowledge Neurons in Pretrained Transformers\\nID: 8. Title: Kformer: Knowledge Injection in Transformer Feed-Forward Layers\\nID: 9. Title: Locating and Editing Factual Associations in GPT',\n",
       "  'ground_truth': '\\n\\nRecent works (@CITE[8]@; @CITE[9]@; ; @CITE[6]@ offer more comprehensive evaluations for fact-editing methods. Inspired by the linear associative memory property of feedforward layers in Transformers (Anderson, 1972; @CITE[1]@, 2022) and success with the approach in convolutional models (@CITE[4]@, recent works have proposed to edit MLP weights directly (@CITE[3]@; @CITE[5]@;. In the encyclopedic factual domain, @CITE[3]@ proposed to edit single facts by fitting a Rank One Model Edit (ROME) to the parameters of an MLP layer, and showed it outperformed prior methods. Our work builds on @CITE[2]@, which extended this approach to thousands of edits by altering the weights of a range of MLP layers. @CITE[7]@ demonstrate that many early edit layers can work well with MEMIT; this partially motivates our extensive layer hyperparameter search. Recent work by proposes a dataset for evaluation of a variety of ripple effects in editing methods with factual knowledge and concludes that models fail to capture these effects. All aforementioned works focus on encyclopedic factual knowledge, unlike ours.\\nReferences:ID: 1. Title: Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark\\nID: 2. Title: MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions\\nID: 3. Title: Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge\\nID: 4. Title: Transformer Feed-Forward Layers Are Key-Value Memories\\nID: 5. Title: Rewriting a Deep Generative Model\\nID: 6. Title: Locating and Editing Factual Associations in GPT\\nID: 7. Title: Knowledge Neurons in Pretrained Transformers\\nID: 8. Title: Kformer: Knowledge Injection in Transformer Feed-Forward Layers\\nID: 9. Title: Locating and Editing Factual Associations in GPT<|eot_id|>'},\n",
       " 7459: {'predict': 'Long text generation has been an active research area for decades. In the pre-LM era, researchers proposed various planning-based approaches (@CITE[11]@; ; @CITE[3]@; @CITE[6]@; @CITE[5]@; @CITE[8]@; @CITE[7]@; @CITE[12]@; @CITE[2]@; @CITE[9]@. Recently, with the success of LMs in text generation, researchers have proposed various prompting-based approaches (@CITE[10]@; @CITE[1]@; @CITE[4]@; @CITE[1]@; @CITE[1]@. For example, @CITE[10]@ proposed a progressive generation approach, which progressively refines content keywords into complete passages. @CITE[1]@ proposed a recursive reprompting and revision framework, which recursively injects contextual information into a language model prompt. @CITE[4]@ proposed a context-tuning approach, which uses continuous inverse prompting to model an inverse generation process from output to input. @CITE[1]@ proposed a prompt-based approach, which generates long text by repeatedly prompting a language model with a sequence of prompts. These approaches have shown promising results in generating long text. However, they are not designed to generate long text in a multi-turn dialogue setting, which is the focus of this paper. In this paper, we propose a novel prompting-based approach, which generates long text in a multi-turn dialogue setting by using a sequence of prompts and a sequence of contexts to model the dialogue flow and the speaker’s intent.\\nReferences:ID: 1. Title: Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text\\nID: 2. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 3. Title: PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation\\nID: 4. Title: Plan-And-Write: Towards Better Automatic Storytelling\\nID: 5. Title: A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation\\nID: 6. Title: Progressive Generation of Long Text with Pretrained Language Models\\nID: 7. Title: Re3: Generating Longer Stories With Recursive Reprompting and Revision\\nID: 8. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning\\nID: 9. Title: Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence\\nID: 10. Title: Context-Tuning: Learning Contextualized Prompts for Natural Language Generation\\nID: 11. Title: Extracting Training Data from Large Language Models\\nID: 12. Title: Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text',\n",
       "  'ground_truth': '\\n\\nPrevious research has highlighted the issue of off-prompt generation and incoherence in long-form text generation (Holtzman et al., 2020; @CITE[5]@). Existing approaches to solve these issues can be divided into two categories based on the generation mechanism. One approach involves generating text in a single pass while improving text coherence by leveraging the hierarchical structure of human-generated texts and modeling high-level semantic features. This includes considering keywords (@CITE[11]@), event sequences (@CITE[2]@), and sentence-level representations (@CITE[9]@). Another category decomposes the text generation process into multiple stages. Some approaches generate keywords or an outline of the text first and then generate and refine the complete text (@CITE[10]@; @CITE[6]@; ; @CITE[7]@). Other techniques employ latent vectors as continuous text plans (@CITE[1]@; @CITE[3]@; @CITE[8]@). The recent success of LLMs has demonstrated that these issues can be effectively mitigated without the need for fine-tuning on specific datasets (@CITE[12]@; @CITE[4]@; ). Despite this progress, it is still necessary to analyze and identify areas where LLMs may encounter limitations or fail to perform optimally.\\nReferences:ID: 1. Title: Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text\\nID: 2. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 3. Title: PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation\\nID: 4. Title: Plan-And-Write: Towards Better Automatic Storytelling\\nID: 5. Title: A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation\\nID: 6. Title: Progressive Generation of Long Text with Pretrained Language Models\\nID: 7. Title: Re3: Generating Longer Stories With Recursive Reprompting and Revision\\nID: 8. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning\\nID: 9. Title: Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence\\nID: 10. Title: Context-Tuning: Learning Contextualized Prompts for Natural Language Generation\\nID: 11. Title: Extracting Training Data from Large Language Models\\nID: 12. Title: Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text<|eot_id|>'},\n",
       " 5415: {'predict': 'The factual precision of text generation has been studied extensively in recent years. Previous work has shown that LLMs often struggle with factual knowledge in the long tail (@CITE[4]@; @CITE[3]@; @CITE[5]@), and that retrieval-augmented LLMs can significantly improve their factual knowledge (@CITE[2]@; @CITE[6]@; @CITE[7]@). Human evaluation studies have also been conducted to assess the factual accuracy of LLMs (@CITE[1]@; @CITE[7]@; @CITE[1]@; @CITE[7]@). However, these studies mainly focus on evaluating factual knowledge in open-domain QA datasets, and there is still a lack of research on the factual precision of LLMs in the context of scientific text generation.\\nReferences:ID: 1. Title: Retrieval Augmentation Reduces Hallucination in Conversation\\nID: 2. Title: Language Models (Mostly) Know What They Know\\nID: 3. Title: Large Language Models Struggle to Learn Long-Tail Knowledge\\nID: 4. Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\\nID: 5. Title: Capabilities of GPT-4 on Medical Challenge Problems\\nID: 6. Title: Evaluating Verifiability in Generative Search Engines\\nID: 7. Title: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection',\n",
       "  'ground_truth': '\\n\\nFactual precision in text generation has been an active area of research in NLP. Most prior work studies factual precision of models supervised for a specific problem such as dialogue (@CITE[1]@, or focuses on question answering with short answers (; @CITE[4]@; ; @CITE[5]@. More recent work has studied factual precision of text generation beyond short answers. Lee et al. (2022) evaluates the factual precision with proxy metrics, e.g., whether named entities in a generation appear in an article of the topic. A series of concurrent work verifies the precision of the citations (attributions) provided by the model (@CITE[6]@; @CITE[2]@; @CITE[3]@; @CITE[7]@. A concurrent work by automates the identification of factual errors in LM generations without using any knowledge source; we use their method as a baseline estimator in Section 4. In contrast, our work (1) considers much longer text generation4 from a variety of state-of-the-art LMs with and without search, (2) provides their fine-grained evaluation both by human experts and through an automated evaluator that closely approaches humans, and (3) applies it to a large set of LMs at scale.\\nReferences:ID: 1. Title: Retrieval Augmentation Reduces Hallucination in Conversation\\nID: 2. Title: Language Models (Mostly) Know What They Know\\nID: 3. Title: Large Language Models Struggle to Learn Long-Tail Knowledge\\nID: 4. Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\\nID: 5. Title: Capabilities of GPT-4 on Medical Challenge Problems\\nID: 6. Title: Evaluating Verifiability in Generative Search Engines\\nID: 7. Title: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection<|eot_id|>'},\n",
       " 6729: {'predict': 'Neural machine translation has made rapid progress in recent years. However, its generalization ability is still limited. @CITE[1]@ propose a novel adversarial augmentation method for neural machine translation. @CITE[6]@ improve the robustness of NMT models by reducing the effect of noisy words. @CITE[5]@ and @CITE[2]@ investigate the compositional skills of NMT. @CITE[7]@ propose a novel architecture to enforce the separation between the lexicon and syntax. @CITE[3]@ propose an extension to sequence-to-sequence models which encourages disentanglement by adaptively re-encoding the source input. @CITE[4]@ propose a neural-symbolic recursive machine that integrates neural perception, syntactic parsing, and semantic reasoning.\\nReferences:ID: 1. Title: AdvAug: Robust Adversarial Augmentation for Neural Machine Translation\\nID: 2. Title: Addressing the Vulnerability of NMT in Input Perturbations\\nID: 3. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 4. Title: On Compositionality in Neural Machine Translation\\nID: 5. Title: Compositional Neural Machine Translation by Removing the Lexicon from Syntax\\nID: 6. Title: Disentangled Sequence to Sequence Learning for Compositional Generalization\\nID: 7. Title: Neural-Symbolic Recursive Machine for Systematic Generalization',\n",
       "  'ground_truth': '\\n\\nRecently, CG and robustness of Neural Machine Translation (NMT) have gained much attention from the research community (@CITE[4]@; @CITE[6]@; @CITE[3]@;, including pre-training (@CITE[5]@, data augmentation (@CITE[1]@, datasets (, and enriching semantic information at token-level (; ;,b; @CITE[2]@. Noteworthily, argue that MT is a suitable and relevant testing ground to test CG in natural language. Different from them, we introduce a composed layer to compose different encoder layers’ information dynamically, which is inspired by previous studies about analyzing Transformer (; @CITE[7]@.\\nReferences:ID: 1. Title: AdvAug: Robust Adversarial Augmentation for Neural Machine Translation\\nID: 2. Title: Addressing the Vulnerability of NMT in Input Perturbations\\nID: 3. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 4. Title: On Compositionality in Neural Machine Translation\\nID: 5. Title: Compositional Neural Machine Translation by Removing the Lexicon from Syntax\\nID: 6. Title: Disentangled Sequence to Sequence Learning for Compositional Generalization\\nID: 7. Title: Neural-Symbolic Recursive Machine for Systematic Generalization<|eot_id|>'},\n",
       " 434: {'predict': 'Few-shot learning is a fundamental learning paradigm, which aims to learn a model that can generalize to unseen classes with only a few labeled examples. Recent years, many approaches have been proposed to tackle few-shot learning problems, including metric-based methods (@CITE[5]@; @CITE[5]@; @CITE[1]@; ; @CITE[3]@, meta-learning methods (@CITE[4]@; @CITE[2]@, and hybrid methods (@CITE[1]@. Among them, metric-based methods learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Meta-learning methods, which are also known as learning to learn, learn a meta-learner that can adapt to new tasks with a few examples. Hybrid methods combine metric-based and meta-learning approaches to achieve better performance.\\nReferences:ID: 1. Title: Matching Networks for One Shot Learning\\nID: 2. Title: Prototypical Networks for Few-shot Learning\\nID: 3. Title: Prototypical Networks for Few-shot Learning\\nID: 4. Title: Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace\\nID: 5. Title: Hierarchically Structured Meta-learning',\n",
       "  'ground_truth': '\\n\\nFew-shot learning (or meta-learning) is to learn from prior experiences to form transferable knowledge for new tasks with few labeled data. Notable approaches have three categories. The first category is metric-based methods (; @CITE[2]@) which learn effective similarity space for few-shot instances. For instance, Prototypical Network (@CITE[2]@) classifies each data sample by computing the distance to prototype representation of each class. The second category is gradient-based methods (@CITE[4]@; ; @CITE[3]@) that aim to quickly optimize the model parameters given the gradients on few-shot data instances. For example, MAML (@CITE[4]@) effectively initializes model parameters via a small number of gradient updates and it can quickly adapt to new few-shot tasks. The last category is memory models () which learn to store prior experience (from seen tasks) and generalizes them to unseen tasks. Unlike previous studies that focus on computer vision (, imitation learning (@CITE[5]@, graph mining (@CITE[1]@), we study few-shot relation learning over knowledge bases in this work.\\nReferences:ID: 1. Title: Matching Networks for One Shot Learning\\nID: 2. Title: Prototypical Networks for Few-shot Learning\\nID: 3. Title: Prototypical Networks for Few-shot Learning\\nID: 4. Title: Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace\\nID: 5. Title: Hierarchically Structured Meta-learning<|eot_id|>'},\n",
       " 3951: {'predict': 'Causal mechanism has been widely explored in the field of domain generalization (@CITE[2]@; @CITE[1]@; ; @CITE[4]@; @CITE[3]@. @CITE[2]@ proposed to learn domain-invariant features by minimizing the dissimilarity across domains. @CITE[1]@ proposed to learn invariant correlations across multiple training distributions. @CITE[4]@ proposed to extract the causal factors from inputs and then reconstruct the invariant causal mechanisms. @CITE[3]@ proposed to construct intrinsically interpretable GNNs by discovering invariant rationale. In this paper, we propose to learn the causal mechanism by learning the invariant causal factors and then reconstruct the invariant causal mechanisms. This work is different from the above works in that it is the first time to explore the causal mechanism for domain generalization in the graph domain.\\nReferences:ID: 1. Title: Domain Generalization via Invariant Feature Representation\\nID: 2. Title: Invariant Risk Minimization\\nID: 3. Title: Causality Inspired Representation Learning for Domain Generalization\\nID: 4. Title: Discovering Invariant Rationales for Graph Neural Networks',\n",
       "  'ground_truth': '\\n\\nCausal Mechanism is crystallized in the invariant learning, suggesting that only associations invariant in the training set should be learned (Peters et al., 2016; @CITE[2]@. Invariant Risk Minimization (IRM) (@CITE[4]@ is a practical implementation of invariant learning, which is an optimization objective modifying the loss with a regularization term to enforce the invariant representations. Recent works have explored Structural Causal Model (SCM) (Schölkopf et al., 2012) to model auxiliary variables and show promising performance, ranging from domain generalization ( in computer vision and intrinsic interpretability ( in graph neural networks to factual knowledge (@CITE[3]@ and text classification (@CITE[1]@ in NLP. In this work, we introduce SCM to PLM debiasing to discover the inherent causal relations between data and labels while achieving better debiasing performance. To our knowledge, this is the first work exploiting causality for debiasing PLMs.\\nReferences:ID: 1. Title: Domain Generalization via Invariant Feature Representation\\nID: 2. Title: Invariant Risk Minimization\\nID: 3. Title: Causality Inspired Representation Learning for Domain Generalization\\nID: 4. Title: Discovering Invariant Rationales for Graph Neural Networks<|eot_id|>'},\n",
       " 6336: {'predict': 'Historically, the diachronic semantic changes of words have been studied by linguists (@CITE[4]@). Recently, the diachronic semantic changes of words have been studied by using distributional methods, particularly prediction-based word embedding models (@CITE[2]@). However, this line of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we focus on the evaluation of temporal generalization of LMs. The temporal generalization of LMs has been studied by (@CITE[3]@; @CITE[1]@). The former work studies the temporal generalization of LMs by using the task of next sentence prediction. The latter work studies the temporal generalization of LMs by using the task of text classification. They find that LMs suffer from poor temporal generalization capability and propose to use continual training to mitigate such a misalignment. However, they only use the text classification task and do not consider other tasks.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Mind the Gap: Assessing Temporal Generalization in Neural Language Models\\nID: 3. Title: Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change\\nID: 4. Title: Diachronic word embeddings and semantic shifts: a survey',\n",
       "  'ground_truth': '\\n\\nAutomatic detection of words whose meanings change over time has provided important insights for diverse fields such as linguistics, lexicology, sociology, and information retrieval (IR) (@CITE[2]@; ; Michel et al., 2011; @CITE[4]@. For example, in IR one must know the seasonal association of keywords used in user queries to provide relevant results pertaining to a particular time period. Moreover, it has been shown that the performance of publicly available pretrained foundation models (Bommasani et al., 2021) declines over time when applied to emerging data (Loureiro et al., 2022; @CITE[1]@ because they are trained using a static snapshot. @CITE[3]@ showed that the temporal generalisation of foundation models is closely related to their ability to detect semantic variations of words. Semantic change detection is modelled in the literature as an unsupervised task of detecting words whose meanings change between two given time-specific corpora (@CITE[4]@;.\\nReferences:ID: 1. Title: Diachronic word embeddings and semantic shifts: a survey\\nID: 2. Title: Mind the Gap: Assessing Temporal Generalization in Neural Language Models\\nID: 3. Title: Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change\\nID: 4. Title: Diachronic word embeddings and semantic shifts: a survey<|eot_id|>'},\n",
       " 7480: {'predict': 'In recent years, many approaches have been proposed to discover new intents. Some of them are based on the idea of clustering. @CITE[6]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[4]@; @CITE[7]@; @CITE[8]@; @CITE[1]@; @CITE[6]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[6]@; @CITE[8]@; @CITE[4]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[4]',\n",
       "  'ground_truth': '\\n\\nIdentifying new intents are key to adaptable conversational agents for better dialogue state understanding (; @CITE[3]@). Previous research on NID can be predominantly categorized into two types: unsupervised and semi-supervised. For the former, early approaches (@CITE[2]@; Li et al., 2013) primarily relied on statistical features of the unlabeled data to cluster similar queries for discovering new user intents. Subsequently, some studies (; ; @CITE[4]@; ) have endeavored to leverage deep neural networks to learn robust representations conducive to new intent clustering. However, none of these fully leveraged supervised signals, such as existing intent labels. To address this, recent studies (@CITE[1]@;,c; Mou et al., 2022b; @CITE[6]@) have extended NID to a semi-supervised setting to achieve prior knowledge transfer, in which the labeled data is incorporated into the training process to assist new intent clustering. For example, @CITE[8]@ and first pre-trained a backbone model with the supervision of the limited labeled data. Then, they employed the pre-trained backbone to generate pseudo labels for the unlabeled data, directing the model to discern novel intents. Additionally, different from those pseudo-labeling based semi-supervised methods, @CITE[8]@ and @CITE[6]@ sought to directly optimize utterance representations with the aid of supervised data. They formulated distinct contrastive learning objectives to learn discriminative utterance representations, facilitating the similar utterance clustering and establishing distinct boundaries for new intent clusters. However, all these methods overemphasize on relations such as similarity or dissimilarity among utterances for better clustering effects, while belittle the usage of semantics inside utterances and intents. Similar to the trivial solution in clustering (; ; ; @CITE[5]@; @CITE[7]@), it brings the problem of in-domain over-fitting and meaningless new intents due to the data distortion. In our work, we enhance model with semantic knowledge and use soft prompts to detect new intents.\\nReferences:ID: 1. Title: Unsupervised Deep Embedding for Clustering Analysis\\nID: 2. Title: Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\\nID: 3. Title: Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\\nID: 4. Title: TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition\\nID: 5. Title: Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery\\nID: 6. Title: Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery\\nID: 7. Title: Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering\\nID: 8. Title: Deep Clustering for Unsupervised Learning of Visual Features<|eot_id|>'},\n",
       " 402: {'predict': 'Zero-shot translation is a fundamental task in multilingual NMT. The pioneering work of multilingual NMT (@CITE[7]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[5]@; @CITE[2]@; @CITE[6]@ shows that a single multilingual model can be trained to translate from multiple source languages into multiple target languages. Recent works have proposed to leverage multilingual NMT for zero-shot translation (@CITE[7]@; @CITE[5]@; @CITE[6]@; ; ; ; @CITE[4]@. The existing approaches can be roughly categorized into two groups: 1) knowledge transfer approaches (; @CITE[6]@; @CITE[5]@, which exploit the knowledge in the parallel corpora of other languages to translate unseen languages; 2) knowledge distillation approaches (@CITE[7]@; @CITE[4]@; ; ;, which utilize the knowledge in the monolingual corpora of the target language to translate unseen languages. In this work, we focus on the second group and propose a novel framework to improve zero-shot translation by utilizing the monolingual data of the target language.\\nReferences:ID: 1. Title: Massively Multilingual Neural Machine Translation\\nID: 2. Title: A Comprehensive Survey of Multilingual Neural Machine Translation\\nID: 3. Title: Massively Multilingual Neural Machine Translation\\nID: 4. Title: Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges\\nID: 5. Title: Rapid Adaptation of Neural Machine Translation to New Languages\\nID: 6. Title: Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation\\nID: 7. Title: Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation',\n",
       "  'ground_truth': '\\n\\nZero-shot translation is direct translation in a language pair unseen during training. @CITE[6]@ analyze the zero-shot performance of MNMT models as a function of the number of language pairs. They observe that having more languages results in better zero-shot performance. However, several artifacts arise, as described by @CITE[4]@; Zhang et al. (2020); @CITE[6]@; @CITE[3]@, like off- target translation and insufficient modeling capacity of the MNMT models. Zhang et al. (2020) use language-aware layer normalization and linear transformation to improve some drawbacks of MNMT; they also rely massively on back- translation to improve zero-shot translation. Adaptation to a new language pair may be addressed by training a multilingual model then fine-tuning it with parallel data in the language pair of interest (@CITE[1]@; Variš and @CITE[2]@; @CITE[5]@. @CITE[7]@ propose plug-and-play encoders and decoders per language, which take advantage of a single representation in each language but at the cost of larger model sizes.\\nReferences:ID: 1. Title: Massively Multilingual Neural Machine Translation\\nID: 2. Title: A Comprehensive Survey of Multilingual Neural Machine Translation\\nID: 3. Title: Massively Multilingual Neural Machine Translation\\nID: 4. Title: Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges\\nID: 5. Title: Rapid Adaptation of Neural Machine Translation to New Languages\\nID: 6. Title: Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation\\nID: 7. Title: Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation<|eot_id|>'},\n",
       " 390: {'predict': 'Self-attention mechanism has been widely used in recent NLP studies. @CITE[2]@ introduced self-attention mechanism in Transformer, which is used as the core component in many NLP tasks. @CITE[4]@ applied the self-attention mechanism to the discourse structure of narrative texts to enhance the performance of reading comprehension. @CITE[1]@ used self-attention mechanism to generate images. @CITE[3]@ extended self-attention mechanism to lattice inputs. In this paper, we use self-attention mechanism to model the relationship between the image and text in the visual-textual context.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension\\nID: 3. Title: Image Transformer\\nID: 4. Title: Self-Attentional Models for Lattice Inputs',\n",
       "  'ground_truth': '\\n\\nproposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long-range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (; Shaw et al., 2018; @CITE[2]@). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. @CITE[1]@ proposed a discourse-aware self-attention encoder for reading comprehension on narrative texts, where event chains, discourse relations, and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (@CITE[3]@) and lattice inputs (@CITE[4]@).\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension\\nID: 3. Title: Image Transformer\\nID: 4. Title: Self-Attentional Models for Lattice Inputs<|eot_id|>'},\n",
       " 193: {'predict': 'Few-shot learning has been extensively studied in the field of NLP (@CITE[2]@; @CITE[1]@; ; @CITE[3]@; @CITE[3]@). In the field of KGs, there are also many works on few-shot learning. For example, @CITE[3]@ propose a meta-relational learning framework to do the common but challenging few-shot link prediction in KGs. @CITE[1]@ propose a relation-aware attention-based graph neural network to model the relation-aware structure in KGs. @CITE[3]@ propose a few-shot knowledge graph embedding model that learns the embeddings of entities and relations in a unified way. @CITE[3]@ propose a novel few-shot relational learning framework for KG completion, which can learn the embeddings of entities and relations in a unified way. However, these works only consider the few-shot learning in KG completion and do not consider the few-shot learning in KG inference. In this paper, we propose a novel few-shot relational learning framework for KG inference, which can learn the embeddings of entities and relations in a unified way.\\nReferences:ID: 1. Title: Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning\\nID: 2. Title: Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval\\nID: 3. Title: Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs',\n",
       "  'ground_truth': '\\n\\nTraditional embedding models always require sufficient training triples for all relations, thus are limited when solving the few-shot problem. Previous few-shot learning studies mainly focus on computer vision (@CITE[1]@, imitation learning ( and sentiment analysis (@CITE[3]@. Recent attempts (@CITE[2]@; ; Zhang et al., 2020) tried to perform few-shot relational learning for long-tail relations. @CITE[2]@ proposed a matching network GMatching, which is the first research on one-shot learning for KGs as far as we know. GMatching exploits a neighbor encoder to enhance entity embeddings from their one-hop neighbors, and uses a LSTM matching processor to perform a multi-step matching by a LSTM block. FSRL (Zhang et al., 2020) extends GMatching to few-shot cases, further capturing local graph structures with an attention mechanism. Chen et al. (2019) proposed a novel meta relational learning framework MetaR by extracting and transferring shared knowledge across tasks from a few existing facts to incomplete ones. However, previous studies learn static representations of entities or references, ignoring their dynamic properties. This work attempts to learn dynamic entity and reference representations by an adaptive attentional network.\\nReferences:ID: 1. Title: Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning\\nID: 2. Title: Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval\\nID: 3. Title: Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs<|eot_id|>'},\n",
       " 7381: {'predict': 'Synthetic dialogue data has been used to improve performance of LLMs on conversational tasks, such as dialogue summarization (@CITE[4]@; @CITE[2]@) and dialogue generation (@CITE[6]@; @CITE[5]@). In the context of educational dialogues, @CITE[1]@; @CITE[3]@ have used synthetic data to evaluate the pedagogical ability of LLMs. @CITE[1]@ have also used synthetic data to study the behavior of LLMs in educational dialogues. However, the use of synthetic data to study the reasoning and learning abilities of LLMs has not been explored in educational dialogues. In this paper, we propose a novel approach to generate synthetic educational dialogues by using LLMs and evaluate their reasoning and learning abilities.\\nReferences:ID: 1. Title: Scaling Instruction-Finetuned Language Models\\nID: 2. Title: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization\\nID: 3. Title: PLACES: Prompting Language Models for Social Conversation Synthesis\\nID: 4. Title: Using cognitive psychology to understand GPT-3\\nID: 5. Title: The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues\\nID: 6. Title: Opportunities and Challenges in Neural Dialog Tutoring',\n",
       "  'ground_truth': '\\n\\nLLMs have recently found their way as synthetic di- alogue dataset generators due to their increasingly human-like behaviour. Both methods using fine- tuning (@CITE[1]@ and prompting (@CITE[3]@; @CITE[6]@ haven been proposed. The human-like behaviour also manifests in them showing similar biases in logical reasoning as humans (@CITE[5]@;, and can be comparable to gold-human annotations for generation tasks (Ziems et al., 2023). Consequently, they have been used to simulate students for teacher training (@CITE[4]@, suggesting that one might also rely upon them to create meaningful tutors. However, ; @CITE[2]@ show that they can not yet perform well as teachers out-of-the-box, because they of- ten incorrectly assess student solutions and reveal answers too quickly.\\nReferences:ID: 1. Title: Scaling Instruction-Finetuned Language Models\\nID: 2. Title: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization\\nID: 3. Title: PLACES: Prompting Language Models for Social Conversation Synthesis\\nID: 4. Title: Using cognitive psychology to understand GPT-3\\nID: 5. Title: The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues\\nID: 6. Title: Opportunities and Challenges in Neural Dialog Tutoring<|eot_id|>'},\n",
       " 4640: {'predict': 'Visual Question Answering (VQA) is a task that requires an AI model to answer a question based on an image. The goal of VQA is to enable machines to understand the visual content of images and answer questions in a way that is similar to humans. VQA has been a very active research area, with numerous datasets and benchmarks created for this task, such as VQA (@CITE[1]@, VQA2.0 (@CITE[4]@, and GQA (@CITE[2]@. The VQA task has been studied extensively in the NLP community, and numerous approaches have been proposed to solve this task, including visual attention (@CITE[5]@; @CITE[6]@, multimodal fusion (@CITE[7]@; @CITE[8]@, and multimodal representation learning (@CITE[3]@. In this paper, we focus on the knowledge-based VQA task, which is a sub-task of VQA that requires an AI model to answer questions that require external knowledge to answer.\\nReferences:ID: 1. Title: VQA: Visual Question Answering\\nID: 2. Title: VQA: Visual Question Answering\\nID: 3. Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\\nID: 4. Title: Learning Visual Question Answering by Bootstrapping Hard Attention\\nID: 5. Title: From Recognition to Cognition: Visual Commonsense Reasoning\\nID: 6. Title: A Comprehensive Survey on Cross-modal Retrieval\\nID: 7. Title: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge\\nID: 8. Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering',\n",
       "  'ground_truth': '\\n\\nEarly prominent work on VQA with a framework encouraging open-ended questions and candidate answers was done by @CITE[3]@. Since then, there have been multiple variations observed. @CITE[3]@ classified the answers by jointly representing images and questions. Others followed by examining cross-modal interactions via attention types not restricted to co/soft/hard-attention mechanisms (Lu et al., 2016; @CITE[6]@; @CITE[4]@, effectively learning the explicit correlations between question tokens and localized image regions. Notably, there was a series of attempts toward incorporating common-sense reasoning (@CITE[2]@; @CITE[1]@, 2017; @CITE[5]@. Many of these studies also leveraged information from external knowledge bases for addressing VQA tasks. General models like UpDn (@CITE[6]@ and LXMERT (@CITE[8]@ explicitly leverage non-linear transformations and Transformers for the VQA task, while others like LMH (Clark et al., 2019) and SSL (@CITE[7]@ addressed the critical language priors constraining the VQA performances, albeit with marginal enhancements.\\nReferences:ID: 1. Title: VQA: Visual Question Answering\\nID: 2. Title: VQA: Visual Question Answering\\nID: 3. Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\\nID: 4. Title: Learning Visual Question Answering by Bootstrapping Hard Attention\\nID: 5. Title: From Recognition to Cognition: Visual Commonsense Reasoning\\nID: 6. Title: A Comprehensive Survey on Cross-modal Retrieval\\nID: 7. Title: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge\\nID: 8. Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering<|eot_id|>'},\n",
       " 2080: {'predict': 'Rationale extraction is a task of identifying a subset of input text that is most relevant to the model’s prediction. Existing work in this area is mostly focused on classification tasks (@CITE[4]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[1]@; @CITE[3]@). The rationale extraction is often formulated as a latent variable selection problem, where the rationale is a binary mask over the input text. To model this selection problem, researchers use either binary latent variables (@CITE[4]@; @CITE[5]@; @CITE[2]@; @CITE[3]@) or attention mechanisms (@CITE[1]@; @CITE[3]@). In this work, we focus on the text classification task and use a binary latent variable model. Our work differs from previous work in two main aspects. First, we consider the task of identifying relevant text spans for multiple labels, where the number of rationales is not fixed. Second, we focus on the task of detecting factuality of a statement, which is a more challenging task than the standard text classification task. In this task, the model needs to identify relevant evidence for multiple labels and to determine whether the evidence supports or contradicts each label.\\nReferences:ID: 1. Title: Rationalizing Neural Predictions\\nID: 2. Title: Interpretable Neural Predictions with Differentiable Binary Variables\\nID: 3. Title: The Explanation Game: Towards Prediction Explainability through Sparse Communication\\nID: 4. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 5. Title: SPECTRA: Sparse Structured Text Rationalization',\n",
       "  'ground_truth': '\\n\\nGiven a model M, we are interested in explaining why M predicted ˆ y for a particular instance x ∈X. An extracted rationale R should therefore represent as accurately as possible the most important subset of the input (R ∈x) which contributed mostly towards the model’s prediction ˆ. Currently, there are two popular approaches for extracting rationales. The first consists of using feature attribution methods that attribute to the input tokens an importance score (i.e. how important an input token is to a model’s M prediction ˆ). We can then form a rationale R, by selecting the K most important tokens (independent or contiguous) as indicated by the feature attribution method. The second select-then-predict approach focuses on training inherently faithful classifiers by jointly training two modules, a rationale extractor and a rationale classifier, trained only on rationales produced by the extractor (@CITE[5]@; @CITE[5]@; @CITE[3]@; @CITE[4]@; @CITE[5]@). Recent studies have used feature attribution approaches as part of the rationale extractor (@CITE[4]@; @CITE[3]@), showing improved classifier predictive performance.\\nReferences:ID: 1. Title: Rationalizing Neural Predictions\\nID: 2. Title: Interpretable Neural Predictions with Differentiable Binary Variables\\nID: 3. Title: The Explanation Game: Towards Prediction Explainability through Sparse Communication\\nID: 4. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 5. Title: SPECTRA: Sparse Structured Text Rationalization<|eot_id|>'},\n",
       " 4997: {'predict': 'Factuality evaluation in document summarization has been a long-standing challenge. @CITE[1]@ analyzed the limitations of standard likelihood training and decoding objectives in neural abstractive summarization models and found that they are prone to hallucinate content that is unfaithful to the input document. @CITE[3]@ conducted a large-scale human evaluation of several neural abstractive summarization systems and found substantial amounts of hallucinated content in all model-generated summaries. The authors also showed that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, they showed that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria. @CITE[2]@ proposed a typology of factual errors and used it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations, they identified the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgment as well as their specific strengths and weaknesses. @CITE[5]@ proposed a question-answering-based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. @CITE[4]@ proposed a summarization metric, FEVER, which uses the FEVER dataset to evaluate the faithfulness of generated summaries. The dataset consists of a set of questions and their corresponding answers from Wikipedia. FEVER evaluates the faithfulness of generated summaries by comparing the answers generated from the summaries against the answers extracted from the Wikipedia articles. In this work, we propose a factuality evaluation metric that utilizes a set of pre-defined templates to extract facts from the generated summaries and compare them with the facts extracted from the original documents. Our metric is designed to be model-agnostic and can be used to evaluate the faithfulness of any summarization model, including both extractive and abstractive models.\\nReferences:ID: 1. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 2. Title: MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\\nID: 3. Title: Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\\nID: 4. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 5. Title: MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning',\n",
       "  'ground_truth': '\\n\\nFactuality evaluation is first proposed in the field of document summarization. @CITE[1]@ conducted a human annotation on the XSUM dataset (Narayan et al., 2018) and found that more than 70% of summaries generated by summarization models have factual errors. Other human annotations(; @CITE[2]@ reach similar conclusions. To measure the factual consistency, researchers proposed different metrics, which can be roughly divided into Entailment-based metrics (@CITE[5]@; Kry´ nski et al., 2020) and QA-based metrics (@CITE[3]@; ; @CITE[4]@.\\nReferences:ID: 1. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 2. Title: MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\\nID: 3. Title: Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics\\nID: 4. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 5. Title: MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning<|eot_id|>'},\n",
       " 4123: {'predict': 'Regularization training has been widely used in neural networks to improve generalization (@CITE[4]@; @CITE[2]@; @CITE[1]@; @CITE[5]@). For example, Temporal Ensembling (@CITE[3]@) and Mean Teacher (@CITE[3]@) are used to improve the generalization ability of neural networks by averaging the predictions of multiple models. R-Drop (@CITE[1]@) uses dropout to enhance the robustness of neural networks by forcing the output distributions of different sub-models to be consistent with each other. GEM ( is used to alleviate forgetting in continual learning by minimizing the KL-divergence between the output distributions of the current model and the previous model. In NLP, regularization training is used to improve the generalization ability of neural networks in tasks such as text generation (@CITE[7]@), machine translation (, and semantic parsing (@CITE[6]@). In this paper, we use regularization training to improve the generalization ability of neural networks in the task of entity disambiguation. The key difference between our work and previous work is that we use regularization training to improve the generalization ability of neural networks in the task of entity disambiguation. In addition, we use a novel regularization training method to improve the generalization ability of neural networks. Specifically, we propose a novel regularization training method that uses the output distribution of the model to generate the target distribution. In addition, we use a novel regularization training method that uses the output distribution of the model to generate the target distribution. We use the output distribution of the model to generate the target distribution by minimizing the KL-divergence between the output distribution of the model and the target distribution. This method is different from previous work that uses dropout to generate the target distribution by minimizing the KL-divergence between the output distribution of the model and the target distribution (@CITE[1]@; @CITE[6]@).\\nReferences:ID: 1. Title: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\\nID: 2. Title: Towards Robust Neural Machine Translation\\nID: 3. Title: R-Drop: Regularized Dropout for Neural Networks\\nID: 4. Title: Gradient Episodic Memory for Continual Learning\\nID: 5. Title: Systematic Generalization with Edge Transformers\\nID: 6. Title: A Contrastive Framework for Neural Text Generation\\nID: 7. Title: Categorizing Semantic Representations for Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nRegularization training has been shown effective in semi-supervised training (Sajjadi et al., 2016; @CITE[5]@, robust training (; @CITE[6]@, continual training (Kirkpatrick et al., 2016; @CITE[3]@, etc. To encourage compositional behavior, @CITE[4]@ softly combine source/target sequence embeddings during training, and @CITE[2]@ introduce gradient-based meta-learning to simulate distribution shift. In addition, contrastive learning serving as regularization has achieved success in various NLP tasks (@CITE[1]@; ; @CITE[7]@. Different from them, we explore the effectiveness of the regularization training on the two different tasks in compositional generalization.\\nReferences:ID: 1. Title: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\\nID: 2. Title: Towards Robust Neural Machine Translation\\nID: 3. Title: R-Drop: Regularized Dropout for Neural Networks\\nID: 4. Title: Gradient Episodic Memory for Continual Learning\\nID: 5. Title: Systematic Generalization with Edge Transformers\\nID: 6. Title: A Contrastive Framework for Neural Text Generation\\nID: 7. Title: Categorizing Semantic Representations for Neural Machine Translation<|eot_id|>'},\n",
       " 7088: {'predict': 'SQuAD (@CITE[4]@) and NewsQA (@CITE[2]@) are two well-known MRC datasets, which are constructed by asking crowd workers to write questions based on Wikipedia articles and news articles. TriviaQA (@CITE[3]@) is another challenging MRC dataset which contains questions written by trivia enthusiasts. TyDiQA (@CITE[1]@) is a multilingual MRC dataset covering 11 typologically diverse languages. ParaShoot ( is a Hebrew MRC dataset. The data of these datasets are collected from Wikipedia articles, news articles, and trivia websites. In contrast, our dataset is collected from the web, which is more diverse and realistic.\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: NewsQA: A Machine Comprehension Dataset\\nID: 3. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 4. Title: TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages\\nID: 5. Title: ParaShoot: A Hebrew Question Answering Dataset',\n",
       "  'ground_truth': '\\n\\nMRC datasets (often these take the form of question-answering (QA) tasks) have been developed by the NLP community for a long time (Hirschman et al., 1999). While newer datasets are being developed, the structure of QA data — a paragraph (from Wikipedia), a question about this paragraph, and an answer — is still the way most datasets are constructed nowadays. Perhaps the most well-known datasets are the SQuAD and SQuAD 2.0 (@CITE[1]@, 2018b) which use Wikipedia as a source for paragraphs, creating 100,000 QA instances. SQuAD 2.0 improves over the first version by including another 50,000 unanswerable questions. Other known datasets are NewsQA ( that were collected from news articles, TriviaQA (@CITE[2]@, a large-scale dataset (650K samples) that used questions from quiz websites aligned with Wikipedia and web results, and CoQA (Reddy et al., 2019), a dataset focused on conversational question answering, offers a unique context-based approach with dialogues and answers. Another recent QA dataset is natural questions (@CITE[5]@, which distinctly uses natural queries from Google searches, aligned with Wikipedia paragraphs. All of the above are English datasets. In contrast, TyDi QA (@CITE[3]@ is designed for multilingual information-seeking QA, encompassing languages from various language types, offering a more global scope for MRC research. While the selection of MRC datasets in English is large, the only MRC dataset in Hebrew is ParaShoot (@CITE[4]@, consisting of Wikipedia articles and constructed in a similar way to SQuAD. We improve over ParaShoot in three aspects: size, diversity, and quality. HeQ has 30K questions compared to 3K; uses news and Wikipedia domains; and was much more rigorously validated. In particular, special attention has been given to the evaluation of flexible span boundaries. While many MRC datasets leverage external sources (user queries and trivia sites), these resources are not available for Hebrew. Therefore, we create HeQ in a similar fashion to SQuAD 2.0. However, revised guidelines have been introduced in order to ensure diversity, question difficulty, and sorting our matters concerning span boundaries.\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: NewsQA: A Machine Comprehension Dataset\\nID: 3. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 4. Title: TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages\\nID: 5. Title: ParaShoot: A Hebrew Question Answering Dataset<|eot_id|>'},\n",
       " 997: {'predict': 'Non-autoregressive models with iterative refinement have been proposed for various tasks. For example, @CITE[2]@ propose the Levenshtein Transformer to perform iterative refinement for machine translation and text summarization. @CITE[3]@ propose the DisCo Transformer to perform iterative refinement for machine translation. @CITE[1]@ propose curriculum learning for NAT to improve translation accuracy. @CITE[1]@ propose iterative refinement for NAT to improve translation accuracy. @CITE[1]@ propose iterative refinement for image captioning. @CITE[1]@ propose iterative refinement for machine translation.\\nReferences:ID: 1. Title: Levenshtein Transformer\\nID: 2. Title: Non-Autoregressive Machine Translation with Disentangled Context Transformer\\nID: 3. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nA series of work are devoted to semi-autoregressive models that refine the outputs with multi-pass iterative decoding (Lee et al., 2018; @CITE[1]@; @CITE[2]@; Ghazvininejad et al., 2019, 2020b; ; @CITE[3]@). Lee et al. (2018) proposed a method of iterative refinement based on denoising autoencoder. @CITE[2]@ utilized insertion and deletion to refine the outputs in inference. Ghazvininejad et al. (2019) trained the model with the masked language model, and the model iteratively replaces masked tokens with new outputs. (@CITE[3]@ first predicts the left token and right token for each position, and decodes the final token at the current position conditioned on the left-and-right tokens predicted before. Despite the relatively better accuracy, the multiple decoding iterations reduce the inference efficiency of non-autoregressive models.\\nReferences:ID: 1. Title: Levenshtein Transformer\\nID: 2. Title: Non-Autoregressive Machine Translation with Disentangled Context Transformer\\nID: 3. Title: Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation<|eot_id|>'},\n",
       " 7050: {'predict': 'End-to-end speech translation (EST) aims to translate a segment of speech into a specific language with an end-to-end model (@CITE[7]@). Conventional EST models (; @CITE[4]@; @CITE[5]@) often suffer from the challenge of data scarcity. To address this, some works (@CITE[6]@; @CITE[3]@; @CITE[1]@; @CITE[8]@) propose to use pre-trained models to improve the performance. Some works (@CITE[9]@; @CITE[2]@; @CITE[10]@) aim to improve the performance by exploiting the auxiliary text translation task. Some works (@CITE[1]@; @CITE[10]@) also focus on the disentangled representation learning for EST. However, these works only consider the disentanglement of the content and speaker, while the prosody factors such as pitch and rhythm are not considered. To this end, we propose to disentangle the content, speaker, pitch, and rhythm of speech and then learn the disentangled representations for EST. Besides, we also propose to use the adversarial learning to enhance the disentanglement.\\nReferences:ID: 1. Title: Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation\\nID: 2. Title: A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks\\nID: 3. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 4. Title: Jointly Trained Transformers models for Spoken Language Translation\\nID: 5. Title: End-to-end Speech Translation via Cross-modal Progressive Training\\nID: 6. Title: Unified Speech-Text Pre-training for Speech Translation and Recognition\\nID: 7. Title: Generating Sentences from Disentangled Syntactic and Semantic Spaces\\nID: 8. Title: STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation\\nID: 9. Title: Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task\\nID: 10. Title: Adversarially learning disentangled speech representations for robust multi-factor voice conversion',\n",
       "  'ground_truth': '\\n\\nTo avoid error propagation in cascaded ST and reduce inference latency, Bérard et al. (2016) and Weiss et al. (2017) propose end-to-end ST that directly translates speech in the source language into text in the target language, without relying on the intermediate transcriptions. However, due to the inherent complexity and variation of speech signals and the scarcity of high-quality E2E ST data, achieving satisfactory performance remains challenging. Over the years, a variety of approaches have been proposed to address these issues, such as pre-training (@CITE[1]@; @CITE[6]@; @CITE[4]@), multi-task learning (@CITE[9]@; @CITE[5]@; @CITE[7]@), data augmentation (@CITE[2]@; Lam et al., 2022), contrastive learning (@CITE[3]@; ), and knowledge distillation (@CITE[8]@; @CITE[10]@). Most of these approaches focus on using the transcription data in speech data triplets to perform MT/ASR tasks, pretrain model components, and mitigate the modality gap between speech and text. Significantly different from them, we attempt to improve translation quality by efficiently exploring the speech representation disentanglement to learn content-centric speech representations for ST.\\nReferences:ID: 1. Title: Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation\\nID: 2. Title: A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks\\nID: 3. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 4. Title: Jointly Trained Transformers models for Spoken Language Translation\\nID: 5. Title: End-to-end Speech Translation via Cross-modal Progressive Training\\nID: 6. Title: Unified Speech-Text Pre-training for Speech Translation and Recognition\\nID: 7. Title: Generating Sentences from Disentangled Syntactic and Semantic Spaces\\nID: 8. Title: STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation\\nID: 9. Title: Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task\\nID: 10. Title: Adversarially learning disentangled speech representations for robust multi-factor voice conversion<|eot_id|>'},\n",
       " 670: {'predict': 'Transformer has been widely used in NLP tasks, such as machine translation (@CITE[6]@), text generation (@CITE[3]@), image generation (@CITE[5]@, 2020), etc. However, the original Transformer architecture is not suitable for long-sequence tasks. To solve this problem, some works try to reduce the quadratic dependency on the sequence length. For example, BlockBERT (@CITE[1]@) and BlockBERTv2 (@CITE[8]@) reduce the dependency by introducing sparse block structures into the attention matrix. BigBird (@CITE[4]@) and ETC (@CITE[7]@, 2022) reduce the quadratic dependency by introducing a sparse attention mechanism. In addition, some works try to reduce the computational complexity of the Transformer architecture. For example, Linformer (Katharopoulos et al., 2020) and Longformer ( use linear attention mechanisms. In this work, we propose a novel attention mechanism which is different from previous works. Our attention mechanism is based on the observation that the information in the sequence is not uniformly distributed. In the beginning of the sequence, the information is rich and dense, and the information in the end of the sequence is sparse and poor. Therefore, we propose a novel attention mechanism which can capture the information in the sequence more effectively. We use the dense information to guide the attention mechanism to capture the information in the sequence, which can effectively reduce the computational complexity of the Transformer architecture.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: A Cascade Approach to Neural Abstractive Summarization with Content Selection and Fusion\\nID: 3. Title: Image Transformer\\nID: 4. Title: Blockwise Self-Attention for Long Document Understanding\\nID: 5. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 6. Title: Big Bird: Transformers for Longer Sequences\\nID: 7. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 8. Title: Sparse Sinkhorn Attention\\nID: 9. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 10. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned',\n",
       "  'ground_truth': '\\n\\nPre-trained transformer models have shown success and become the starting point for various NLP problems such as BERT ( in contextual representation, GPT2 in text generation (@CITE[7]@, or BART in seq2seq tasks (@CITE[5]@. However, the memory and time requirements for transformer models grow quadratically with the sequence length, and for long-span tasks this quickly leads to GPU running out of memory in training. To mitigate the quadratic nature, a wide range of modified architectures have recently been proposed (@CITE[6]@. They reduce the quadratic complexity of the full self-attention mechanism by using fixed attention patterns (; ; @CITE[7]@; @CITE[7]@; ; @CITE[8]@;, learnable patterns (Kitaev et al., 2020; @CITE[10]@, low-rank matrix approximation (, or kernel method (Choromanski et al., 2021). Alternatively, it has been shown that some attention heads are redundant and can be pruned to reduce model size (@CITE[5]@; @CITE[9]@. Knowledge distillation reduces memory and compute by compressing a large model to a smaller one (; @CITE[6]@.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: A Cascade Approach to Neural Abstractive Summarization with Content Selection and Fusion\\nID: 3. Title: Image Transformer\\nID: 4. Title: Blockwise Self-Attention for Long Document Understanding\\nID: 5. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 6. Title: Big Bird: Transformers for Longer Sequences\\nID: 7. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 8. Title: Sparse Sinkhorn Attention\\nID: 9. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 10. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned<|eot_id|>'},\n",
       " 6937: {'predict': \"Data cartography is a technique for visualizing the underlying structure of a dataset. In this work, we use the term to refer to any form of analysis or visualization of the dataset, such as data maps or scatter plots. This technique has been previously used to identify biases in datasets (@CITE[3]@; @CITE[4]@, to identify outliers (@CITE[1]@, and to identify challenging examples in a dataset for active learning (@CITE[2]@. In this work, we use data cartography to identify examples in a dataset that are likely to be challenging for models to classify, and to identify the features of those examples that are most challenging.\\nReferences:ID: 1. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them\\nID: 2. Title: Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering\\nID: 3. Title: Cartography Active Learning\\nID: 4. Title: WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation\",\n",
       "  'ground_truth': \"\\n\\n@CITE[1]@ demonstrated how weak models, as opposed to strong ones, recreate similar data maps by leveraging annotation artifacts and biases. On the subject of instance difficulty, Ethayarajh et al. (2022) proposed V-usable information, a generalization of Shannon information (@CITE[2]@), which provides an aggregate measure of data difficulty in relation to a model; the pointwise V −Information, an extension of V, has been shown to correlate strongly with data map coordinates, and offers the additional benefit of explaining why an instance can be challenging for a model. Data maps have been recently applied to a variety of tasks such as syntactic parsing (), hate speech detection (@CITE[4]@), and visual question answering (). Furthermore, data maps have been shown to be an effective data selection tool for active learning (@CITE[3]@; ).\\nReferences:ID: 1. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them\\nID: 2. Title: Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering\\nID: 3. Title: Cartography Active Learning\\nID: 4. Title: WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation<|eot_id|>\"},\n",
       " 3617: {'predict': 'Transformer quantization has attracted much attention recently. @CITE[1]@ propose a Hessian-based method to quantize BERT models to ultra-low precision. @CITE[5]@ propose to quantize BERT to 8-bit integers. @CITE[6]@ propose to quantize transformer weights and embeddings to ultra-low bit-widths. @CITE[8]@ propose to quantize transformer weights and embeddings to 2-bit integers. @CITE[4]@ propose to quantize the transformer model to 3-bit integers for machine translation. @CITE[2]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[3]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[7]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[4]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[8]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[7]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[1]@ propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[8]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[7]@ propose to quantize the transformer model to 8-bit integers for machine translation. @CITE[7]@ propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quantize the transformer model to 8-bit integers for machine translation. propose to quant',\n",
       "  'ground_truth': '\\n\\nIn comparison to GPT-2, BERT is easier to quantize. It can be quantized with PTQ under a limited performance drop (@CITE[5]@. QAT on BERT for a given downstream task recovers full-precision (FP) performance even with ultra-low precision (@CITE[8]@; @CITE[6]@, or with integer-only operations for non-linear layers (Kim et al., 2021). On the other hand, quantization studies on autoregressive transformers are relatively limited in their scope, using weight-only quantization (@CITE[1]@ or requiring full-fledged training (@CITE[4]@; @CITE[2]@. Please note that these works focus on quantizing GPT-2 that is finetuned on a downstream task whereas ours quantizes the original pretrained GPT-2. Quantization techniques directly relevant to our work are cross-layer-equalization (CLE) (@CITE[7]@ and adaptive rounding (AdaRound) (@CITE[3]@. Similarly to CLE, Qudapter rescales associated model weights to lessen the quantization burden. AdaRound and our proposed method are alike in training foldable helper parameters to minimize the block-wise quantization error. In addition, learned step size (LSQ) (@CITE[1]@ and its extension (LSQ+) (@CITE[3]@ train the quantization-related parameters during QAT, to which Quadapter bears similarity.\\nReferences:ID: 1. Title: Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\\nID: 2. Title: Q8BERT: Quantized 8Bit BERT\\nID: 3. Title: Understanding and Overcoming the Challenges of Efficient Transformer Quantization\\nID: 4. Title: Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation\\nID: 5. Title: Fully Quantized Transformer for Machine Translation\\nID: 6. Title: Compression of Generative Pre-trained Language Models via Quantization\\nID: 7. Title: Data-Free Quantization Through Weight Equalization and Bias Correction\\nID: 8. Title: LSQ+: Improving low-bit quantization through learnable offsets and better initialization<|eot_id|>'},\n",
       " 1897: {'predict': 'Contrastive learning is a self-supervised learning technique that aims to learn representations by contrasting similar and dissimilar data samples. Contrastive learning has been widely used in computer vision (@CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[3]@). Recently, contrastive learning has been applied to NLP tasks such as text classification (Gao et al., 2021), named entity recognition (Zhu et al., 2020), and text generation (Wu et al., 2021). However, there is limited work on applying contrastive learning to dialogue summarization. @CITE[1]@ propose a contrastive learning framework to learn sentence representations for dialogue summarization. They use dropout to augment the input sequences as negative samples. However, the generated negative samples are not similar to the original sequence, which can lead to suboptimal results. In this paper, we propose a contrastive learning framework to learn sentence representations for dialogue summarization. We use the augmented sentences as positive samples. Our method is more effective and efficient than previous methods.\\nReferences:ID: 1. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 2. Title: Supervised Contrastive Learning\\nID: 3. Title: Contrastive Self-Supervised Learning for Commonsense Reasoning\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings',\n",
       "  'ground_truth': '\\n\\nThe application of contrastive learning for various tasks has been investigated recently, mainly in the computer vision domain. The contrastive predictive coding ( has been studied for data-efficient image recognition (@CITE[3]@. Without using specialized architectures or a memory bank, learning visual representations in a contrastive manner outperforms various baselines with self-supervised, semi-supervised, and transfer learning (. @CITE[4]@ proposed a fully-supervised contrastive loss which achieved new state-of-the-art results on the image classification task, surpassing the cross-entropy loss. This work also demonstrates that, compared to the traditional cross-entropy loss, the proposed supervised contrastive loss performs more stably to different hyperparameter settings, like data augmentations and optimizers. Moreover, @CITE[1]@ introduced contrastive margin as a regularizer for commonsense reasoning where a pairwise contrastive auxiliary prediction task is constructed. Fang et al. (2020) proposed to pre-train language models with contrastive self-supervised learning at the sentence level, which learns to predict whether two sentences originate from the same one. @CITE[2]@ proposed a supervised contrastive learning objective which allows working with cross-entropy and lead to significant performance gains. The contrastive learning is also introduced to learn the sentence embeddings (. The above applications of contrastive learning are for computer vision or natural language understanding domains, while, in this work, we introduce the contrastive learning to the abstractive dialogue summarization task, which is a typical generation task.\\nReferences:ID: 1. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 2. Title: Supervised Contrastive Learning\\nID: 3. Title: Contrastive Self-Supervised Learning for Commonsense Reasoning\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings<|eot_id|>'},\n",
       " 3063: {'predict': 'Vector Quantised Variational Autoencoder (VQ-VAE) (@CITE[3]@ is a VAE-based model that uses discrete latent variables. The model quantises the continuous latent variables using a fixed-size codebook. It has been successfully applied to speech (@CITE[4]@; @CITE[1]@), text (@CITE[2]@; @CITE[5]@), and dialogue (@CITE[6]@; ) generation tasks. In this paper, we propose a VQ-VAE-based model for dialogue response generation. Our model first encodes the dialogue context and the response into a continuous latent variable, and then quantises the latent variable into a discrete latent variable. The discrete latent variable is used to guide the decoder to generate the response.\\nReferences:ID: 1. Title: Neural Discrete Representation Learning\\nID: 2. Title: Unsupervised speech representation learning using WaveNet autoencoders\\nID: 3. Title: Fast Decoding in Sequence Models using Discrete Latent Variables\\nID: 4. Title: Discrete Latent Variable Representations for Low-Resource Text Classification\\nID: 5. Title: PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\\nID: 6. Title: Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder',\n",
       "  'ground_truth': '\\n\\nRather than learning continuous representations, VQ-VAE (@CITE[1]@) learns quantized representations via vector quantization. It was first applied to speech representation (@CITE[2]@) and video generation (@CITE[3]@). Recently, it has attracted lots of attention in a variety of NLP tasks. In machine translation, applied VQ-VAE to decode the target language sequence from the discrete space, making the model inference faster and the model performance only slightly compromised. In text classification, @CITE[5]@ explored general text representations induced by VQ-VAE for the classification especially in low-resource domains. @CITE[6]@ designed a semi-supervised VQ-VAE for new event type induction, while @CITE[4]@ equipped the encode-decoder architecture with the VQ-VAE to automatically select relevant evidence. In this paper, we move one step further by guiding the vector quantization with topics learned by NTMs.\\nReferences:ID: 1. Title: Neural Discrete Representation Learning\\nID: 2. Title: Unsupervised speech representation learning using WaveNet autoencoders\\nID: 3. Title: Fast Decoding in Sequence Models using Discrete Latent Variables\\nID: 4. Title: Discrete Latent Variable Representations for Low-Resource Text Classification\\nID: 5. Title: PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\\nID: 6. Title: Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder<|eot_id|>'},\n",
       " 47: {'predict': 'The dataset for KG-to-text generation is relatively limited. @CITE[1]@ collected a dataset of 5,000 scientific articles and their corresponding KGs. @CITE[4]@ collected 700k biographies from Wikipedia and the corresponding KGs. However, these datasets are relatively small and lack diversity in terms of the type of entities, the length of the text, and the complexity of the KG. To the best of our knowledge, our dataset is the first KG-to-text generation dataset that contains a large number of diverse entities, texts, and KGs. The task of KG-to-text generation is closely related to other NLP tasks such as semantic role labeling (SRL) (@CITE[2]@, semantic parsing (Kasai et al., 2019), and abstract meaning representation (AMR) parsing and generation (@CITE[3]@. The main difference is that KG-to-text generation requires a model to understand the complex relationships between entities and generate a coherent text. In this paper, we propose a two-stage framework that first generates a sequence of abstract sentences and then generates the final text based on the sequence of abstract sentences.\\nReferences:ID: 1. Title: Text Generation from Knowledge Graphs with Graph Transformers\\nID: 2. Title: Neural Text Generation from Structured Data with Application to the Biography Domain\\nID: 3. Title: Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling\\nID: 4. Title: Neural AMR: Sequence-to-Sequence Models for Parsing and Generation',\n",
       "  'ground_truth': \"\\n\\nThere is an increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. introduced the WebNLG challenge, which aimed to generate text from a small set of RDF knowledge triples (no more than 7) that are well-aligned with the text. To avoid the high cost of preparing such well-aligned data, researchers also studied how to leverage automatically obtained partially-aligned data in which some portion of the output text cannot be generated from the input triples (@CITE[2]@. @CITE[1]@ introduced AGENDA dataset, which aimed to generate paper abstract from a title and a small KG built by information extraction system on the abstracts and has at most 7 relations. In our work, we directly create a knowledge graph for the main entities and topic-related entities from Wikidata without looking at the relations in our output. Scale-wise, our dataset consists of 110k instances while AGENDA is 40k. introduced WIKIBIO dataset that generates the first sentence of biographical articles from the key-value pairs extracted from the article's infobox. introduced E2E dataset in the restaurant domain, which aimed to generate restaurant recommendations given 3 to 8 slot-value pairs. These two datasets were only for a single domain, while ours focuses on multiple domains of over 100 categories, including people, event, location, organization, etc. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not key-value pairs. Another popular task is AMR-to-text generation (@CITE[3]@. The structure of AMR graphs is rooted and denser, which is quite different from the KG-to-text task. Researchers also studied how to generate texts from a few given entities or prompts (; @CITE[4]@. However, they did not explore the knowledge from a KG.\\nReferences:ID: 1. Title: Text Generation from Knowledge Graphs with Graph Transformers\\nID: 2. Title: Neural Text Generation from Structured Data with Application to the Biography Domain\\nID: 3. Title: Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling\\nID: 4. Title: Neural AMR: Sequence-to-Sequence Models for Parsing and Generation<|eot_id|>\"},\n",
       " 5201: {'predict': 'There have been many works on question generation, which can be roughly categorized into retrieval-based and generation-based approaches. For retrieval-based approaches, they retrieve questions from a question database based on the context (@CITE[2]@; @CITE[5]@; @CITE[1]@. For generation-based approaches, they generate questions based on the context using neural networks (@CITE[4]@; @CITE[3]@; @CITE[3]@. However, the generated questions are often of low quality and lack diversity, which is not suitable for question answering tasks. In this paper, we use the retrieval-based approach, but we retrieve questions from a question database based on the context and generate questions based on the retrieved questions. To the best of our knowledge, this is the first work that uses the retrieval-based approach to generate questions for question answering tasks.\\nReferences:ID: 1. Title: Learning to Ask: Neural Question Generation for Reading Comprehension\\nID: 2. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 3. Title: The Curious Case of Neural Text Degeneration\\nID: 4. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 5. Title: Hierarchical Neural Story Generation',\n",
       "  'ground_truth': '\\n\\nEarly attempts on QG are rule-based (; @CITE[3]@, which are inflexible and labor-intensive. In addition, such methods are not able to generate questions from a larger context. Sequence-to-sequence-based methods (@CITE[2]@; Kumar et al., 2019) are able to overcome such issues, leading to better results. Recently, supervised fine-tuning pre-trained language models (PLM) have shown to achieve significant improvement (;. These systems, however, mostly focus on consistency, whereas diversity is also essential for downstream tasks such as QA. Prior attempts at diversity can be divided into two main categories, those that make use of internal knowledge such as content selection (@CITE[4]@; ; and improved decoding (@CITE[1]@;, and those that exploit external patterns (Deschamps et al., 2021; @CITE[5]@. Our work falls into the latter category but attempts to do so without samples for question rewriting.\\nReferences:ID: 1. Title: Learning to Ask: Neural Question Generation for Reading Comprehension\\nID: 2. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 3. Title: The Curious Case of Neural Text Degeneration\\nID: 4. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 5. Title: Hierarchical Neural Story Generation<|eot_id|>'},\n",
       " 3775: {'predict': 'SAM is an optimization technique that encourages the model to converge to a flat minimum, rather than a sharp one. The key idea is to introduce an auxiliary regularization term that is the product of the loss and the norm of the gradient. The gradient norm is used to measure the sharpness of the loss landscape. The motivation of SAM comes from adversarial training (@CITE[1]@, 2021), which uses adversarial examples to perturb the input. The gradient norm can be seen as the measure of the adversarial perturbation. SAM has been applied to various deep learning tasks, including image classification (@CITE[4]@, language modeling (@CITE[2]@, and reinforcement learning (@CITE[3]@. In this paper, we use SAM to improve the robustness of the model against the adversarial attack.\\nReferences:ID: 1. Title: Regularizing Neural Networks via Adversarial Model Perturbation\\nID: 2. Title: Sharpness-Aware Minimization Improves Language Model Generalization\\nID: 3. Title: High-Performance Large-Scale Image Recognition Without Normalization\\nID: 4. Title: Sharpness-Aware Minimization Improves Language Model Generalization',\n",
       "  'ground_truth': '\\n\\nForet et al. (2020) leverage the correlation between flat minima and better model generalization and propose SAM for training deep neural models that are robust to ad- versarial weight perturbations. It has demonstrated effectiveness in tasks on both vision (Chen et al., 2022; @CITE[3]@ and language (@CITE[1]@ modalities. Several variants of SAM have been proposed to improve its efficiency or effective- ness. For efficiency, @CITE[2]@ propose to speed up SAM by perturbing fewer instances in the batch; @CITE[1]@ introduce stochastic weight perturbation and sharpness-sensitive data selection to reduce the computational overhead. For effec- tiveness; @CITE[4]@ propose to adaptively set the SAM’s radius such that it is invariant to parameters’ scales; Zhuang et al. (2022) introduce a gradient ascent optimization step in the perturbed model’s orthogonal direction to achieve better flat- Algorithm 1: SAM and δ-SAM Input: model fw, training set S ≜{(xi, yi)}|S| i=1, loss function l : W × X × Y →R+, batch size N, neighborhood size ρ ∈R+, optimizer h. Output: a flat solution ˆ. Initialize model weights w. while not converge do ness. In contrast to the aforementioned studies, this paper, for the first time, tackles how to narrow the gap of per-instance and per-batch weight perturba- tion. Accordingly, we propose an efficient approxi- mation to per-instance weight perturbation, which shows improved results on several NLP tasks while does not bring much computational overheads.\\nReferences:ID: 1. Title: Regularizing Neural Networks via Adversarial Model Perturbation\\nID: 2. Title: Sharpness-Aware Minimization Improves Language Model Generalization\\nID: 3. Title: High-Performance Large-Scale Image Recognition Without Normalization\\nID: 4. Title: Sharpness-Aware Minimization Improves Language Model Generalization<|eot_id|>'},\n",
       " 7146: {'predict': 'GEC has been studied for decades, with the first GEC shared task held in 2007. The task has been tackled with rule-based approaches, statistical machine translation, and neural network-based models. Rule-based approaches (@CITE[2]@; @CITE[4]@) have been shown to be effective for detecting and correcting grammatical errors. However, they are limited in their ability to handle complex and context-dependent errors. Statistical machine translation approaches (@CITE[3]@; ) have been shown to be effective in detecting and correcting grammatical errors. However, they are limited in their ability to capture the syntactic and semantic information of the sentence. Recently, neural network-based models have become increasingly popular in GEC. These models can capture the syntactic and semantic information of the sentence and have been shown to be effective in detecting and correcting grammatical errors. For example, @CITE[1]@ proposed a fluency boost learning and inference mechanism for GEC. They used a convolutional sequence-to-sequence model and proposed a fluency boosting learning mechanism to generate diverse error-corrected sentence pairs during training. @CITE[1]@ proposed a generic and language-independent strategy for multilingual GEC. They used a pretrained cross-lingual language model and created diverse parallel GEC data without any language-specific operations. @CITE[4]@ proposed to fine-tune a pretrained masked language model (MLM), such as BERT (@CITE[4]@), into an encoder-decoder model for GEC. The fine-tuned MLM was used as additional features in the GEC model. In this paper, we propose to use a pretrained MLM to detect grammatical errors. We use the MLM to predict the probability of the token being grammatically correct. We use the predicted probability as a score to determine whether the token should be corrected.\\nReferences:ID: 1. Title: Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study\\nID: 2. Title: A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model\\nID: 3. Title: Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'ground_truth': \"\\n\\nGEC is a fundamental NLP task that has received wide attention over the past decades. Besides of the early statistical methods, the currently mainstream neural GEC methods are categorized into two groups, i.e., Seq2Seq methods and Seq2Edit methods, in general. Seq2Seq methods treat GEC as a monolingual translation task, regarding errorful sentences as the source language and error-free sentences as the target language (. Some works (@CITE[3]@; @CITE[1]@ generate considerable synthetic data based on the symmetry of the Seq2Seq's structure for data augmentation. In addition, some works (@CITE[4]@; Zhang et al., 2022b) feed additional features into the neural network to improve GEC, such as the BERT (@CITE[2]@ presentation or syntactic structure of the input sentence.\\nReferences:ID: 1. Title: Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study\\nID: 2. Title: A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model\\nID: 3. Title: Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<|eot_id|>\"},\n",
       " 6962: {'predict': 'Temporal reasoning task is to answer questions that require reasoning about time. @CITE[1]@ propose SituatedQA, a large-scale QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. @CITE[4]@ introduce StreamingQA, a new large-scale dataset to study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge. @CITE[3]@ propose a three-stage TextTableQA framework, which comprises of retriever, selector, and reasoner. @CITE[2]@ introduce a comprehensive probing dataset to evaluate the temporal reasoning capability of large language models.\\nReferences:ID: 1. Title: SituatedQA: Incorporating Extra-Linguistic Contexts into QA\\nID: 2. Title: StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models\\nID: 3. Title: S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering\\nID: 4. Title: Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models',\n",
       "  'ground_truth': '\\n\\nThere have been plenty of works to tackle the temporal reasoning task. @CITE[4]@ was introduced to tackle open-domain time-sensitive question answering, with a particular emphasis on analyzing how answers differ based on extra-linguistic factors, such as the time of inquiry. Kasai et al. (2022) extended time question answering to scenarios where real-time news serves as context, and requested the model to retrieve the latest temporal evidence to answer the question. StreamingQA (@CITE[2]@ introduced the first QA dataset and task for studying adaptation to new information over time in open and close-book settings with temporally non-overlapping training and evaluation sets. TimeQA (Chen et al., 2021) built the first dataset to investigate whether existing models can understand time-sensitive facts. There are a few major differences between the aforementioned works and MenatQA: 1) MenatQA encompasses various temporal factors, such as the scope factor, order factor, and counterfactual factor, involving a significant amount of reasoning about implicit temporal information. This aspect of temporal reasoning ability, which is neglected by previous works, is the most important. 2) MenatQA is not only the first dataset designed specifically for evaluating the time understanding and reasoning capabilities of LLMs, but also provides some simple optimization methods and baseline comparisons, which offer valuable references for evaluating the time reasoning of LLMs in the future. 3) Considering the existence of hallucinations in generative models, we introduce unanswerable types to penalize the illusory outputs of LLMs in MenatQA. These unanswerable type questions are impossible for humans to answer as well, and enable a genuine assessment of whether LLMs truly grasp temporal knowledge. One concurrent work (published on 15 @CITE[3]@ similar to ours is (@CITE[1]@, which proposed a comprehensive probing dataset TEMPREASON to evaluate the temporal reasoning capability of language models. They also proposed a temporal span extraction and time-sensitive reinforcement learning framework to improve the temporal reasoning capability of large language models. However, they only evaluated three models, T5-Large (780M), Flan-T5-Large (780M), and GPT-3.5-turbo (175B), and mainly focused on using fine-tuning to improve the time reasoning ability of T5-Large and Flan-T5-Large. Besides, the fine-tuning based improvement methods are not applicable to large language models, such as OPT-175B. Our work aims to evaluate the time reasoning capability of current mainstream LLMs on three time-sensitive factors, and conducts preliminary investigations to improve the current LLMs on different time factors by designing various specific prompts and tool learning.\\nReferences:ID: 1. Title: SituatedQA: Incorporating Extra-Linguistic Contexts into QA\\nID: 2. Title: StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models\\nID: 3. Title: S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering\\nID: 4. Title: Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models<|eot_id|>'},\n",
       " 6027: {'predict': 'Language models are the core of many NLP applications. They are trained on large amounts of text data, and can be used for text generation, text classification, machine translation, and other tasks. Language models have seen tremendous progress in recent years, with the introduction of the Transformer architecture (@CITE[1]@, the development of large pre-trained models like GPT2 (@CITE[3]@, and the scaling of these models to billions of parameters (@CITE[2]@. While the scaling of language models has led to significant improvements on many tasks, it has also led to concerns about bias and toxicity (Weideman et al., 2022; @CITE[3]@. Recent work has also explored the use of external memory to improve language models. For example, Transformer-XL (@CITE[3]@ uses a large external memory to store previously seen tokens, and retrieves them when needed. Similarly, Adaptive Semiparametric Language Models (ASLM) (@CITE[3]@ uses a non-parametric episodic memory component to store and retrieve previously seen tokens. Our approach differs from these methods in that we use external memory to store the predictions of the model, rather than the tokens themselves. This allows us to leverage the model’s own predictions, rather than relying on external data. Additionally, we use a different memory mechanism, which is based on a graph neural network (GNN) rather than a traditional memory network.\\nReferences:ID: 1. Title: Scaling Language Models: Methods, Analysis & Insights from Training Gopher\\nID: 2. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 3. Title: Adaptive Semiparametric Language Models',\n",
       "  'ground_truth': '\\n\\nLarge LMs trained on a vast amount of text are shown to perform a wide range of downstream tasks in a zero-shot manner by converting a task into a cloze format (; @CITE[2]@). This is possible because a variety of knowledge is encoded in the parameters of the models. Recent work has scaled parametric LMs by adding more parameters (@CITE[2]@; ; @CITE[3]@), which can be very expensive in practice. Moreover, such models struggle with predicting rare words or entities, and cannot be updated over time. There has been a recent body of work that incorporates the nonparametric component with a parametric LM. We distinguish (1) work that concatenates retrieved text to the input and trains the model with a standard LM objective (Borgeaud et al. (2022); @CITE[1]@; so-called retrieve-and-generate approaches) from (2) work that retrieves tokens from a large text corpus to estimate a probability distribution that is interpolated with the output distribution from a standard LM (Khandelwal et al. (2020); ; Zhong et al. (2022b); ; so-called kNN models). Our work is closely related to such a line of work and can be seen as an extreme version of the kNN approach with no interpolation. However, our work is the first that models a fully nonparametric distribution by entirely removing the softmax over a finite vocabulary. This offers a range of new functionalities, such as modeling a distribution over phrases, or predicting rare or unseen words.\\nReferences:ID: 1. Title: Scaling Language Models: Methods, Analysis & Insights from Training Gopher\\nID: 2. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 3. Title: Adaptive Semiparametric Language Models<|eot_id|>'},\n",
       " 2912: {'predict': 'In the field of text classification, input salience methods are widely used to explain the predictions of the model. For example, @CITE[1]@ proposed a method to measure the importance of words in a sentence by computing the gradient of the output with respect to the input. @CITE[3]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the embedding of the word. @CITE[4]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s one-hot encoding. @CITE[2]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s embedding and the context. @CITE[2]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s embedding and the context. @CITE[2]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s embedding and the context. @CITE[2]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s embedding and the context. @CITE[2]@ proposed a method to measure the importance of words by computing the gradient of the output with respect to the word’s embedding and the context.\\nReferences:ID: 1. Title: What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play\\nID: 2. Title: The (Un)reliability of saliency methods\\nID: 3. Title: Debugging Tests for Model Explanations\\nID: 4. Title: Benchmarking Attribution Methods with Relative Feature Importance',\n",
       "  'ground_truth': '\\n\\nResearch on input salience methods for text classification is prolific and diverse in terms of the definitions used (Camburu et al., 2020), applications (@CITE[4]@), desiderata (Sundararajan et al., 2017b), etc. The importance of getting faithful salience explanations has been recognized early on (; @CITE[2]@) and there exist formal definitions of explanation fidelity (@CITE[1]@). However, these have not been connected to model debugging where it is the top of a salience ranking that matters most. In the vision domain, our work is closest to, 2022), who also explore the debugging scenario with salience maps, @CITE[3]@, who use synthetic data to obtain the ground truth for pixel importance, and Hooker et al. (2019), who contrast the performance of the same model trained on original and modified data when evaluating feature importance.\\nReferences:ID: 1. Title: What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play\\nID: 2. Title: The (Un)reliability of saliency methods\\nID: 3. Title: Debugging Tests for Model Explanations\\nID: 4. Title: Benchmarking Attribution Methods with Relative Feature Importance<|eot_id|>'},\n",
       " 2139: {'predict': 'The existing speaker-aware dialogue models mainly focus on the speaker role in the dialogue context, and they can be divided into two categories: (1) Speaker-aware pre-trained language models: @CITE[3]@, @CITE[2]@, and @CITE[5]@ propose to mask the speaker information in the pre-trained language models (PrLMs) to disentangle the speaker role in the dialogue context. (2) Speaker-aware task-specific models: @CITE[3]@ and @CITE[1]@ propose to model the speaker role in the dialogue context by adding a speaker-aware module to the pre-trained language models (PrLMs). However, they only focus on the speaker role in the dialogue context and ignore the speaker information in the response. In contrast, our proposed model focuses on the speaker information in both the dialogue context and the response. The speaker information in the dialogue context is captured by the speaker-aware module, while the speaker information in the response is captured by the speaker-aware response module. The speaker information in the dialogue context and the response is then fused to capture the speaker information in the dialogue-response pair.\\nReferences:ID: 1. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 2. Title: Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue\\nID: 3. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 4. Title: Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue\\nID: 5. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement',\n",
       "  'ground_truth': '\\n\\nSpeaker role, as a feature of dialogue passage, has received growing attention recently. On the one hand, speaker embedding facilities research of dialogues. Speaker-aware modeling has also made contributions to response retrieval (@CITE[2]@; @CITE[5]@. SA-BERT (@CITE[2]@ add a speaker embedding to the input of a PrLM, while MDFN (@CITE[5]@ modifies self-attention to enhance speaker switches. Persona has been utilized for smoother dialogue generation. In recent work (@CITE[2]@, the speaker-aware information is modeled by adding a reward of persona proximity to the reinforcement learning of generation, based on a persona-annotated dataset (@CITE[4]@. On the other hand, speakers role is a valuable research object for personal knowledge analysis, since the persona can be extracted from one’s words in dialogues. Relationship prediction task has been better handled through observing interactions of dialogue speakers (@CITE[1]@; @CITE[3]@. @CITE[3]@ make use of speaker identity by a SA-BERT (@CITE[2]@-like embedding but in utterance-level representation. Relations between utterances have been studied for a long time. Earlier works mostly based on pioneer datasets, Penn Discourse TreeBank.\\nReferences:ID: 1. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 2. Title: Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue\\nID: 3. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement\\nID: 4. Title: Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue\\nID: 5. Title: DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement<|eot_id|>'},\n",
       " 3543: {'predict': 'The analysis of PLMs has been a hot topic in NLP research. The main focus of the analysis is on the interpretability of PLMs. @CITE[4]@ and @CITE[2]@ studied the linguistic information stored in PLMs and found that PLMs store linguistic information in a pipeline fashion. @CITE[3]@; @CITE[6]@; @CITE[3]@; @CITE[7]@; @CITE[5]@ investigated the transferability of PLMs, showing that the representations learned by PLMs can be transferred to downstream tasks. Probing is a popular method for analyzing PLMs. @CITE[4]@; @CITE[6]@; @CITE[3]@; @CITE[7]@; @CITE[5]@; @CITE[1]@; @CITE[3]@; @CITE[6]@; @CITE[7]@; @CITE[5]@; @CITE[1]@ investigated the linguistic knowledge captured by PLMs. @CITE[6]@; @CITE[3]@ investigated the impact of fine-tuning on the linguistic knowledge captured by PLMs. @CITE[7]@ investigated the impact of the size of the training corpus on the linguistic knowledge captured by PLMs. @CITE[5]@ investigated the impact of the task type on the linguistic knowledge captured by PLMs. @CITE[1]@ investigated the impact of the model architecture on the linguistic knowledge captured by PLMs.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives\\nID: 3. Title: Linguistic Knowledge and Transferability of Contextual Representations\\nID: 4. Title: Linguistic Knowledge and Transferability of Contextual Representations\\nID: 5. Title: Investigating Transferability in Pretrained Language Models\\nID: 6. Title: On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers\\nID: 7. Title: Probing Classifiers: Promises, Shortcomings, and Advances',\n",
       "  'ground_truth': '\\n\\nRepresentations from PLMs have been widely studied using probing methods. There is evidence showing that pre-trained features from intermediate layers are more transferable (; @CITE[4]@). Additionally, @CITE[3]@ showed that masked language modelling objective introduces an autoencoder-like structure in the PLM, meaning the behavior of the top most layers is similar to the bottom layers. Studies have also shown that the effects of fine-tuning are non-uniformly spread across different layers (; Merchant et al., 2020; ; Phang et al., 2021). Those findings challenge the default choice of tuning the entire PLM for adapting it to downstream tasks. While probing tools have been used to study task-specific layer importance (@CITE[2]@; @CITE[5]@), the probing paradigm is parametric, hard to interpret, and is known to be unreliable (@CITE[4]@; @CITE[7]@; @CITE[6]@; @CITE[1]@; Pimentel et al., 2020). Instead, we propose to measure the layer-wise task-specialty of PLMs using a non-parametric tool that quantifies the task-specific variability of the hidden features.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives\\nID: 3. Title: Linguistic Knowledge and Transferability of Contextual Representations\\nID: 4. Title: Linguistic Knowledge and Transferability of Contextual Representations\\nID: 5. Title: Investigating Transferability in Pretrained Language Models\\nID: 6. Title: On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers\\nID: 7. Title: Probing Classifiers: Promises, Shortcomings, and Advances<|eot_id|>'},\n",
       " 6701: {'predict': 'Toxic and biased generations from LLMs have been widely studied in recent years. A large number of studies (@CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[6]@ have demonstrated that LLMs are prone to toxic and biased generations, such as gender and race biases. These studies have shown that the toxic and biased generations can be triggered by using universal adversarial triggers (Gururangan et al., 2018; @CITE[6]@. Some studies (@CITE[1]@; @CITE[2]@ have proposed a few-shot prompt tuning method to reduce toxic and biased generations. However, these methods require additional human-annotated data and are not effective for all types of toxic and biased generations. In this paper, we propose a few-shot prompt learning method to reduce toxic and biased generations without additional human-annotated data.\\nReferences:ID: 1. Title: Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior\\nID: 2. Title: A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity\\nID: 3. Title: Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes\\nID: 4. Title: Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings\\nID: 5. Title: Social Biases in NLP Models as Barriers for Persons with Disabilities\\nID: 6. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP',\n",
       "  'ground_truth': '\\n\\nCaliskan et al. (2017); ; @CITE[3]@; @CITE[2]@ show problematic biases in pre-training, which could reinforce existing stereotypes and prejudices. @CITE[6]@; Zhuo et al. (2023); ; Kurita et al. (2019); @CITE[4]@; Sheng et al. (2019); ; Zhao et al. (2019); @CITE[1]@; Basta et al. (2019) reveal systematic biases and stereotypical correlations in LLMs. @CITE[5]@; highlight the brittleness of LLMs by generating toxic outputs through “trigger” words.\\nReferences:ID: 1. Title: Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior\\nID: 2. Title: A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity\\nID: 3. Title: Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes\\nID: 4. Title: Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings\\nID: 5. Title: Social Biases in NLP Models as Barriers for Persons with Disabilities\\nID: 6. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP<|eot_id|>'},\n",
       " 2263: {'predict': 'Prompting has been shown to be an effective way to elicit knowledge from PLMs (@CITE[3]@; @CITE[1]@; @CITE[5]@; @CITE[4]@. There are two main ways to construct prompts: (1) manual design (@CITE[2]@; @CITE[6]@; ; @CITE[6]@; ; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[6]',\n",
       "  'ground_truth': '\\n\\nNatural language prompting freezes pretrained models and reformats the natural language input with example prompts. GPT-3 (Brown et al., 2020) introduces in-context learning, using manually designed and discrete text prompts. Sun et al. (@CITE[3]@ also leverage prompts as keywords to control the sentiment or topic of the generated sentence. AutoPrompt (@CITE[1]@ searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from a masked LM. Different from the discrete text prompt, some methods examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Prefix-Tuning (@CITE[2]@ prepends a sequence of continuous task-specific vectors as virtual tokens to the input. (; @CITE[5]@; @CITE[4]@ introduce continuous templates following manual prompt templates. P- tuning (@CITE[6]@ uses continuous prompts which are learned by inserting trainable variables into the embedded input. Ptr (@CITE[2]@ adopts manually crafted sub-templates and generates complete templates by logic rules. In ProbES, we prepend continuous task-specific vectors to the embedding of the input instruction and directly tune the embeddings of these vectors. After prompt tuning, the model can be adapted to VLN and REVERIE tasks.\\nReferences:ID: 1. Title: Conditioned Natural Language Generation using only Unconditioned Language Model: An Exploration\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: PTR: Prompt Tuning with Rules for Text Classification\\nID: 4. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 5. Title: WARP: Word-level Adversarial ReProgramming\\nID: 6. Title: GPT Understands, Too<|eot_id|>'},\n",
       " 495: {'predict': 'Few-shot learning (FSL) aims to learn a model that can be fine-tuned with a few examples to perform well on a new task. In the context of NLP, FSL has been studied in various settings, such as text classification, machine translation, and language modeling. @CITE[1]@ proposed a simple yet effective approach to learn a prototype for each class, and the distance between the prototype and the representation of an example is used to make the prediction. @CITE[3]@ proposed a method to learn a meta-learner, which is trained to learn a model from a few examples of a task. @CITE[2]@ proposed a method to learn a model by training a model with a few examples of a task in a meta-learning manner. In addition, there are many other approaches to FSL, such as generative methods (@CITE[2]@, attention-based methods (@CITE[2]@, and adversarial methods (@CITE[1]@. However, these approaches are not designed for the specific task of NER.\\nReferences:ID: 1. Title: Matching Networks for One Shot Learning\\nID: 2. Title: Prototypical Networks for Few-shot Learning\\nID: 3. Title: A Simple Neural Attentive Meta-Learner',\n",
       "  'ground_truth': '\\n\\nFew-shot learning refers to problems where classifiers are required to generalize to unseen classes with only a few training examples per class (). To overcome challenges of potential overfitting, most FSL methods adopt a meta-learning approach where knowledge is extracted and transferred across multiple tasks. There are two major approaches towards FSL: (1) metric-based approach whose goal is to learn feature extractor that extracts and generalizes to emerging classes (; @CITE[2]@; ), and (2) optimization-based approach that aims to optimize model parameters from few samples (@CITE[1]@; ; @CITE[3]@; ). In this work, we focus mostly on the metric-based learning approach. Specifically, we extend Prototypical Network (PN) (@CITE[2]@) in which prototypes are not only represented by support samples but also matching information between support and query samples. Traditionally, FSL methods are evaluated in an episodic procedure due to the major principle that test and train conditions must match (). Each episode represents a meta-learning task in which the models explicitly \"learn to learn\" minimize the loss on an unlabeled/query set given the support/labeled set. However, we claim that this evaluation lacks practicality for two main reasons. First, evaluation on random samples could not help us understand the strengths or weaknesses of the model. For instance, if the trained model overfits a subset of novel classes, it is impossible to pinpoint the overfitting classes with episodic evaluation. Secondly, in realistic applications, there is a need to categorize unlabeled samples into one of the novel/joint classes, rather than a set of sampled classes. Episodic testing does not provide an end-to-end systematic evaluation. Therefore, in our work, we propose a more challenging but realistic non-episodic evaluation setting where unlabeled samples are only inferred once with a probability distribution over a fixed set of classes in novel or joint label space.\\nReferences:ID: 1. Title: Matching Networks for One Shot Learning\\nID: 2. Title: Prototypical Networks for Few-shot Learning\\nID: 3. Title: A Simple Neural Attentive Meta-Learner<|eot_id|>'},\n",
       " 5484: {'predict': 'Critical reasoning is a type of reasoning that requires the understanding of the underlying logical rules and the ability to draw conclusions based on them. Critical reasoning has been extensively studied in the NLP community, with the majority of the work focusing on logical reasoning of text. The most common task is the logical entailment task, which requires the model to determine whether the conclusion follows logically from the premise. This task has been shown to be challenging for LLMs (@CITE[3]@; @CITE[1]@. However, LLMs have shown strong performance on other critical reasoning tasks, such as abductive reasoning (@CITE[2]@, which requires the model to come up with a plausible explanation for a given situation. The ability to perform logical reasoning has been shown to be important for understanding natural language arguments (@CITE[3]@. This has led to the development of argumentation mining datasets, such as ArgMining (@CITE[3]@ and Argumentation Mining (@CITE[3]@. These datasets are composed of natural language arguments and their corresponding logical forms. The logical forms are used to evaluate the models’ ability to perform logical reasoning. In this work, we use the logical forms to generate explanations for the arguments, which are then used to evaluate the models’ ability to perform abductive reasoning.\\nReferences:ID: 1. Title: Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text\\nID: 2. Title: Abductive Commonsense Reasoning\\nID: 3. Title: Probing Neural Network Comprehension of Natural Language Arguments',\n",
       "  'ground_truth': '\\n\\nCritical reasoning is one of the core abilities of logical reasoning that humans perform, along with analytical reasoning (@CITE[1]@ and abductive reasoning (@CITE[3]@. This reasoning is related to understanding the structure of practical arguments that is generally composed of ground (premise), warrant (rationale), and claim (conclusion). As formulated by @CITE[2]@, given facts or data as the ground, we provide the warrant that acts as a bridge between the ground and the claim we are making. Recent research includes developing ways to model this behavior in tasks such as argument mining and question answering (QA) (e.g., ReClor). For example, Habernal et al. (2018) propose a task of identifying implicit rationale (i.e., warrant) in arguments. However, find that successful systems on the argument reasoning task exploit superficial input features. Similarly, QA systems have been shown to exhibit shallow understanding by input perturbation (; ;. For example, demonstrate that QA performance significantly decreases when incorrect options are replaced with irrelevant texts in an adversarial manner. This means that successful models on those datasets do not necessarily exhibit generalizable capabilities in other datasets. These findings necessitate the explainability of the (informal) logical reasoning process for better evaluation of intended reasoning abilities (e.g., the critical reasoning in this study).\\nReferences:ID: 1. Title: Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text\\nID: 2. Title: Abductive Commonsense Reasoning\\nID: 3. Title: Probing Neural Network Comprehension of Natural Language Arguments<|eot_id|>'},\n",
       " 40: {'predict': 'Recent work has explored unsupervised question answering, which aims to learn from unlabeled data without any supervision. @CITE[7]@ propose to leverage a pre-trained language model to generate pseudo questions and answers. @CITE[1]@; @CITE[2]@; @CITE[6]@; @CITE[3]@; @CITE[5]@; @CITE[4]@ generate pseudo questions or answers using the pre-trained language models. @CITE[7]@ propose to use the retrieved sentences to generate pseudo questions and answers. @CITE[7]@; @CITE[4]@; @CITE[1]@ propose to train QA models on the generated pseudo data. @CITE[3]@ propose to use a template-based method to generate pseudo questions and answers. Our work is most similar to @CITE[3]@, which also uses a template-based method to generate pseudo questions and answers. However, our work focuses on generating more complex questions and answers. In addition, our work uses a more powerful model to generate pseudo data, which achieves better performance.\\nReferences:ID: 1. Title: Supervised and Unsupervised Transfer Learning for Question Answering\\nID: 2. Title: Leveraging Knowledge Bases in LSTMs for Improving Machine Reading\\nID: 3. Title: Simple and Effective Semi-Supervised Question Answering\\nID: 4. Title: Commonsense for Generative Multi-Hop Question Answering Tasks\\nID: 5. Title: Latent Retrieval for Weakly Supervised Open Domain Question Answering\\nID: 6. Title: Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering\\nID: 7. Title: Training Question Answering Models From Synthetic Data',\n",
       "  'ground_truth': '\\n\\nRecent work on unsupervised question answering approach the problem in two ways, a domain adaptation or transfer learning problem (, or a data augmentation problem (@CITE[6]@; @CITE[5]@; @CITE[3]@; Alberti et al., 2019). The work of (@CITE[4]@; ; use style transfer or template-based question, context, and answer triple generation, and learn using these to perform unsupervised extractive question answering. There is another approach to learning generative models, generating the answer given a question or clarifying explanations and questions, such as GPT-2 (Radford et al., 2019) to perform unsupervised question answering (; @CITE[1]@; @CITE[1]@. In the visual domain, zero-shot visual question answering is studied in (@CITE[7]@, and a self-supervised learning method for logical compositions of visual questions is proposed in (@CITE[2]@. In contrast, our work focuses on learning from knowledge graphs and generate vector representations or sequences of tokens not restricted to the answer but including the context and the question using the masked language modeling objective.\\nReferences:ID: 1. Title: Supervised and Unsupervised Transfer Learning for Question Answering\\nID: 2. Title: Leveraging Knowledge Bases in LSTMs for Improving Machine Reading\\nID: 3. Title: Simple and Effective Semi-Supervised Question Answering\\nID: 4. Title: Commonsense for Generative Multi-Hop Question Answering Tasks\\nID: 5. Title: Latent Retrieval for Weakly Supervised Open Domain Question Answering\\nID: 6. Title: Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering\\nID: 7. Title: Training Question Answering Models From Synthetic Data<|eot_id|>'},\n",
       " 5385: {'predict': 'Most existing datasets for machine-generated text detection are based on pre-trained models and are mainly focused on detecting text generated by pre-trained models (@CITE[8]@; @CITE[5]@; @CITE[6]@; @CITE[3]@; @CITE[2]@; @CITE[4]@; @CITE[7]@; @CITE[1]@. These datasets are usually limited to specific domains (e.g. news, reviews, and social media) and are often biased towards certain types of generators. In contrast, we aim to build a general-purpose dataset for detecting machine-generated text in a wide range of domains and applications.\\nReferences:ID: 1. Title: Defending Against Neural Fake News\\nID: 2. Title: TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation\\nID: 3. Title: SynSciPass: detecting appropriate uses of scientific text generation\\nID: 4. Title: Generating Sentiment-Preserving Fake Online Reviews Using Neural Language Models and Their Human- and Machine-based Detection\\nID: 5. Title: TweepFake: about Detecting Deepfake Tweets\\nID: 6. Title: Deepfake Text Detection: Limitations and Opportunities\\nID: 7. Title: Stylometric Detection of AI-Generated Text in Twitter Timelines\\nID: 8. Title: Instruction Tuning with GPT-4\\nID: 9. Title: Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian',\n",
       "  'ground_truth': '\\n\\nSince transformer-based generative language models became ubiquitous in 2018, researchers have released datasets to address the new problem of machine-generated text detection. The most popular datasets are GPT-22 (Radford et al., 2019) and GROVER3 (@CITE[8]@ generated texts. However, as more Language Models (LM) got deployed, the need for more datasets from different LMs arose. Therefore Uchendu et al. (2020) released the first multi-generator (8 LMs) dataset in the news domain and released the first benchmark dataset (19 LMs) and environment for machine-generated text detection. Next, researchers released datasets for different domains - Academic papers (Liyanage et al., 2022;, News, Reddit posts & Recipes (@CITE[7]@, Amazon reviews (@CITE[2]@, multi-modal (i.e., images & texts) news articles (Tan et al., 2020), Tweets (, COVID-19 articles (, deepfake text in-the-wild (@CITE[9]@, gamification of MGT detection (@CITE[3]@, essays (, prompt-generation (;, and multi-generator (27 LLMs) multi-domain (i.e., news, story, question, argument, scientific, etc.) data (@CITE[6]@. However, the vast majority of these datasets are in English. A few researchers released non-English datasets in Russian (@CITE[1]@, Chinese (@CITE[5]@, Iberian languages (Sarvazyan et al., 2023), and M4 which contains Russian, Chinese, Urdu, Indonesian, & Arabic languages (@CITE[4]@.\\nReferences:ID: 1. Title: Defending Against Neural Fake News\\nID: 2. Title: TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation\\nID: 3. Title: SynSciPass: detecting appropriate uses of scientific text generation\\nID: 4. Title: Generating Sentiment-Preserving Fake Online Reviews Using Neural Language Models and Their Human- and Machine-based Detection\\nID: 5. Title: TweepFake: about Detecting Deepfake Tweets\\nID: 6. Title: Deepfake Text Detection: Limitations and Opportunities\\nID: 7. Title: Stylometric Detection of AI-Generated Text in Twitter Timelines\\nID: 8. Title: Instruction Tuning with GPT-4\\nID: 9. Title: Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian<|eot_id|>'},\n",
       " 4108: {'predict': 'Synthetic dialogue data generation has been studied extensively. Some approaches are based on pre-trained language models. For instance, @CITE[5]@ and @CITE[3]@ use BERT to fill in the gaps between sentences to generate dialogue data. @CITE[4]@ use GPT2 to simulate the interaction between crowd workers by creating a user bot and an agent bot. @CITE[1]@ propose an automatic data augmentation technique grounded on documents through a generative dialogue model. Some approaches are based on template generation. For instance, @CITE[2]@ use a template-based approach to generate a large-scale dialogue dataset for task-oriented dialogue systems. @CITE[1]@ propose a data augmentation technique for task-oriented dialogue systems. Our work is different from the above works in that we focus on generating dialogue data for the document-grounded conversation.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Dialog Inpainting: Turning Documents into Dialogs\\nID: 3. Title: Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions\\nID: 4. Title: DG2: Data Augmentation Through Document Grounded Dialogue Generation\\nID: 5. Title: Dialog Inpainting: Turning Documents into Dialogs',\n",
       "  'ground_truth': '\\n\\nWith the superior development of pre-trained mod- els (; Brown et al., 2020; Bom- masani et al., 2021), many researchers have started to exploit the generation of synthetic dialogue data for training better dialogue models (; @CITE[5]@; Mehri et al., 2022). @CITE[1]@ and @CITE[3]@ both employ two pre-trained models, as a user bot and an agent bot respectively, to simulate the in- teraction between two human annotators. use template prompts to guide GPT- J, a large-scale pre-trained LM with 6B parame- ters, to generate emotional support conversations. Knowledge Heuristic Sampling Dialogue Flow Synthetic Dialogue (a) Dialogue Flow Construction su,nf sf,nf (b) Dialogue Content Realization (c) Two-level Filtering Figure 2: The overview of SynDG. We omit the superscript s for ui and fi for simplicity. When generating each synthetic dialogue, SynDG (a) constructs a dialogue flow by heuristic sampling from unstructured knowledge data; (b) incrementally realizes every knowledge piece in the flow as an utterance by a fine-tuned T5 model, producing a synthetic dialogue; (c) scores the synthetic dialogue both at the flow-level and utterance-level with two T5-based scorer. Finally, SynDG filters out the synthetic dialogues of low quality based on their scores to obtain the final set of synthetic dialogues. @CITE[5]@ propose to transform a document into a dialogue between the writer and the reader, in which they sequentially treat each sentence in the document as the writer’s utterance and gener- ate the reader’s questions by pre-trained models. Closely related to our work, @CITE[4]@ ex- plore using the reinforcement learning-based gen- erative conversational networks (@CITE[2]@ to generate synthetic conversational data for knowledge-grounded dialogue models. Yet, all the reviewed solutions above are ded- icated to directly generating synthetic dialogues. The significance of explicitly simulating or model- ing dialogue flows is overlooked, which is closely related to the quality of the synthetic dialogue. In this work, we explicitly construct a dialogue flow before turning it into a synthetic dialogue.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Dialog Inpainting: Turning Documents into Dialogs\\nID: 3. Title: Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions\\nID: 4. Title: DG2: Data Augmentation Through Document Grounded Dialogue Generation\\nID: 5. Title: Dialog Inpainting: Turning Documents into Dialogs<|eot_id|>'},\n",
       " 7168: {'predict': 'Following the success of language model pre-training, several works have been proposed for vision pre-training. These works can be divided into two categories: masked image modeling (MIM) (@CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@) and masked feature prediction (@CITE[6]@; @CITE[3]@). In addition, there are also some works that combine visual and textual features to learn better representation (@CITE[8]@; @CITE[7]@; @CITE[9]@; @CITE[10]@; @CITE[10]@; @CITE[10]@; @CITE[10]@; @CITE[10]@; @CITE[10]@). In this work, we focus on MIM and propose a new method to learn better visual representation by using the visual feature as the input of language model.\\nReferences:ID: 1. Title: Masked Autoencoders Are Scalable Vision Learners\\nID: 2. Title: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\\nID: 3. Title: Masked Feature Prediction for Self-Supervised Visual Pre-Training\\nID: 4. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 5. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 6. Title: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\\nID: 7. Title: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\\nID: 8. Title: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\\nID: 9. Title: Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts\\nID: 10. Title: FLAVA: A Foundational Language And Vision Alignment Model',\n",
       "  'ground_truth': '\\n\\n... vision pre-training (@CITE[9]@; ; ; @CITE[7]@; @CITE[1]@ and vision-language pre-training (@CITE[5]@; @CITE[6]@; @CITE[10]@, 2022; ; Wang et al., 2021a; @CITE[7]@; @CITE[8]@, 2022) with Transformer as the backbone have also made significant progress recently, pushing the state-of-the-art of various understanding tasks of language, vision, and vision-language. Recently, the fact that Transformer can model multi-modal data within a single architecture has inspired research to develop general foundation models that can solve language, vision, and vision-language tasks at the same time. UNIMO (@CITE[2]@,c) jointly learns vision representations, language representations, and vision-language alignments in a shared space. FLAVA ( performs pre-training with masked uni-modal and multi-modal modeling objectives. OFA ( formulates vision-language tasks as sequence-to-sequence (seq2seq) problems and pre-trains a seq2seq model in multi-task learning. SimVLM (@CITE[4]@ pre-trains a seq2seq model with a single objective of language generation (prefix language modeling). DaVinci (@CITE[3]@ combines prefix language modeling and prefix image modeling to learn a general foundation model for a wide range of tasks. Uni-Perceiver (Zhu et al., 2021, 2022) builds a unified perception architecture that processes various modalities and tasks with a single Transformer and shared parameters.\\nReferences:ID: 1. Title: Masked Autoencoders Are Scalable Vision Learners\\nID: 2. Title: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\\nID: 3. Title: Masked Feature Prediction for Self-Supervised Visual Pre-Training\\nID: 4. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 5. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 6. Title: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\\nID: 7. Title: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\\nID: 8. Title: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\\nID: 9. Title: Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts\\nID: 10. Title: FLAVA: A Foundational Language And Vision Alignment Model<|eot_id|>'},\n",
       " 1301: {'predict': \"Language models are typically fine-tuned on a task-specific dataset to adapt to a target task. This approach is widely used in the NLP community and has been shown to be effective (@CITE[2]@; @CITE[3]@). However, it is known that this approach is sensitive to the size of the target dataset and can lead to catastrophic forgetting (@CITE[4]@). To address this issue, previous works have proposed to use a multi-phase pretraining and fine-tuning approach, where the model is first pretrained on unlabeled data and then fine-tuned on task-specific data (@CITE[1]@; @CITE[1]@). We use a similar approach, but instead of using unlabeled data, we use a set of auxiliary tasks that can be solved with a similar architecture as the target task. This approach has been shown to be effective for few-shot learning (@CITE[1]@) and can be used to address catastrophic forgetting (@CITE[4]@). In our work, we use a similar approach to improve the generalization of the model to unseen tasks.\\nReferences:ID: 1. Title: Learning and Evaluating General Linguistic Intelligence\\nID: 2. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\\nID: 3. Title: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks\\nID: 4. Title: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks\",\n",
       "  'ground_truth': \"\\n\\nFine-tuning has been the most common approach for applying pre-trained language models to downstream tasks. However, it typically requires a target dataset of thousands to hundreds of thousands of examples to work well (; Brown et al., 2020). Many methods have been proposed to improve performance and stability of pre-trained language models on small datasets, including language model fine-tuning on unlabeled data from the target domain (@CITE[3]@; @CITE[2]@), intermediate-task fine-tuning (@CITE[1]@), multi-task pre-finetuning (), better design choices and training strategies (; Zhang et al., 2021), and regularization-oriented techniques (@CITE[4]@; Aghajanyan et al., 2021b). More related to our work is research on intermediate-task fine-tuning that makes use of data-rich tasks (@CITE[1]@), tasks that require complex reasoning and inference (Pruksachatkun et al., 2020), and beneficial relationships among tasks ().\\nReferences:ID: 1. Title: Learning and Evaluating General Linguistic Intelligence\\nID: 2. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\\nID: 3. Title: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks\\nID: 4. Title: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks<|eot_id|>\"},\n",
       " 1918: {'predict': 'Soft control is a popular method for controlling language models. It is based on the idea of modifying the output distribution of a language model by using a discriminative model. The discriminative model is usually trained to predict the target attribute given the partial text. In this way, the discriminative model is used to guide the language model to generate the desired text. PPLM (@CITE[1]@) is a pioneering work of soft control. It uses a classifier to predict the target attribute given the hidden states of the language model. GeDi (@CITE[2]@) and FUDGE (@CITE[3]@) are two other works that use the discriminative model to guide the language model. GeDi is based on the Bayes rule, while FUDGE is based on the Bayesian decomposition of the conditional distribution of the language model given the target attribute. Compared with these works, we use the discriminative model to guide the language model in a different way. Instead of using the hidden states of the language model as the input to the discriminative model, we use the input tokens as the input to the discriminative model. This is because the hidden states of the language model are not directly accessible in the inference stage, and the input tokens are more relevant to the discriminative model.\\nReferences:ID: 1. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 2. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 3. Title: FUDGE: Controlled Text Generation With Future Discriminators',\n",
       "  'ground_truth': '\\n\\nRelated to our approach of using semantic spaces to control generation, Chang et al. (2021) use the space of GloVe embeddings to define control topics. However, this method is not plug-and-play since it requires fine-tuning a GPT-2 encoder to generate text aligned with the control topics. A number of plug-and-play methods for soft control exist that use external discriminators to steer language generation. Holtzman et al. (2018) train discriminators on different linguistic properties to improve the quality of generated text. @CITE[3]@ use the gradients of an external discriminator to direct the generation of a pre-trained language model towards the target topic. Similarly, @CITE[2]@ use a contrastive strategy to soft-control language generation. @CITE[1]@ directly modify the output probabilities of a (pre-trained) language model using the output of a discriminator that determines whether future text will contain the desired attribute, e.g., formality. Unfortunately, the use of external discriminators limits the applicability of these methods since they require training data for each of the target topics or attributes. Our method elegantly dispenses with the need for a discriminator by using the geometric properties of semantic spaces.\\nReferences:ID: 1. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 2. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 3. Title: FUDGE: Controlled Text Generation With Future Discriminators<|eot_id|>'},\n",
       " 3664: {'predict': 'Event-centric QA (ECQA) has been gaining increasing attention in recent years. @CITE[5]@ propose a dataset for event-centric temporal reasoning. @CITE[2]@ introduce a dataset of temporal ordering questions for event-centric reading comprehension. @CITE[4]@ propose a multi-axis annotation scheme for event temporal relations. @CITE[1]@ propose a dataset of temporal ordering questions for event-centric reading comprehension. @CITE[3]@ propose a dataset for event-centric QA. There are also some works on event-centric QA using graph neural networks (@CITE[3]@;. However, these datasets and methods are not suitable for our task because they are not specifically designed for event-centric QA with event knowledge. Moreover, they do not have a specific focus on event-centric temporal reasoning, which is the main focus of our work.\\nReferences:ID: 1. Title: EventKG - the Hub of Event Knowledge on the Web - and Biographical Timeline Generation\\nID: 2. Title: TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions\\nID: 3. Title: A Multi-Axis Annotation Scheme for Event Temporal Relations\\nID: 4. Title: Logic-Guided Data Augmentation and Regularization for Consistent Question Answering\\nID: 5. Title: Learning with Instance Bundles for Reading Comprehension',\n",
       "  'ground_truth': '\\n\\nThe growing interest into event understanding has recently led to the development of new resources for event-centric QA and event relation extraction. Souza Costa et al. (2020) proposed EventQA, an event-centric QA dataset to access semantic information stores in knowledge graphs. The questions are created via a random walking on the EventKG (@CITE[5]@, then manually translated into natural language. modified and converted an event temporal relation extraction dataset – MATRES (@CITE[3]@ into a reading comprehension format focused on event temporal ordering questions, named TORQUE. Instead of solely focusing on simple arguments or temporal relations, the ESTER dataset (@CITE[1]@ was developed to highlight how events are semantically related in terms of five most common semantic relations: Causal, Conditional, Counterfactual, Sub-event, and Coreference relations. Aforementioned work built dataset baselines with popular entity-based PLMs, and thus leave significant performance gaps compared with human evaluation. @CITE[2]@, @CITE[4]@ and leverage features of closely related questions to capture temporal difference to deal with certain types of event-centric questions. Compared to the existing works, we target various types of event-centric questions. Therefore, we introduce an invertible event transformation to (i) model the event semantic relations through an auxiliary classification task, and to (ii) realign the event latent representations via contrastive learning in the space of the transformed events.\\nReferences:ID: 1. Title: EventKG - the Hub of Event Knowledge on the Web - and Biographical Timeline Generation\\nID: 2. Title: TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions\\nID: 3. Title: A Multi-Axis Annotation Scheme for Event Temporal Relations\\nID: 4. Title: Logic-Guided Data Augmentation and Regularization for Consistent Question Answering\\nID: 5. Title: Learning with Instance Bundles for Reading Comprehension<|eot_id|>'},\n",
       " 7035: {'predict': \"Biases in VL benchmarks have been a long-standing issue, and several works have been proposed to mitigate them. One approach is to introduce datasets with more diverse and balanced answer distributions (; @CITE[12]@; @CITE[15]@. Another approach is to regularize models with auxiliary question-only models (@CITE[14]@; @CITE[3]@; @CITE[1]@; @CITE[10]@; @CITE[2]@; @CITE[7]@; @CITE[13]@; @CITE[6]@. However, these methods do not address the issue of models relying on irrelevant information in the image. @CITE[4]@ and @CITE[11]@ propose to analyze and mitigate the reliance on irrelevant objects in the image. However, these works do not consider the language bias in the question. @CITE[8]@ and @CITE[5]@ propose to analyze and mitigate the reliance on visual context. However, these works do not consider the language bias in the question. @CITE[9]@ propose to analyze the reliance on visual context by analyzing the model's performance on a set of questions that require visual reasoning. However, this approach does not consider the language bias in the question.\\nReferences:ID: 1. Title: VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions\\nID: 2. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 3. Title: e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks\\nID: 4. Title: Exploring Models and Data for Image Question Answering\\nID: 5. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 6. Title: Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering\\nID: 7. Title: Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering\\nID: 8. Title: Yin and Yang: Balancing and Answering Binary Visual Questions\\nID: 9. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 10. Title: Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks\\nID: 11. Title: Counterfactual Samples Synthesizing for Robust Visual Question Answering\\nID: 12. Title: SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering\\nID: 13. Title: From Recognition to Cognition: Visual Commonsense Reasoning\\nID: 14. Title: A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge\\nID: 15. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\",\n",
       "  'ground_truth': \"\\n\\nPrevious studies have predominantly focused on biases in VQA-Short (@CITE[1]@; @CITE[8]@; @CITE[5]@; @CITE[13]@; @CITE[7]@. These benchmarks lack sample-specific candidate options, resulting in VL models being supervised to classify image-question pairs using shared class labels consisting of short answers. In these datasets, researchers have pointed out that models often downplay visual information and instead focus on learning biases in the text (@CITE[9]@; @CITE[2]@; @CITE[4]@; Manjunatha et al., 2019). This is exemplified by models directly learning the shallow mapping between prior question words and shared class labels in the absence of sample-specific contextualized candidate options. Consequently, models develop false visual dependency (@CITE[11]@; @CITE[12]@) as they may succeed in resolving VQA tasks (@CITE[15]@; @CITE[10]@; @CITE[3]@) utilizing irrelevant visual cues. However, the biases previously examined may not apply to VQA-Long due to its unique characteristics, such as the absence of shared class labels, the presence of sample-specific candidate options, and the diverse question types. Consequently, it is challenging for models to conclude the question types and determine the most popular answer choices for each question type. Given its difficulty and complex visual scenes, VQA-Long has gained popularity in recent years (@CITE[6]@; Tapaswi et al., 2016; ; Zhu et al., 2016; @CITE[11]@), but bias analysis in this specific context has not been explored extensively. While a recent study (@CITE[14]@) briefly addressed bias issues in VCR, it only focused on the exact matching of pronoun words. In contrast, our research delves into a comprehensive analysis of biases across all textual components, cross-modal correlations in the input, and the process of generating distractors. We identify more general bias problems and demonstrate their prevalence.\\nReferences:ID: 1. Title: VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions\\nID: 2. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 3. Title: e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks\\nID: 4. Title: Exploring Models and Data for Image Question Answering\\nID: 5. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 6. Title: Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering\\nID: 7. Title: Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering\\nID: 8. Title: Yin and Yang: Balancing and Answering Binary Visual Questions\\nID: 9. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 10. Title: Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks\\nID: 11. Title: Counterfactual Samples Synthesizing for Robust Visual Question Answering\\nID: 12. Title: SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering\\nID: 13. Title: From Recognition to Cognition: Visual Commonsense Reasoning\\nID: 14. Title: A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge\\nID: 15. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models<|eot_id|>\"},\n",
       " 6173: {'predict': 'Prompt tuning is a parameter-efficient approach to adapt PLMs for downstream tasks by learning a few trainable parameters that can be inserted into the input of a frozen PLM (@CITE[1]@; @CITE[3]@; @CITE[4]@; @CITE[6]@; @CITE[2]@. The soft prompt is usually initialized by a randomly generated vector and then updated by gradient descent. In this work, we use the prompt tuning method as the base method for our proposed method. Prompt tuning has shown to be effective in improving the performance of PLMs in a variety of downstream tasks. However, it has also been shown to be sensitive to the initialization of the soft prompt (@CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[2]@. We show that our proposed method can improve the robustness of prompt tuning to the initialization of the soft prompt.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 3. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 4. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 5. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 6. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 7. Title: The Power of Scale for Parameter-Efficient Prompt Tuning',\n",
       "  'ground_truth': '\\n\\nMany studies (@CITE[1]@; @CITE[5]@; @CITE[3]@; @CITE[7]@ have focused on how to design prompts since good prompts can narrow the gap between pretrained language models and downstream tasks. Depending on the prompt types, existing researches can be divided into two main categories: manually designed ones (@CITE[1]@; @CITE[5]@; @CITE[3]@; @CITE[7]@ and automatically created ones (discrete prompts (@CITE[4]@; Schick and Schütze, 2020) or continuous prompts (@CITE[6]@; @CITE[2]@), where continuous prompts focus on utilizing learnable continuous embeddings as prompt templates rather than label words. However, these prompts construction still lack coverage and bring considerable bias and high variances to the results. Recently, @CITE[7]@ propose to utilize external knowledge data to solve this issue. However, these works can not stimulate knowledge without external data directly.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 3. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 4. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 5. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 6. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification\\nID: 7. Title: The Power of Scale for Parameter-Efficient Prompt Tuning<|eot_id|>'},\n",
       " 141: {'predict': 'Recently, pre-trained language models have shown great potential in generating data for data augmentation. @CITE[3]@ propose contextual augmentation, which replaces words with paradigmatic relations predicted by a bi-directional language model. @CITE[1]@ propose TextFooler, which uses BERT (@CITE[2]@ to generate augmented data for text classification tasks. @CITE[3]@ propose Data Manipulation, which uses reinforcement learning to learn different manipulation schemes, including text transformation, with the same gradient-based algorithm. @CITE[3]@ propose Data Augmentation with Language Models (DALM), which uses reinforcement learning to learn to generate augmented data with language models. @CITE[3]@ propose Contextual Augmentation with Language Models (CALM), which uses a combination of contextual and language models to generate augmented data. In this paper, we use a combination of contextual and language models to generate augmented data, but we use a different reinforcement learning algorithm, which can learn more complex manipulation schemes.\\nReferences:ID: 1. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Learning Data Manipulation for Augmentation and Weighting',\n",
       "  'ground_truth': '\\n\\nPrevious work showed that pre-trained models are very effective at generalizing natural language knowledge in a zero- and few-shot fashion (Radford et al., 2019; @CITE[2]@. These models have been used to expand training data for various NLP classification tasks, by fine-tuning the model on a small seed dataset, then using conditioning on the class label to generate more data (@CITE[3]@; Kumar et al., 2020). proposed using a bidirectional LSTM-based language model to substitute words that fit the context, conditioning on the class label to prevent augmentation from changing the class label. Wu et al. (2019) used BERT ( in a similar way, and @CITE[1]@ improved upon it by jointly fine-tuning BERT and the classifier. Semnani et al. (2019) explored data augmentation for domain transfer using BERT. These approaches rely on an initial dataset with many examples in each class, and therefore are not suitable for semantic parsing, where each logical form has only a few or even just one example.\\nReferences:ID: 1. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Learning Data Manipulation for Augmentation and Weighting<|eot_id|>'},\n",
       " 3859: {'predict': 'Code generation is a long-standing problem in NLP, with a rich history of work in both program synthesis and code infilling. Recent advances in large language models (LLMs) have led to the development of systems capable of generating code from natural language specifications (@CITE[1]@; @CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[6]@; @CITE[7]@. In contrast to our work, most code generation work focuses on generating small programs from short natural language descriptions, and does not involve complex reasoning or symbolic computations. Recent work has also explored the use of LLMs for program synthesis, where the goal is to generate a program that solves a given problem, as opposed to generating a program that performs a given task. This is typically done by generating code in a left-to-right fashion, and has been shown to be effective on a range of problems, including math problems (@CITE[2]@; @CITE[8]@ and programming problems (@CITE[7]@; @CITE[4]@. In contrast, we focus on generating programs that solve math word problems, and do so in a right-to-left fashion, where the LLM generates the program in reverse, and then uses a symbolic solver to execute the program and check its correctness. This is more similar to the approach used in InCoder (@CITE[8]@, where the LLM generates code in a bidirectional fashion, and then uses a symbolic solver to check the correctness of the generated code.\\nReferences:ID: 1. Title: Jigsaw: Large Language Models meet Program Synthesis\\nID: 2. Title: Evaluating Large Language Models Trained on Code\\nID: 3. Title: Measuring Coding Challenge Competence With APPS\\nID: 4. Title: Programming Puzzles\\nID: 5. Title: Program Synthesis with Large Language Models\\nID: 6. Title: Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\\nID: 7. Title: InCoder: A Generative Model for Code Infilling and Synthesis\\nID: 8. Title: Training Verifiers to Solve Math Word Problems',\n",
       "  'ground_truth': '\\n\\nThanks to the recent development of pre-training techniques, a lot of pre-trained language models have been proposed for code-related tasks. Almost all of them focus on standalone code, while JigSaw (@CITE[1]@ and CERT (@CITE[3]@ are presented for generating code using public libraries. In this paper, we aim to generate code invoking private APIs, which is a common scenario in practice. It is more challenging because pre-trained language models have never seen any information about private libraries. As for benchmarks, HumanEval (@CITE[8]@, APPs (@CITE[4]@, P3 (, MBPP (@CITE[7]@, BIG-bench (@CITE[2]@, and CodeContests (@CITE[6]@ were proposed to evaluate the performance of generating standalone code. GSM8K-Python (@CITE[5]@ and MathQA-Python (@CITE[7]@ were engaged in evaluating the capability of solving mathematical problems. PandasEval and NumpyEval (@CITE[3]@ were released to evaluate the code generation for public library. We propose three benchmarks, called TorchDataEval, MonkeyEval, and BeatNumEval, aiming to evaluate the performance of code generation for private library.\\nReferences:ID: 1. Title: Jigsaw: Large Language Models meet Program Synthesis\\nID: 2. Title: Evaluating Large Language Models Trained on Code\\nID: 3. Title: Measuring Coding Challenge Competence With APPS\\nID: 4. Title: Programming Puzzles\\nID: 5. Title: Program Synthesis with Large Language Models\\nID: 6. Title: Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\\nID: 7. Title: InCoder: A Generative Model for Code Infilling and Synthesis\\nID: 8. Title: Training Verifiers to Solve Math Word Problems<|eot_id|>'},\n",
       " 4746: {'predict': 'Temporal video grounding aims to localize the most relevant video segment for a given natural language query. Recent studies have made great progress in this task, including the supervised and weakly-supervised methods. Supervised methods (@CITE[1]@; @CITE[2]@; @CITE[3]@; ; @CITE[4]@; @CITE[6]@; @CITE[7]@; @CITE[5]@) require temporal annotations of video segments for training, which are usually unavailable in real-world scenarios. Weakly-supervised methods (@CITE[4]@; @CITE[5]@; ; @CITE[6]@; @CITE[7]@) do not require temporal annotations but rely on video-sentence pairs. However, they also suffer from the lack of temporal annotations. Our work focuses on the weakly-supervised setting and introduces a novel self-supervised learning framework to learn temporal video representations.\\nReferences:ID: 1. Title: TALL: Temporal Activity Localization via Language Query\\nID: 2. Title: Localizing Moments in Video with Natural Language\\nID: 3. Title: Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video\\nID: 4. Title: WSLLN: Weakly Supervised Natural Language Localization Networks\\nID: 5. Title: Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation\\nID: 6. Title: Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos\\nID: 7. Title: Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification',\n",
       "  'ground_truth': '\\n\\nTemporal video grounding aims to localize the moment corresponding to the query. Under the supervised setting, existing methods can be categorized into the top-down and bottom-up frameworks. The top-down methods (@CITE[1]@; ; ; @CITE[6]@; Zhang et al., 2019) first generate proposals and then estimate cross-modal alignment scores for them. And the bottom-up methods (, 2020; @CITE[4]@; @CITE[2]@; ) directly calculate the frame-level probabilities of being temporal boundaries. Under the weakly-supervised setting, the methods can be categorized into the multiple instance learning (MIL) and the reconstruction frameworks. The MIL methods learn the latent visual-textual alignment by distinguishing the matched video-language pairs from the unmatched pairs. For example, @CITE[5]@ devise an alignment and a detection module. develop contrastive learning between counterfactual results. @CITE[7]@ explore cross-sentence relational constraints. The reconstruction methods reconstruct the query from visual contents during training and utilize intermediate results to localize. For example, utilize language reconstruction to rank proposals. further employ the attention weight. @CITE[3]@ mine negatives within the same video. Recent work (@CITE[3]@) studies the spoken video grounding task and represent speech with video-guided contrastive predictive coding. We consider the intensive labor and introduce the weakly-supervised setting.\\nReferences:ID: 1. Title: TALL: Temporal Activity Localization via Language Query\\nID: 2. Title: Localizing Moments in Video with Natural Language\\nID: 3. Title: Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video\\nID: 4. Title: WSLLN: Weakly Supervised Natural Language Localization Networks\\nID: 5. Title: Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation\\nID: 6. Title: Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos\\nID: 7. Title: Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification<|eot_id|>'},\n",
       " 7388: {'predict': 'A line of work uses PLMs to learn knowledge embeddings. KG-BERT ( is the first work that uses PLMs for KG completion. It uses the PLM to encode entities and relations, and then uses the encoded triples to learn the scoring function. KEPLER (@CITE[3]@ proposes a unified model for KE and PLM. It encodes the entity descriptions with PLMs and jointly optimizes the KE and language modeling objectives. LMKE (@CITE[1]@ adopts PLMs to derive KE. It formulates description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. P@CITE[2]@ proposes a method to learn KG embeddings from pre-trained language models. It first uses PLMs to encode the entity descriptions, and then uses the encoded descriptions to learn KG embeddings. However, these methods are not designed for long-tail entity learning. They either do not consider the long-tail problem or use the expensive negative sampling. In contrast, our method is specifically designed for long-tail entity learning. We use the entity descriptions to learn the entity embeddings, which can effectively alleviate the long-tail problem. Moreover, we use the entity descriptions to learn the relation embeddings, which can effectively alleviate the long-tail problem.\\nReferences:ID: 1. Title: KG-BERT: BERT for Knowledge Graph Completion\\nID: 2. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 3. Title: Language Models as Knowledge Embeddings',\n",
       "  'ground_truth': '\\n\\nIn light of the success of Natural Language Processing (NLP), text-based knowledge graph completion is gaining more attention. As another mode of knowledge different from the structured KG, text can provide rich semantic information. DKRL () first introduces textual descriptions into entity embeddings produced by a convolutional neural network. Subsequent works (such as KG-BERT (@CITE[2]@), KEPLER (), and Pretrain-KGE (@CITE[3]@) use a pre-trained language model (PLM) to encode the text descriptions. More recently, LMKE (@CITE[1]@) proposed a contrastive learning framework that uses the PLM to obtain entity and relation embeddings in the same space as word tokens, and demonstrated its effectiveness on long-tail problem. These methods generally rely on the language models to process text descriptions and require finetuning for different knowledge graphs. The proposed KICGPT, which uses LLM directly, is more efficient because it is training-free and requires no finetuning.\\nReferences:ID: 1. Title: KG-BERT: BERT for Knowledge Graph Completion\\nID: 2. Title: KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\\nID: 3. Title: Language Models as Knowledge Embeddings<|eot_id|>'},\n",
       " 1426: {'predict': 'The Relation Networks (RN) (, a feed-forward network, was originally proposed for visual reasoning in a multi-agent setting. The RN module consists of a relation network, which is a feed-forward network that takes two agent representations as input and produces a relation representation. Then, the agent representations are updated with the relation representation, and this is repeated for all agents. The RN module is trained end-to-end, and it can be used to augment any neural network. In our work, we focus on the object-level RN module (ORN) proposed by @CITE[3]@, which is a lightweight module that is easy to integrate into existing object detection pipelines. The ORN module is similar to the RN module, but it is designed for object detection. It is trained to predict the relations between objects in an image, and the relation representations are used to refine the object features. The ORN module has been shown to improve object detection in various settings, including object detection in the wild (@CITE[2]@, and object detection in the medical domain (,b). Recently, @CITE[4]@ proposed a general conditioning layer called FiLM, which is similar to the relation network module. The FiLM module is a simple feed-forward network that takes two inputs, and it produces a feature-wise affine transformation that is applied to the input features. The FiLM module has been shown to be effective for visual reasoning tasks, including visual question answering (@CITE[4]@, and visual grounding (.\\nReferences:ID: 1. Title: A simple neural network module for relational reasoning\\nID: 2. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 3. Title: Relation Networks for Object Detection\\nID: 4. Title: FiLM: Visual Reasoning with a General Conditioning Layer',\n",
       "  'ground_truth': '\\n\\nRelation Networks (RN) originated as an alternative to other kinds of graph-based neural models to develop relation-based representations and were designed to overcome the limitations of CNNs and MLPs for reasoning problems in vision, NLP, and symbolic domains such as physics (@CITE[2]@). RN performance on a visual question answering dataset CLEVR (@CITE[4]@) surpassed human performance. RNs also prove effective at improving object detection models (@CITE[3]@). Moreover, RNs have become a general framework for few-shot learning (). We adapt RNs for the three-way relation represented by the questions, reference answers, and student answers in ASAG datasets. RNs typically combine learned representations of relational vectors using vector addition. We combine the relation vectors that represent multiple reference answers for the same question and student answer using a learned relation fusion function. For the fusion function, we use feature-wise transformation, based on its success with multi-modal data, e.g., images and text. @CITE[1]@ developed FiLM, an approach to merge information from language and visual input. A language vector serves as conditioning input, to control the scaling and shifting of the visual feature map in a feature-wise fashion. Similarly, we train a function with learnable parameters to combine QRA triples for a given student answer. We have not seen feature-wise transformation used for vector combination in ASAG research, which typically relies instead on fixed arithmetic operations or concatenation.\\nReferences:ID: 1. Title: A simple neural network module for relational reasoning\\nID: 2. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 3. Title: Relation Networks for Object Detection\\nID: 4. Title: FiLM: Visual Reasoning with a General Conditioning Layer<|eot_id|>'},\n",
       " 288: {'predict': 'The information bottleneck (IB) is a framework that aims to find a compact representation of the input data that contains the most information about the target variable (@CITE[1]@). In the context of NLP, the IB is used to select the most relevant features for the prediction. This can be done by learning a binary mask over the input words and then feeding the masked input to the model. The IB is used in several works for text classification, such as in @CITE[2]@, where the authors use the IB for the purpose of explaining the predictions of a text classifier. @CITE[5]@ and @CITE[6]@ use the IB to select the most relevant words for the prediction. @CITE[4]@ use the IB to improve the robustness of a text classifier against adversarial attacks. @CITE[3]@ propose a variational version of the IB, which is used for the purpose of generating rationales for the predictions of a text classifier. @CITE[3]@ use the IB for the purpose of generating rationales for the predictions of a text classifier. @CITE[6]@ use the IB to generate rationales for the predictions of a text classifier. @CITE[4]@ propose a method for extracting rationales that are coherent and relevant to the prediction. @CITE[3]@ use the IB for the purpose of generating rationales for the predictions of a text classifier. @CITE[6]@ use the IB to generate rationales for the predictions of a text classifier. @CITE[4]@ propose a method for extracting rationales that are coherent and relevant to the prediction.\\nReferences:ID: 1. Title: The information bottleneck method\\nID: 2. Title: Interpretable Neural Predictions with Differentiable Binary Variables\\nID: 3. Title: The Challenge of Crafting Intelligible Intelligence\\nID: 4. Title: Deep Variational Information Bottleneck\\nID: 5. Title: Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control\\nID: 6. Title: Deep Variational Information Bottleneck',\n",
       "  'ground_truth': '\\n\\nInformation Bottleneck (IB) (@CITE[4]@ has recently been adapted in a number of downstream applications like parsing (@CITE[2]@, extractive summarization (@CITE[6]@, and image classification (@CITE[3]@; @CITE[5]@. @CITE[3]@ and @CITE[2]@ use IB for optimal compression of hidden representations of images and words respectively. We are interested in compressing the number of cognitive units (like sentences) to ensure interpretability of the bottleneck representation, similar to West et al. (2019). However, while @CITE[6]@ use brute-force search to optimize IB for summarization, we directly optimize a parametric variational bound on IB for rationales. IB has also been previously used for interpretability—Zhmoginov et al. (2019) use a VAE to estimate the prior distribution over z for image classification. @CITE[1]@ use IB for post-hoc explanation of sentiment classification. They do not enforce a sparse prior, and as a result, cannot guarantee that the rationale is strictly smaller than the input. Controlling sparsity to manage the accuracy-conciseness trade-off is also not possible in their model.\\nReferences:ID: 1. Title: The information bottleneck method\\nID: 2. Title: Interpretable Neural Predictions with Differentiable Binary Variables\\nID: 3. Title: The Challenge of Crafting Intelligible Intelligence\\nID: 4. Title: Deep Variational Information Bottleneck\\nID: 5. Title: Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control\\nID: 6. Title: Deep Variational Information Bottleneck<|eot_id|>'},\n",
       " 404: {'predict': 'Dialog Improving dialog systems with RL has largely been restricted to task-oriented dialog systems. @CITE[9]@; @CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[1]@; @CITE[6]@; @CITE[2]@; @CITE[7]@; @CITE[8]@. For example, @CITE[9]@; @CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[1]@; @CITE[6]@; @CITE[2]@; @CITE[7]@; @CITE[8]@ use RL to train a dialog agent to achieve a specific task, such as booking a flight or searching for a product. In these works, the agent is trained to maximize the cumulative reward in a simulated environment. In contrast, we focus on improving the general conversational skills of the agent, and propose a framework that uses RL to optimize the agent’s ability to engage in a wide range of conversations.\\nReferences:ID: 1. Title: Policy Networks with Two-Stage Training for Dialogue Systems\\nID: 2. Title: Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models\\nID: 3. Title: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems\\nID: 4. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 5. Title: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems\\nID: 6. Title: Sentiment Adaptive End-to-End Dialog Systems\\nID: 7. Title: Doubly Robust Off-policy Value Evaluation for Reinforcement Learning\\nID: 8. Title: A Deep Reinforcement Learning Chatbot\\nID: 9. Title: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor',\n",
       "  'ground_truth': '\\n\\nwhich have a limited number of task-specific actions (; Gaˇ c et al., 2011; @CITE[6]@; @CITE[5]@; @CITE[3]@. Some of these approaches incorporate human input through explicit, manual feedback (@CITE[5]@ or implicit signals (e.g. the user interrupting the system or starting over) (@CITE[4]@. RL in the open-domain dialog setting is less explored (@CITE[8]@, 2017b, 2018). Authors may choose to use a highly restricted action space; for example, using RL to choose which dialog model to invoke (@CITE[9]@. used explicit human feedback to improve the summarization and text continuation performance of a large-scale language model. Although implicit signals such as sentiment (@CITE[1]@ and conversation length (@CITE[7]@ have been used in maximum likelihood estimation (MLE) systems, the idea of using such signals as a reward for RL is relatively unexplored. @CITE[2]@ combine using reinforcement learning to optimize dialog reward with using supervised learning to restrict the conversation to be close to the training data. Shin et al. (2019) use on-policy learning in conjunction with a user-sentiment approximator to improve a seq2seq model, but are unable to learn directly from user feedback. To the best of our knowledge, we are the first to use offline RL to train dialog models on real human interactions.\\nReferences:ID: 1. Title: Policy Networks with Two-Stage Training for Dialogue Systems\\nID: 2. Title: Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models\\nID: 3. Title: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems\\nID: 4. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 5. Title: Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems\\nID: 6. Title: Sentiment Adaptive End-to-End Dialog Systems\\nID: 7. Title: Doubly Robust Off-policy Value Evaluation for Reinforcement Learning\\nID: 8. Title: A Deep Reinforcement Learning Chatbot\\nID: 9. Title: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor<|eot_id|>'},\n",
       " 3722: {'predict': 'The text-to-SQL task has been studied extensively in recent years. Many works focus on the schema-dependent text-to-SQL task (@CITE[3]@; @CITE[9]@; @CITE[7]@; @CITE[4]@; @CITE[6]@; @CITE[5]@; @CITE[8]@; @CITE[1]@; @CITE[2]@. For the schema-dependent task, the models are trained on the text-to-SQL dataset, which is constructed by pairing the natural language questions with the SQL queries on a fixed database schema. The schema-dependent text-to-SQL task is relatively easier since the model can directly access the database schema. In contrast, the schema-dependent text-to-SQL task is much harder since the model has to figure out the schema information from the text. In this paper, we focus on the schema-dependent text-to-SQL task, which is more challenging. Most recently, @CITE[2]@ propose a framework that jointly learns to generate SQL queries and to detect column mentions in the utterances. @CITE[1]@ propose a model pre-training framework, Generation-Augmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. @CITE[6]@ propose a graph-based model that jointly learns to generate SQL queries and to detect column mentions in the utterances. In contrast to previous works, we propose a unified model that can generate SQL queries and detect column mentions in the utterances simultaneously. Moreover, we propose a novel method to construct the schema graph, which is more robust to the schema changes. Furthermore, we propose a novel method to construct the context graph, which is more robust to the context changes.\\nReferences:ID: 1. Title: RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers\\nID: 2. Title: LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations\\nID: 3. Title: S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers\\nID: 4. Title: PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\\nID: 5. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\\nID: 6. Title: CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases\\nID: 7. Title: CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases\\nID: 8. Title: IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation\\nID: 9. Title: Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training',\n",
       "  'ground_truth': '\\n\\nParsing of previous text-to-SQL works focused on the context-independent setting. Notably, the graph-based parser, e.g., RAT-SQL (@CITE[9]@), LGESQL (), S2SQL (@CITE[5]@), and the T5-based parser, e.g., PICARD (@CITE[3]@), achieving the impressive performance on SPIDER (@CITE[4]@). In recent years, context-dependent (multi-turn) text-to-SQL parsing has attracted increasing attention due to its broad applications and realistic setting. SPARC (@CITE[7]@) and COSQL () are two benchmark datasets for context-dependent text-to-SQL parsing. Several works (; @CITE[8]@; @CITE[1]@; @CITE[6]@; ; @CITE[2]@) were proposed, which consider contextual information or conversation history to synthesize the correct SQL query. In particular, exploited the conversation history by editing the previous predicted SQL to improve the generation quality. The schema interaction graph in IGSQL (@CITE[8]@) and two kinds of interaction states in IST-SQL (@CITE[1]@) are designed to capture the historical schema evolution in context. Furthermore, @CITE[2]@ improved contextual accuracy by incorporating additional SQL encoders to integrate historical SQL into the input. In contrast to above works, STAR focuses on the pre-training stage, expecting to extract general knowledge from large-scale unsupervised or self-supervised data that will be useful for downstream parsing tasks.\\nReferences:ID: 1. Title: RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers\\nID: 2. Title: LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations\\nID: 3. Title: S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers\\nID: 4. Title: PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\\nID: 5. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\\nID: 6. Title: CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases\\nID: 7. Title: CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases\\nID: 8. Title: IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation\\nID: 9. Title: Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training<|eot_id|>'},\n",
       " 5791: {'predict': 'The research on charts is mainly divided into two directions: chart understanding and chart generation. Chart understanding mainly focuses on chart QA, where a question is asked to obtain the answer from a chart. DVQA (@CITE[8]@, FigureQA (@CITE[6]@, and PlotQA (@CITE[5]@ are typical chart QA datasets. Besides, there are also works on chart reconstruction, which can be achieved by a series of questions (@CITE[8]@ or by generating charts from text (@CITE[3]@; @CITE[4]@; @CITE[7]@; @CITE[9]@. As for chart generation, the research mainly focuses on generating captions for charts. @CITE[1]@ proposed a dataset of chart captions for scientific figures. @CITE[2]@ introduced a model for generating captions for charts. @CITE[9]@ proposed a model for generating captions for scientific figures. @CITE[9]@ proposed a model for generating charts from captions. @CITE[5]@ proposed a model for generating captions for charts. There are also works on generating charts from text (@CITE[3]@; @CITE[4]@; @CITE[7]@. However, these works do not consider the problem of chart generation from table. In this paper, we propose a new task of chart generation from table, and propose a method to solve this problem.\\nReferences:ID: 1. Title: DVQA: Understanding Data Visualizations via Question Answering\\nID: 2. Title: FigureQA: An Annotated Figure Dataset for Visual Reasoning\\nID: 3. Title: PlotQA: Reasoning over Scientific Plots\\nID: 4. Title: Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\\nID: 5. Title: Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\\nID: 6. Title: Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model\\nID: 7. Title: SciCap: Generating Captions for Scientific Figures\\nID: 8. Title: Answering Questions about Data Visualizations using Efficient Bimodal Fusion\\nID: 9. Title: DVQA: Understanding Data Visualizations via Question Answering',\n",
       "  'ground_truth': '\\n\\nResearching chart understanding in V+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) (@CITE[1]@; @CITE[6]@; @CITE[4]@; @CITE[9]@; Chaudhry et al., 2020), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) (@CITE[9]@; @CITE[5]@, which requires machine learning models to create a summary of key insights conveyed by a chart. @CITE[7]@ collected a large-scale scientific figures captioning dataset from research papers where many images are chart plots. There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images (@CITE[2]@; @CITE[3]@; Chaudhry et al., 2020; @CITE[1]@; @CITE[7]@; @CITE[8]@. The other approach assumes the table data of charts is accessible from the dataset (Kim et al., 2020; or can be extracted from the chart images using vision to table techniques (@CITE[4]@; @CITE[9]@; @CITE[9]@. Then, the researchers will either use a table-to-text generation model (Kim et al., 2020; ; @CITE[4]@ or combine the embedding of tables and charts via a multi-modal fusion method to generate the text output (@CITE[9]@; @CITE[9]@. It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model’s capability to understand and interpret chart information. Following the table-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre-training on chart-table pairs.\\nReferences:ID: 1. Title: DVQA: Understanding Data Visualizations via Question Answering\\nID: 2. Title: FigureQA: An Annotated Figure Dataset for Visual Reasoning\\nID: 3. Title: PlotQA: Reasoning over Scientific Plots\\nID: 4. Title: Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\\nID: 5. Title: Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\\nID: 6. Title: Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model\\nID: 7. Title: SciCap: Generating Captions for Scientific Figures\\nID: 8. Title: Answering Questions about Data Visualizations using Efficient Bimodal Fusion\\nID: 9. Title: DVQA: Understanding Data Visualizations via Question Answering<|eot_id|>'},\n",
       " 2471: {'predict': 'The unimodal task-oriented dialog system aims to generate a response based on the dialogue history, which is usually represented as a sequence of user requests and system responses. The response generation model can be divided into two parts: the encoder and the decoder. The encoder takes the dialogue history as input and outputs a fixed-size representation, which is then fed into the decoder to generate the response. The decoder is usually a sequence-to-sequence model, such as the Transformer (@CITE[7]@), which generates the response token by token. To incorporate external knowledge into the encoder, recent works employ memory-augmented architectures, such as Memory Network (Weston et al., 2015), Memory Fusion Network (Sukhbaatar et al., 2016), and Memory Transformer Network (@CITE[6]@), which use the memory to store the external knowledge. In addition, recent works also employ the copy mechanism (Vinyals et al., 2015) to copy the tokens from the memory to the response. The memory-augmented architectures and the copy mechanism can be combined to form a unified model (@CITE[9]@; @CITE[10]@; @CITE[4]@; @CITE[5]@; @CITE[11]@; @CITE[3]@; @CITE[8]@; @CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[4]@; @CITE[2]@; @CITE[11]@. However, these models are usually designed for a specific domain and are not easily extended to a new domain. Our model is different from these models in that we employ the cross-domain knowledge to improve the response generation and can be easily extended to a new domain.\\nReferences:ID: 1. Title: Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation\\nID: 2. Title: Open Domain Dialogue Generation with Latent Images\\nID: 3. Title: Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System\\nID: 4. Title: Continual Learning in Task-Oriented Dialogue Systems\\nID: 5. Title: Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\\nID: 6. Title: Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs\\nID: 7. Title: A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue\\nID: 8. Title: Learning End-to-End Goal-Oriented Dialog\\nID: 9. Title: Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation\\nID: 10. Title: Multi-level Memory for Task Oriented Dialogs\\nID: 11. Title: Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever',\n",
       "  'ground_truth': '\\n\\nRecent years has witnessed the remarkable success in textual dialog systems, which can be roughly divided into two categories: open-domain conversations with casual chi-chat (; ; Chan et al., 2021; @CITE[7]@ and task-oriented dialog systems (@CITE[8]@; Santra et al., 2021; ; @CITE[5]@; ; ;, which are designed to help users achieve specific goals. Early efforts mainly adopt a sequence-to-sequence (Seq2Seq) architecture, but cannot work well in KB retrieval and reasoning. To alleviate this problem, copy mechanism ( have been adopted and many memory augmented Seq2Seq models have been proposed (@CITE[7]@; @CITE[11]@; Madotto et al., 2018; Wu et al., 2019; ; @CITE[6]@; ; @CITE[8]@, which achieve promising results.\\nReferences:ID: 1. Title: Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation\\nID: 2. Title: Open Domain Dialogue Generation with Latent Images\\nID: 3. Title: Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System\\nID: 4. Title: Continual Learning in Task-Oriented Dialogue Systems\\nID: 5. Title: Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\\nID: 6. Title: Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs\\nID: 7. Title: A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue\\nID: 8. Title: Learning End-to-End Goal-Oriented Dialog\\nID: 9. Title: Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation\\nID: 10. Title: Multi-level Memory for Task Oriented Dialogs\\nID: 11. Title: Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever<|eot_id|>'},\n",
       " 4903: {'predict': 'Multilingual prompting has been explored by many researchers. @CITE[1]@ evaluate LMs on 42 scenarios across 16 languages and show that multilingual LMs are better than monolingual LMs on multilingual tasks. @CITE[4]@ apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. @CITE[3]@ develop a learning framework named Polyglot Prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering. @CITE[2]@ evaluate the reasoning abilities of large language models in multilingual settings. @CITE[5]@ propose a multilingual prompting method that uses language-specific prompts to improve the performance of multilingual LMs on multilingual tasks. However, these works do not consider the problem of multilingual prompting with prompts in a single language. In contrast, our work focuses on the multilingual prompting problem with prompts in a single language. We explore the multilingual prompting problem in a single language by training a multilingual model with prompts in English. Our work is the first to investigate the multilingual prompting problem in a single language. We conduct extensive experiments to study the multilingual prompting problem and analyze the effects of different factors, including model size, task types, and language similarity.\\nReferences:ID: 1. Title: Holistic Evaluation of Language Models\\nID: 2. Title: Crosslingual Generalization through Multitask Finetuning\\nID: 3. Title: Polyglot Prompt: Multilingual Multitask PrompTraining\\nID: 4. Title: Efficient Large Scale Language Modeling with Mixtures of Experts\\nID: 5. Title: Language Models are Multilingual Chain-of-Thought Reasoners',\n",
       "  'ground_truth': '\\n\\nPrompting reformulates NLP tasks into masked or generative language modeling problem, depending on the models’ pretraining objective. Zhao and Schütze (2021) and @CITE[4]@ show that finetuning XLM-R on cloze-style prompts yield better performance than standard finetuning under a low-resource regime for XNLI. On the other hand, @CITE[2]@ find that standard finetuning of XLM-R outperforms prompt-based learning for sentiment prediction in low-resource Indonesian dialects. Some work shows that multitask prompt-based training on a variety of tasks and English or translated prompts improves zero-shot cross-lingual and cross-task performance (@CITE[5]@; @CITE[1]@). Multilingual prompt-based learning can also be achieved without performing gradient updates for downstream tasks. For instance, @CITE[3]@ demonstrate success in prompting GPT-like pretrained models with in-context learning for NLU tasks, using either English or translated prompt templates. find that when language models scale up, they can perform better multilingual chain-of-thought reasoning.\\nReferences:ID: 1. Title: Holistic Evaluation of Language Models\\nID: 2. Title: Crosslingual Generalization through Multitask Finetuning\\nID: 3. Title: Polyglot Prompt: Multilingual Multitask PrompTraining\\nID: 4. Title: Efficient Large Scale Language Modeling with Mixtures of Experts\\nID: 5. Title: Language Models are Multilingual Chain-of-Thought Reasoners<|eot_id|>'},\n",
       " 1779: {'predict': 'In the past, many approaches have been proposed to utilize teacher-student paradigms for knowledge transfer. The most well-known one is knowledge distillation (@CITE[5]@), which is to transfer the knowledge from a teacher model to a student model. In knowledge distillation, the teacher model is trained on a large amount of labeled data, and the student model is trained on a small amount of labeled data with the guidance of the teacher model. In addition to knowledge distillation, some approaches have been proposed to utilize teacher-student paradigms for semi-supervised learning. For example, @CITE[4]@ proposes to utilize unlabeled data with the teacher model as a regularizer. @CITE[6]@ proposes to utilize teacher-student paradigms for multi-domain learning. For example, @CITE[7]@ proposes to utilize teacher-student paradigms to transfer the domain knowledge from the teacher model to the student model. @CITE[2]@ proposes to utilize teacher-student paradigms to transfer the knowledge from the teacher model to the student model. In addition, some approaches have been proposed to utilize teacher-student paradigms for transfer learning. For example, @CITE[1]@ proposes to utilize teacher-student paradigms to transfer the knowledge from the teacher model to the student model. @CITE[3]@ proposes to utilize teacher-student paradigms to transfer the knowledge from the teacher model to the student model. In this paper, we propose a novel teacher-student paradigm to transfer the knowledge from the teacher model to the student model. In our paradigm, the teacher model is trained on the labeled data with the guidance of the student model, and the student model is trained on the unlabeled data with the guidance of the teacher model. In addition, the teacher model is trained on the unlabeled data with the guidance of the student model, and the student model is trained on the labeled data with the guidance of the teacher model. Therefore, the teacher model and the student model are trained in a mutual way, which is different from the traditional teacher-student paradigms.\\nReferences:ID: 1. Title: Distilling the Knowledge in a Neural Network\\nID: 2. Title: Revisiting Self-Training for Neural Sequence Generation\\nID: 3. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 4. Title: Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks\\nID: 5. Title: Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge\\nID: 6. Title: Teach an all-rounder with experts in different domains\\nID: 7. Title: Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks',\n",
       "  'ground_truth': '\\n\\nTeacher-student paradigms are widely used for knowledge distillation (@CITE[3]@; Li et al., 2014; @CITE[7]@. We aim to let a student model outperform its teacher model for performance improvements and thus use the same architecture for all teacher and student models. Our work is related to self-training (@CITE[4]@; @CITE[6]@. The main differences are (i) noise is introduced by retrieved context instead of noisy answers, (ii) we generate weakly-labeled data based on existing large-scale QA data cover- ing a wide range of domains, instead of the same domain (; @CITE[1]@; @CITE[2]@; or at least approx- imately in-domain ( as the target MRC task, and (iii) ground-truth labels of weakly- labeled data are used directly or indirectly to train teacher models. Note that we use teacher models to generate new soft labels for ﬁxed weakly-labeled data instead of new pseudo data with noisy labels from unlabeled data (e.g., (@CITE[5]@). Compared with previous multi-teacher student paradigms (; @CITE[2]@;, to train models to be strong teachers, we conduct iterative training and leverage large-scale weakly-labeled data rather than using clean, human-labeled data of similar tasks.\\nReferences:ID: 1. Title: Distilling the Knowledge in a Neural Network\\nID: 2. Title: Revisiting Self-Training for Neural Sequence Generation\\nID: 3. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 4. Title: Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks\\nID: 5. Title: Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge\\nID: 6. Title: Teach an all-rounder with experts in different domains\\nID: 7. Title: Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks<|eot_id|>'},\n",
       " 5568: {'predict': 'E2E-TOD systems employ different strategies to incorporate KB information for response generation. @CITE[2]@ use KB as a soft constraint to filter out irrelevant information for response generation. @CITE[1]@ propose to decouple KB retrieval from response generation and introduce a multi-grained knowledge retriever that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes. @CITE[3]@ propose to embed the KB into the model parameters and the resulting model does not require any DST or template responses, nor the KB as input. In this paper, we propose to leverage the KB to generate diverse responses. To the best of our knowledge, our work is the first to use KB to generate diverse responses for E2E-TOD.\\nReferences:ID: 1. Title: Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs\\nID: 2. Title: Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog\\nID: 3. Title: Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems',\n",
       "  'ground_truth': '\\n\\nFirstly, memory networks are utilized to store knowledge, using multi-hop interactions to aggregate relevant information. For instance, Mem2seq (Madotto et al., 2018) employs multi-hop attention over memory cells to select KB-related tokens during response generation. GLMP (Wu et al., 2019) introduces a global-to-local memory pointer network to retrieve relevant triplets and complete the response template. CD-NET (@CITE[1]@ retrieves relevant KB records by computing a distillation distribution based on dialogue context. Secondly, the entire linearized KB is encoded by pre-trained language models and taken as input to a generator to generate the final system response. For instance, UnifiedSKG (@CITE[2]@ uses a unified text-to-text framework, while Q-TOD ( rewrites the dialogue context into a natural language query for knowledge retrieval. MAKER (@CITE[3]@ introduces a multi-grained retrieval with both entity and attribute selection. Thirdly, knowledge bases are stored in model parameters for implicit retrieval during response generation. GPT-KE ( embeds the KB into pre-trained model parameters through data augmentation. Following GPT-KE, ECO ( first generates the most relevant entity with trie constraint before response generation to ensure entity consistency in the response.\\nReferences:ID: 1. Title: Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs\\nID: 2. Title: Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog\\nID: 3. Title: Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems<|eot_id|>'},\n",
       " 4510: {'predict': 'Another line of research is two-step summarization, which first generates a summary and then edits it to generate the final output. @CITE[4]@ first generates a summary and then edits it to generate the final output. @CITE[2]@ first generates a summary and then selects the best summary from a set of summaries generated by a model with different random seeds. @CITE[3]@ first generates a summary and then edits it to generate the final output. @CITE[1]@ first generates a summary and then edits it to generate the final output. @CITE[6]@ first generates a summary and then edits it to generate the final output. @CITE[5]@ first generates a summary and then edits it to generate the final output. Two-step summarization can improve the quality of the output summary. However, the second step is usually done by a rule-based method, which may not be able to capture the semantic meaning of the original text. Our approach is different from two-step summarization in that we use the output of the first step as the input of the second step, and the second step is done by a language model, which can capture the semantic meaning of the original text.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection\\nID: 3. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 4. Title: BRIO: Bringing Order to Abstractive Summarization\\nID: 5. Title: Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation\\nID: 6. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering',\n",
       "  'ground_truth': '\\n\\nRe-ranking candidate summaries can address the “exposure bias” problem (@CITE[1]@ from standard maximum likelihood teacher forcing by allowing an external model to coordinate system outputs with evaluation metrics. Re-ranking diverse candidates can lead to improved faithfulness (@CITE[4]@; @CITE[3]@ or relevance (as measured by ROUGE) (@CITE[6]@; ; @CITE[5]@;. Ranking can also be incorporated into training by adding a contrastive loss to the standard MLE loss for a multi-task objective (@CITE[2]@; @CITE[5]@. This work is related to, yet distinct from, our work, as we focus on the impact of candidate generation methods on explicit re-ranking.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection\\nID: 3. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 4. Title: BRIO: Bringing Order to Abstractive Summarization\\nID: 5. Title: Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation\\nID: 6. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering<|eot_id|>'},\n",
       " 3131: {'predict': 'Pipelined task-oriented dialogue systems (PTOD) are one of the most popular approaches in TORS. They usually consist of three modules: user understanding, dialogue policy, and response generation. User understanding module is responsible for recognizing the user’s intent and extracting the dialogue states. Dialogue policy module decides the next action based on the dialogue states. Response generation module generates the response based on the action. The three modules are trained separately, which makes it easy to integrate and fine-tune them. However, the errors in one module will be propagated to the next module, which may affect the overall performance of the system. To alleviate this problem, @CITE[8]@ proposed to use a copy mechanism to carry over the old dialogue states to the new one. @CITE[5]@ proposed to use a minimal generation length to track the dialogue states. @CITE[4]@ proposed to use a back and denoising reconstruction approach to improve the accuracy of dialogue states. @CITE[1]@ proposed to use a gated mechanism to weigh suitable unlabeled dialog samples. In addition, many PTOD systems adopt pre-trained language models as the backbone. @CITE[6]@ proposed a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. @CITE[7]@ proposed to use a minimalist transfer learning framework to simplify the system design process. @CITE[2]@ proposed to use a generative pre-trained model to enhance the task-oriented dialogue system. @CITE[3]@ proposed to use a unified sequence-to-sequence model to handle the task-oriented dialogue systems. @CITE[4]@ proposed to use a back and denoising reconstruction approach to improve the accuracy of dialogue states. @CITE[6]@ proposed to use a generative pre-trained model to enhance the task-oriented dialogue system. @CITE[2]@ proposed to use a generative pre-trained model to enhance the task-oriented dialogue system.\\nReferences:ID: 1. Title: A Network-based End-to-End Trainable Task-oriented Dialogue System\\nID: 2. Title: A Simple Language Model for Task-Oriented Dialogue\\nID: 3. Title: MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems\\nID: 4. Title: BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog\\nID: 5. Title: GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection\\nID: 6. Title: A Network-based End-to-End Trainable Task-oriented Dialogue System\\nID: 7. Title: A Simple Language Model for Task-Oriented Dialogue\\nID: 8. Title: MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems',\n",
       "  'ground_truth': '\\n\\nMeanwhile, there are many pipelined task-oriented dialogue systems, which strip out the component of knowledge retrieval (Young et al., 2013; @CITE[7]@; @CITE[2]@; ; ; @CITE[3]@; @CITE[8]@; @CITE[4]@. They usually decompose a task-oriented dialogue system into several pipelined modules: natural language understanding, dialogue state tracking, dialogue policy learning, and system response generation (Young et al., 2013; @CITE[7]@. Some recent works formulate all pipelined modules as a cascaded generation task using pre-trained language model (@CITE[2]@; ; ; @CITE[5]@; @CITE[1]@; @CITE[3]@. To further boost the performance, some models attempt to introduce the pre-training strategy into task-oriented dialogue systems (@CITE[6]@; @CITE[8]@; @CITE[4]@. However, these pipelined systems rely on the predefined schema to retrieve knowledge from an external knowledge base, leading to difficulties in adapting to unseen domains....\\nReferences:ID: 1. Title: A Network-based End-to-End Trainable Task-oriented Dialogue System\\nID: 2. Title: A Simple Language Model for Task-Oriented Dialogue\\nID: 3. Title: MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems\\nID: 4. Title: BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog\\nID: 5. Title: GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection\\nID: 6. Title: A Network-based End-to-End Trainable Task-oriented Dialogue System\\nID: 7. Title: A Simple Language Model for Task-Oriented Dialogue\\nID: 8. Title: MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems<|eot_id|>'},\n",
       " 6906: {'predict': 'The knowledge-intensive NLP and retrieval has been explored in many works. @CITE[4]@ propose a unified framework to tackle open-domain question answering using Wikipedia as the unique knowledge source. @CITE[3]@ propose a dense passage retriever to select candidate contexts. @CITE[6]@ and @CITE[5]@ show that generative models for open domain question answering can benefit from retrieving text passages. @CITE[1]@ propose a unified text-to-text transformer for transfer learning. @CITE[2]@ and @CITE[7]@ explore how to elicit reasoning in large language models. However, these works focus on the retrieval of explicit knowledge. In contrast, our work aims to retrieve implicit knowledge, which is a more challenging task.\\nReferences:ID: 1. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 2. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 5. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 6. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 7. Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',\n",
       "  'ground_truth': '\\n\\nKnowledge-intensive NLP, such as open-domain QA ( and fact verification (Thorne et al., 2018), requires substantial factual knowledge that may change over time. Therefore, these tasks were originally envisioned to incorporate the retrieval of relevant passages from the knowledge corpus (. In the typical retrieve-then-read pipeline (@CITE[6]@, inter alia), a pipeline of models first selects k passages from a retrieval function which are then used to condition answer generation from the reader (@CITE[4]@, inter alia). Meanwhile, the success of pre-trained language models (@CITE[5]@ and the associative memory properties learned during training ( has allowed researchers to revisit closed-book QA, in which models answer the questions without being provided a passage. Closed-book QA was demonstrated to be effective both in in-context learning (@CITE[1]@ and supervised learning (@CITE[5]@. Recent works on chain-of-thought prompting have shown that generating intermediate steps before giving an answer improves the reasoning capability of LLMs (@CITE[2]@;. Inspired by this, recent works prompt LLMs to generate the intermediate step of QA, which is the passages (@CITE[3]@; @CITE[7]@. These passages are subsequently fed into the reader, either supervised FiD (@CITE[4]@ or in-context learning LLMs (@CITE[1]@. Despite their success, these methods require a very large scale and risk generating passages containing stale or non-factual information. Moreover, it is not fully explained why generating passages may have advantages over retrieval.\\nReferences:ID: 1. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 2. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 5. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 6. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 7. Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models<|eot_id|>'},\n",
       " 2468: {'predict': 'Interpretability of neural networks has been a topic of research in recent years, with various methods proposed to explain the predictions of neural networks. These methods can be broadly classified into two categories: feature attribution methods and model-agnostic methods. Feature attribution methods (@CITE[7]@; @CITE[5]@; @CITE[1]@; @CITE[2]@; @CITE[9]@; @CITE[3]@; @CITE[8]@; @CITE[10]@) aim to explain the predictions of a model by attributing the input features. They typically work by perturbing the input features and measuring the effect on the output of the model. Model-agnostic methods (@CITE[1]@; @CITE[4]@; @CITE[6]@; @CITE[10]@) are more general, as they do not require access to the model architecture. They typically work by perturbing the input features and measuring the effect on the output of the model. Our work is similar to model-agnostic methods, but we focus on providing explanations that are more natural and interpretable. Our method is similar to @CITE[1]@, but we focus on generating explanations that are more natural and interpretable. Our method is similar to @CITE[1]@, but we focus on generating explanations that are more natural and interpretable. Our method is similar to @CITE[6]@, but we focus on generating explanations that are more natural and interpretable. Our method is similar to @CITE[6]@, but we focus on generating explanations that are more natural and interpretable. Our method is similar to @CITE[6]@, but we focus on generating explanations that are more natural and interpretable.\\nReferences:ID: 1. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 2. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 3. Title: Towards Explainable NLP: A Generative Explanation Framework for Text Classification\\nID: 4. Title: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\\nID: 5. Title: NILE : Natural Language Inference with Faithful Natural Language Explanations\\nID: 6. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 7. Title: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task\\nID: 8. Title: Pathologies of Neural Models Make Interpretations Difficult\\nID: 9. Title: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\\nID: 10. Title: Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models',\n",
       "  'ground_truth': '\\n\\nWith the great success of natural language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (@CITE[5]@; @CITE[8]@; @CITE[7]@; ; @CITE[8]@). Three forms of interpretation are provided by these works: (1) feature-based interpretation (Chen et al., 2016a, 2018; @CITE[5]@, 2018; @CITE[6]@; @CITE[7]@; @CITE[5]@; Gururangan et al., 2018) such as attention distribution (@CITE[9]@), heatmap (@CITE[10]@), alignment rationale (), gradients (@CITE[6]@), magnitude of hidden states (Linzen et al., 2016), etc.; (2) token-level interpretation that is relatively easy to comprehend but prone to ambiguity (@CITE[5]@; @CITE[7]@; ); and (3) sentence-level interpretation which has the best human-readability Camburu et al. (2018); ; @CITE[8]@. Different from the previous work which only includes one-side promotion, we proposed the mutual promotion mechanism that can improve the performance of both inference and sentence-level interpretation.\\nReferences:ID: 1. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 2. Title: e-SNLI: Natural Language Inference with Natural Language Explanations\\nID: 3. Title: Towards Explainable NLP: A Generative Explanation Framework for Text Classification\\nID: 4. Title: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\\nID: 5. Title: NILE : Natural Language Inference with Faithful Natural Language Explanations\\nID: 6. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 7. Title: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task\\nID: 8. Title: Pathologies of Neural Models Make Interpretations Difficult\\nID: 9. Title: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\\nID: 10. Title: Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models<|eot_id|>'},\n",
       " 2174: {'predict': 'Hierarchical RL (@CITE[1]@; @CITE[5]@; @CITE[3]@; @CITE[4]@; @CITE[6]@; @CITE[2]@) decomposes complex tasks into simpler ones and trains agents with different levels of abstraction. In this paper, we use hierarchical RL to learn to reason about complex natural language instructions. Our work is inspired by previous works in vision and control domains. In vision, @CITE[1]@ and @CITE[3]@ use a hierarchical RL framework to learn to solve complex tasks such as object sorting and assembly. In control, @CITE[2]@ and @CITE[4]@ use a hierarchical RL framework to learn to solve complex tasks such as robotic manipulation. In NLP, @CITE[6]@ use a hierarchical RL framework to learn to solve complex natural language instructions. However, in all these works, the hierarchical RL framework is used to decompose a complex task into simpler sub-tasks. In contrast, we use a hierarchical RL framework to decompose a complex natural language instruction into simpler sub-instructions. Our work is also inspired by previous works that use language as a natural way to represent the hierarchical structure of complex tasks. @CITE[5]@ use language as a way to represent the hierarchical structure of complex tasks in the vision domain. @CITE[2]@ use language as a way to represent the hierarchical structure of complex tasks in the text-based domain. However, in all these works, the language is used to represent the hierarchical structure of the task, and the agent is trained with a flat RL objective. In contrast, we use language to represent the hierarchical structure of the instruction, and the agent is trained with a hierarchical RL objective.\\nReferences:ID: 1. Title: TACO: Learning Task Decomposition via Temporal Alignment for Control\\nID: 2. Title: Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies\\nID: 3. Title: Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\\nID: 4. Title: FeUdal Networks for Hierarchical Reinforcement Learning\\nID: 5. Title: Language as an Abstraction for Hierarchical Deep Reinforcement Learning\\nID: 6. Title: Generalization in Text-based Games via Hierarchical Reinforcement Learning',\n",
       "  'ground_truth': '\\n\\nOur work is closely related to task decomposition (; @CITE[6]@; @CITE[4]@) and hierarchical reinforcement learning (; @CITE[3]@; @CITE[1]@). Similar to our efforts, @CITE[2]@ and @CITE[5]@ designed a meta-policy for task decomposition and subtask selection, and a sub-policy for goal-conditioned decision-making. Typically, these works either assume the access to a set of available subtasks or decompose a task through pre-defined rules, while we aim to achieve automatic task decomposition through pre-training and remove the requirement for expert knowledge during reinforcement learning. Besides, existing work assumes that unlimited interaction data can be obtained to train the whole model through RL. In contrast, we consider the more practical situation where the interaction data is limited, and focus on improving the RL agent’s data efficiency. Regarding the sub-policy, we do not assume the access to the termination states of the subtasks. We also do not require additional handcrafted operations in reward shaping (Bahdanau et al., 2019).\\nReferences:ID: 1. Title: TACO: Learning Task Decomposition via Temporal Alignment for Control\\nID: 2. Title: Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies\\nID: 3. Title: Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\\nID: 4. Title: FeUdal Networks for Hierarchical Reinforcement Learning\\nID: 5. Title: Language as an Abstraction for Hierarchical Deep Reinforcement Learning\\nID: 6. Title: Generalization in Text-based Games via Hierarchical Reinforcement Learning<|eot_id|>'},\n",
       " 4892: {'predict': 'Sign language translation (SLT) is a task that aims to translate spoken language into sign language. Existing SLT methods can be categorized into two groups: gloss-based and gloss-free. Gloss-based methods first recognize sign language into sign glosses and then translate the glosses into spoken language. Gloss-free methods directly translate sign language videos into spoken language. The most popular gloss-based method is the sequence-to-sequence model (@CITE[4]@; @CITE[5]@; @CITE[6]@; @CITE[1]@. The most popular gloss-free method is the attention-based model (@CITE[2]@; @CITE[3]@. There are also some other methods that are proposed for SLT, such as the graph-based model (, and the reinforcement learning-based model (@CITE[6]@. Most of the existing datasets for SLT are collected from the same signer, which can lead to domain bias. To mitigate this problem, a new dataset called How2Sign is proposed in (.\\nReferences:ID: 1. Title: Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation\\nID: 2. Title: A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation\\nID: 3. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 4. Title: Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition\\nID: 5. Title: Keypoint based Sign Language Translation without Glosses\\nID: 6. Title: How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language',\n",
       "  'ground_truth': '\\n\\nSign language translation (SLT) aims to translate a sign video containing multiple signs to the corresponding spoken text. first proposed the PHOENIX14T dataset that enables the study of direct translation from sign videos to spoken translation. Due to the data scarcity issues caused by the cost of labeling gloss in large quantities. Most works on SLT focus on exploit gloss annotations ( or techniques like back translation (Zhou et al., 2021) between gloss and spoken text. transfers powerful pre-trained models (@CITE[6]@; @CITE[3]@ to the sign domain through progressively pre-training and a mapper network. PET (Jin et al., 2022) utilizes the part-of-speech tag as prior knowledge to guide the text generation. However, they all rely on gloss annotations. There have been attempts to conduct SLT in a gloss-free manner (; @CITE[4]@; @CITE[1]@, but their results are subpar compared to those that use gloss annotation. Recently, there have emerged large-scale SLT datasets like How2Sign (@CITE[2]@ and OpenASL (@CITE[5]@. They both surpass PHOENIX14T in quantity and are not limited to a certain domain. However, these two datasets don’t provide gloss annotations. By far, there are few frameworks have been developed to tackle this challenging scenario except for the baseline methods of the datasets.\\nReferences:ID: 1. Title: Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation\\nID: 2. Title: A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation\\nID: 3. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 4. Title: Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition\\nID: 5. Title: Keypoint based Sign Language Translation without Glosses\\nID: 6. Title: How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language<|eot_id|>'},\n",
       " 5332: {'predict': 'Text style transfer aims to transfer the style of text while preserving its content. Most prior work on style transfer uses parallel training data (@CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[9]@). However, collecting parallel data is expensive and laborious, which limits the development of style transfer models. Therefore, there has been a growing interest in unsupervised text style transfer (USTST) (@CITE[3]@; @CITE[4]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[1]@). In this paper, we focus on unsupervised style transfer for formal language generation. Our work is related to the work of @CITE[6]@, which introduces a novel training objective for unsupervised style transfer. However, their work requires the availability of a style classifier, which is not available for formal language generation. Our work is also related to the work of @CITE[1]@, which proposes a novel method of deleting style attributes from the source sentence by exploiting the inner workings of the Transformer. However, their work requires parallel data and does not consider the evaluation of generated text.\\nReferences:ID: 1. Title: Text Style Transfer: A Review and Experimental Evaluation\\nID: 2. Title: Parallel Data Augmentation for Formality Style Transfer\\nID: 3. Title: Style Transfer Through Back-Translation\\nID: 4. Title: Unsupervised Text Style Transfer using Language Models as Discriminators\\nID: 5. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 6. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 7. Title: Toward Controlled Generation of Text\\nID: 8. Title: Style Transfer from Non-Parallel Text by Cross-Alignment\\nID: 9. Title: Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer',\n",
       "  'ground_truth': '\\n\\nText style transfer aims to modify the style of text without inherently affecting its content (; ; @CITE[2]@. The concept of style can take many forms, including formality (; @CITE[4]@, sentiment (@CITE[5]@;, and authorship (. Text fact transfer is the counterpart to text style transfer, as we focus on transferring the factual content of text between topics without affecting its underlying style. Hence, our task emphasizes generating new, factual text, which is not the main focus of style transfer tasks. Several methods have been developed for text style transfer, such as training neural models on parallel corpora (@CITE[7]@; @CITE[8]@, latently disentangling content and style (@CITE[9]@; @CITE[6]@, or prototype editing (; @CITE[1]@; @CITE[3]@. ModQGA is most similar to the Delete-Retrieve-Generate model (, which extracts attribute markers, transfers attributes across styles, and generates an output. We apply a similar technique for text fact transfer, but notably use a novel combination of end-to-end question generation and specificity-aware question answering, which has not been explored in prior work.\\nReferences:ID: 1. Title: Text Style Transfer: A Review and Experimental Evaluation\\nID: 2. Title: Parallel Data Augmentation for Formality Style Transfer\\nID: 3. Title: Style Transfer Through Back-Translation\\nID: 4. Title: Unsupervised Text Style Transfer using Language Models as Discriminators\\nID: 5. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 6. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 7. Title: Toward Controlled Generation of Text\\nID: 8. Title: Style Transfer from Non-Parallel Text by Cross-Alignment\\nID: 9. Title: Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer<|eot_id|>'},\n",
       " 1704: {'predict': 'The layer-wise or head-wise information has been studied in recent years. @CITE[4]@ and @CITE[1]@ analyzed the layer-wise information in BERT and found that lower layers are more morphological and higher layers are more semantic. @CITE[3]@ found that the lower layers are more invariant during fine-tuning. @CITE[2]@ analyzed the head-wise information in BERT and found that the higher heads are more important. However, none of the previous studies have explored the layer-wise and head-wise information in a unified framework. In this paper, we propose a framework to study the layer-wise and head-wise information in BERT and show that the layer-wise and head-wise information are complementary.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: Dissecting Contextual Word Embeddings: Architecture and Representation\\nID: 3. Title: Visualizing and Understanding the Effectiveness of BERT\\nID: 4. Title: Improving Transformer Models by Reordering their Sublayers',\n",
       "  'ground_truth': '\\n\\n@CITE[2]@ shows that the ﬁrst layers of BERT focus on encoding short dependency relationships at the syntactic level (e.g., subject-verb agreement). In contrast, top layers focus on encoding long-range dependencies (e.g., subject-object dependencies). @CITE[1]@ supports similar declarations for Convolutional, LSTM, and self-attention architectures. While these studies also support that the top layers appear to encode semantic information, the evidence to support this claim is not conclusive or contradictory with other works. For example, Jawahar et al. (2019) could only identify one SentEval semantic task that topped at the last layer. In terms of information flow, Voita et al. (2019a) report that information about the past in left-to-right language models gets vanished as the information flows from bottom to top BERT’s layers. @CITE[3]@ shows that the lower layers of BERT change less during fine-tuning, suggesting that layers close to inputs learn more transferable language representations. @CITE[4]@ shows that increasing self-attention at the bottom layers improves language modeling performance based on BERT.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: Dissecting Contextual Word Embeddings: Architecture and Representation\\nID: 3. Title: Visualizing and Understanding the Effectiveness of BERT\\nID: 4. Title: Improving Transformer Models by Reordering their Sublayers<|eot_id|>'},\n",
       " 4375: {'predict': 'A variety of works have tried to explain the phenomenon of in-context learning. @CITE[4]@ showed that LMs can learn from in-context examples and that this ability is correlated with the model scale. @CITE[3]@ showed that the performance of LMs on in-context learning tasks is closely related to the model’s ability to perform linear classification in a setting where the labels are semantically unrelated to the inputs. @CITE[2]@ showed that LMs can recover a function of the posterior distribution over the latent variables of a hidden Markov model (HMM) from in-context examples. @CITE[1]@ showed that LMs can perform implicit Bayesian inference to solve downstream tasks. @CITE[4]@ showed that LMs can be viewed as gradient descent on a regression loss. @CITE[4]@ showed that LMs can learn an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. @CITE[4]@ showed that LMs can learn a linear model by gradient descent in their forward pass. @CITE[4]@ showed that LMs can learn a linear model by gradient descent in their forward pass and that this is related to the induction head mechanism. In this paper, we show that LMs can perform in-context learning by gradient descent on the self-attention weights.\\nReferences:ID: 1. Title: Larger language models do in-context learning differently\\nID: 2. Title: Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning\\nID: 3. Title: An Explanation of In-context Learning as Implicit Bayesian Inference\\nID: 4. Title: Transformers learn in-context by gradient descent',\n",
       "  'ground_truth': '\\n\\nA number of empirical studies have attempted to characterize the behavior of ICL and explain why it works. These studies have found that ICL can be overly sensitive to certain aspects of the prompt, such as the order of the demonstration examples (@CITE[2]@ and choice of label words (Zhao et al., 2021), but also surprisingly insensitive to others. In particular, show that LLMs can largely ignore the relationship between inputs and labels in the demonstration example and @CITE[3]@ show that the performance of ICL can perform well on NLI even if the prompt is irrelevant or misleading. Relatedly, ; Pan et al. (2023) show that LLMs can perform ICL well with flipped labels or semantically-unrelated labels, but such ability of overriding semantic priors emerges with scale. Some theoretical work has also attempted to explain why prompt-based methods work, by drawing connections between the prompting setting and properties of the pretraining distribution (Saunshi et al., 2021; @CITE[1]@; or by arguing that Transformers can act as meta-learners, implicitly performing gradient descent on the in-context examples (von @CITE[4]@; Akyürek et al., 2023). Our results provide empirical evidence that there is a strong task bias from pretraining when the LLMs must infer the task by input-output relations.\\nReferences:ID: 1. Title: Larger language models do in-context learning differently\\nID: 2. Title: Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning\\nID: 3. Title: An Explanation of In-context Learning as Implicit Bayesian Inference\\nID: 4. Title: Transformers learn in-context by gradient descent<|eot_id|>'},\n",
       " 4276: {'predict': 'Recently, many works have been proposed for generating videos from texts. Existing methods can be roughly categorized into two classes: (1) video generation from text descriptions, and (2) video generation from text-to-image models. For the former, some methods (@CITE[3]@; @CITE[7]@; @CITE[4]@; @CITE[8]@; @CITE[1]@; @CITE[5]@; @CITE[2]@; @CITE[6]@; @CITE[1]@) adopt a text-to-image model as a pre-trained encoder to extract the semantic features of the input text, and then generate videos from the features. However, the quality of the generated videos is not satisfactory. Some methods (@CITE[7]@; @CITE[4]@; @CITE[5]@; @CITE[1]@; @CITE[6]@) decompose the video generation into two steps: generating images and then generating videos from the images. However, this decomposition method cannot ensure the consistency between images and videos. Some methods (@CITE[8]@; @CITE[1]@) use the text-to-image model to generate images, and then use a video generation model to generate videos from the images. However, the text-to-image model cannot understand the semantic of the text. For the latter, some methods (@CITE[3]@; @CITE[7]@; @CITE[4]@) directly apply text-to-image models to generate videos. However, the generated videos are not satisfactory. Some methods (@CITE[6]@; @CITE[5]@; @CITE[2]@) decompose the video generation into two steps: generating images and then generating videos from the images. However, this decomposition method cannot ensure the consistency between images and videos.\\nReferences:ID: 1. Title: CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: Denoising Diffusion Probabilistic Models\\nID: 4. Title: High-Resolution Image Synthesis with Latent Diffusion Models\\nID: 5. Title: To Create What You Tell: Generating Videos from Captions\\nID: 6. Title: Video Generation From Text\\nID: 7. Title: MoCoGAN: Decomposing Motion and Content for Video Generation\\nID: 8. Title: Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures',\n",
       "  'ground_truth': '\\n\\nImage Generation has made many progresses, auto-regressive methods (; @CITE[4]@; @CITE[2]@; leverage VQVAE to tokenize the images into discrete tokens and use Transformers (@CITE[3]@ to model the dependency between tokens. DDPM (@CITE[5]@ presents high-quality image synthesis results. LDM ( performs a diffusion process on latent space, showing significant efficiency and quality improvements. Similar advances have been witnessed in video generation, (Vondrick et al., 2016; Saito et al., 2017; @CITE[1]@; @CITE[7]@; @CITE[8]@ extend GAN to video generation. Syncdraw ( uses a recurrent VAE to automatically generate videos. GODIVA (Wu et al., 2021) proposes a three-dimensional sparse attention to map text tokens to video tokens. VideoGPT (@CITE[4]@ adapts Transformer-based image generation models to video generation with minimal modifications. NUWA (@CITE[6]@ with 3D Nearby Attention extends GODIVA (Wu et al., 2021) to various generation tasks in a unified representation. Cogvideo ( leverages a frozen T2I model ( by adding additional temporal attention modules. More recently, diffusion methods (Ho et al., 2022b; ; Ho et al., 2022a) have also been applied to video generation. Among them, VDM (Ho et al., 2022b) replaces the typical 2D U-Net for modeling images with a 3D U-Net. Make-a-video ( successfully extends a diffusion-based T2I model to T2V without text-video pairs. Imagen Video (Ho et al., 2022a) leverages a cascade of video diffusion models to text-conditional video generation. Different from these works, which concentrate on short video generation, we aim to address the challenges associated with long video generation.\\nReferences:ID: 1. Title: CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: Denoising Diffusion Probabilistic Models\\nID: 4. Title: High-Resolution Image Synthesis with Latent Diffusion Models\\nID: 5. Title: To Create What You Tell: Generating Videos from Captions\\nID: 6. Title: Video Generation From Text\\nID: 7. Title: MoCoGAN: Decomposing Motion and Content for Video Generation\\nID: 8. Title: Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures<|eot_id|>'},\n",
       " 6059: {'predict': 'The Transformer model (@CITE[5]@, based on self-attention, has been widely used in long document summarization (LDS) and query-focused summarization (QFS). Previous works on QFS mainly focus on using pre-trained language models (PLMs) (@CITE[5]@; @CITE[5]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[2]@ as the backbone for extracting relevant information from the document and generating summaries. However, most of the existing works only consider the QFS task as a multi-document summarization task and ignore the query information. In this work, we propose a novel multi-task framework for QFS, which can effectively capture the query information and improve the summarization performance.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Big Bird: Transformers for Longer Sequences\\nID: 3. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 4. Title: Exploring Neural Models for Query-Focused Summarization\\nID: 5. Title: Exploring Neural Models for Query-Focused Summarization',\n",
       "  'ground_truth': '\\n\\nTransformers (@CITE[4]@ have shown remarkable success in document summarization. Standard Transformer-based models can accept up to only 512-1024 tokens at once due to the high computational cost of the self-attention mechanism. Recently, various methods have been proposed to overcome this limitation (@CITE[5]@, such as the sparse attention mechanism used in Big Bird (. FiD (@CITE[1]@ is a Transformer encoder-decoder model that allows multiple documents to be input. Although initially designed for open-domain question answering, it can be applied to MDS tasks (@CITE[2]@;. Query-focused summarization (QFS) aims to generate summaries related to user-specified queries (. Recent studies have applied Transformers to QFS, but most of them simply concatenate queries into input documents (; @CITE[3]@. As mentioned in Section 3, SciReviewGen has an average input length longer than 1024 tokens and contains the titles of literature reviews and chapters as queries. Therefore, we extend FiD for query-focused summarization to tackle the task of literature review generation. Our proposed QFiD explicitly considers the relevance of each input document to the queries.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Big Bird: Transformers for Longer Sequences\\nID: 3. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 4. Title: Exploring Neural Models for Query-Focused Summarization\\nID: 5. Title: Exploring Neural Models for Query-Focused Summarization<|eot_id|>'},\n",
       " 350: {'predict': 'Neural networks enhanced by rules have been explored in various NLP tasks. The idea is to combine the expressiveness of neural networks with the precision of symbolic rules, which can be applied to tasks where high-precision output is required. @CITE[6]@ proposed a structured latent-variable approach to encode task-specific knowledge into neural generative models. @CITE[6]@ introduced a method to learn from rules and exemplars jointly. @CITE[6]@ proposed a semantic loss function to guide the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. @CITE[6]@ proposed a structure learning method for learning sparse, parameter-efficient NLP models. @CITE[6]@ proposed a method to exploit the rich expressiveness of regular expressions at different levels within a neural network. @CITE[5]@ proposed a method to integrate rule-based models with neural networks, which are effective in exploiting the available training data. @CITE[4]@ proposed a method to learn rule-based models with neural networks in a multi-task setting. @CITE[3]@ proposed a method to use rule-based models to improve the robustness of neural networks. @CITE[1]@ proposed a method to use rule-based models to improve the interpretability of neural networks. @CITE[2]@ proposed a method to use rule-based models to improve the performance of neural networks. However, these works only focus on learning from rules, and do not consider the problem of learning with rules.\\nReferences:ID: 1. Title: Posterior Control of Blackbox Generation\\nID: 2. Title: Learning from Rules Generalizing Labeled Exemplars\\nID: 3. Title: Posterior Control of Blackbox Generation\\nID: 4. Title: A Semantic Loss Function for Deep Learning with Symbolic Knowledge\\nID: 5. Title: RNN Architecture Learning with Sparse Regularization\\nID: 6. Title: Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding',\n",
       "  'ground_truth': '\\n\\nHu et al. (2016); @CITE[5]@ use rules to constrain neural networks by knowledge distillation and posterior regularization. @CITE[4]@ inject rule knowledge into neural networks using multi-task learning. @CITE[5]@ train a trigger matching network using additional annotation and use the output of trigger matching results as the attention of a sequence labeler. Rocktäschel et al. (2015); @CITE[3]@; @CITE[1]@ use parsed rule results to regularize neural network predictions by additional loss terms. @CITE[2]@; inject declarative knowledge in the form of parsed RE results or first-order expressions into neural networks by hacking the prediction logits or the attention scores. @CITE[6]@; @CITE[1]@ use rules as additional input features. All these previous methods use matching results or truth values of rules to enhance existing neural models. In contrast, we directly turn REs into a novel type of trainable networks.\\nReferences:ID: 1. Title: Posterior Control of Blackbox Generation\\nID: 2. Title: Learning from Rules Generalizing Labeled Exemplars\\nID: 3. Title: Posterior Control of Blackbox Generation\\nID: 4. Title: A Semantic Loss Function for Deep Learning with Symbolic Knowledge\\nID: 5. Title: RNN Architecture Learning with Sparse Regularization\\nID: 6. Title: Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding<|eot_id|>'},\n",
       " 4826: {'predict': 'To achieve controlled text generation, some works have been proposed to generate diverse text by using different decoding strategies, such as sampling-based methods (@CITE[1]@; @CITE[6]@; @CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[2]@; ), beam search-based methods (Hokamp and Liu, 2017; Anderson et al., 2017; ), and reinforcement learning-based methods (@CITE[7]@; ). However, most of these works focus on controlling the diversity of text generation, while ignoring the overall quality of generated text. In this paper, we propose a novel method to balance the trade-off between the diversity and quality of generated text by using a multi-objective optimization algorithm.\\nReferences:ID: 1. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 2. Title: Semantic Diversity in Dialogue with Natural Language Inference\\nID: 3. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 4. Title: FUDGE: Controlled Text Generation With Future Discriminators\\nID: 5. Title: Guided Open Vocabulary Image Captioning with Constrained Beam Search\\nID: 6. Title: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation\\nID: 7. Title: Trading Off Diversity and Quality in Natural Language Generation',\n",
       "  'ground_truth': '\\n\\nThe diversity of text generation has been a key challenge for applications like dialogue (@CITE[5]@; Zhang et al., 2020; @CITE[7]@, MT (@CITE[2]@ and conditional text generation (@CITE[3]@. Beam search has also been developed to generate more diverse outputs (Vijayakumar et al., 2018; @CITE[6]@; @CITE[1]@. Prior work also studies the trade-off between diversity and quality in text generation (@CITE[4]@.\\nReferences:ID: 1. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 2. Title: Semantic Diversity in Dialogue with Natural Language Inference\\nID: 3. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 4. Title: FUDGE: Controlled Text Generation With Future Discriminators\\nID: 5. Title: Guided Open Vocabulary Image Captioning with Constrained Beam Search\\nID: 6. Title: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation\\nID: 7. Title: Trading Off Diversity and Quality in Natural Language Generation<|eot_id|>'},\n",
       " 5552: {'predict': \"Morphological segmentation is the task of decomposing words into their constituent morphemes. It is a fundamental problem for many NLP tasks such as morphological inflection generation (@CITE[1]@; @CITE[2]@; @CITE[4]@, machine translation (Bentz et al., 2016), and language modeling (Ardila et al., 2021). In this paper, we focus on word-level morphological segmentation. The task of morphological segmentation was previously explored in the SIGMORPHON shared tasks (Cinková et al., 2015; @CITE[1]@, where participants were required to decompose words into morphemes. The task of morphological segmentation is closely related to the task of morphological inflection generation (@CITE[3]@; @CITE[2]@, where the task is to generate the inflected form of a given lemma corresponding to a particular linguistic transformation. The two tasks are related since morphological segmentation is a pre-requisite for morphological inflection generation. In this paper, we focus on the task of morphological segmentation.\\nReferences:ID: 1. Title: The SIGMORPHON 2022 Shared Task on Morpheme Segmentation\\nID: 2. Title: Morphological Inflection Generation Using Character Sequence to Sequence Learning\\nID: 3. Title: The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection\\nID: 4. Title: (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance\",\n",
       "  'ground_truth': \"\\n\\nDecompounding can be seen as a special case of morphological segmentation (@CITE[4]@). However, a large amount of work in morphological segmentation focuses on derivational and inflectional morphology (; @CITE[3]@; @CITE[1]@; McCarthy et al., 2019; ), which is reflected by datasets such as UniMorph (Batsuren et al., 2022b) and MorphyNet (@CITE[2]@) annotating inflectional and derivational affixes, but not compound constituents. The SIGMORPHON-2022 Shared Task (@CITE[4]@, SMST 2022) breaks this pattern by providing a dataset for segmentation into compound constituents in addition to inflectional and derivational affixes. We improve on the SMST 2022 dataset by broadening coverage from 9 to 56 languages, as well as handling negatives (i.e., non-compounds) more carefully (§3.1).\\nReferences:ID: 1. Title: The SIGMORPHON 2022 Shared Task on Morpheme Segmentation\\nID: 2. Title: Morphological Inflection Generation Using Character Sequence to Sequence Learning\\nID: 3. Title: The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection\\nID: 4. Title: (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance<|eot_id|>\"},\n",
       " 3930: {'predict': 'The robustness of local explanation methods has been studied in various ways. @CITE[5]@ show that existing explanation methods are sensitive to factors that do not contribute to the model’s prediction. @CITE[4]@ demonstrate that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. @CITE[3]@ show that existing explanation methods lack reliability and propose a novel Bayesian framework for generating local explanations along with their associated uncertainty. @CITE[2]@ show that the conclusion for how faithful interpretations are can vary substantially based on different notions. @CITE[1]@ propose a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning. These works mainly focus on the sensitivity of explanation methods to the input perturbations, which is a specific aspect of robustness. Our work, in contrast, studies the robustness of explanation methods in a broader sense, including their sensitivity to the model, the data, and the algorithmic choices.\\nReferences:ID: 1. Title: On the Robustness of Interpretability Methods\\nID: 2. Title: The (Un)reliability of saliency methods\\nID: 3. Title: Reliable Post hoc Explanations: Modeling Uncertainty in Explainability\\nID: 4. Title: On the Sensitivity and Stability of Model Interpretations in NLP\\nID: 5. Title: Towards A Rigorous Science of Interpretable Machine Learning',\n",
       "  'ground_truth': '\\n\\nDespite being widely adopted, there has been a long discussion on the actual quality of explanation methods. Recently, people have found that explanation methods can assign substantially different attributions to similar inputs (@CITE[5]@; Ghorbani et al., 2019; ; @CITE[3]@; ; @CITE[1]@), i.e., they are not robust enough, which adds to the concerns about how faithful these explanations are (@CITE[4]@; Adebayo et al., 2018; @CITE[2]@). In addition to previous work focusing on robustness against input perturbations, we demonstrate that even just changing the random seeds can cause the estimated Shapley Values to be weakly-correlated with each other, unless a large number of perturbation samples are used (which incurs high computational cost).\\nReferences:ID: 1. Title: On the Robustness of Interpretability Methods\\nID: 2. Title: The (Un)reliability of saliency methods\\nID: 3. Title: Reliable Post hoc Explanations: Modeling Uncertainty in Explainability\\nID: 4. Title: On the Sensitivity and Stability of Model Interpretations in NLP\\nID: 5. Title: Towards A Rigorous Science of Interpretable Machine Learning<|eot_id|>'},\n",
       " 1176: {'predict': 'Our work is closely related to the recent efforts in understanding pretrained language models. @CITE[1]@ provides a comprehensive survey of analysis methods in neural language processing, including probing and interpretability. Recent works have applied probing to pretrained models to study their capabilities and limitations in a variety of tasks (@CITE[2]@; @CITE[3]@; @CITE[3]@. In the context of pretrained vision-language models, @CITE[2]@; @CITE[3]@; @CITE[3]@; ; ; ; propose various probing tasks to analyze the cross-modal alignment and the ability of pretrained models to understand visual and textual information. Our work differs from previous probing studies in that we focus on the linguistic information encoded in pretrained models and the extent to which it can be retrieved from the attention matrices.\\nReferences:ID: 1. Title: Analysis Methods in Neural Language Processing: A Survey\\nID: 2. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 3. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models',\n",
       "  'ground_truth': '\\n\\nSignificant efforts have been devoted to understanding what is learned by text-only pretrained language models (Rogers et al., 2020), which can be categorised as probing based on diagnostic classifier, analysing attention weights, or direct evaluations (@CITE[1]@). However, much less work has looked at V&L pretraining. Two studies have examined learned attention weights: @CITE[3]@ find that there are attention heads specializing in entity phrase grounding in Visual-BERT, but do not investigate text-to-vision attention. @CITE[3]@ find that the textual modality dominates in UNITER and LXMERT, both in terms of overall attention and more specifically during visual co-reference resolution tasks. In a direct evaluation analysis, @CITE[2]@ evaluate VilBERT and LXMERT on a counting task and find more evidence of dataset bias (predicting frequent numbers) than accurate image grounding. These results all indicate that cross-modal interaction within these models may not be as strong or as symmetric as often assumed, which is further confirmed by the methods introduced in this paper.\\nReferences:ID: 1. Title: Analysis Methods in Neural Language Processing: A Survey\\nID: 2. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 3. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models<|eot_id|>'},\n",
       " 3894: {'predict': 'Previous works on adaptive policy can be categorized into two groups: the first group (@CITE[4]@; @CITE[2]@; @CITE[3]@; ; @CITE[5]@; @CITE[1]@; @CITE[1]@) designs a policy to directly decide whether to READ or WRITE based on the current context. However, these methods suffer from the following issues: (1) The policy is difficult to learn because the policy needs to consider the whole context and make decisions based on the context. (2) The policy is not interpretable because the policy is a black box. The second group (@CITE[1]@; @CITE[5]@) designs a policy to predict the aligned source position of each target word, and accordingly waits until its aligned position to start translating. However, these methods also suffer from the following issues: (1) The policy is not adaptive because it does not consider the current context. (2) The policy is not interpretable because the policy is a black box. In this paper, we propose a novel adaptive policy that can predict the aligned source position of each target word based on the current context. The policy is adaptive because it can consider the current context and make decisions based on the context. The policy is interpretable because the policy is based on the attention weights.\\nReferences:ID: 1. Title: Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation\\nID: 2. Title: Monotonic Multihead Attention\\nID: 3. Title: Simultaneous Translation Policies: From Fixed to Adaptive\\nID: 4. Title: Modeling Dual Read/Write Paths for Simultaneous Machine Translation\\nID: 5. Title: Gaussian Multi-head Attention for Simultaneous Machine Translation',\n",
       "  'ground_truth': '\\n\\nFor adaptive policy, @CITE[5]@ trained the agent with oracle actions generated by full-sentence neural machine translation model. Arivazhagan et al. (2019) proposed MILk to decide the READ and WRITE actions by introducing a Bernoulli variable. proposed MMA, which implemented MILk on Transformer. @CITE[3]@ implemented the adaptive policy through a composition of several fixed policies. @CITE[1]@ proposed a generative framework to perform the adaptive policy for SiMT. @CITE[2]@ introduced duality constraints to direct the learning of translation paths during training. Instead of predicting the READ and WRITE actions, @CITE[4]@ implemented the adaptive policy by predicting the aligned source positions of each target token. Our method focuses on the accuracy of READ and WRITE actions during inference. Our PE policy can evaluate the rationality of actions by utilizing the increment of source content before taking them, which reduces the probability of incorrect actions. Besides, our method achieves good performance under all latency with a unified model.\\nReferences:ID: 1. Title: Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation\\nID: 2. Title: Monotonic Multihead Attention\\nID: 3. Title: Simultaneous Translation Policies: From Fixed to Adaptive\\nID: 4. Title: Modeling Dual Read/Write Paths for Simultaneous Machine Translation\\nID: 5. Title: Gaussian Multi-head Attention for Simultaneous Machine Translation<|eot_id|>'},\n",
       " 7360: {'predict': 'While negation is a crucial aspect of natural language, there has been relatively little work on understanding negation in language models. @CITE[4]@ use the BioScope corpus to train NegBERT, a transfer learning model that achieves a token-level F1 score of 92.36 on scope resolution. They also evaluate NegBERT on negation detection and find that it outperforms a BiLSTM baseline. @CITE[3]@ investigate the ability of LSTMs to understand negative polarity items and find that LSTMs are capable of correctly processing a subset of such constructions. They also find that LSTMs are able to correctly identify the scope of the negation. @CITE[1]@ use a variety of linguistic phenomena, including negation, to evaluate the performance of NLI models. They find that NLI models are able to correctly identify negation, but struggle to handle negation in conjunction with other phenomena. @CITE[2]@ explore the use of negative examples in training neural language models and find that the use of negative examples can improve the model’s robustness to complex syntactic constructions, such as long-distance agreement.\\nReferences:ID: 1. Title: NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\\nID: 2. Title: Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items\\nID: 3. Title: Stress Test Evaluation for Natural Language Inference\\nID: 4. Title: An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models',\n",
       "  'ground_truth': '\\n\\nSome recent work studies negation understanding in the area of negation detection (), negation scope detections (Fancellu et al., 2016; @CITE[2]@; ), attention analysis in negation scope (Zhao and Bethard, 2020), and focus detection (@CITE[3]@). Authors () study linguistic phenomena such as antonyms, negation, and spelling mismatch and find that these models rely on syntactic clues to make the NLE inferences. @CITE[1]@ proposes a benchmark for natural language inference tasks in the form of a text-hypothesis pair in which negation plays a critical role. Also, the authors find that the current state-of-the-art transformers struggle to understand negation, which often results in wrong inferences. shows the utilities of explicit negative examples to improve the syntactic abilities of models for the NLI tasks. The studies (@CITE[1]@; Laverghetta Jr. and @CITE[4]@) show that these state-of-the-art transformer models struggle to understand the negation and there exists the need to inject negation understanding in these models. These works investigate the negation understanding of LMs. However, only a few works try to fix this negation bias in LMs. Hosseini et al. (2021) augment negation understanding in BERT by modeling objectives with an unlikelihood based on negated generic sentences. The proposed method successfully injects knowledge into BERT-base; however, it fails in the BERT-large model.\\nReferences:ID: 1. Title: NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\\nID: 2. Title: Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items\\nID: 3. Title: Stress Test Evaluation for Natural Language Inference\\nID: 4. Title: An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models<|eot_id|>'},\n",
       " 7103: {'predict': 'Existing works on lyrics generation with alignment prediction can be divided into two categories: (1) lyrics generation with alignment prediction for the lyrics-to-melody task, and (2) lyrics generation with alignment prediction for the melody-to-lyrics task. For the lyrics-to-melody task, @CITE[1]@ propose a sequence-to-sequence model with attention mechanism to generate lyrics conditioned on the melody. @CITE[4]@ propose an unsupervised alignment model for song translation. @CITE[2]@ propose an end-to-end melody-conditioned lyrics generation system based on sequence generative adversarial networks. @CITE[3]@ propose a song-level masked sequence to sequence pre-training and attention-based alignment modeling for lyrics-to-melody generation. For the melody-to-lyrics task, @CITE[4]@ propose a sequence-to-sequence model with attention mechanism to generate melody conditioned on the lyrics. @CITE[1]@ propose a Transformer-based model to generate rap with both rhymes and rhythms.\\nReferences:ID: 1. Title: Automatic Song Translation for Tonal Languages\\nID: 2. Title: Melody-Conditioned Lyrics Generation with SeqGANs\\nID: 3. Title: SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint\\nID: 4. Title: DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling',\n",
       "  'ground_truth': '\\n\\nLyrics Generation with Alignment Prediction, one of the most important tasks in automatic song production, has received much attention recently. In recent works of machine translation, graph neural networks (GNNs) are widely used for alignment prediction between the source and target sentences (@CITE[1]@, 2023). In our researched domain, most of the current works (; @CITE[2]@; @CITE[3]@; @CITE[4]@; ; adopt the sequence generation method, but with different objectives. Some constrain rhythmic alignment, others theme and target genre. Other works (@CITE[3]@; @CITE[4]@ apply the attention mechanism and find the lyrics-melody alignment via dynamic programming on the attention weights matrix. This method sometimes results in non-monotonic output. Most importantly, it seems that their alignment component is akin to a post-processing module rather than an integrated unit that learns the dynamic alignment such that the lyrics generation is constrained. In our proposal, we take advantage of the monotonic nature of the lyrics-melody alignments and design a light neural network for alignment prediction in parallel to the translation process. Transformer Decoder Transformer Encoder Note-pooling Embedding Note-pooling Embedding [2zh][LYRICS]But you play it to the beat [bos]但你没有好好 但你没有好好珍（惜） #$%:\\' Alignment Decoder Pre-training 𝑒()*\"+(-) 𝑒()*\"+(/$%&:() #$%:\\' Figure 2: The overall overview of our proposed architecture, illustrated at the j-th decoding step. The transformer decoder will output the target token and the alignment decoder will derive the number of aligned notes.\\nReferences:ID: 1. Title: Automatic Song Translation for Tonal Languages\\nID: 2. Title: Melody-Conditioned Lyrics Generation with SeqGANs\\nID: 3. Title: SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint\\nID: 4. Title: DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling<|eot_id|>'},\n",
       " 2452: {'predict': 'Neuro-Symbolic Reasoning (NSR) is an emerging paradigm that aims to combine the benefits of neural and symbolic approaches. NSR aims to learn symbolic reasoning rules from data and combine them with neural networks to perform complex reasoning tasks. NSR has been successfully applied in various tasks, such as visual question answering (VQA) (@CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE',\n",
       "  'ground_truth': '\\n\\nNeuro-Symbolic reasoning has attracted a lot of research attentions recently due to its advantage of exploiting the representational power of neural networks and the compositionality of symbolic reasoning for more robust and interpretable models (; ; ; Vedantam et al., 2019; @CITE[5]@; Vedantam et al., 2019; van @CITE[2]@. The main difference between neuro-symbolic vs. pure neural networks lies in how the former combines basic rules or modules to model complex functions. Rocktäschel and @CITE[3]@ propose a neuro-symbolic model that can jointly learn sub-symbolic representations and interpretable rules from data via standard back-propagation. In visual QA, propose neural module networks to compose a chain of differentiable modules wherein each module implements an operator from a latent program. @CITE[5]@ propose to discover symbolic program trace from the input question and then execute the program on the structured representation of the image for visual question answering. However, these approaches cannot be easily adapted to task-oriented dialogues due to the error propagation issue caused by multi-hop reasoning on large-scale KBs. Thus, we aim to bridge this gap by developing a neuro-symbolic approach for improving task-oriented dialogues.\\nReferences:ID: 1. Title: Neural Module Networks\\nID: 2. Title: Learning to Reason: End-to-End Module Networks for Visual Question Answering\\nID: 3. Title: Compositional Attention Networks for Machine Reasoning\\nID: 4. Title: Analyzing Differentiable Fuzzy Logic Operators\\nID: 5. Title: Neural Module Networks<|eot_id|>'},\n",
       " 7261: {'predict': 'Dense Passage Retrieval (DPR) (@CITE[5]@) is a dense retrieval method, which retrieves relevant documents from a large corpus by computing the cosine similarity between the query and each document. DPR has become the mainstream method for information retrieval in recent years. DPR is trained with a contrastive loss function. In the training phase, DPR samples a positive passage from the corpus and a set of negative passages as the hard negatives, then computes the similarity between the query and the positive passage and the query and the negative passages, and maximizes the former and minimizes the latter. DPR has been proven to be effective for many NLP tasks, such as open-domain question answering (@CITE[3]@), passage ranking (, etc. However, DPR has two main limitations. One is that DPR is sensitive to the training data quality. The other is that DPR requires large batches to robustly learn the embedding space. To address the above problems, some researchers have proposed a variety of methods to improve DPR. @CITE[4]@ proposes an approximate nearest neighbor negative contrastive estimation method to construct more realistic negative training instances. @CITE[1]@ proposes a corpus-aware language model pre-training method to warm up the passage embedding space. @CITE[2]@ proposes a passage-centric similarity relation method to improve DPR.\\nReferences:ID: 1. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 2. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 3. Title: Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\\nID: 4. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval\\nID: 5. Title: PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval',\n",
       "  'ground_truth': '\\n\\nRecent advancements in PLMs such as BERT (De- vlin et al., 2018), Roberta (@CITE[1]@, and GPT (@CITE[3]@ have enabled numerous unsupervised techniques to derive dense represen- tations of queries and passages for retrieval. These approaches primarily use a Bi-Encoder structure to embed text in a low-dimensional space and learn similarity relations via contrastive learning, con- trasting traditional sparse retrieval methods like BM25 or DeepCT (@CITE[2]@;. DPR ( pi- oneered an unsupervised dense passage retrieval framework, afﬁrming the feasibility of using dense representations for retrieval independently. This ef- ﬁcient and operational approach was further reﬁned by subsequent studies (@CITE[4]@; ; ; @CITE[5]@ that focused on high-quality negative sample min- ing, additional passage relation analysis, and extra training. The essence of these methods is to repre- sent texts in a target space where queries are closer to relevant and distant from irrelevant passages.\\nReferences:ID: 1. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 2. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 3. Title: Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\\nID: 4. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval\\nID: 5. Title: PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval<|eot_id|>'},\n",
       " 4298: {'predict': 'Multilingual LLMs are typically trained on large amounts of data in high-resource languages (e.g. English, Spanish, French, German, Chinese). This means that the majority of the data is in languages with large amounts of available training data, while low-resource languages are underrepresented in the training data. Therefore, the model will have a bias towards the high-resource languages. Recent work has shown that models fine-tuned on high-resource languages can perform surprisingly well in low-resource languages, even when the target language is not present in the training data (@CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[1]@. This is often referred to as zero-shot transfer. However, as the zero-shot transfer performance is heavily dependent on the choice of source language, recent work has shown that other languages such as German and Russian can transfer more effectively than English (@CITE[4]@. This highlights the importance of considering a diverse range of source languages when training the model.\\nReferences:ID: 1. Title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\\nID: 2. Title: Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer\\nID: 3. Title: Beyond English-Centric Multilingual Machine Translation\\nID: 4. Title: Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages\\nID: 5. Title: NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis',\n",
       "  'ground_truth': '\\n\\nA key benefit of “horizontally” scaled multilingual LLMs is transfer from high- to low-resource languages. Our evaluation suggests that Glot500-m excels at this, but this is not the main focus of our paper. There is a large body of work on crosslingual transfer: (@CITE[4]@; Imani-Googhari et al., 2022; @CITE[2]@; Conneau et al., 2020; ; @CITE[1]@; @CITE[3]@; ; @CITE[5]@, inter alia.\\nReferences:ID: 1. Title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\\nID: 2. Title: Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer\\nID: 3. Title: Beyond English-Centric Multilingual Machine Translation\\nID: 4. Title: Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages\\nID: 5. Title: NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis<|eot_id|>'},\n",
       " 2771: {'predict': 'Embedding-based methods encode the entities and relations in a KG into dense vectors, and measure the plausibility of a triple by a scoring function. They can be roughly divided into three categories: bilinear methods, tensor-based methods, and complex-based methods. Bilinear methods (@CITE[1]@; Lin et al., 2015; Bordes et al., 2013) use a bilinear function to score triples. However, they cannot model asymmetric relations. To address this problem, tensor-based methods (@CITE[2]@; Socher et al., 2013) use a tensor to encode relations. But they suffer from high computational complexity. Recently, complex-based methods (@CITE[3]@; ; @CITE[3]@) use a complex-valued vector to encode entities and relations. They can model asymmetric relations and are efficient. In this paper, we focus on the complex-based methods. We propose a novel complex-based model to predict the entity types in KGs. Our model can not only model asymmetric relations but also capture the semantic information of entities and relations. In addition, we design a new negative sampling method to improve the performance of our model. Experiments show that our model can outperform state-of-the-art models.\\nReferences:ID: 1. Title: Complex Embeddings for Simple Link Prediction\\nID: 2. Title: RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space\\nID: 3. Title: Connecting Embeddings for Knowledge Graph Entity Typing',\n",
       "  'ground_truth': '\\n\\nThe entities with known types can be treated as special triples with a unique relation “Has type”, e.g., (Newton, Has type, Physicist). In this way, the KGET can be formulated as a link prediction task. Existing knowledge graph embedding (KGE) methods (Bor- des et al., 2013; @CITE[1]@; @CITE[3]@; @CITE[2]@; Chao et al., 2021) can be used to infer the missing types of Newton by completing the triple (Newton, Has type,?). Although using KGE methods can address the KGET task to some degree, it lacks the diversity of relation types. Therefore, Moon et al. (2017) propose the ETE model to tackle this problem. ETE first learns entity embeddings and relation embeddings with a KGE model on KGs that do not have entity types, then trains the embedding of each entity to be closer to the embedding of its type. For better expressing and reasoning capability, propose the ConnectE model, which considers both local entity typing information and global triple knowledge in KGs. ConnectE first uses TransE (Bordes et al., 2013) to obtain the entity embeddings, then infers the missing types according to two inference mechanisms. One is E2T mechanism that focuses on mapping entities from entity space to entity types space. Another is TRT mechanism, which is based on the assumption that the relation can remain unchanged when replacing the entities in the triple with their types.\\nReferences:ID: 1. Title: Complex Embeddings for Simple Link Prediction\\nID: 2. Title: RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space\\nID: 3. Title: Connecting Embeddings for Knowledge Graph Entity Typing<|eot_id|>'},\n",
       " 5185: {'predict': 'Prompt tuning is a parameter-efficient approach to fine-tune PLMs for downstream tasks. It involves adding learnable continuous prompts to the input sequence and only tuning the prompt parameters. Recent works have shown that prompt tuning is competitive with full model tuning in terms of performance (@CITE[2]@; @CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[5]@; @CITE[1]@; @CITE[1]@. In this paper, we focus on prompt tuning for NLG tasks. One of the key challenges for prompt tuning is that the generated text is often unfamiliar to the PLM, which makes it hard to generate high-quality text. To tackle this issue, we propose to use a PLM to generate prompts instead of directly learning prompts from scratch. We also propose to use a sequence-to-sequence PLM to generate prompts, which allows us to generate more diverse and high-quality prompts.\\nReferences:ID: 1. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 2. Title: Parameter-Efficient Transfer Learning with Diff Pruning\\nID: 3. Title: Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models\\nID: 4. Title: Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models\\nID: 5. Title: Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization',\n",
       "  'ground_truth': '\\n\\nPrompt tuning (@CITE[3]@; @CITE[1]@) inserts learnable parameters to the model as virtual tokens where the insertion can happen at the model input (@CITE[4]@) or each layer (). Later variants improve prompt tuning methods for NLU (@CITE[2]@) and NLG (@CITE[2]@) tasks respectively through a series of optimizations and adaptations. More recent studies add residual connections to improve the performance and stability of prompt tuning () and extend prompt tuning to the continual learning setting (Razdaibiedina et al., 2023a). However, most of these methods only simply add prompts to input layers, which greatly limited their performances. Most recently, mixture prompt tuning has been proposed in MixPrompt (@CITE[5]@) and E2VPT (@CITE[3]@), which combines the input prompts with key-value prompts. These methods can be treated as special cases of our attention prompt tuning approach.\\nReferences:ID: 1. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 2. Title: Parameter-Efficient Transfer Learning with Diff Pruning\\nID: 3. Title: Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models\\nID: 4. Title: Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models\\nID: 5. Title: Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization<|eot_id|>'},\n",
       " 2577: {'predict': 'Semantic generalization is the ability to generalize to new semantic concepts, such as new relations, entities, and types of events, which is a challenging task for semantic parsing models. Previous works have shown that semantic generalization can be improved by injecting structural prior knowledge into models (@CITE[8]@; @CITE[7]@; @CITE[6]@; @CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[9]@; @CITE[1]@; @CITE[2]@. However, these works are mostly limited to semantic parsing, which is a relatively simple task. In this work, we propose a new method for semantic generalization by using a pre-trained language model to paraphrase the input into a controlled language. We show that this method can be applied to a wide range of NLP tasks, including relation extraction, question answering, and semantic parsing.\\nReferences:ID: 1. Title: Learning to Paraphrase for Question Answering\\nID: 2. Title: Improving Compositional Generalization in Semantic Parsing\\nID: 3. Title: Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing\\nID: 4. Title: Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering\\nID: 5. Title: Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations\\nID: 6. Title: Data Recombination for Neural Semantic Parsing\\nID: 7. Title: Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data\\nID: 8. Title: CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\\nID: 9. Title: Constrained Language Models Yield Few-Shot Semantic Parsers',\n",
       "  'ground_truth': '\\n\\nRecently, the generalization problem has become a research hotspot in semantic parsing. To generalize on various natural language expressions, semantic-invariance knowledge is introduced by paraphrasing (@CITE[2]@; ; @CITE[5]@; ). There are also many studies for achieving generalization on meaning composition (@CITE[7]@; @CITE[4]@; ; @CITE[8]@; @CITE[3]@). To generalize in low-resource settings, data augmentation (@CITE[1]@; @CITE[6]@) and constrained decoding (; @CITE[9]@) methods are proposed. In this paper, the generalization of representations is improved by pulling the utterances and MRs with similar overall semantics closer in representation space.\\nReferences:ID: 1. Title: Learning to Paraphrase for Question Answering\\nID: 2. Title: Improving Compositional Generalization in Semantic Parsing\\nID: 3. Title: Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing\\nID: 4. Title: Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering\\nID: 5. Title: Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations\\nID: 6. Title: Data Recombination for Neural Semantic Parsing\\nID: 7. Title: Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data\\nID: 8. Title: CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\\nID: 9. Title: Constrained Language Models Yield Few-Shot Semantic Parsers<|eot_id|>'},\n",
       " 2672: {'predict': 'Prompt-based methods have shown remarkable performance in few-shot learning (; @CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[3]@. These methods usually rely on a manually designed verbalizer, which is a function that maps the label to a label word. The verbalizer can be either a handcrafted word or a learned embedding. @CITE[2]@ and @CITE[3]@ propose to learn the verbalizer from the training data. In this paper, we focus on designing a verbalizer that is easy to learn and generalize well to unseen classes. To this end, we propose to learn a verbalizer from the semantic embedding of the class name.\\nReferences:ID: 1. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 2. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 3. Title: Prototypical Verbalizer for Prompt-based Few-shot Tuning\\nID: 4. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification',\n",
       "  'ground_truth': '\\n\\nPrompt-based methods perform zero-shot learning in a natural way by mimicking human behaviors when solving NLP tasks (. Many existing works on prompts focus on text classification, where a template is used to transform the classification task into a cloze task, and a verbalizer maps the predicted words into classification labels (Schick and Schütze, 2021). With carefully designed templates and verbalizers, prompt-based methods can perform comparably to supervised methods in text classification. Various methods have been explored for designing templates (@CITE[3]@; @CITE[1]@ and verbalizers (@CITE[4]@. Other researchers leverage external knowledge. @CITE[2]@ expand label names with knowledge bases, and Chen et al. (2022) re-train PLMs by adaptively retrieving extra data. SimPTC shares the idea of utilizing natural language templates and class names. Nevertheless, instead of reformulating the classification task, SimPTC uses natural language templates and class names to construct class-related texts, which are used to compute initial cluster positions and shapes.\\nReferences:ID: 1. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 2. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 3. Title: Prototypical Verbalizer for Prompt-based Few-shot Tuning\\nID: 4. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification<|eot_id|>'},\n",
       " 5735: {'predict': 'Tabular schema linking is an essential component of the text-to-SQL pipeline. Existing works on tabular schema linking can be broadly categorized into two types: rule-based approaches and neural-based approaches. Rule-based approaches rely on predefined rules to map NL expressions to the corresponding table and column (@CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[2]@; ; @CITE[5]@. Neural-based approaches adopt neural models to learn the mapping rules between NL expressions and the schema. For example, @CITE[4]@ propose to use the attention mechanism to calculate the importance of schema elements to the NL expressions. @CITE[2]@ propose a two-stage neural model to learn the mapping rules between NL expressions and the schema. Different from the existing works, we propose to use a contrastive learning framework to learn the mapping rules between NL expressions and the schema. Our framework is more effective than the existing works in learning the complex mapping rules between NL expressions and the schema.\\nReferences:ID: 1. Title: Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation\\nID: 2. Title: Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing\\nID: 3. Title: On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries\\nID: 4. Title: A Tale of Two Linkings: Dynamically Gating between Schema Linking and Structural Linking for Text-to-SQL Parsing\\nID: 5. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task',\n",
       "  'ground_truth': '\\n\\nSchema linking (; @CITE[2]@) is an instance of entity linking in the context of linking to the relational database schema. Previous research shows that good schema linking (@CITE[4]@; Katsakioris et al., 2022; @CITE[1]@; @CITE[2]@; @CITE[3]@) can substantially improve downstream tasks such as Text-to-SQL parsing. However, entity mentions in existing benchmarks such as Spider () can almost exactly match the corresponding schema entities (@CITE[3]@). Therefore, current Text-to-SQL semantic parsers normally address this problem with string-matching heuristics (@CITE[3]@) or embedding matching modules (@CITE[3]@; @CITE[5]@; ; @CITE[2]@). However, due to the diversity and ambiguity in natural language mentions, such heuristics are hard to generalize to new domains (@CITE[3]@; @CITE[5]@).\\nReferences:ID: 1. Title: Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation\\nID: 2. Title: Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing\\nID: 3. Title: On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries\\nID: 4. Title: A Tale of Two Linkings: Dynamically Gating between Schema Linking and Structural Linking for Text-to-SQL Parsing\\nID: 5. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task<|eot_id|>'},\n",
       " 5441: {'predict': 'Iterative refinement is a common approach to improve the performance of LLMs on complex tasks. The approach typically involves generating an initial solution, then iteratively refining it based on feedback from an external tool or human. Self-Debugging (@CITE[4]@) is an example of iterative refinement in code generation, which uses LLMs to generate code and then refines it based on the execution result. In the field of math, @CITE[1]@ and @CITE[3]@ use iterative refinement to generate math solutions. REFINER (@CITE[2]@) is a framework for iterative refinement in the reasoning task, which uses a critic model to provide feedback on the reasoning. In this paper, we extend the idea of iterative refinement to the problem of generating explanations. We use a simple classifier as the critic model to provide feedback on the generated explanations, and use the feedback to refine the explanations. We show that iterative refinement can significantly improve the quality of the generated explanations.\\nReferences:ID: 1. Title: Teaching Large Language Models to Self-Debug\\nID: 2. Title: Solving Math Word Problems by Combining Language Models With Symbolic Solvers\\nID: 3. Title: REFINER: Reasoning Feedback on Intermediate Representations\\nID: 4. Title: Llama 2: Open Foundation and Fine-Tuned Chat Models',\n",
       "  'ground_truth': '\\n\\nOne stream of work is dedicated to iteratively enhancing LLMs by continuously reevaluating and refining outputs until the desired quality is achieved. @CITE[4]@ prompts the model to write feedback based on previously generated drafts and leverages the feedback to generate high-quality outputs. Similarly, @CITE[1]@ iteratively debugs the code by utilizing external program execution results and code explanations generated by the model itself. In order to avoid repetitive mistakes, @CITE[2]@ builds a memory of previous errors, while @CITE[3]@ collects all mistakes during the training phase to provide a global insight. When considering sources of hints to guide rethinking, focuses on intermediate reasoning steps, while directly utilizes the previously generated answers. proposes to emulate the divide-and-conquer fashion of human thinking strategy and involve self-questioning and recursive thinking processes in the problem-solving framework. Although these approaches contribute to improving the reasoning quality of LLMs, they are limited in retrying without looking around for other possible thoughts. In contrast, our proposed method aims to explore alternative solutions, and it is orthogonal to iterative refinement, as we have the flexibility to switch solutions when refining no longer leads to further improvement.\\nReferences:ID: 1. Title: Teaching Large Language Models to Self-Debug\\nID: 2. Title: Solving Math Word Problems by Combining Language Models With Symbolic Solvers\\nID: 3. Title: REFINER: Reasoning Feedback on Intermediate Representations\\nID: 4. Title: Llama 2: Open Foundation and Fine-Tuned Chat Models<|eot_id|>'},\n",
       " 2478: {'predict': 'Recent years have seen a resurgence of interest in neural summarization, with a focus on the extractive approach (Nallapati et al., 2016; See et al., 2017; @CITE[5]@). The most popular neural extractive summarization models are based on the sequence-to-sequence framework (Rush et al., 2015; Nallapati et al., 2016), which use a recurrent neural network (RNN) or a transformer to generate summaries. These models are typically trained using supervised learning and are evaluated on single-document summarization tasks. Neural multi-document summarization is more challenging than single-document summarization, as it requires models to identify important sentences across multiple documents and to generate a coherent summary. To address this challenge, several neural multi-document summarization models have been proposed, including a hierarchical transformer model (@CITE[4]@; @CITE[4]@) and a pointer-generator network (@CITE[3]@). However, these models are limited by the size of the training data and require significant computational resources. In contrast, our model is based on a transformer encoder-decoder model and is trained using a large-scale dataset of news articles, which enables us to achieve high-quality summaries at a lower cost.\\nReferences:ID: 1. Title: Abstract Meaning Representation for Multi-Document Summarization\\nID: 2. Title: A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\\nID: 5. Title: Hierarchical Transformers for Multi-Document Summarization',\n",
       "  'ground_truth': '\\n\\nThese models can be categorized into two classes, graph-based models (; @CITE[5]@; ; and hierarchical models (; ; @CITE[5]@. Graph-based models often require auxiliary information (e.g., AMR, discourse structure) to build an input graph, making them reliant on auxiliary models and less general. Hierarchical models are another class of models for multi-document summarization, examples of which include multi-head pooling and inter-paragraph attention (, MMR-based attention (;, and attention across representations of different granularity (words, sentences, and documents) (@CITE[5]@. Prior work has also shown the advantages of customized optimization in multi-document summarization (e.g., RL;. Such models are often dataset-specific and difficult to develop and adapt to other datasets or tasks.\\nReferences:ID: 1. Title: Abstract Meaning Representation for Multi-Document Summarization\\nID: 2. Title: A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\\nID: 5. Title: Hierarchical Transformers for Multi-Document Summarization<|eot_id|>'},\n",
       " 5636: {'predict': 'Zero-shot learning (ZSL) aims to train a model to classify a text sample without seeing the target labels. In ZSL, the training set only contains the source classes, and the model is required to generalize to the unseen target classes. ZSL has been studied in the context of text classification (@CITE[8]@; @CITE[2]@; @CITE[4]@; @CITE[1]@; @CITE[3]@; @CITE[5]@; @CITE[7]@; @CITE[6]@; @CITE[9]@; @CITE[6]@; @CITE[6]@). In our work, we adopt a zero-shot learning setting, where the model is trained on the source classes and tested on the target classes.\\nReferences:ID: 1. Title: Text Classification Using Label Names Only: A Language Model Self-Training Approach\\nID: 2. Title: Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches\\nID: 3. Title: Matching Networks for One Shot Learning\\nID: 4. Title: Neural ranking models for document retrieval\\nID: 5. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\\nID: 6. Title: GILE: A Generalized Input-Label Embedding for Text Classification\\nID: 7. Title: Train Once, Test Anywhere: Zero-Shot Learning for Text Classification\\nID: 8. Title: Neural ranking models for document retrieval\\nID: 9. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT',\n",
       "  'ground_truth': '\\n\\nZero-shot learning for text classification involves classifying texts into classes which were not seen during the learning stage (unseen classes). To this end, zero-shot techniques often rely on transferring knowledge from seen to unseen classes by leveraging semantic attributes or descriptions of classes (@CITE[9]@, inter-class correlations (@CITE[5]@, or joint embeddings of classes and examples (@CITE[7]@; @CITE[2]@. Given the complexity of class descriptions in our datasets, we focus on methods that use semantic attributes. Matching models, one such approach, are of particular interest. These models take pairs of texts as input—in our case a class description and example text—and output a score. They differ from “matching networks” (@CITE[3]@, which are instead aimed at few-shot learning. Matching models can be categorized into a taxonomy (@CITE[4]@; @CITE[8]@ based on the amount of interaction they allow between input pairs. In increasing order of interaction, performance, and computational complexity, we have (1) representation-based models (or bi-encoders) (Pappas and ; @CITE[6]@, which embed the inputs separately before computing their matching score; (2) late interaction models (@CITE[4]@; @CITE[8]@, which allow more interplay between input pairs, but still have separate encoding stacks; and (3) all-to-all models (or cross-encoders) (; ; @CITE[1]@, which facilitate immediate interaction. Matching models also have close ties to information retrieval (see Appendix A).\\nReferences:ID: 1. Title: Text Classification Using Label Names Only: A Language Model Self-Training Approach\\nID: 2. Title: Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches\\nID: 3. Title: Matching Networks for One Shot Learning\\nID: 4. Title: Neural ranking models for document retrieval\\nID: 5. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\\nID: 6. Title: GILE: A Generalized Input-Label Embedding for Text Classification\\nID: 7. Title: Train Once, Test Anywhere: Zero-Shot Learning for Text Classification\\nID: 8. Title: Neural ranking models for document retrieval\\nID: 9. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT<|eot_id|>'},\n",
       " 6200: {'predict': 'There are several approaches to modeling solutions. Span-based approaches model programs as trees and then generate solutions by traversing the tree. For example, SpanBasedSP (@CITE[6]@) generates solutions by traversing the tree, and SpanTree (@CITE[5]@) generates the tree itself. Tagging approaches (@CITE[4]@; @CITE[2]@) decompose decoding into two phases, where an input utterance is first tagged with semantic symbols representing the meaning of individual words, and then a sequence-to-sequence model is used to predict the final meaning representation. Another line of work focuses on data augmentation, which aims to improve compositional generalization by generating new training examples that have the same semantic structure as the original examples (@CITE[1]@; @CITE[3]@; @CITE[3]@; @CITE[2]@; @CITE[2]@; @CITE[1]@). These approaches have shown strong performance in improving compositional generalization, but they are not easily applicable to semantic parsing with multiple outputs, as they are not designed to handle multiple outputs. In this paper, we propose a novel approach to modeling solutions, which is designed to handle multiple outputs.\\nReferences:ID: 1. Title: Span-based Semantic Parsing for Compositional Generalization\\nID: 2. Title: Learning to Recombine and Resample Data for Compositional Generalization\\nID: 3. Title: Compositional Generalization via Semantic Tagging\\nID: 4. Title: Data Recombination for Neural Semantic Parsing\\nID: 5. Title: Good-Enough Compositional Data Augmentation\\nID: 6. Title: Improving Compositional Generalization in Semantic Parsing',\n",
       "  'ground_truth': '\\n\\nAnother line of work focuses on modeling solutions, mostly ways to explicitly instil compositional bias into neural models. This can be achieved by adopting a more conventional grammar-based approach (@CITE[1]@) or incorporating a lexicon or lexicon-style alignments into sequence models (; @CITE[4]@). Other work employs heuristics, grammars, and generative models to synthesize examples for data augmentation (; Akyürek et al., 2021; @CITE[5]@; @CITE[6]@; Qiu et al., 2022) or modifies standard training objectives with new supervision signals like attention supervision or meta-learning (@CITE[2]@; @CITE[6]@; @CITE[3]@).\\nReferences:ID: 1. Title: Span-based Semantic Parsing for Compositional Generalization\\nID: 2. Title: Learning to Recombine and Resample Data for Compositional Generalization\\nID: 3. Title: Compositional Generalization via Semantic Tagging\\nID: 4. Title: Data Recombination for Neural Semantic Parsing\\nID: 5. Title: Good-Enough Compositional Data Augmentation\\nID: 6. Title: Improving Compositional Generalization in Semantic Parsing<|eot_id|>'},\n",
       " 4679: {'predict': 'Relational triple extraction (RTE) is a challenging task in NLP. It has attracted much attention in recent years due to its great importance in knowledge graph construction. Most of the existing RTE methods follow the tagging scheme (@CITE[4]@; @CITE[1]@; @CITE[5]@; @CITE[3]@; @CITE[2]@; @CITE[2]@;. However, the tagging scheme-based methods suffer from error accumulation. To alleviate the problem, some methods (@CITE[5]@; @CITE[2]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[1]@ adopt a two-step framework, which first identifies the head and tail entities, and then extracts the relation type. However, the two-step framework is prone to error accumulation. To solve the problem, some methods (@CITE[5]@; @CITE[2]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[1]@ adopt a one-step framework, which first extracts all the entities and then extracts all the triples. However, the one-step framework is prone to error accumulation. In this paper, we propose a two-step framework, which first extracts all the entities, and then extracts all the triples. The two-step framework can avoid error accumulation. Besides, the two-step framework can obtain more precise entity positions and relation types.\\nReferences:ID: 1. Title: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\\nID: 2. Title: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\\nID: 3. Title: A Simple but Effective Bidirectional Framework for Relational Triple Extraction\\nID: 4. Title: TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking\\nID: 5. Title: Relational Triple Extraction: One Step is Enough',\n",
       "  'ground_truth': '\\n\\nThe RTE methods can be roughly divided into following four categories based on the idea of relational triple extraction. The first category is the tagging-based method, which utilizes several correlated sequence labeling modules to annotate head entities, tail entities and even relations. For example, the Novel Tagging Scheme proposed by @CITE[1]@ firstly converts the RTE task into a tagging problem. Then the CASREL proposed by @CITE[4]@ models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. And Zheng et al. (2021) proposed an extractor based on Potential Relation and Global Correspondence to alleviate the redundancy of relation prediction. BiRTE (@CITE[2]@) proposes a bidirectional extraction framework based method that extracts triples based on the entity pairs extracted from two complementary directions. The second category is the table-filling method, which determines the head and tail entities by classifying the relationships of token pairs. The typical representation of the table-filling methods is TPLinker (@CITE[5]@), which introduces a novel handshaking tagging scheme that aligns the boundary tokens of BERT Encoder Relations Negative Sampling Global relational triple region detection Relation-specific vertice heatmaps Vertice tagging matrix Tom and Jerry live New York Jerry Tom and Jerry live New York Jerry Relation: place_lived (‘Tom’, place_lived, ‘New York’) (‘Jerry’, place_lived, ‘New York’) Bi-DW decoding algorithm generate the triples. And the fourth category is the graph-linking method (@CITE[3]@), which models the triple extraction problem as a bipartite graph linking problem of enumerated candidate entities.\\nReferences:ID: 1. Title: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\\nID: 2. Title: A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\\nID: 3. Title: A Simple but Effective Bidirectional Framework for Relational Triple Extraction\\nID: 4. Title: TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking\\nID: 5. Title: Relational Triple Extraction: One Step is Enough<|eot_id|>'},\n",
       " 6681: {'predict': 'Penalizing negative examples is a common technique in machine learning to prevent models from generating undesired outputs. @CITE[3]@ use the contrastive loss to train a model to avoid generating toxic text. @CITE[1]@ train a model to avoid generating text with a high perplexity. @CITE[2]@ use the CRINGE loss to train a model to avoid generating text that contradicts the input. @CITE[4]@ use a similar approach to train a model to avoid generating text that contradicts the input in a dialogue setting. These approaches can be applied to the task of text summarization by penalizing the model for generating summaries that are similar to the input text. We propose a new approach that penalizes the model for generating summaries that are similar to the input text, but also encourages the model to generate summaries that are similar to the ground truth summary.\\nReferences:ID: 1. Title: Text Detoxification using Large Pre-trained Neural Models\\nID: 2. Title: Neural Text Generation with Unlikelihood Training\\nID: 3. Title: BRIO: Bringing Order to Abstractive Summarization\\nID: 4. Title: The CRINGE Loss: Learning what language not to model',\n",
       "  'ground_truth': '\\n\\nPrevious work on detoxification did not systematically address penalizing the toxic samples. For instance, ParaGeDi (@CITE[3]@ proposes a loss to penalize the generated texts that are labeled as toxic by toxicity classifiers. However, the multiple objective nature of text style transfer (preserve content, preserve fluency, change style) has not been properly and systematically addressed as ParaGeDi relies on heuristics for content preservation. Unlikelihood training has been used in controllable text generation applications to avoid undesirable tokens with a high probability (@CITE[1]@. CLICK (Zheng et al., 2023), SLiC (Zhao et al., 2022), BRIO (@CITE[2]@ and CRINGE (@CITE[4]@ also use unlikelihood training for various text generation applications such as summarization and sentiment control. Despite the popularity of unlikelihood training in text generation, it has not been widely applied to the text style transfer task. We empirically observe that unlikelihood (@CITE[1]@ training for text style transfer often fails to properly penalize the toxic samples since it has two training objectives that often conflict. Specifically, penalizing the likelihood of generating the toxic sample contradicts with maximizing the likelihood of the gold-standard target since the gold-standard rephrasing often overlaps substantially with the input and the loss cannot discriminate the toxic from the non-toxic content. Addressing this deficiency of unlikelihood for detoxification is our primary focus in this work.\\nReferences:ID: 1. Title: Text Detoxification using Large Pre-trained Neural Models\\nID: 2. Title: Neural Text Generation with Unlikelihood Training\\nID: 3. Title: BRIO: Bringing Order to Abstractive Summarization\\nID: 4. Title: The CRINGE Loss: Learning what language not to model<|eot_id|>'},\n",
       " 1061: {'predict': 'The Transformer model has been widely adopted for text-based tasks and has achieved state-of-the-art results in many NLP tasks (@CITE[4]@. In recent years, there has been an increasing interest in applying Transformers to tabular data. proposed a method for open question answering over tables and text. They used an early fusion technique to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. They also proposed a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. proposed a method for encoding long and structured inputs in Transformers. They introduced a novel global-local attention mechanism between global tokens and regular input tokens. They also showed that combining global-local attention with relative position encodings and a contrastive predictive coding pre-training objective allows to encode structured inputs. @CITE[3]@ proposed a method for understanding tables with intermediate pre-training. They adapted TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. They also created a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. @CITE[2]@ proposed a method for tabular question answering. They adapted the T5 model (Raffel et al., 2019) to answer tabular questions. They also introduced a novel table-aware attention mechanism to better capture the table structure. @CITE[1]@ proposed a method for tabular data imputation. They adapted the T5 model (Raffel et al., 2019) to impute missing values in tables. They also introduced a novel table-aware attention mechanism to better capture the table structure.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Open Question Answering over Tables and Text\\nID: 3. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 4. Title: Understanding tables with intermediate pre-training',\n",
       "  'ground_truth': '\\n\\nTraditionally, tasks involving tables were tackled by searching for logical forms in a semantic parsing setting. More recently Transformers (@CITE[4]@ have been used to train end-to-end models on tabular data as well (@CITE[2]@. For example, TAPAS ( relies on Transformer- based masked language model pre-training and spe- cial row and column embeddings to encode the ta- ble structure. @CITE[1]@ use a variant of ETC ( on an open-domain ver- sion of HYBRIDQA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length @CITE[3]@ propose heuristic column selection techniques, and they also pro- pose pre-training on synthetic data. Krichene et al. (2021) propose a model based cell selection tech- nique that is differentiable and trained end-to-end together with the main task model. Our approach is orthogonal to these methods, and can be usefully combined with them, as shown in Table 4. Recently, @CITE[2]@ proposed SAT, which uses an attention mask to restrict attention to tokens in the same row and same column. SAT also computes an additional histogram row appended at the bottom of the table and encodes the table content as text only (unlike TAPAS). The proposed method is not head dependent as ours is, which pre- vents it from being implemented efﬁciently to allow scaling to larger sequence lengths. Controlling for model size and pre-training for a fair comparison, we show that our model is both faster (Table 4) and more accurate (Table 6) than SAT.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Open Question Answering over Tables and Text\\nID: 3. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 4. Title: Understanding tables with intermediate pre-training<|eot_id|>'},\n",
       " 4124: {'predict': 'Uncertainty measures are commonly used to select the most informative samples for active learning. This is because the uncertainty of a model can serve as a proxy for the potential gain in performance that can be achieved by adding the sample to the labeled set. For example, @CITE[6]@ proposed to use the uncertainty of a model to select the most informative samples. @CITE[5]@ proposed to use the entropy of the model to select the most informative samples. @CITE[1]@ proposed to use the entropy of the model to select the most informative samples. @CITE[3]@ proposed to use the entropy of the model to select the most informative samples. @CITE[4]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. @CITE[2]@ proposed to use the entropy of the model to select the most informative samples. However, these methods may not perform well when the model is over-confident, i.e., the model has high confidence in its predictions, but the predictions are incorrect. In this paper, we propose to use a combination of uncertainty and confidence to select the most informative samples for active learning.\\nReferences:ID: 1. Title: A Survey of Deep Active Learning\\nID: 2. Title: Cost-Effective Active Learning for Deep Image Classification\\nID: 3. Title: Bayesian Active Learning for Classification and Preference Learning\\nID: 4. Title: Deep Bayesian Active Learning with Image Data\\nID: 5. Title: Consistency-based Semi-supervised Active Learning: Towards Minimizing Labeling Cost\\nID: 6. Title: Active Learning by Acquiring Contrastive Examples',\n",
       "  'ground_truth': '\\n\\nActive learning typically adopts uncertainty to estimate the informativeness of a sample (@CITE[1]@, which mainly consists of two directions: estimating the uncertainty of a model’s direct output (@CITE[4]@; and the calibrated uncertainty with a group of models (@CITE[6]@; @CITE[3]@; Kirsch et al., 2019). Besides, some work estimates the uncertainty of a sample Table 8: Performance of two settings of combining active learning and counterfactual construction. by comparing it with other related samples. (@CITE[5]@ proposes to calculate the variance (i.e., inconsistency) of predictions over a random set of data augmentation over the given sample. And (@CITE[2]@ proposes to select the contrastive samples by calculating the KL-divergence between a sample and its nearest neighbors in the labeled set. These variance-based methods only focus on the prediction of the current model, which provides little information about whether the model relies on spurious correlation to predict the sample. However, our acquisition strategy focuses on the dynamic process of how the model changes its prediction along the training, and the variability recovers the existence of the spurious correlations that are informative for OOD generalization.\\nReferences:ID: 1. Title: A Survey of Deep Active Learning\\nID: 2. Title: Cost-Effective Active Learning for Deep Image Classification\\nID: 3. Title: Bayesian Active Learning for Classification and Preference Learning\\nID: 4. Title: Deep Bayesian Active Learning with Image Data\\nID: 5. Title: Consistency-based Semi-supervised Active Learning: Towards Minimizing Labeling Cost\\nID: 6. Title: Active Learning by Acquiring Contrastive Examples<|eot_id|>'},\n",
       " 1773: {'predict': 'Approaches to acceptability classification can be divided into two categories: unsupervised methods and supervised methods. Unsupervised methods learn from unlabeled sentences and use statistical models to predict acceptability. For example, @CITE[4]@ use a probabilistic suffix tree to learn the distribution of words and phrases in sentences and predict acceptability. @CITE[5]@ use a probabilistic dependency parser to extract syntactic features from sentences and predict acceptability. Supervised methods learn from labeled sentences and use statistical models to predict acceptability. For example, @CITE[1]@ use a recurrent neural network to learn from labeled sentences and predict acceptability. @CITE[3]@ use a BiLSTM-CRF model to learn from labeled sentences and predict acceptability. Recently, pre-trained language models (PLMs) have been used to predict acceptability. For example, @CITE[2]@ use a pre-trained BERT model to learn from labeled sentences and predict acceptability. @CITE[4]@ use a pre-trained BERT model to predict acceptability by using entailment to reformulate the acceptability classification task. However, the above methods only consider the acceptability of sentences, while we consider the acceptability of sentences in a context.\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: Neural Network Acceptability Judgments\\nID: 3. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 4. Title: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\\nID: 5. Title: Entailment as Few-Shot Learner',\n",
       "  'ground_truth': '\\n\\nThe CoLA corpus was presented together with a number of experiments aimed at assessing the performance of neural networks on a novel binary acceptability task (. The best performance was achieved with a pooling classifier and ELMo-style embeddings, yielding 0.341 MCC on in-domain data and 0.281 on the out- of-domain test set. Matthews Correlation Coefficient (MCC) was chosen as an evaluation measure because it is more appropriate than F1 or accuracy for binary classification with unbalanced data (@CITE[5]@. More recently, ( extended the classification experiments by comparing a BiLSTM baseline with the performance achieved by transformer encoders such as GPT and BERT. The best approach is obtained by fine-tuning BERTlarge with a mean MCC of 0.582. Other approaches, instead, focus on unsupervised learning, for example (Lau et al., 2015, 2020) compare different types of language models to infer the probability of a sentence, which is then mapped onto acceptability. Since CoLA has been included in the GLUE dataset (, a very popular multi- task benchmark for English natural language understanding, and an acceptability challenge has been launched on Kaggle,2 the number of studies dealing with binary acceptability has remarkably increased. Unfortunately, most studies using GLUE report accuracy instead of MCC, making it difficult to identify the best approach. Nevertheless, all top-ranked systems rely on variations of transformer-based models, including ALBERT 2https://www.kaggle.com/c/ cola-in-domain-open-evaluation/ (@CITE[4]@ (69.1 Accuracy) and StructBERT (@CITE[1]@ (69.2 Acc.). More recently, also reformulating acceptability as an entailment task and using smaller language models to few-shot fine-tuning has showed a great potential (, outperforming existing BERT-based approaches (86.4 Acc.). Concerning acceptability on Italian, a shared task has been organized for the first time at Evalita 2020 Evaluation campaign, proposing a joint classification of complexity and acceptability (Brunato et al., 2020). The dataset, which we use for our out-of- domain evaluation (Section 4.2) was originally created merging data from different psycholinguistic studies, and includes 1,683 sentences with a manually assigned value of acceptability between 1 and 7. Two participants submitted three runs in total. In order to cope with the limited number of training instances, the best performing approach (@CITE[3]@ implemented an ensemble of fine-tuned models to annotate a large corpus of unlabeled text, and lever- aged new annotations in a multi-task setting to obtain final predictions over the original test set. The second system (@CITE[2]@ was rule-based, implementing a set of syntactic and semantic constraints to check to what an extent a sentence can be considered acceptable.\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: Neural Network Acceptability Judgments\\nID: 3. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 4. Title: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\\nID: 5. Title: Entailment as Few-Shot Learner<|eot_id|>'},\n",
       " 4319: {'predict': 'MRC is a task that aims to answer questions based on the context. In this task, the input is a context and the output is an answer. MRC can be divided into two types: extractive MRC and generative MRC. Extractive MRC aims to extract a span from the context as the answer. Generative MRC aims to generate the answer. In extractive MRC, the model is required to predict the start and end positions of the answer. The most widely used extractive MRC dataset is SQuAD (@CITE[1]@). In generative MRC, the model is required to generate a sequence of tokens as the answer. The most widely used generative MRC dataset is WikiSQL (@CITE[2]@). In addition to the above two types, some works aim to solve the MRC task in a unified way. @CITE[3]@ formulate nested NER as an extractive MRC task and propose a unified framework for nested NER. @CITE[4]@ unify the seven subtasks of ABSA into a generative MRC task. @CITE[6]@ propose a unified framework to solve the MRC task in a semi-supervised way. @CITE[7]@ propose a unified framework to solve the MRC task in a multitask way. @CITE[5]@ propose a unified framework to solve the MRC task with a generative and extractive way. In this paper, we propose a unified framework to solve the MRC task with a generative and extractive way. The difference is that we introduce the auxiliary task to help the model generate the answer in a unified way. The generative and extractive MRC tasks are solved in a unified way by the same model.\\nReferences:ID: 1. Title: Teaching Machines to Read and Comprehend\\nID: 2. Title: A Unified MRC Framework for Named Entity Recognition\\nID: 3. Title: A Unified Generative Framework for Aspect-Based Sentiment Analysis\\nID: 4. Title: CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 7. Title: The Natural Language Decathlon: Multitask Learning as Question Answering',\n",
       "  'ground_truth': '\\n\\nMRC is to extract an answer span from a relevant context conditioned on a given query. It is initially designed to solve question answering tasks (, while recent trends have shown great advantages in formulating NLP tasks as MRC problems. In the context of SpanID, @CITE[2]@; @CITE[6]@, 2023b) address the nested NER issues by decomposing nested entities under multiple queries. Mao et al. (2021); @CITE[1]@ tackle ABSA in a unified MRC framework. @CITE[3]@ tackle CCE with MRC to deal with the extraction of long clauses. Moreover, other tasks such as relation extraction (@CITE[5]@, event detection (@CITE[7]@, 2021a), and summarization (@CITE[4]@ are also reported to benefit from the MRC paradigm.\\nReferences:ID: 1. Title: Teaching Machines to Read and Comprehend\\nID: 2. Title: A Unified MRC Framework for Named Entity Recognition\\nID: 3. Title: A Unified Generative Framework for Aspect-Based Sentiment Analysis\\nID: 4. Title: CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 7. Title: The Natural Language Decathlon: Multitask Learning as Question Answering<|eot_id|>'},\n",
       " 6743: {'predict': 'Recently, some works utilize the bottom-up strategy. Mun et al. (2020) proposed a regression-based model to extract a collection of mid-level features for semantic phrases in a text query, which corresponds to important semantic entities described in the query, and reflect bi-modal interactions between the linguistic features of the query and the visual features of the video in multiple levels. @CITE[2]@ proposed a latent graph co-attention network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query. @CITE[4]@ proposed a joint visual-semantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. @CITE[5]@ proposed a two-stage model that tackles this problem in a coarse-to-fine manner. @CITE[6]@ proposed a multi-level attentional reconstruction network (MARN) that only relies on video-sentence pairs during the training stage. @CITE[1]@ proposed a two-stage model that tackles this problem in a coarse-to-fine manner. @CITE[7]@ proposed a weakly-supervised temporal grounding model, which uses a combination of the bottom-up and top-down strategy. @CITE[3]@ proposed a weakly-supervised temporal grounding model that only relies on video-sentence pairs during the training stage, which is an extension of the work proposed in (@CITE[7]@. Different from these works, we propose a weakly-supervised temporal grounding model that uses a combination of the bottom-up and top-down strategy, and the bottom-up and top-down strategy are used to learn the proposal-level and clip-level features, respectively.\\nReferences:ID: 1. Title: Local-Global Video-Text Interactions for Temporal Grounding\\nID: 2. Title: LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval\\nID: 3. Title: Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video\\nID: 4. Title: Weakly Supervised Video Moment Retrieval From Text Queries\\nID: 5. Title: Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video\\nID: 6. Title: Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video\\nID: 7. Title: Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos',\n",
       "  'ground_truth': '\\n\\nRecently, some works utilize the bottom-up strategy (@CITE[7]@; @CITE[3]@; @CITE[7]@), which directly regresses the boundary of the target segment or predicts boundary probabilities frame-wisely. To alleviate the reliance to a certain extent, some state-of-the-art turn to the weakly-supervised setting (@CITE[5]@; ; @CITE[2]@; @CITE[6]@; @CITE[6]@; ; @CITE[4]@), where only video-query pairs are annotated without precise segment boundaries. To further mitigate the reliance, an unsupervised work (@CITE[1]@) clusters the semantic information of the whole query set by a complex training process for grounding. Different from the above methods, we address the challenging unsupervised TSG task from a novel perspective: collecting and transferring the general cross-modal knowledge from other simple yet cheap tasks.\\nReferences:ID: 1. Title: Local-Global Video-Text Interactions for Temporal Grounding\\nID: 2. Title: LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval\\nID: 3. Title: Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video\\nID: 4. Title: Weakly Supervised Video Moment Retrieval From Text Queries\\nID: 5. Title: Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video\\nID: 6. Title: Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video\\nID: 7. Title: Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos<|eot_id|>'},\n",
       " 6226: {'predict': 'The adversarial IR literature has explored various types of adversarial attacks on retrieval models. @CITE[4]@ proposed an adversarial attack that manipulates the query and document to make the retrieval model rank the target document higher. @CITE[6]@ explored a position-biased attack on the retrieval model. @CITE[3]@ proposed a typo-based attack to generate adversarial examples for the retrieval model. @CITE[7]@ proposed an imitation adversarial attack that leverages the ranking imitation model to manipulate the ranking results. @CITE[1]@ proposed a pseudo relevance-based adversarial ranking attack that learns a surrogate model based on pseudo relevance feedback to generate gradients for finding the adversarial perturbations. @CITE[5]@ proposed a multi-step adversarial attack that adds a new step to the attack process. @CITE[2]@ proposed a semantic adversarial attack that leverages the semantic embedding to manipulate the ranking results. Our work differs from these studies in that we explore the adversarial attacks on the ranking model with the given query and document, rather than generating the adversarial examples from scratch. In addition, our work focuses on the black-box attack setting, where the attacker cannot access the internal information of the ranking model.\\nReferences:ID: 1. Title: Ranking Robustness Under Adversarial Document Manipulations\\nID: 2. Title: Ranking-Incentivized Quality Preserving Content Modification\\nID: 3. Title: Mitigating the Position Bias of Transformer Models in Passage Re-Ranking\\nID: 4. Title: Dealing with Typos for BERT-based Passage Retrieval and Ranking\\nID: 5. Title: One word at a time: adversarial attacks on retrieval models\\nID: 6. Title: Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models\\nID: 7. Title: PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking Models',\n",
       "  'ground_truth': '\\n\\nThe adversarial IR has been studied for an extended period, such as black-hat SEO, which refers to the intentional manipulation of Web pages with the goal of obtaining an unjustified ranking position, resulting in a decline in the quality of search results and an inundation of irrelevant pages (Gyöngyi and. In this context, mainstream research focuses on studying the adversarial manipulations through various aspects, such as detection (@CITE[3]@; @CITE[1]@, theoretical and empirical analysis (@CITE[6]@, robustness of LTR-based ranking functions (, automatic content modification (@CITE[4]@, and other research directions (. Recently, there has been significant progress in NRMs, particularly those leveraging PLMs, which have shown exceptional performance in text ranking (. Concurrently, an increasing number of studies have shed light on the robustness concerns of NRMs in various scenarios, including the presence of query typos or variations (; Penha et al., 2022; @CITE[5]@, textual noises (@CITE[7]@, and adversarial attacks (;. Although current attack methods like PRADA (@CITE[2]@, Brittle-BERT (, and PAT ( have shown the ability to deceive NRMs successfully, they introduce additional quality issues and often heavily rely on surrogate NRMs for document manipulation. Instead, our proposed IDEM effectively overcomes the limitations of these existing attack methods and showcases remarkable attack performance.\\nReferences:ID: 1. Title: Ranking Robustness Under Adversarial Document Manipulations\\nID: 2. Title: Ranking-Incentivized Quality Preserving Content Modification\\nID: 3. Title: Mitigating the Position Bias of Transformer Models in Passage Re-Ranking\\nID: 4. Title: Dealing with Typos for BERT-based Passage Retrieval and Ranking\\nID: 5. Title: One word at a time: adversarial attacks on retrieval models\\nID: 6. Title: Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models\\nID: 7. Title: PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking Models<|eot_id|>'},\n",
       " 3482: {'predict': 'The recent progress of PLMs has led to a new paradigm of relation classification fine-tuning PLMs for RC. Early works (@CITE[4]@; @CITE[5]@; @CITE[7]@; @CITE[6]@) leverage the pre-trained knowledge of PLMs to enhance the performance of RC. @CITE[9]@ propose a novel method to utilize the pre-trained knowledge of PLMs for RC. @CITE[8]@ propose to use a unified learnable verbalizer to generate the entity span sequence and entity categories. @CITE[1]@ propose a new framework to solve many structured prediction language tasks including RC. @CITE[3]@ propose a text-to-triple translation framework. @CITE[2]@ propose to reformulate RC as an entailment task. @CITE[4]@ propose to use a pre-trained textual entailment engine for RC. @CITE[4]@ propose to use a pre-trained textual entailment engine for RC. @CITE[4]@ propose to use a pre-trained textual entailment engine for RC. @CITE[4]@ propose to use a pre-trained textual entailment engine for RC. @CITE[4]@ propose to use a pre-trained textual entailment engine for RC.\\nReferences:ID: 1. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 2. Title: GDPNet: Refining Latent Multi-View Graph for Relation Extraction\\nID: 3. Title: Relation Classification with Entity Type Restriction\\nID: 4. Title: An Improved Baseline for Sentence-level Relation Extraction\\nID: 5. Title: Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction\\nID: 6. Title: LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting\\nID: 7. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 8. Title: Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\\nID: 9. Title: Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction',\n",
       "  'ground_truth': '\\n\\nFine-tuning PLMs for RC (@CITE[6]@; Yamada et al., 2020; ; @CITE[9]@ has achieved promising performance. @CITE[5]@ achieves state-of-the-art results by incorporating entity type information into entity markers. Another interesting line is converting information extraction into generation form. @CITE[7]@ and @CITE[1]@ propose seq2seq models to extract relational facts. @CITE[8]@ present a generative framework for document-level entity-based extraction tasks. There are other works converting RC to translation framework (@CITE[2]@; @CITE[4]@ and entailment task (@CITE[3]@.\\nReferences:ID: 1. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 2. Title: GDPNet: Refining Latent Multi-View Graph for Relation Extraction\\nID: 3. Title: Relation Classification with Entity Type Restriction\\nID: 4. Title: An Improved Baseline for Sentence-level Relation Extraction\\nID: 5. Title: Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction\\nID: 6. Title: LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting\\nID: 7. Title: Structured Prediction as Translation between Augmented Natural Languages\\nID: 8. Title: Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\\nID: 9. Title: Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction<|eot_id|>'},\n",
       " 4245: {'predict': 'Network pruning aims to reduce the number of parameters and FLOPs of a neural network while preserving its original performance. The lottery ticket hypothesis (@CITE[1]@) suggests that the dense network contains a sparse subnetwork (winning ticket) that can be trained independently and achieve comparable performance to the original network. SNIP (Molchanov et al., 2019) and GraSP (Cheng et al., 2017) are two popular pruning methods. SNIP calculates the importance of each connection based on the gradient of the output with respect to the input. GraSP is based on the gradient of the loss with respect to the weights. Both methods can be used to prune the network in a greedy manner. To improve the performance of the pruned network, many works (@CITE[4]@; @CITE[3]@; @CITE[2]@) propose to fine-tune the pruned network from scratch. However, these methods require a large amount of computational resources and memory. In contrast, our method can improve the performance of the pruned network by simply updating the parameters of the adapter.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Rigging the Lottery: Making All Tickets Winners\\nID: 3. Title: SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters\\nID: 4. Title: The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\\nID: 5. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "  'ground_truth': '\\n\\nPast works in network pruning have explored effective techniques to find efficient subnetworks (@CITE[4]@; @CITE[1]@; @CITE[2]@) and zero out redundant parameters. According to the lottery ticket hypothesis (LTH) pioneered by @CITE[3]@, dense, randomly initialized, feed-forward networks contain the subnetwork (winning tickets) that maintains comparable test performance of the original network after training for the same iterations. This hypothesis inspires a series of follow-up works in network pruning. However, these methods always sacrifice performance because of pruned parameters. As for dynamic networks, instead of directly pruning dynamic parameters, we considered changing them to static ones. In Section 5.4, we show our approach significantly and consistently outperforms fully dynamic networks in the GLUE benchmark (@CITE[5]@), while the pruned model performed worse than the original network.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Rigging the Lottery: Making All Tickets Winners\\nID: 3. Title: SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters\\nID: 4. Title: The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\\nID: 5. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding<|eot_id|>'},\n",
       " 3383: {'predict': 'Multilingual representation learning is an active area of research. There has been a line of work that focuses on learning multilingual representations using parallel data (@CITE[6]@; @CITE[2]@; @CITE[4]@; @CITE[5]@. These approaches typically involve learning multilingual representations by training a single model on a large amount of parallel data in multiple languages. For example, @CITE[4]@ pre-train a multilingual denoising auto-encoder on a large amount of monolingual data in multiple languages. @CITE[1]@ pre-train a multilingual masked language model on a large amount of filtered CommonCrawl data in multiple languages. More recently, @CITE[3]@; @CITE[1]@; @CITE[5]@ have explored pre-training multilingual models on large-scale monolingual corpora in multiple languages. In contrast, we focus on learning multilingual representations using monolingual data in a single language. We demonstrate that this approach is effective for learning multilingual representations that can be used for cross-lingual transfer.\\nReferences:ID: 1. Title: Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation\\nID: 2. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 3. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 4. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 5. Title: mT5: A massively multilingual pre-trained text-to-text transformer\\nID: 6. Title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond',\n",
       "  'ground_truth': '\\n\\nThere have been extensive efforts in learning massive cross-lingual representations. Such models are pretrained with a large amount of unlabeled data from multiple languages with the intention to benefit low-resource languages with the rich-resource languages through shared vocabulary, genetic relatedness ( or contact relatedness (@CITE[6]@. Some of the widely adopted models are mBERT (Devlin et al., 2019), XLM (@CITE[5]@, mBART (@CITE[5]@, MARGE (@CITE[6]@, XLM-R (@CITE[6]@, and mT5 (@CITE[5]@. Other models also leverage cross-lingual signals (large-scale parallel data) with a translation language model objective, including LASER (, VECO ( and ERNIE-M (.\\nReferences:ID: 1. Title: Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation\\nID: 2. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 3. Title: Multilingual Denoising Pre-training for Neural Machine Translation\\nID: 4. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 5. Title: mT5: A massively multilingual pre-trained text-to-text transformer\\nID: 6. Title: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond<|eot_id|>'},\n",
       " 7172: {'predict': 'Energy-based models are widely used in computer vision and natural language processing. @CITE[1]@ proposed a distributional approach to controlled text generation from pre-trained LMs. They used an EBM to specify the target LM distribution. @CITE[2]@ proposed a latent space energy-based prior model for text generation and classification. The model stands on a generator network that generates the text sequence based on a continuous latent vector. The energy term of the prior model couples a continuous latent vector and a symbolic one-hot vector. @CITE[4]@ proposed a latent space energy-based model of symbol-vector coupling for text generation and classification. They proposed a latent space energy-based model of symbol-vector coupling for text generation and classification. The model stands on a generator network that generates the text sequence based on a continuous latent vector. The energy term of the prior model couples a continuous latent vector and a symbolic one-hot vector. @CITE[3]@ proposed a simple but effective method for investigating the regularity of entity spans in Chinese NER. They proposed a simple but effective method for investigating the regularity of entity spans in Chinese NER. The proposed model consists of two branches: a regularity-aware module and a regularityagnostic module. The regularity-aware module captures the internal regularity of each span for better entity type prediction, while the regularity-agnostic module is employed to locate the boundary of entities and relieve the excessive attention to span regularity. An orthogonality space is further constructed to encourage two modules to extract different aspects of regularity features.\\nReferences:ID: 1. Title: A Distributional Approach to Controlled Text Generation\\nID: 2. Title: Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification\\nID: 3. Title: Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition\\nID: 4. Title: Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification',\n",
       "  'ground_truth': '\\n\\nRecently, energy-based models (EBMs) have emerged as a flexible generative framework capable of handling diverse configurations (@CITE[3]@; ). These models allow for the incorporation of arbitrary functions into the energy function, which is minimized during inference. As a result, many recent works leverage EBMs to model complex distributions (@CITE[2]@; @CITE[4]@) and incorporate multiple constraints and attributes (Nie et al., 2021; @CITE[2]@; @CITE[1]@; ). For example, Mix-and-Match (Mireshghallah et al., 2022) employs EBMs to combine arbitrary black-box scorers for guiding text generation, while COLD (@CITE[1]@) utilizes the energy function to impose arbitrary constraints during the decoding process. LatentOps () introduces composable text control operations utilizing classifier-based EBMs. However, these works primarily focus on plain-text generation domains, whereas our approach applies EBM to dialogue-generation scenarios, specifically modeling complex personality as a composition of multiple personal attributes based on CVAE architecture. We also adapt the ODE sampling method to effectively sample personalized dialogue responses.\\nReferences:ID: 1. Title: A Distributional Approach to Controlled Text Generation\\nID: 2. Title: Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification\\nID: 3. Title: Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition\\nID: 4. Title: Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification<|eot_id|>'},\n",
       " 703: {'predict': 'Data scarcity has been a long-standing challenge in semantic parsing. One approach is to use unsupervised or weakly supervised data, such as denotations (@CITE[7]@; @CITE[2]@; @CITE[5]@), paraphrases (@CITE[9]@), or abstract representations (@CITE[6]@; @CITE[3]@). Another approach is to use generative models to generate synthetic data from rules (@CITE[1]@), word embeddings (@CITE[1]@), or other sources (; @CITE[8]@). However, these approaches typically require domain-specific rules or lexical knowledge. In contrast, our approach focuses on generalizing semantic parsing across domains. We also note that our approach is orthogonal to existing work on cross-domain semantic parsing, which aims to adapt a parser from one domain to another (@CITE[4]@; @CITE[1]@; @CITE[9]@; @CITE[6]@; @CITE[8]@).\\nReferences:ID: 1. Title: Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing\\nID: 2. Title: Inferring Logical Forms From Denotations\\nID: 3. Title: Weakly-supervised Semantic Parsing with Abstract Examples\\nID: 4. Title: StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing\\nID: 5. Title: Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning\\nID: 6. Title: Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning\\nID: 7. Title: Data Recombination for Neural Semantic Parsing\\nID: 8. Title: Cross-domain Semantic Parsing via Paraphrasing\\nID: 9. Title: Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing',\n",
       "  'ground_truth': '\\n\\nWitnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; @CITE[5]@; ; @CITE[5]@; @CITE[6]@), such as denotation-based learning (; ), iterative searching (). Semi-supervised semantic parsing is also proposed (), such as variational auto-encoding (@CITE[7]@), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (@CITE[9]@), and back-translation (@CITE[9]@). One other strategy is to generate data for semantic parsing, e.g., construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudo-labeled data. @CITE[8]@ create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (@CITE[5]@; @CITE[6]@; ; Zhong et al., 2020). Goldwasser et al. (2011); ; Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning.\\nReferences:ID: 1. Title: Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing\\nID: 2. Title: Inferring Logical Forms From Denotations\\nID: 3. Title: Weakly-supervised Semantic Parsing with Abstract Examples\\nID: 4. Title: StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing\\nID: 5. Title: Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning\\nID: 6. Title: Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning\\nID: 7. Title: Data Recombination for Neural Semantic Parsing\\nID: 8. Title: Cross-domain Semantic Parsing via Paraphrasing\\nID: 9. Title: Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing<|eot_id|>'},\n",
       " 5166: {'predict': 'The Transformer model (@CITE[4]@, which is based on self-attention, has been widely used in text classification tasks. The pretrained language model BERT (@CITE[2]@ is based on the Transformer model, which is trained on a large corpus and achieves good performance in many natural language processing tasks. BERT is also used in text classification tasks, and it has achieved good performance in many text classification tasks (@CITE[3]@; ; ;. However, due to the large number of parameters in the BERT model, it is difficult to apply it to low-resource and low-computation environments. Therefore, there are many studies on the distillation of BERT models, which can reduce the number of parameters and improve the efficiency of the model (@CITE[1]@;. In addition, the fine-tuning process of BERT models also has a certain impact on the performance of the model. Some studies have shown that fine-tuning can distort the pre-trained features of the model, and the performance of the model is not optimal (@CITE[3]@. Therefore, some studies have proposed the linear probing method, which only updates the last layer of the model, to improve the performance of the model in the fine-tuning process.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 4. Title: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution',\n",
       "  'ground_truth': '\\n\\nTransformer-based (@CITE[1]@ Pre-trained Language Models (PLMs) (; @CITE[3]@; can be fine-tuned to build sentence embedding models (. Since the original goal of training sentence embeddings is to better model the sentence similarity for applications such as dense retrieval and sentence clustering (, their usage is less explored in text classification. Though frozen sentence embeddings can directly serve as input features in text classification (Perone et al., 2018;, the performance is limited compared to standard full fine-tuning of PLMs (@CITE[2]@. To compensate this performance loss, concatenate encodings from various Sentence Transformers to form semantically richer sentence representations, achieving results comparable to standard fine-tuning, but at the cost of slower inference. More recently, SetFit (@CITE[4]@ significantly improves the few-shot classification by contrastively fine-tuning a pre-trained sentence-embedding model before training a classification head. Despite efficiently utilizing the limited labeled samples, SetFit does not leverage the abundant in-domain unlabeled data that can provide more domain knowledge for the task.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 4. Title: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution<|eot_id|>'},\n",
       " 3027: {'predict': 'In the past few years, several benchmarks have been proposed to test the compositional generalization of models. The most popular one is SCAN (@CITE[1]@, which is a sequence-to-sequence dataset for evaluating the compositional generalization of models. The dataset consists of simple commands paired with action sequences. SCAN has been extended to a new dataset, NACS (@CITE[8]@, where models are required to map actions back to the original commands. In the context of semantic parsing, COGS (@CITE[3]@ is a dataset that contains multiple systematic gaps that require compositional generalization. gSCAN (@CITE[6]@ is a grounded language understanding benchmark for evaluating compositional generalization. It is a grid-world navigation task that requires models to understand how adjectives and adverbs combine with new verbs. COVR (@CITE[4]@ is a visually-grounded compositional generalization benchmark with real images. It focuses on questions that require complex reasoning, including higher-order operations such as quantification and aggregation. In the context of text-to-SQL, @CITE[5]@ proposes a dataset with systematic gaps that require compositional generalization. They also propose a new method for constructing compositional generalization benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets. They use this method to construct a large and realistic natural language question answering dataset and analyze the compositional generalization ability of three machine learning architectures. @CITE[7]@ propose a method for constructing compositional generalization benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets. They use this method to construct a new compositionality benchmark on top of the existing SCAN dataset. They also demonstrate that this method can be used to construct new compositionality benchmarks on top of existing datasets, such as COGS and gSCAN. Our dataset is similar to the above-mentioned benchmarks, but it is a text-to-SQL dataset that requires compositional generalization in a more realistic way. In our dataset, the compositional generalization is not only limited to the level of words, but also to the level of phrases and sentences.\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Jump to better conclusions: SCAN both left and right\\nID: 3. Title: COGS: A Compositional Generalization Challenge Based on Semantic Interpretation\\nID: 4. Title: A Benchmark for Systematic Generalization in Grounded Language Understanding\\nID: 5. Title: Improving Text-to-SQL Evaluation Methodology\\nID: 6. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 7. Title: Measuring Compositional Generalization: A Comprehensive Method on Realistic Data',\n",
       "  'ground_truth': '\\n\\nCompositional splits are have been defined either manually (@CITE[7]@; @CITE[4]@; Bahdanau et al., 2019; @CITE[3]@; @CITE[1]@; @CITE[2]@) or automatically in a dataset-agnostic manner (as in § 6.1). Automatic methods include splitting by output length (@CITE[7]@), by anonymizing programs (@CITE[5]@), and by maximizing divergence between the training and test sets (@CITE[8]@; @CITE[6]@).\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Jump to better conclusions: SCAN both left and right\\nID: 3. Title: COGS: A Compositional Generalization Challenge Based on Semantic Interpretation\\nID: 4. Title: A Benchmark for Systematic Generalization in Grounded Language Understanding\\nID: 5. Title: COVR: A test-bed for Visually Grounded Compositional Generalization with real images\\nID: 6. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 7. Title: Improving Text-to-SQL Evaluation Methodology\\nID: 8. Title: Measuring Compositional Generalization: A Comprehensive Method on Realistic Data<|eot_id|>'},\n",
       " 4047: {'predict': 'Generative Information Extraction (GIE) aims to extract information from the input text and generate the target structure, such as triples, tables, or graphs. GIE has been widely studied in the field of knowledge graph construction (@CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[2]@. In the field of IE, many works have been proposed to extract information using generative methods, such as generating entity pairs (@CITE[4]@, relations (@CITE[2]@, and tables (@CITE[1]@. However, these methods only focus on a single IE task, and the extracted information is in the form of a flat sequence. In contrast, our work focuses on generating complex event structures, which contain multiple events and are more expressive than flat sequences. To the best of our knowledge, our work is the first to explore generating complex event structures using generative methods.\\nReferences:ID: 1. Title: Generative Knowledge Graph Construction: A Review\\nID: 2. Title: Exploring Generative Models for Joint Attribute Value Extraction from Product Titles\\nID: 3. Title: Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy\\nID: 4. Title: Unified Structure Generation for Universal Information Extraction',\n",
       "  'ground_truth': '\\n\\nGenerative Information Extraction, a rising technique in these two years (@CITE[3]@, is also an inspiration for proposing this research. A contemporaneous work ( adopts sequence generation models and preliminarily shows the potential of generative models in open-world attribute extraction. Alongside sequence-based generation models, structure generation models are also widely studied and have shown power in other IE tasks. For example, REBEL (@CITE[2]@ introduces a structure-linearized model for relation extraction; Seq2UMTree (@CITE[4]@ conducts a sequence-to-unordered-multi-tree generation model for extracting entities and relations jointly; UIE (@CITE[1]@ proposes a text-to-structure generation framework that can universally model different IE tasks based on the guidance of the pre-defined schema. Though both attribute extraction and generative models have been widely explored, we are the first to design a novel tree generation model for AE and demonstrate the effectiveness on our unified real-world paradigm.\\nReferences:ID: 1. Title: Generative Knowledge Graph Construction: A Review\\nID: 2. Title: Exploring Generative Models for Joint Attribute Value Extraction from Product Titles\\nID: 3. Title: Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy\\nID: 4. Title: Unified Structure Generation for Universal Information Extraction<|eot_id|>'},\n",
       " 5191: {'predict': 'Figurative language is a rich source of creative expression in literature and poetry. Recently, there has been a surge of interest in generating figurative language, including similes (@CITE[1]@), metaphors (@CITE[2]@; @CITE[6]@; @CITE[4]@; ), and idioms (@CITE[3]@). @CITE[5]@; @CITE[3]@; @CITE[6]@; @CITE[4]@; @CITE[5]@ generate visual metaphors with LLMs and diffusion models. However, these works do not explicitly consider the visual grounding of metaphors. @CITE[6]@ and @CITE[5]@ generate visual metaphors with a two-stage process, where LLMs are used to generate the textual metaphors and then diffusion models are used to generate the visual metaphors. In contrast, our work directly generates visual metaphors with LLMs and diffusion models. Our work is also related to the multimodal understanding of figurative language. @CITE[4]@ and @CITE[5]@ release a dataset that contains images and textual metaphors. @CITE[6]@ release a dataset that contains images and textual similes. Our work focuses on generating visual metaphors from textual metaphors, while these works focus on recognizing textual metaphors from images. The closest work to ours is @CITE[5]@, which also generates visual metaphors from textual metaphors. However, their work is limited to generating visual metaphors from textual metaphors in a fixed domain, while our work is more general and can generate visual metaphors from textual metaphors in various domains.\\nReferences:ID: 1. Title: Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation\\nID: 2. Title: Neural Simile Recognition with Cyclic Multitask Learning and Local Attention\\nID: 3. Title: I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors\\nID: 4. Title: IRFL: Image Recognition of Figurative Language\\nID: 5. Title: MetaCLUE: Towards Comprehensive Visual Metaphors Research\\nID: 6. Title: MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding',\n",
       "  'ground_truth': '\\n\\nThere has been extensive research on textual figurative language such as metaphor generation (; @CITE[3]@; @CITE[6]@), idiom generation and paraphrasing (@CITE[2]@; ), and simile recognition and interpretation (; ). Visualizing figurative language, on the other hand, has received less attention. Existing approaches primarily revolved around constructing datasets with images and annotations for metaphors, similes, and idioms (@CITE[4]@; @CITE[5]@; @CITE[1]@; ). However, these datasets are small and rely on expert knowledge. Despite the benefits of such datasets, they do not provide a fully automated process in figurative language visualization. We, for the first time, present a lightweight and robust model tailored for assisting text-to-image models in visualizing figurative language. Our model is not only robust and open-source but also requires neither human annotations nor additional images.\\nReferences:ID: 1. Title: Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation\\nID: 2. Title: Neural Simile Recognition with Cyclic Multitask Learning and Local Attention\\nID: 3. Title: I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors\\nID: 4. Title: IRFL: Image Recognition of Figurative Language\\nID: 5. Title: MetaCLUE: Towards Comprehensive Visual Metaphors Research\\nID: 6. Title: MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding<|eot_id|>'},\n",
       " 2459: {'predict': 'Many studies have proposed methods to address model bias in various tasks. For example, @CITE[5]@; @CITE[6]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[7]@; @CITE[2]@; @CITE[8]@; @CITE[9]@; @CITE[8]@; @CITE[10]@; @CITE[1]@; @CITE[6]@; @CITE[2]@; @CITE[7]@; @CITE[8]@; @CITE[2]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[2]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@',\n",
       "  'ground_truth': '\\n\\nPrior works for mitigating social biases feature adversarial learning (@CITE[4]@; @CITE[1]@, counterfactual data augmentation (Zmigrod et al., 2019; or dataset balancing (@CITE[7]@, and more recently, using an interpretability-driven approach to uncover and controllably demote hidden biases (@CITE[8]@. Techniques for adapting to linguistic variants and mixed-language data include adversarial learning to pick up on key linguistic cues (@CITE[2]@, augmenting datasets with synthetic text (@CITE[6]@ or examples of variants that models underperform on (, discriminative learning (Gonen and @CITE[5]@, and transfer learning with morphological cues (@CITE[3]@.\\nReferences:ID: 1. Title: Adversarial Removal of Demographic Attributes from Text Data\\nID: 2. Title: Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints\\nID: 3. Title: Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates\\nID: 4. Title: Topics to Avoid: Demoting Latent Confounds in Text Classification\\nID: 5. Title: Code-Switched Language Models Using Neural Based Synthetic Data from Parallel Sentences\\nID: 6. Title: Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching\\nID: 7. Title: Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training\\nID: 8. Title: From English to Code-Switching: Transfer Learning with Strong Morphological Clues<|eot_id|>'},\n",
       " 3185: {'predict': 'Dialogue summarization has been studied in the past years. The SAMSum corpus ( @CITE[4]@ is a popular benchmark for abstractive dialogue summarization, which contains 8,000 conversations with summaries. MediaSum (@CITE[3]@ is a large-scale media interview dataset for dialogue summarization, which contains 463.6K transcripts with abstractive summaries. To address the challenges of dialogue summarization, researchers have proposed various techniques. @CITE[1]@ propose a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations. @CITE[6]@ propose a two-stage generation strategy that generates a preliminary summary sketch serving as the basis for the final summary. @CITE[2]@ show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator. @CITE[5]@ propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (\"who-doing-what\") in utterances through structured graphs to better encode conversations.\\nReferences:ID: 1. Title: SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\nID: 2. Title: MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization\\nID: 3. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 4. Title: Controllable Abstractive Dialogue Summarization with Sketch Supervision\\nID: 5. Title: Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization\\nID: 6. Title: Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs',\n",
       "  'ground_truth': '\\n\\nDialogue summarization aims to summarize the dialogue document into a shorter passage. AMI () and ICSI (Janin et al., 2003) are two early meeting corpora that both contain hundreds of samples. Recently, the SAMSum corpus (@CITE[1]@) is introduced, which contains over 16k chat conversations with human-labeled summaries. MediaSum (@CITE[3]@) collects about 464k interview transcripts and uses the overview descriptions as summaries. GupShup (@CITE[2]@) develops the first code-switched dialogue summarization dataset whose dialogues are in Hindi-English. Based on these datasets, a lot of work (@CITE[5]@; @CITE[4]@; @CITE[6]@; ; @CITE[6]@) models the conversation characteristics and achieves great performance. All these efforts are given to monolingual or code-switched scenarios while we focus on cross-lingual scenario in this paper.\\nReferences:ID: 1. Title: SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\nID: 2. Title: MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization\\nID: 3. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 4. Title: Controllable Abstractive Dialogue Summarization with Sketch Supervision\\nID: 5. Title: Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization\\nID: 6. Title: Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs<|eot_id|>'},\n",
       " 5092: {'predict': 'Human-AI collaboration has been studied in a variety of domains, including question answering (, recommendation systems (@CITE[6]@, medical diagnosis (, and human-computer dialogue (@CITE[5]@. In these domains, human-AI collaboration can be achieved by having the AI provide explanations for its predictions (, or by having the AI provide recommendations for the human to accept or reject (@CITE[6]@. In medical diagnosis, human-AI collaboration has been shown to improve performance by having the AI provide explanations for its predictions (, or by having the AI provide recommendations for the human to accept or reject (@CITE[6]@. In recommendation systems, human-AI collaboration has been shown to improve performance by having the AI provide explanations for its predictions (@CITE[6]@, or by having the AI provide recommendations for the human to accept or reject (@CITE[6]@. In human-computer dialogue, human-AI collaboration has been shown to improve performance by having the AI provide explanations for its predictions (, or by having the AI provide recommendations for the human to accept or reject (@CITE[5]@.\\nReferences:ID: 1. Title: Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork\\nID: 2. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 3. Title: The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering\\nID: 4. Title: How model accuracy and explanation fidelity influence user trust\\nID: 5. Title: Teaching Humans When To Defer to a Classifier via Exemplars\\nID: 6. Title: Causal Perception in Question-Answering Systems\\nID: 7. Title: The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering',\n",
       "  'ground_truth': '\\n\\nOptimizing for cooperation with humans is more productive than focusing solely on model performance (@CITE[1]@). Human-AI collaboration research has focused on AI systems explaining their predictions () or examining the relationship between trust and AI system’s accuracy (Rechkemmer and @CITE[3]@; ). Related to our work, @CITE[4]@; Bansal et al. (2021b); ; Papenmeier et al. (2022) examined the influence of explanations and found that inaccurate ones act as deceptive experiences which erode trust. @CITE[2]@; study the development of mental models which create further collaboration expectations. This mental model, or the associated expectations, can be violated, which results in degraded trust in the system and hindered collaboration (). The field of NLP offers several applications where trust plays a vital role, such as chatbots for various tasks or multi-domain question answering (@CITE[6]@; @CITE[7]@; ) and transparency and controllability are one of the key components that increase users’ trust @CITE[5]@;.\\nReferences:ID: 1. Title: Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork\\nID: 2. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 3. Title: The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering\\nID: 4. Title: How model accuracy and explanation fidelity influence user trust\\nID: 5. Title: Teaching Humans When To Defer to a Classifier via Exemplars\\nID: 6. Title: Causal Perception in Question-Answering Systems\\nID: 7. Title: The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering<|eot_id|>'},\n",
       " 7230: {'predict': 'Query-focused MDS is a challenging task as it requires the model to not only summarize the content of multiple documents but also focus on the query. @CITE[2]@ propose a sentence compression-based framework to query-focused MDS. They use a beam search decoder to find highly probable compressions. @CITE[1]@ present two QMDS training datasets, which are constructed using two data augmentation methods. They also introduce new hierarchical encoders that enable a more efficient encoding of the query together with multiple documents. @CITE[3]@ propose a neural network model that uses a dense retriever to retrieve relevant documents and a neural summarizer to generate the summary. They also propose a novel query-focused summarization metric, called QF-SUM. @CITE[4]@ propose a framework that uses a unified text-to-text transformer to model the query and documents. They also propose a novel query-focused summarization metric, called QF-RANK.\\nReferences:ID: 1. Title: A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization\\nID: 2. Title: Data Augmentation for Abstractive Query-Focused Multi-Document Summarization\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',\n",
       "  'ground_truth': '\\n\\nIn query-focused MDS (QMDS, @CITE[1]@; ; ; @CITE[4]@, a query is provided alongside a set of topic-related input documents and used to guide summarization. For example, extractive QMDS methods use query relevance to select the sentences that form the summary. No retrieval from a document index is performed. Here, we propose and investigate the more challenging scenario where, given only a query, the input documents must be retrieved from a large index containing mostly irrelevant documents. Previous attempts at open-domain MDS et al. (2018) proposed the WikiSum dataset. Given the title of a Wikipedia article and a collection of non-Wikipedia reference documents, the goal is to generate the first section of the article. The proposed extractive-abstractive approach resembles open-domain MDS. However, the document index is small (10s to 100s of documents) and composed of relevant documents (references of the article plus ten pages of search results using the title as query). We study the more challenging and more general setting where the index is large (»10,000 documents, see Table 6) and contains many more irrelevant documents than relevant documents. @CITE[2]@, a method similar to retrieve-then-summarize is proposed, using a pre-trained dense passage retriever (@CITE[3]@ and T5 ( as summarizer.4 The model is trained & evaluated on a dataset constructed from existing QMDS datasets. This dataset is small (∼90 training examples) and does not appear to be publicly available. Here, we conduct a large-scale analysis on multiple datasets from different domains (each consisting of thousands of examples) and evaluate several of the top-performing multi-document summarizers currently available (§4). We also extensively simulate document retrieval errors to probe their relative impact on summarization (§7). Together, this allows us to draw broader conclusions about open-domain MDS and provide detailed practical advice for future work.\\nReferences:ID: 1. Title: A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization\\nID: 2. Title: Data Augmentation for Abstractive Query-Focused Multi-Document Summarization\\nID: 3. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<|eot_id|>'},\n",
       " 1464: {'predict': 'Event coreference resolution has been studied extensively, and many approaches have been proposed to resolve events within and across documents. Within-document event coreference resolution has been studied extensively, with many approaches proposed for this task (@CITE[5]@; @CITE[2]@; ; @CITE[4]@; @CITE[1]@; ; @CITE[3]@; ; @CITE[3]@; @CITE[3]@; ;. However, cross-document event coreference resolution has received much less attention. @CITE[3]@ proposed a method to model the semantics of events and their components using RDF representations and compared these descriptions to solve a cross-document event coreference task. @CITE[3]@ presented a method to detect event descriptions in different news articles and to model the semantics of events and their components using RDF representations. @CITE[3]@ proposed a neural architecture for cross-document coreference resolution, which jointly models entity and event coreference. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a mention-level model with a sentence-level model. @CITE[3]@ proposed a method to solve cross-document event coreference by combining a predicate-argument structure-based model with a word-level representation-based model. @CITE[3]@',\n",
       "  'ground_truth': '\\n\\nMany CDCR algorithms use hand engineered event features to perform classification. Such systems have a low pairwise classification cost and therefore ignore the quadratic scaling and perform no pruning (@CITE[5]@; ; ; Bugert et al., 2020a). Other such systems choose to include document clustering to increase precision, which can be done with very little tradeoff for the ECB+ corpus (@CITE[1]@; @CITE[2]@. @CITE[4]@ explore an approach that avoids pairwise classification entirely, instead relying purely on representation learning and clustering within an embedding space. They propose a novel distance-based regularization term for their classifier that encourages representations that can be used for clustering. This approach is more scalable than pairwise classification approaches, but its performance lags behind the state-of-the-art as it cannot use pairwise information. Most recent systems use neural models for pairwise classification (; ; ; Zeng et al., 2020; @CITE[3]@; Caciularu et al., 2021). These algorithms each use document clustering, a pairwise neural classifier to construct distance matrices within each topic, and agglomerative clustering to compute the final clusters. Innovation has focused on the pairwise classification stage, with variants of document clustering as the only pruning option. Caciularu et al. (2021) sets the previous state of the art for both events and entities in ECB+ using a cross-document language model with a large context window to cross-encode and classify a pair of mentions with the full context of their documents.\\nReferences:ID: 1. Title: A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution\\nID: 2. Title: Identity and Granularity of Events in Text\\nID: 3. Title: Resolving Event Coreference with Supervised Representation Learning and Clustering-Oriented Regularization\\nID: 4. Title: Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution\\nID: 5. Title: Paraphrasing vs Coreferring: Two Sides of the Same Coin<|eot_id|>'},\n",
       " 2778: {'predict': 'Automatic evaluation of text generation has been extensively studied in the literature. Early works focus on the lexical similarity between the system output and the reference text (@CITE[6]@; @CITE[2]@. With the rise of deep learning, researchers propose to use contextualized representations to compare the system output with the reference text (@CITE[4]@; @CITE[3]@; @CITE[5]@. However, these methods still rely on lexical overlap between the system output and the reference text. @CITE[1]@ and @CITE[1]@ propose to use the distribution of the system output to measure the quality of the text generation model. The distribution can be estimated by the model’s prediction on the validation set. @CITE[1]@ propose to use the model’s prediction distribution to measure the quality of the text generation model. They use the model’s prediction distribution to measure the quality of the text generation model. However, these methods are not efficient and not robust to the reference text, and cannot be used to evaluate the text generation model on the test set. In this paper, we propose to use the model’s prediction distribution to evaluate the text generation model. We use the model’s prediction distribution to measure the quality of the text generation model. We use the model’s prediction distribution to measure the quality of the text generation model. We use the model’s prediction distribution to measure the quality of the text generation model.\\nReferences:ID: 1. Title: NLTK: The Natural Language Toolkit\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 4. Title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\\nID: 5. Title: MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers\\nID: 6. Title: Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nTraditional evaluation metrics include perplexity, and n-gram overlap metrics for translation-type problems such as BLEU (@CITE[5]@), ROUGE (@CITE[2]@), METEOR (), and NIST (@CITE[1]@. In recent years, with the emergence of neural models that learn contextual representations (; @CITE[3]@), researchers propose to project text to contextual representations and compute distance in this space (@CITE[6]@; @CITE[4]@; ). The closest work to ours is, which evaluates different decoding strategies in machine translation by comparing the statistics of the produced text. While these past works mainly concern word-level string/meaning representation matching, the goal of our work is to check the high-level aspects of the generated text such as coherence. Besides, word-level matching is not suitable for evaluating open-ended generation tasks due to the existence of too many plausible references (Celikyilmaz et al., 2020), while our work projects text to a more manageable lower-dimensional latent space to make the evaluation of open-ended generation feasible.\\nReferences:ID: 1. Title: NLTK: The Natural Language Toolkit\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 4. Title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\\nID: 5. Title: MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers\\nID: 6. Title: Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation<|eot_id|>'},\n",
       " 6149: {'predict': 'Prompt learning is a parameter-efficient approach to improve the performance of PLMs by learning task-specific prompts. AutoPrompt (@CITE[1]@) learns discrete prompts to elicit knowledge from PLMs. However, discrete prompts are hard to generalize across tasks and domains. Soft prompts, which are continuous vectors, can be optimized by gradient descent (@CITE[3]@; @CITE[5]@). Recently, many works have explored prompt transfer to improve the performance of PLMs (@CITE[4]@; @CITE[6]@; ; @CITE[2]@; ; @CITE[6]@). However, these works mainly focus on the zero-shot or few-shot setting, where only a few examples are available. In contrast, our work focuses on the few-shot setting, where more examples are available, and explores how to leverage the knowledge from XNLI to improve the performance of PLMs on other NLI tasks. Besides, our work focuses on the multilingual setting, which is more challenging than the monolingual setting.\\nReferences:ID: 1. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\\nID: 4. Title: Syntax-augmented Multilingual BERT for Cross-lingual Transfer\\nID: 5. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 6. Title: On Transferability of Prompt Tuning for Natural Language Processing',\n",
       "  'ground_truth': '\\n\\nRecently, prompt learning (@CITE[1]@; ; @CITE[6]@; ; @CITE[3]@; @CITE[5]@; ) has shown promising results in many NLP tasks under the few-shot setting. The key idea of prompt learning for XNLI is reformulating the text classification problem into a masked language modeling problem by constructing cloze-style questions. @CITE[2]@ propose a novel prompt-based transfer learning approach, which first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. @CITE[4]@ adopt separate soft prompts to learn embeddings enriched with domain knowledge. Schick and Schütze (2021a) explore discrete prompt learning to NLI with manually defined templates. Zhao and Schütze (2021) demonstrate that prompt learning outperforms fine-tuning for few-shot XNLI by simply applying soft, discrete, and mixed prompting with a uniform template in English. @CITE[2]@ proposed a discrete prompt learning framework that constructs an augmented sample by randomly sampling a template in another language. However, discrete prompts of human-designed template words require extensive external expert knowledge and are not flexible enough to adapt to different languages. In our work, we adopt trainable soft prompts to capture correspondence between different languages by comparing the augmented multilingual and original questions.\\nReferences:ID: 1. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\\nID: 4. Title: Syntax-augmented Multilingual BERT for Cross-lingual Transfer\\nID: 5. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 6. Title: On Transferability of Prompt Tuning for Natural Language Processing<|eot_id|>'},\n",
       " 725: {'predict': 'Contrastive learning has been successfully applied to various areas, including computer vision (@CITE[5]@; @CITE[7]@; @CITE[1]@; @CITE[2]@), natural language processing (@CITE[6]@; @CITE[11]@; @CITE[8]@; @CITE[3]@; @CITE[9]@), and graph neural networks (@CITE[4]@; @CITE[10]@; @CITE[12]@). In this paper, we apply contrastive learning to the pre-training of AMR parsers. Specifically, we design two contrastive learning tasks, i.e., entity-level contrastive learning and subgraph-level contrastive learning, to learn the representations of entities and subgraphs. The former task aims to learn the representations of entities and their types, while the latter task aims to learn the representations of subgraphs and their topological structures.\\nReferences:ID: 1. Title: Learning deep representations by mutual information estimation and maximization\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 4. Title: GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training\\nID: 5. Title: Graph Contrastive Learning with Augmentations\\nID: 6. Title: Improving AMR Parsing with Sequence-to-Sequence Pre-training\\nID: 7. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 8. Title: A Mutual Information Maximization Perspective of Language Representation Learning\\nID: 9. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 10. Title: QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization\\nID: 11. Title: Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning\\nID: 12. Title: Learning from Context or Names? An Empirical Study on Neural Relation Extraction',\n",
       "  'ground_truth': '\\n\\nContrastive learning was initiated by following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for self- supervised representation learning in various domains, such as computer vision (@CITE[4]@; Oord et al., 2018; @CITE[5]@; @CITE[10]@; @CITE[6]@ and graph (@CITE[12]@; ;. In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (@CITE[8]@, BERT (@CITE[11]@; @CITE[1]@ and ELECTRA (@CITE[7]@. Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (@CITE[11]@, discourse modeling (Iter et al., 2020), natural language inference (@CITE[3]@ and relation extraction (@CITE[2]@.\\nReferences:ID: 1. Title: Learning deep representations by mutual information estimation and maximization\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 4. Title: GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training\\nID: 5. Title: Graph Contrastive Learning with Augmentations\\nID: 6. Title: Improving AMR Parsing with Sequence-to-Sequence Pre-training\\nID: 7. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 8. Title: A Mutual Information Maximization Perspective of Language Representation Learning\\nID: 9. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\\nID: 10. Title: QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization\\nID: 11. Title: Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning\\nID: 12. Title: Learning from Context or Names? An Empirical Study on Neural Relation Extraction<|eot_id|>'},\n",
       " 2645: {'predict': 'Table-to-text generation aims to generate a natural language description for a given table, which is a fundamental task in natural language generation. Previous work mainly focuses on generating a surface-level description for a table (@CITE[2]@; @CITE[1]@; @CITE[3]@; @CITE[1]@. However, generating a logical-level description is more challenging due to the lack of logical-level datasets and the complexity of logical reasoning. Some datasets, such as Logic2Text (@CITE[1]@ and HiTab (@CITE[3]@, provide logical-level descriptions for tables. However, these datasets are mainly used for question answering and table reasoning, and the descriptions are not logical-level in the sense of logical reasoning. For example, Logic2Text contains descriptions in the form of logical forms, and HiTab contains descriptions in the form of natural language. Therefore, they are not suitable for logical-level table-to-text generation. To the best of our knowledge, we are the first to propose a logical-level table-to-text generation task and provide a large-scale dataset to support this task. Existing work mainly focuses on generating a natural language description for a given table. @CITE[3]@ propose a method to generate a natural language description for a given table based on a graph neural network. @CITE[1]@ propose a method to generate a natural language description for a given table based on a reinforcement learning framework. @CITE[2]@ propose a method to generate a natural language description for a given table based on a sequence-to-sequence framework. However, these methods are not suitable for logical-level table-to-text generation due to the lack of logical-level datasets and the complexity of logical reasoning.\\nReferences:ID: 1. Title: Logic2Text: High-Fidelity Natural Language Generation from Logical Forms\\nID: 2. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 3. Title: Logic-Consistency Text Generation from Semantic Parses',\n",
       "  'ground_truth': '\\n\\nEarly table-to-text generation tasks are limited to surface-level generation with little focus on logical inference (Lebret et al., 2016). LOGICNLG is the first dataset to focus on logical table-to-text generation, with Wikipedia tables and human-annotated logical descriptions. @CITE[3]@ proposed a de-confounded variational encoder-decoder model to encourage the model to generate non-surface-level predictions; however, the logical reasoning process is not explicitly considered, leading to low fidelity scores on human evaluation. @CITE[2]@ proposed to annotate logical forms to guide the generation and released a LOGIC2TEXT dataset. In this work, we focus on direct logical table-to-text generation without any explicit logical forms. Another related line of datasets are ToTTo ( and HiTab (, which incorporate highlighted cells to promote controllable generation. The CONTLOG dataset we propose is similar to the task settings of these datasets but differs in that we focus on logical-level generation. At the same time, only a small portion of examples in ToTTo and HiTab involve logical reasoning. ROTOWIRE (@CITE[1]@ and Numeric-NLG ( also involve numerical reasoning over table records, while they focus on document-level table summarization instead of sentence generation.\\nReferences:ID: 1. Title: Logic2Text: High-Fidelity Natural Language Generation from Logical Forms\\nID: 2. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 3. Title: Logic-Consistency Text Generation from Semantic Parses<|eot_id|>'},\n",
       " 55: {'predict': 'The GLUE benchmark (@CITE[5]@ is a collection of 9 NLU tasks. We focus on three tasks in GLUE: MNLI (@CITE[4]@, QQP (, and CoLA (@CITE[1]@. MNLI is a natural language inference task that aims to determine whether two sentences are related. QQP is a question answering task that requires identifying whether two given sentences are the same. CoLA is a task that requires determining whether a sentence is grammatically correct. We also consider two other NLU tasks: SQuAD (@CITE[3]@ and STS-B (@CITE[2]@. SQuAD is a question answering task that requires identifying the answer to a question given a passage. STS-B is a semantic textual similarity task that aims to measure the semantic similarity between two sentences.\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation\\nID: 3. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 4. Title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\\nID: 5. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 6. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "  'ground_truth': '\\n\\nDataset Train Metric Corpus of Linguistic Acceptability Judgements (@CITE[3]@ Matthews SST-2 The Stanford Sentiment Treebank ( accuracy Microsoft Research Paraphrase Corpus (@CITE[4]@ accuracy STS-B Semantic Textual Similarity Benchmark ( Pearson Quora Question Pairs1 (@CITE[1]@ accuracy The Multi-Genre NLI Corpus (matched) ( accuracy Question NLI (@CITE[2]@; @CITE[1]@ accuracy Recognizing Textual Entailment (Dagan et al., 2005; Haim et al., 2006; ; @CITE[6]@ accuracy Winograd NLI (@CITE[5]@ accuracy). Table 1: GLUE tasks (@CITE[1]@, dataset sizes and the metrics reported in this study.\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation\\nID: 3. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 4. Title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\\nID: 5. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 6. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding<|eot_id|>'},\n",
       " 6468: {'predict': 'Traditional methods for knowledge graph completion mainly fall into two categories: Translational models (@CITE[2]@; ; and non-translational models (; @CITE[4]@. Translational models consider the knowledge graph as a vector space and learn vector representations for entities and relations. The most popular translational models are TransE (@CITE[2]@, TransH (@CITE[3]@, and ConvE (@CITE[1]@. Non-translational models consider the knowledge graph as a graph and learn graph neural network representations for entities and relations. The most popular non-translational models are GCN (Kipf et al., 2016) and R-GCN (@CITE[4]@. These models are not applicable to KGs with non-binary relations and are not scalable to large KGs. Recently, some works have been proposed to solve KG completion tasks on KGs with non-binary relations. For example, RGCN (@CITE[4]@ and RGCN+ (@CITE[3]@ are designed to learn relation-aware representations for entities and relations. However, these models are not applicable to KGs with textual information. Therefore, these models are not applicable to our task.\\nReferences:ID: 1. Title: Convolutional 2D Knowledge Graph Embeddings\\nID: 2. Title: End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion\\nID: 3. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 4. Title: A Survey on Knowledge Graphs: Representation, Acquisition and Applications',\n",
       "  'ground_truth': '\\n\\nMany knowledge graph completion methods have been proposed, which can be classified into three types: embedding-based completion methods, path-finding-based completion methods, and logical rule-based completion methods. The embedding-based method learned the relation and node embedding by shortening the distance between the head-relation pair representation and the tail node representation, which had good scalability. Moreover, convolution has been proven to be an effective operation for acquiring the head-relation representation (@CITE[1]@; in the embedding-based method. The path-based methods used the random walk inference based algorithm to find the related path. They achieved the prediction of missing tuples by comparing the related path with the relation to be predicted (; @CITE[4]@). The rule-based methods utilized the induction rules to simplify the path-finding process in the knowledge graphs (; @CITE[3]@). We refer the reader to (@CITE[2]@ for more details about knowledge graph completion.\\nReferences:ID: 1. Title: Convolutional 2D Knowledge Graph Embeddings\\nID: 2. Title: End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion\\nID: 3. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 4. Title: A Survey on Knowledge Graphs: Representation, Acquisition and Applications<|eot_id|>'},\n",
       " 7194: {'predict': 'Adapter-based models are a type of fine-tuning-based methods that inject knowledge into pre-trained language models (PLMs) by adding a lightweight adapter module on top of the PLM, which can be trained with a small number of parameters. The adapter is usually composed of a feed-forward network (FFN) with learnable parameters, and is inserted between the encoder layers of the PLM. When processing an input sequence, the adapter can modify the hidden states of the PLM and thus enable the PLM to learn new knowledge. Adapter-based models have been shown to be effective in various applications, such as knowledge representation learning (, knowledge graph completion (@CITE[4]@;, and question answering ( and entity typing (@CITE[3]@. In this paper, we propose to use adapter-based models to improve the performance of the KGPT model. Our adapter-based model is trained with a small number of parameters, and can be easily combined with the KGPT model to improve its performance. Moreover, our adapter-based model can be used as a plug-in for the KGPT model, which is easy to be integrated with other models. The performance of our adapter-based model is comparable to the performance of the KGPT model, and is superior to that of the KGPT model in some tasks.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: Integrating Graph Contextualized Knowledge into Pre-trained Language Models\\nID: 3. Title: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model\\nID: 4. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters',\n",
       "  'ground_truth': '\\n\\nPLMs, pre-trained on large-scale text corpus with unsupervised objectives, have established state-of-the-art performances on various NLP downstream tasks. Many studies fine-tune PLMs with language modeling and downstream task objectives to obtain better performance (@CITE[2]@; @CITE[3]@; ; @CITE[4]@. To leverage the powerful PLMs more efficiently, add adapter layers, small neural networks, into each transformer layer to obtain near state-of-the-art performance on the GLUE benchmark while updating only the parameters of adapters. Inspired by this work, more adapter-based models (@CITE[1]@; Liu et al., 2021; @CITE[1]@ are proposed to inject task-specific knowledge into PLMs with adapters. Inspired by the adapter architecture, we propose Adapter-TST, which trains different neural adapters to capture different stylistic attributes to perform the multiple-attribute TST. The proposed adapter framework has two configurations that support multiple stylistic attribute outputs and compositional editing.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: Integrating Graph Contextualized Knowledge into Pre-trained Language Models\\nID: 3. Title: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model\\nID: 4. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters<|eot_id|>'},\n",
       " 238: {'predict': 'Community question answering (CQA) is a popular web service where users post questions and others answer them. The goal of CQA is to rank answers to a given question. Traditional CQA datasets are collected from question answering websites like Yahoo! Answers (@CITE[3]@), Wiki Answers (@CITE[6]@), and Stack Exchange (@CITE[1]@). In recent years, many datasets are collected from e-commerce platforms like Alibaba (, Amazon (@CITE[4]@), and Yelp (@CITE[5]@). The CQA tasks are usually framed as a ranking task, where the goal is to rank the most relevant answer to a given question. In this paper, we focus on the task of fact-checking CQA, which aims to determine whether a given answer to a question is true or not. This task has been studied in the past in several datasets (@CITE[2]@; @CITE[1]@; @CITE[5]@; @CITE[6]@).\\nReferences:ID: 1. Title: Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture\\nID: 2. Title: Adversarial Training for Community Question Answer Selection Based on Multi-scale Matching\\nID: 3. Title: Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering\\nID: 4. Title: Answer Ranking for Product-Related Questions via Multiple Semantic Relations Modeling\\nID: 5. Title: Fact Checking in Community Forums\\nID: 6. Title: SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums',\n",
       "  'ground_truth': '\\n\\nExisting methods in community question answering (CQA) mainly focus on the answer selection task (@CITE[5]@; @CITE[3]@; @CITE[2]@;, where an answer is considered to be positive if it is semantically relevant to the question regardless of its veracity. Some studies further measure the quality of answers, trying to predict the answer helpfulness in PQA platforms (Zhang et al., 2020b) or ranking all available answers for a given question (@CITE[1]@. One closely related work in the CQA context is a recent attempt of investigating the fact checking task in QA settings (@CITE[4]@, which was later adopted as the SemEval-2019 Task 8 (@CITE[6]@. Its goal is to classify an answer in the Qatar forum1 into true, false or non-factual. However, only QA pairs are given in this shared task to predict the answer veracity, making it less practical due to the lack of evidence sources. Moreover, the small number of training data consisting only 495 QA pairs restricts the possibility of trying some powerful machine learning models such as deep neural networks. 1http://www.qatarliving.com As pointed out in @CITE[6]@, verifying the verdict of answers in CQA requires using rich world knowledge. However, gathering relevant information as evidence can be difficult due to the open-domain nature of those questions. Compared with general CQA forums, PQA provides product-specific forums, making the evidence collection process more realistic and controllable. Also, as will be described in Section 3, the high proportion of factual type QA pairs also makes it suitable for studying the fact checking problem on PQA.\\nReferences:ID: 1. Title: Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture\\nID: 2. Title: Adversarial Training for Community Question Answer Selection Based on Multi-scale Matching\\nID: 3. Title: Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering\\nID: 4. Title: Answer Ranking for Product-Related Questions via Multiple Semantic Relations Modeling\\nID: 5. Title: Fact Checking in Community Forums\\nID: 6. Title: SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums<|eot_id|>'},\n",
       " 7032: {'predict': 'Self-adaption for Extractive QAs. As shown in Figure 2, a QA model is expected to learn the question-answer relation from the training data. However, when encountering a new language, the model has to rely on the pre-trained knowledge and cannot learn the language-specific relation. To address this issue, we propose a self-adaption mechanism to adjust the model’s parameters based on the language information. In this work, we use the question-answer pair as the language information. The idea is inspired by the recent work on synthetic data generation for domain adaptation of question answering systems (@CITE[2]@. The synthetic data is generated by a decoder that is trained on a large-scale corpus. Similarly, we use the decoder to generate synthetic data for each question-answer pair. The decoder is trained to generate a question-answer pair from a passage. The generated data is used as the language information to adjust the model’s parameters. The language information can also be used to adjust the model’s parameters in the inference stage. In this work, we propose to use the language information to adjust the model’s parameters in the inference stage. The idea is inspired by the recent work on efficient test-time adapter ensembling for low-resource language varieties (@CITE[1]@. The idea is to ensemble the model parameters from multiple languages to make the model more robust to the unseen language. We use the language information to adjust the model’s parameters to make the model more robust to the unseen language. The model parameters are adjusted by a language adapter. The language adapter is trained on a large-scale corpus. The language adapter is used to adjust the model’s parameters by minimizing the entropy of the predictions.\\nReferences:ID: 1. Title: TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages\\nID: 2. Title: End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems\\nID: 3. Title: Efficient Test Time Adapter Ensembling for Low-resource Language Varieties',\n",
       "  'ground_truth': '\\n\\nSeveral previous studies explored the self-adaptive capabilities of the traditional pre-trained language models, but they mainly addressed classification problems under the extractive setting (@CITE[3]@; ; ; @CITE[1]@; @CITE[2]@. However, we focus on the generative setting, which makes a large difference due to fundamentally different objectives. To be specific, in an extractive setting, self-adaptation is based on probabilities, while in a generative setting, self-adaptation is done using generated text. In situations where filtering is further applied, filtering is based on probabilities in the extractive setting, whereas, it is done using the generated text in the generative setting. Furthermore, Li et al. (2020) and assume an unsupervised QA setting, where the context-question-answer triplets are not available, thus requiring an additional query-generation module. Such a pair generation approach is different and orthogonal to ours, since we aim to enhance answer generation directly from the provided context and question.\\nReferences:ID: 1. Title: TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages\\nID: 2. Title: End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems\\nID: 3. Title: Efficient Test Time Adapter Ensembling for Low-resource Language Varieties<|eot_id|>'},\n",
       " 3690: {'predict': \"PLM analysis on attention mechanism and embeddings has been a popular topic in recent years. The majority of studies focus on understanding the attention mechanism in PLMs. For example, @CITE[1]@; @CITE[6]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[3]@ analyze the attention patterns in PLMs and find that attention heads are specialized to target different linguistic features, such as syntax and semantics. @CITE[5]@; @CITE[3]@ analyze the attention patterns across different layers and find that different layers are specialized to capture different linguistic features. On the other hand, some studies focus on understanding the embeddings in PLMs. For example, @CITE[4]@; @CITE[3]@ analyze the concept representations in PLMs and find that PLMs can learn novel concepts that do not strictly adhere to any pre-defined categorization. @CITE[1]@; @CITE[2]@ analyze the linguistic properties of embeddings in PLMs and find that embeddings in PLMs contain rich linguistic information, such as syntax and semantics.\\nReferences:ID: 1. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\\nID: 2. Title: Analyzing the Structure of Attention in a Transformer Language Model\\nID: 3. Title: Editing Factual Knowledge in Language Models\\nID: 4. Title: What Does BERT Look At? An Analysis of BERT's Attention\\nID: 5. Title: Discovering Latent Concepts Learned in BERT\\nID: 6. Title: Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space\",\n",
       "  'ground_truth': \"\\n\\nPLMs have been largely studied in the last years, with most analysis focusing on the attention mechanism (@CITE[6]@; @CITE[4]@ and on the role of embeddings (Rogers et al., 2020; @CITE[3]@; @CITE[1]@. However, none of those efforts study the notion of types that we introduce. One exception is the recent studies of how concepts are encoded in PLMs. One work analyzes BERT by clustering contextual representations across layers, followed by a manual annotation to label clusters with meaningful concepts (@CITE[2]@. Another work starts from treating the feedforward network of a transformer as a key-value memory and studies how certain vectors encode concepts in the vocabulary space (@CITE[5]@. Our effort is different in two ways. First, we do not require the labeling of artifacts from the PLM, but rather we rely on user-specified tokens to model their common type. Second, we focus on type, which is one semantic concept, leaving the others, such as syntactic, morphological, and lexical to future work (Section 6).\\nReferences:ID: 1. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\\nID: 2. Title: Analyzing the Structure of Attention in a Transformer Language Model\\nID: 3. Title: Editing Factual Knowledge in Language Models\\nID: 4. Title: What Does BERT Look At? An Analysis of BERT's Attention\\nID: 5. Title: Discovering Latent Concepts Learned in BERT\\nID: 6. Title: Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space<|eot_id|>\"},\n",
       " 6493: {'predict': 'Aspect Category Sentiment Analysis (ACSA) aims to predict the sentiment polarity of a sentence towards a given aspect category. Most existing methods focus on learning aspect category-specific representations of the input sentence and then predict the sentiment polarity towards the aspect category. Previous works (@CITE[4]@; @CITE[6]@; @CITE[5]@; @CITE[1]@; @CITE[3]@; @CITE[2]@; @CITE[7]@; @CITE[1]@; ; @CITE[7]@; @CITE[2]@) can be divided into two categories: (1) pipeline methods, which first detect the aspect categories and then predict the sentiment polarity towards the detected aspect categories; (2) joint methods, which jointly detect the aspect categories and predict the sentiment polarity towards the aspect categories. The joint methods can achieve better performance than pipeline methods. However, the joint methods still suffer from the problem of data deficiency of some aspect categories. To alleviate this problem, @CITE[1]@ propose a novel joint model which contains a shared sentiment prediction layer. The shared sentiment prediction layer transfers the sentiment knowledge between aspect categories and alleviates the problem caused by data deficiency. However, this method cannot be used for ACSA with more than two aspect categories, because it is difficult to design a shared sentiment prediction layer for more than two aspect categories. In this paper, we propose a novel joint model which can be easily extended to ACSA with more than two aspect categories. The proposed model can achieve better performance than the pipeline and joint methods.\\nReferences:ID: 1. Title: Aspect Based Sentiment Analysis with Gated Convolutional Networks\\nID: 2. Title: A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis\\nID: 3. Title: Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis\\nID: 4. Title: Solving Aspect Category Sentiment Analysis as a Text Generation Task\\nID: 5. Title: Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer',\n",
       "  'ground_truth': '\\n\\nNumerous attempts have been made to improve ACSA. propose an LSTM-based model combined with an attention mechanism to attend for suitable words with given aspects. Ruder et al. (2016) capture inter-sentence relations within a review using a hierarchical bidirectional LSTM model. @CITE[3]@ extract features using CNN and output features related to the categories of aspects by using a gated mechanism. Xing et al. (2019), and @CITE[6]@ incorporate aspect category information into a sentence decoder for generating representations specific to both the aspect and its context. Sun et al. (2019) construct auxiliary sentences from aspects for performing ACSA as a sentence-pair classification task. propose a new capsule network which captures relationship between the aspects and the contexts. @CITE[4]@ aggregate the sentiments of the words that indicate an aspect category, so as to predict the sentiment of this category. @CITE[1]@ use a template-based method to perform ACSA as a generation task; this can leverage the knowledge of a pre-trained language model. @CITE[2]@ use additional syntactic information to enhance the sentiment features. utilize commonsense knowledge graph and data augmentation to overcome the shortage of training data. To avoid error propagation, joint models which perform ACSA and ACD simultaneously have been proposed. @CITE[5]@ propose two models with LSTM and CNN, which output an aspect category and its corresponding polarity at the same time. @CITE[7]@ apply orthogonal and sparseness constraints on attention weights. Wang et al. (2019) design an AS-Capsules model to explore the correlation of aspects with sentiments through shared modules. propose a joint model with a shared sentiment prediction layer.\\nReferences:ID: 1. Title: Aspect Based Sentiment Analysis with Gated Convolutional Networks\\nID: 2. Title: A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis\\nID: 3. Title: Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis\\nID: 4. Title: Solving Aspect Category Sentiment Analysis as a Text Generation Task\\nID: 5. Title: Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer<|eot_id|>'},\n",
       " 3467: {'predict': 'Pretraining has been used to improve entity representations in many NLP tasks. For biomedical entity linking, many of the current approaches use pretraining to learn a better representation of the entities. The entity linking task is often framed as a classification problem, where the goal is to predict the entity label given a mention. Pretraining is done using a masked language model (MLM) objective, where some of the tokens in the input are replaced with a [MASK] token and the model is trained to predict the original token. @CITE[1]@ propose a pretraining scheme called BioSyn that uses a synonym marginalization objective. They train the model to predict the synonym of a mention from a set of candidate mentions. @CITE[4]@ propose a pretraining scheme called SapBERT that self-aligns the representation space of biomedical entities. They train the model to predict the similarity between two mentions using a metric learning framework. @CITE[3]@ propose a pretraining scheme that uses a contrastive learning framework to learn a better representation of the entities. They train the model to predict whether two mentions are the same or not. @CITE[5]@ propose a pretraining scheme that uses a contrastive learning framework to learn a better representation of the entities. They train the model to predict whether two mentions are the same or not. @CITE[2]@ propose a pretraining scheme that uses a contrastive learning framework to learn a better representation of the entities. They train the model to predict whether two mentions are the same or not. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity. They propose a method to improve the performance of entity linking models by using the contextual information of the mention and the entity.\\nReferences:ID: 1. Title: Biomedical Entity Representations with Synonym Marginalization\\nID: 2. Title: Self-Alignment Pretraining for Biomedical Entity Representations\\nID: 3. Title: BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks\\nID: 4. Title: Knowledge-Rich Self-Supervision for Biomedical Entity Linking\\nID: 5. Title: Self-Alignment Pretraining for Biomedical Entity Representations',\n",
       "  'ground_truth': '\\n\\nMany recent researches use synonyms to conduct biomedical entity representation learning (@CITE[5]@; @CITE[4]@; @CITE[1]@; Angell et al., 2021; @CITE[3]@; Kong et al., 1https://github.com/whaleloops/KEPT 2021; @CITE[2]@. Our work is most similar to @CITE[4]@, which uses additional pretraining scheme that self-aligns the representation space of biomedical entities from pretrained medical LM. They collect self-supervised synonym examples from the biomedical ontology UMLS, and use multi-similarity contrastive loss to keep the representation of similar entities closer to each other, before fine-tuning them to the downstream specific task. However, their work differs from ours in (1) their testing being limited to only medical entity linking tasks and (2) not using hierarchical information, which has been shown to be useful in KRISSBERT (@CITE[3]@. In contrast to KRISSBERT, our contrastive learning selects negative samples from siblings (1-hop nodes) instead of random nodes in the graph. Our method follows InfoMin proposition that selected samples should contain as much task-relevant information while discarding as much irrelevant information in the input as possible (.\\nReferences:ID: 1. Title: Biomedical Entity Representations with Synonym Marginalization\\nID: 2. Title: Self-Alignment Pretraining for Biomedical Entity Representations\\nID: 3. Title: BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks\\nID: 4. Title: Knowledge-Rich Self-Supervision for Biomedical Entity Linking\\nID: 5. Title: Self-Alignment Pretraining for Biomedical Entity Representations<|eot_id|>'},\n",
       " 1940: {'predict': 'Recent work has demonstrated that deep learning models are not systematically compositional, i.e., they do not generalize to novel compositions of familiar concepts (@CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[3]@). These studies have been conducted on models trained in a supervised manner, and have typically used a single task to test for compositional generalization. In contrast, we consider a scenario where the model has been trained on a variety of tasks, and must generalize to novel compositions of familiar concepts. Our approach is related to the recent work on meta-learning (@CITE[3]@), which has been used to improve the compositional generalization abilities of deep learning models. However, meta-learning is not directly applicable to our scenario, as we are interested in a model that has been trained on a variety of tasks, rather than a model that has been trained on a single task that requires compositional generalization.\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Compositional generalization through meta sequence-to-sequence learning\\nID: 3. Title: Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks',\n",
       "  'ground_truth': '\\n\\nPrior work has demonstrated that neural networks do not possess systematicity (@CITE[3]@; @CITE[2]@; ; @CITE[1]@, a property where the capacity to learn certain concepts implies the capacity to learn novel, structurally-related concepts. @CITE[2]@ empirically investigates novel concept learning on the SCAN dataset. They consider models that have not seen the concept jump during training, but only in isolation, where jump is represented by the program JUMP, whose denotation is also JUMP. At test time, the models have to correctly predict the output sequence for jump in various contexts, e.g., jump and look. Models are expected to be able to do so since they have seen walk and look and run and look during training, and are expected to extrapolate these examples to jump. They ﬁnd that several sequence-to-sequence models perform very poorly on this task. However, the kind of concepts they use to assess generalization are very limited. For instance, in SCAN, the jump primitive is an independent concept with no relation to the existing concepts such as walk, look, run, after, twice, around. In contrast, we consider a more general notion of a novel concept as a program that may be composed of existing concepts, and compare the ability of different models as well as our approach to learn such concepts from few examples.\\nReferences:ID: 1. Title: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks\\nID: 2. Title: Compositional generalization through meta sequence-to-sequence learning\\nID: 3. Title: Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks<|eot_id|>'},\n",
       " 5506: {'predict': 'Traditionally, index-based document retrieval methods use a bag-of-words representation to represent queries and documents. The term weights are usually learned from a pre-trained language model (@CITE[4]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[6]@. In contrast, dense representation-based retrieval methods use dense vectors as the representation of queries and documents (@CITE[1]@; @CITE[6]@; @CITE[1]@; @CITE[2]@. The dense vectors are usually learned from a pre-trained language model through a dual-encoder framework (@CITE[1]@. In this paper, we focus on the dense representation-based retrieval methods.\\nReferences:ID: 1. Title: Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval\\nID: 2. Title: SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval\\nID: 3. Title: SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval\\nID: 4. Title: Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\\nID: 5. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 6. Title: Optimizing Dense Retrieval Model Training with Hard Negatives',\n",
       "  'ground_truth': '\\n\\nTraditionally, index-based document retrieval methods encompass both sparse and dense retrieval techniques. Sparse retrieval methods (@CITE[1]@, 2020; @CITE[4]@;, such as BM25 (, utilize inverted indexes to assess term importance and calculate matching scores between queries and documents. Dense retrieval methods have been employed to overcome the limitations of word mismatching (. These methods convert queries and documents into dense vectors and employ ANNS (Jégou et al., 2011) for efficient vector-based search. The dual encoder approach incorporating pre-trained language models has shown improved performance in dense retrieval. Techniques like hard negative sampling (@CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[6]@ further enhance effectiveness.\\nReferences:ID: 1. Title: Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval\\nID: 2. Title: SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval\\nID: 3. Title: SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval\\nID: 4. Title: Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\\nID: 5. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 6. Title: Optimizing Dense Retrieval Model Training with Hard Negatives<|eot_id|>'},\n",
       " 5924: {'predict': 'Optimal Transport (OT) is a well-established tool for comparing distributions and has been used in various applications (@CITE[1]@; @CITE[8]@; @CITE[2]@; ; @CITE[4]@; @CITE[5]@; @CITE[6]@; @CITE[3]@; @CITE[7]@; @CITE[9]@; ;. The idea is to find the optimal mapping between two distributions in order to minimize the distance between them. In the context of embeddings, OT has been used to improve the quality of word embeddings by aligning different word embeddings (@CITE[1]@; ;. It has also been used to perform semi-supervised learning by aligning labeled and unlabeled data (@CITE[8]@; @CITE[2]@; ; @CITE[4]@; @CITE[5]@;. Recently, @CITE[9]@ has used OT for semi-supervised learning in the context of knowledge graphs. In this work, we use OT for the purpose of aligning different data distributions, which can be used for semi-supervised learning, active learning, or data augmentation.\\nReferences:ID: 1. Title: Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction\\nID: 2. Title: Vocabulary Learning via Optimal Transport for Neural Machine Translation\\nID: 3. Title: Complex Query Answering with Neural Link Predictors\\nID: 4. Title: Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries\\nID: 5. Title: Fast Sinkhorn I: An O(N) algorithm for the Wasserstein-1 metric\\nID: 6. Title: Query2Particles: Knowledge Graph Reasoning with Particle Embeddings\\nID: 7. Title: SEAL: Simultaneous Label Hierarchy Exploration And Learning\\nID: 8. Title: Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions\\nID: 9. Title: Learning Embeddings into Entropic Wasserstein Spaces',\n",
       "  'ground_truth': '\\n\\nOptimal transport (OT) (Peyré et al., 2019) introduces the power metric between probabilistic distributions and facilitates many applications in language and graph data (@CITE[8]@; @CITE[3]@; @CITE[5]@; ; @CITE[1]@; @CITE[6]@; @CITE[4]@; @CITE[9]@). It is particularly efficient when embedding graph vertices and words as probabilistic distributions in the Wasserstein space (@CITE[2]@; ). The Wasserstein-Fisher-Rao (WFR) metric (@CITE[7]@) generalizes the OT between distributions to the measures by balancing local comparison and global transport with a transport radius η. Existing investigations (Zhao et al., 2020b) demonstrated that the WFR metric is a robust and effective measurement for embedding alignment. Previous work measures pretrained embeddings in the WFR space (), while this work is the first to learn embeddings in the WFR space. Moreover, we validate the advantage of WFR space in the context of query embedding.\\nReferences:ID: 1. Title: Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction\\nID: 2. Title: Vocabulary Learning via Optimal Transport for Neural Machine Translation\\nID: 3. Title: Complex Query Answering with Neural Link Predictors\\nID: 4. Title: Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries\\nID: 5. Title: Fast Sinkhorn I: An O(N) algorithm for the Wasserstein-1 metric\\nID: 6. Title: Query2Particles: Knowledge Graph Reasoning with Particle Embeddings\\nID: 7. Title: SEAL: Simultaneous Label Hierarchy Exploration And Learning\\nID: 8. Title: Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions\\nID: 9. Title: Learning Embeddings into Entropic Wasserstein Spaces<|eot_id|>'},\n",
       " 2604: {'predict': \"Text summarization is a long-standing problem in natural language processing, which has been widely studied in the past few decades. The existing methods for text summarization can be divided into extractive summarization and abstractive summarization. The extractive summarization methods extract the most important sentences from the original text to form a summary, which can be further divided into unsupervised methods (@CITE[1]@) and supervised methods (@CITE[4]@; ; @CITE[3]@). However, extractive summarization methods may not be able to generate coherent summaries, which can be addressed by introducing reinforcement learning (@CITE[2]@; @CITE[5]@). The abstractive summarization methods generate the summary in a word-to-word fashion, which can be further divided into sequence-to-sequence methods (@CITE[5]@) and sequence-to-sequence methods with reinforcement learning (@CITE[5]@; @CITE[5]@; @CITE[5]@).\\nReferences:ID: 1. Title: Searching for Effective Neural Extractive Summarization: What Works and What's Next\\nID: 2. Title: Learning to Extract Coherent Summary via Deep Reinforcement Learning\\nID: 3. Title: Generative Adversarial Network for Abstractive Text Summarization\\nID: 4. Title: Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\\nID: 5. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\",\n",
       "  'ground_truth': \"\\n\\nTechnically, Semantic Overlap Summarization can be viewed as a multi-document summarization task, i.e., multi-seq-to-seq task, with an extra commonality constraint (). Over the past two decades, many document summarizing approaches have been investigated (). The two most popular among them are extractive approaches (Cao et al., 2018; Narayan et al., 2018; ; @CITE[1]@) and abstractive approaches (@CITE[2]@; @CITE[4]@; ). Some researchers have also tried combining extractive and abstractive approaches (@CITE[5]@; ; @CITE[3]@).\\nReferences:ID: 1. Title: Searching for Effective Neural Extractive Summarization: What Works and What's Next\\nID: 2. Title: Learning to Extract Coherent Summary via Deep Reinforcement Learning\\nID: 3. Title: Generative Adversarial Network for Abstractive Text Summarization\\nID: 4. Title: Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\\nID: 5. Title: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting<|eot_id|>\"},\n",
       " 6650: {'predict': 'DSRE aims to extract relational facts from text data, which is widely used in many downstream tasks such as knowledge graph construction and question answering. Most existing DSRE methods focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels (@CITE[8]@; @CITE[6]@; @CITE[10]@; @CITE[5]@; @CITE[11]@; ; ; ; @CITE[2]@; @CITE[3]@; @CITE[7]@; @CITE[1]@; @CITE[4]@; @CITE[9]@; @CITE[12]@; ; @CITE[12]@; @CITE[12]@. The majority of existing DSRE methods are designed to extract relational facts from text data, which is widely used in many downstream tasks such as knowledge graph construction and question answering. Most existing DSRE methods focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels (@CITE[8]@; @CITE[6]@; @CITE[10]@; @CITE[5]@; @CITE[11]@; @CITE[1]@; @CITE[3]@; @CITE[7]@; @CITE[2]@; @CITE[9]@; @CITE[4]@; @CITE[12]@; @CITE[12]@; @CITE[12]@.\\nReferences:ID: 1. Title: HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction\\nID: 2. Title: Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction\\nID: 3. Title: Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction\\nID: 4. Title: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions\\nID: 5. Title: CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\\nID: 6. Title: Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction\\nID: 7. Title: Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning\\nID: 8. Title: H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction\\nID: 9. Title: Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training\\nID: 10. Title: Reinforcement Learning for Relation Classification from Noisy Data\\nID: 11. Title: Deep Self-Learning From Noisy Labels\\nID: 12. Title: Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction',\n",
       "  'ground_truth': '\\n\\nRelation extraction (RE) is a fundamental process for constructing knowledge graphs (; @CITE[5]@; Zhang et al., 2023b). To generate large-scale auto-labeled data without human effort, (@CITE[2]@ first use Distant Supervision to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To tackle the predicament of noise, most of the existing studies on DSRE are founded on the multi-instance learning framework. This approach is leveraged to handle noisy sentences in each bag and train models by capitalizing on the constructed trustworthy bag-level representations. Usually, these methods employ attention mechanisms to assign less weights to the probable noisy sentences in the bag (@CITE[10]@; @CITE[4]@; @CITE[6]@; @CITE[1]@; ; @CITE[8]@; @CITE[9]@, apply adversarial training or reinforcement learning to remove the noisy sentences from the bag (Zeng et al., 2015; @CITE[12]@; @CITE[7]@; ; @CITE[3]@. However, the studies (; @CITE[11]@; ; indicate that the bag-level DSRE methods are ineffective for sentence-level relation extraction. This work focuses on extracting relations at the sentence level, ( applied reinforcement learning to identify confident instances based on the reward of noisy labels. (@CITE[11]@ involve building initial reliable sentences based on several manually defined frequent relation patterns. ( assigning complementary labels that cooperate with negative training to filter out noisy instances. Unlike previous studies, our method only discard noisy labels and keep the unlabeled samples. We use pseudo labeling to effectively utilize unlabeled samples, which helps to learn better representation.\\nReferences:ID: 1. Title: HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction\\nID: 2. Title: Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction\\nID: 3. Title: Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction\\nID: 4. Title: Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions\\nID: 5. Title: CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\\nID: 6. Title: Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction\\nID: 7. Title: Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning\\nID: 8. Title: H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction\\nID: 9. Title: Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training\\nID: 10. Title: Reinforcement Learning for Relation Classification from Noisy Data\\nID: 11. Title: Deep Self-Learning From Noisy Labels\\nID: 12. Title: Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction<|eot_id|>'},\n",
       " 3330: {'predict': 'Our work is related to the line of work that focuses on diversity in text generation (@CITE[7]@; @CITE[6]@; @CITE[5]@; ; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[2]@. In contrast to our work, existing methods mostly focus on generating multiple responses for a given input. In this work, we aim to generate multiple responses for a given input with different reasoning patterns. The work of @CITE[3]@ is the most related to our work. They propose to generate multiple responses with different reasoning patterns for a given input by using a conditional language model. However, their work is limited to generating only one or two responses for each input. Our work is more comprehensive as we generate multiple responses with different reasoning patterns for each input. Our work also differs from theirs in that we focus on generating commonsense responses rather than generating multiple responses with different reasoning patterns.\\nReferences:ID: 1. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 2. Title: A Deep Generative Framework for Paraphrase Generation\\nID: 3. Title: MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations\\nID: 4. Title: Injecting Entity Types into Entity-Guided Text Generation\\nID: 5. Title: Target Conditioning for One-to-Many Generation\\nID: 6. Title: Abductive Commonsense Reasoning\\nID: 7. Title: A Deep Generative Framework for Paraphrase Generation',\n",
       "  'ground_truth': '\\n\\nGenerating multiple valid outputs given a source sequence has a wide range of applications, such as machine translation (@CITE[5]@, paraphrase generation (@CITE[7]@, question generation (Cho et al., 2019), dialogue system (@CITE[5]@, and story generation (@CITE[6]@. For example, in machine translation, there are often many plausible and semantically equivalent translations due to information asymmetry between different languages (@CITE[5]@. Methods of improving diversity in NLG have been explored from various perspectives. Sampling-based decoding is one of the most effective solutions to improve diversity. For example, nucleus sampling (@CITE[6]@ samples next tokens from the dynamic nucleus of tokens containing the vast majority of the probability mass, instead of decoding text by maximizing the likelihood. Another line of work focused on introducing random noise (@CITE[7]@ or changing latent variables (@CITE[5]@ to produce uncertainty. In addition, @CITE[5]@ adopted a mixture of experts to diversify machine translation, where a minimum-loss predictor is assigned to each source input. @CITE[7]@ employed an inverse reinforcement learning approach for unconditional diverse text generation. However, no existing work considered performing diverse knowledge reasoning to generate multiple reasonable outputs of different contents.\\nReferences:ID: 1. Title: Mixture Models for Diverse Machine Translation: Tricks of the Trade\\nID: 2. Title: A Deep Generative Framework for Paraphrase Generation\\nID: 3. Title: MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations\\nID: 4. Title: Injecting Entity Types into Entity-Guided Text Generation\\nID: 5. Title: Target Conditioning for One-to-Many Generation\\nID: 6. Title: Abductive Commonsense Reasoning\\nID: 7. Title: A Deep Generative Framework for Paraphrase Generation<|eot_id|>'},\n",
       " 4350: {'predict': 'MWP solvers have been proposed to solve MWP problems. These solvers can be categorized into two types: task-specific models and general-purpose models. Task-specific models are designed to solve MWP problems and are trained on MWP datasets. For example, MWP-BERT (@CITE[5]@) is trained on MWP dataset and uses a contextualized representation learning schema to solve MWP problems. MWP-BERT is a kind of task-specific model, and the model is trained on MWP dataset. General-purpose models are trained on general text and can be used to solve MWP problems. For example, GenBERT (@CITE[1]@) is a general-purpose model trained on DROP dataset and can be used to solve MWP problems. In addition, MWP solvers can also be categorized into two types: symbolic solvers and neural solvers. Symbolic solvers are based on symbolic reasoning, such as equation-based solvers (@CITE[4]@; @CITE[2]@). Neural solvers are based on neural networks, such as MWP-BERT (@CITE[5]@) and GenBERT (@CITE[1]@). Neural solvers are trained on MWP dataset and can solve MWP problems. However, neural solvers are sensitive to dataset and may not generalize well to other MWP datasets. Symbolic solvers are not sensitive to dataset and can solve MWP problems on different datasets. In this paper, we propose a symbolic solver, which is based on symbolic reasoning and can solve MWP problems on different datasets.\\nReferences:ID: 1. Title: Adversarial Examples for Evaluating Math Word Problem Solvers\\nID: 2. Title: MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving\\nID: 3. Title: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\\nID: 4. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 5. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval',\n",
       "  'ground_truth': '\\n\\nSome models have been specifically created to solve maths problems by outputting expressions (@CITE[4]@, 2022; Patel et al., 2021) or pseudo-programs (@CITE[5]@; @CITE[2]@ which are then evaluated using an external module. Notwithstanding the performance of these models, they can only be used to solve maths problems that, moreover, need to be represented in a closed arithmetic form. This restricts the versatility of these models both in terms of the maths and tasks that they can solve. Unlike the other maths solvers, GenBERT (@CITE[1]@ and NT5 (@CITE[3]@ generate the final output as text, making them more general-purpose. Both are pre-trained on numerical and textual tasks to solve mathematical problems. Both of these models are evaluated on DROP (@CITE[2]@ which only provides an accuracy score, so their general numerical skill performance is not.\\nReferences:ID: 1. Title: Adversarial Examples for Evaluating Math Word Problem Solvers\\nID: 2. Title: MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving\\nID: 3. Title: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\\nID: 4. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 5. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval<|eot_id|>'},\n",
       " 3269: {'predict': 'Adversarial attacks in NLP have been explored in various forms. @CITE[3]@ proposed a geometry-inspired attack to generate natural language adversarial examples. The attack generates adversarial examples by iteratively approximating the decision boundary of DNNs. @CITE[1]@ introduced imitation attacks to steal black-box NLP systems. The imitation models are trained to imitate the outputs of the target models. @CITE[2]@ proposed syntactically controlled paraphrase networks (SCPNs) to generate adversarial examples. SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. @CITE[1]@ proposed a combination of automated and human evaluation to show that SCPNs can generate paraphrases that follow their target specifications without decreasing paraphrase quality. @CITE[2]@ proposed a gradient-based attack to generate adversarial examples. The attack generates adversarial examples by iteratively updating the input sentence with gradient-based optimization. @CITE[3]@ proposed a text-based attack to generate adversarial examples. The attack generates adversarial examples by iteratively updating the input sentence with a text-based optimization method. @CITE[1]@ proposed a word-based attack to generate adversarial examples. The attack generates adversarial examples by iteratively updating the input sentence with a word-based optimization method. These attacks are based on the gradient-based optimization method. They can generate adversarial examples by iteratively updating the input sentence with gradient-based optimization. However, they are based on the gradient-based optimization method, which is not suitable for discrete data. Our attack is based on the word-based optimization method, which is suitable for discrete data.\\nReferences:ID: 1. Title: A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples\\nID: 2. Title: Imitation Attacks and Defenses for Black-box Machine Translation Systems\\nID: 3. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks\\nID: 1. Title: A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples',\n",
       "  'ground_truth': '\\n\\nAdversarial attacks generate adversarial samples against a victim model F, which is a PrLM-based text classifier in this paper. Given an input sequence X, the victim model F predicts its label F(X) = y. The corresponding adversarial sample Xadv should alter the prediction of the victim model and meanwhile be similar to the original sequence: F(Xadv) ≠ F(X). d(Xadv, X) < σ, where d() measures the size of perturbations, and σ is a predefined threshold. Classiﬁcation of attacks: Adversarial attacks can be conducted in both white-box and black-box scenarios. In the white-box scenario (@CITE[3]@, adversarial attacks can access all information of their victim models. In the black-box scenario, adversarial attacks can only get the output of the victim models: if they get prediction scores, they are score-based attacks (@CITE[3]@; if they get the prediction label, they are decision-based attacks (. According to the granularity of perturbations, textual attacks can be classified into character-level, word-level, and sentence-level attacks. Character-level attacks ( introduce noises by replacing, inserting, or deleting a character in several words. Word-level attacks substitute several words by their synonyms to fool the model (@CITE[3]@; @CITE[3]@. Sentence-level attacks generate adversarial samples by paraphrasing the original sentence ( or using a generative adversarial network (GAN) (.\\nReferences:ID: 1. Title: A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples\\nID: 2. Title: Imitation Attacks and Defenses for Black-box Machine Translation Systems\\nID: 3. Title: Adversarial Example Generation with Syntactically Controlled Paraphrase Networks<|eot_id|>'},\n",
       " 5113: {'predict': 'In this work, we investigate the use of human gaze to guide the attention of language models. In a similar vein, @CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[3]@ have used gaze data to improve the performance of NLP models, including language models. However, these works have focused on using gaze to improve the performance of models, whereas our work focuses on using gaze to improve the interpretability of models. Furthermore, our work is the first to use gaze to guide the attention of language models in a multimodal setting. In this work, we use gaze data to guide the attention of language models to improve the interpretability of the models. Our approach is complementary to previous work that has used gaze to improve the performance of models, and we believe that gaze can be used to improve the interpretability of models in a wide variety of NLP tasks.\\nReferences:ID: 1. Title: Entity Recognition at First Sight: Improving NER with Eye Movement Information\\nID: 2. Title: Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze\\nID: 3. Title: Multimodal Integration of Human-Like Attention in Visual Question Answering',\n",
       "  'ground_truth': '\\n\\nWhen humans read, they naturally engage in the cognitive process of comprehending language, which, in turn, is reflected in their gaze behav- ior (@CITE[3]@. In a nutshell, a scanpath (i.e., sequence of consecutive fixations) on a stimulus text approximates the reader’s at- tention, which can be exploited to inform Natural Language Processing (NLP) tasks. Gaze data has been shown to be beneficial in vari- ous NLP tasks, such as part-of-speech-tagging (Bar- rett et al., 2016), named entity recognition (Hol- lenstein and @CITE[1]@, generating image cap- tions (@CITE[2]@ and question answer- ing (. Researchers have explored 1https://github.com/aeye-lab/ EMNLP-SyntheticScanpaths-NLU-PretrainedLM.\\nReferences:ID: 1. Title: Entity Recognition at First Sight: Improving NER with Eye Movement Information\\nID: 2. Title: Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze\\nID: 3. Title: Multimodal Integration of Human-Like Attention in Visual Question Answering<|eot_id|>'},\n",
       " 2791: {'predict': 'Adapters are lightweight, parameter-efficient modules that can be added to a pre-trained language model to enable zero-shot transfer to a new task. They can be used in a variety of settings, including multi-task learning (Houlsby et al., 2019; @CITE[5]@; @CITE[2]@; @CITE[1]@; ; @CITE[4]@; @CITE[6]@; ), few-shot learning (Zaken et al., 2020), and cross-lingual transfer (@CITE[3]@).\\nReferences:ID: 1. Title: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning\\nID: 2. Title: AdapterFusion: Non-Destructive Task Composition for Transfer Learning\\nID: 3. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 4. Title: What to Pre-Train on? Efficient Intermediate Task Selection\\nID: 5. Title: Towards a Unified View of Parameter-Efficient Transfer Learning\\nID: 6. Title: UNKs Everywhere: Adapting Multilingual Language Models to New Scripts',\n",
       "  'ground_truth': '\\n\\nAs a parameter-efficient alternative to standard fine-tuning, adapters have been used for quick training (Rücklé et al., 2021), multi-task learning (@CITE[6]@ and knowledge composition (@CITE[5]@; @CITE[4]@; @CITE[2]@. Moreover, Mahabadi et al. (2021a) and @CITE[3]@ extended adapters for better performance with fewer parameters. In the context of multilingual transfer, adapters enable allocation of additional language-specific capacity, thereby mitigating the ‘curse of multilinguality’ (Üstün et al., 2020). Such language adapters (; achieve high zero-shot results when combined with task adapters and enable generalization to languages unseen during pre-training via MLM-based adaptation (. Philip et al. (2020) and Üstün et al. (2021) also used monolingual adapters for zero-shot and unsupervised NMT. Hypernetworks in propose a multi-task model that uses a hypernetwork to condition on input to learn task-specific reparametrizations. Similarly, Mahabadi et al. (2021b) generate task-specific adapters via a hypernetwork. Recently, @CITE[1]@ use a hypernetwork to generate prompts. For multilingual learning, where the input sources correspond to language embeddings, Üstün et al. (2020) and learn these embeddings from the typological feature vectors of languages, enabling generalization to unseen languages based on a hypernetwork. In a similar spirit to our work, parameter space factorization (PSF;, learns task and language-specific embeddings from seen task-language combinations. However, unlike our model, these embeddings are used for task/language-specific parametrization in the softmax layer.\\nReferences:ID: 1. Title: BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning\\nID: 2. Title: AdapterFusion: Non-Destructive Task Composition for Transfer Learning\\nID: 3. Title: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\\nID: 4. Title: What to Pre-Train on? Efficient Intermediate Task Selection\\nID: 5. Title: Towards a Unified View of Parameter-Efficient Transfer Learning\\nID: 6. Title: UNKs Everywhere: Adapting Multilingual Language Models to New Scripts<|eot_id|>'},\n",
       " 167: {'predict': 'Emoji prediction with neural networks has been studied by many researchers in the past years. For example, @CITE[3]@ use a convolutional neural network (CNN) to predict emojis in text messages. In their work, they show that a CNN is better than a recurrent neural network (RNN) for this task. @CITE[2]@ use a multimodal approach for predicting emojis in Instagram posts. They show that text and image modalities encode different information and therefore can complement each other. In a similar approach, @CITE[1]@ use a multimodal approach to predict emojis in tweets. They use a CNN for text and a convolutional neural network for images, and they show that the combination of both modalities improves the accuracy of the task. Our work differs from the existing work in that we use a transformer architecture (@CITE[3]@, which is more effective than CNNs for this task. Moreover, we use a different dataset than the existing work. Our dataset is composed of tweets, but it contains only a subset of the emojis in the existing dataset. Finally, we use a different approach for training the model. We use a multi-task learning approach, which allows us to predict the presence of emojis and the meaning of the emojis.\\nReferences:ID: 1. Title: Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\\nID: 2. Title: Multimodal Emoji Prediction\\nID: 3. Title: Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm',\n",
       "  'ground_truth': '\\n\\nIn this paper, we use tweet-level and passage-based annotations interchangeably. Available upon request. or classifying the linguistic purposes of emojis (Na’aman et al., 2017). With the help of Recurrent Neural Networks (RNNs), @CITE[2]@ and @CITE[1]@ could predict proper emojis from tweet posts. Going one step further, combined image and text features for predicting emojis. Due to the overly large amount of data required to train neural-network-based classifiers with tens or even hundreds of labels, most emoji prediction datasets were labeled automatically, using heuristics or predefined rules. The logic previous researchers used to label these datasets were simple, assuming that every emoji appearing in a social media post qualified as a label of the text. Our work also relies on this assumption since the data we use consists of tweets posted by authorized accounts, who are not likely to often use emojis arbitrarily or randomly. We also extend the annotation method to be able to generate more complex, aspect-level annotations automatically. Additionally, it is commonly agreed that emojis are closely related to emotions, sentiments, sarcasm, irony, etc. @CITE[2]@ showed that the language representations learned from the emoji prediction task were useful in emotion recognition, sentiment analysis, and sarcasm detection tasks. @CITE[3]@ designed experiments to show the interconnections between emoji usage and ironic expressions. also evaluated the influence of emojis on irony detection and sentiment analysis tasks, but they replaced emojis with descriptive text in this process. Based on previous knowledge about emojis and the emoji prediction task, we also use sentiment analysis, emotion recognition, and formality classification tasks to validate the quality of our annotations in this paper.\\nReferences:ID: 1. Title: Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\\nID: 2. Title: Multimodal Emoji Prediction\\nID: 3. Title: Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm<|eot_id|>'},\n",
       " 5014: {'predict': 'Automatic evaluation of LLMs has attracted increasing attention recently. @CITE[4]@ studied the performance of GPT-4 on planning problems in PDDL domains. @CITE[5]@ evaluated the capabilities of GPT-4 on various tasks in different domains, including mathematics, coding, vision, medicine, law, and psychology. @CITE[2]@ demonstrated GPT-4’s ability to solve chemistry-related tasks. @CITE[1]@ evaluated GPT-4 on a wide range of tasks in multiple domains. @CITE[3]@ evaluated GPT-4 on logical reasoning tasks. In this work, we propose a comprehensive evaluation framework for LLMs, which covers a wide range of tasks in different domains. Our framework also provides a benchmark suite, which can be used to evaluate LLMs in the future.\\nReferences:ID: 1. Title: Generalized Planning in PDDL Domains with Pretrained Large Language Models\\nID: 2. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\nID: 3. Title: ChemCrow: Augmenting large-language models with chemistry tools\\nID: 4. Title: Instruction Tuning with GPT-4\\nID: 5. Title: Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4',\n",
       "  'ground_truth': '\\n\\nWhile evaluation of the performance of an LLM in games (@CITE[4]@ or planning in PDDL domains ( can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (@CITE[1]@; @CITE[5]@; @CITE[2]@; @CITE[3]@;. However, these have been found to contradict human evaluation (@CITE[5]@ or be systematically biased (Wang et al., 2023b), where the order of the provided responses affects the predicted ranking. In comparison to these works, we use an LLM to generate pseudo-ground truth data on an easy task, in which the model consistently performs well at, which we use to evaluate on a more difficult task with real-world implications.\\nReferences:ID: 1. Title: Generalized Planning in PDDL Domains with Pretrained Large Language Models\\nID: 2. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\nID: 3. Title: ChemCrow: Augmenting large-language models with chemistry tools\\nID: 4. Title: Instruction Tuning with GPT-4\\nID: 5. Title: Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4<|eot_id|>'},\n",
       " 2112: {'predict': 'Counterfactual generation is a sub-task of controlled text generation (@CITE[6]@; @CITE[2]@; @CITE[4]@; @CITE[3]@, which aims to generate sentences that are as similar as possible to a given sentence, except for one or more specified attributes. Counterfactual generation has been explored in several areas, including data augmentation (@CITE[1]@; @CITE[5]@; @CITE[3]@, model evaluation (@CITE[5]@; @CITE[3]@, and model explanation (@CITE[5]@. However, previous work has not considered the problem of generating counterfactuals for multiple attributes, which is essential for many real-world applications.\\nReferences:ID: 1. Title: Exploring Controllable Text Generation Techniques\\nID: 2. Title: Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation\\nID: 3. Title: An Empirical Survey of Data Augmentation for Limited Data Learning in NLP\\nID: 4. Title: Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond\\nID: 5. Title: Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models\\nID: 6. Title: CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation',\n",
       "  'ground_truth': '\\n\\nControllable generation refers to generation of text while controlling for specific attributes (@CITE[2]@. The controlled attributes can range from style (e.g., politeness and sentiment) to content (e.g., keywords and entities) and even topic. Keskar et al. (2019) propose to control the generated text by training an LM on datasets annotated with the controlled attributes, and modify the model’s decoding method. Recently, @CITE[3]@ introduced a global sampling conditional variational autoencoder (VAE), augmenting text while controlling for attributes such as label and verb tense. However, controlling for the task label is challenging in scarce labeled data scenarios (@CITE[1]@, since generative models require large amounts of labeled data. Counterfactual generation lies at the intersection of controllable generation and causal inference (. Only few works deal with counterfactual generation, mostly by intervening on the task label. @CITE[4]@ train a model on textual examples and their manually generated counterfactuals. Other works present methods for controlling for the text domain and semantics (@CITE[6]@; @CITE[5]@, yet they all experiment with short texts, while our model can generate longer texts, consisting of multiple sentences. A recent work by focuses on generation of new target-domain examples for aspect-based sentiment analysis (ABSA) (Pontiki et al., 2016). However, this method is designed specifically for ABSA, utilizing predefined knowledge, and is only suitable for UDA setups where source domain labeled data is abundant. Our work presents a novel domain counterfactual generation algorithm, which can be trained in an unsupervised manner, and its generated outputs are demonstrated to be effective in multiple low-resource DA tasks.\\nReferences:ID: 1. Title: Exploring Controllable Text Generation Techniques\\nID: 2. Title: Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation\\nID: 3. Title: An Empirical Survey of Data Augmentation for Limited Data Learning in NLP\\nID: 4. Title: Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond\\nID: 5. Title: Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models\\nID: 6. Title: CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation<|eot_id|>'},\n",
       " 7377: {'predict': 'Smart reply, which aims to generate a set of diverse replies to a given input, has been widely studied in the field of natural language processing (@CITE[6]@; @CITE[1]@; @CITE[3]@; @CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[7]@; @CITE[5]@. However, these methods are mainly based on the assumption that the input is a short message, and they are not suitable for the scenario where the input is a long text. In this work, we propose a novel method to generate a set of diverse replies to a long input, which is particularly suitable for the scenario of generating replies for long-form text such as articles. In particular, we propose a method to generate a set of diverse replies to a long input by applying the prompt-based language model to generate a set of diverse prompts, and then applying the retrieval-based language model to retrieve a set of diverse replies from a large-scale reply dataset. Our method is inspired by the recent development of the prompt-based language model (@CITE[5]@; @CITE[7]@, which is particularly suitable for generating a set of diverse replies to a long input. In addition, we propose a method to retrieve a set of diverse replies from a large-scale reply dataset by using the prompt-based language model as the query, which is particularly suitable for the scenario of generating replies for long-form text such as articles.\\nReferences:ID: 1. Title: A Dataset and Baselines for Multilingual Reply Suggestion\\nID: 2. Title: A Conditional Generative Matching Model for Multi-lingual Reply Suggestion\\nID: 3. Title: Model-Based Simulation for Optimising Smart Reply\\nID: 4. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 5. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 6. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 7. Title: OCC: A Smart Reply System for Efficient In-App Communications',\n",
       "  'ground_truth': '\\n\\nThe proprietary nature of data from email and chat applications has led several previous works to use publicly-available dialogue datasets (@CITE[1]@; @CITE[6]@; @CITE[4]@ to benchmark SR methods, due to their analogous conversational nature. While early SR systems used generative models (@CITE[7]@, current production systems favour retrieval methods due to their greater controllability of outputs and superior latency (@CITE[5]@. Increasing the diversity of reply suggestions is a key focus of previous work, which has been attempted by: (1) mapping replies to discrete intents / topics (@CITE[7]@; ; @CITE[2]@; (2) re-weighting replies according to their similarity with other replies in the set (@CITE[3]@; @CITE[5]@; (3) learning continuous latent variables to generate multiple queries (Zhao et al., 2017; @CITE[5]@; (4) using model-based simulation to iteratively search and evaluate the relevance of candidate reply sets (@CITE[4]@. Our proposed method differs from all of these approaches in that our model learns to account for the interdependencies between replies through end-to-end backpropagation.\\nReferences:ID: 1. Title: A Dataset and Baselines for Multilingual Reply Suggestion\\nID: 2. Title: A Conditional Generative Matching Model for Multi-lingual Reply Suggestion\\nID: 3. Title: Model-Based Simulation for Optimising Smart Reply\\nID: 4. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 5. Title: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\\nID: 6. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 7. Title: OCC: A Smart Reply System for Efficient In-App Communications<|eot_id|>'},\n",
       " 7083: {'predict': 'Human evaluation is a widely used method to evaluate the quality of generated texts. It is commonly used in summarization tasks to evaluate whether the generated summary can convey the main idea of the source text (@CITE[9]@; @CITE[6]@; @CITE[5]@; @CITE[7]@; @CITE[4]@; @CITE[1]@. In this paper, we also use human evaluation to evaluate whether the generated text can convey the main idea of the source text. In addition, human evaluation is also used to evaluate whether the generated text can convey the main idea of the source text in the context of multi-document summarization (@CITE[8]@; @CITE[2]@; @CITE[3]@; @CITE[1]@. In this paper, we also use human evaluation to evaluate whether the generated text can convey the main idea of the source text in the context of multi-document summarization. In contrast to the above works, we use human evaluation to evaluate whether the generated text can convey the main idea of the source text in the context of multi-document summarization with a different way. In this paper, we use human evaluation to evaluate whether the generated text can convey the main idea of the source text in the context of multi-document summarization with a different way. We ask the annotators to evaluate the generated text in terms of the main idea of the source text and the context of the source text. We ask the annotators to evaluate the generated text in terms of the main idea of the source text and the context of the source text. In contrast to the above works, we use human evaluation to evaluate whether the generated text can convey the main idea of the source text in the context of multi-document summarization with a different way.\\nReferences:ID: 1. Title: CTRLsum: Towards Generic Controllable Text Summarization\\nID: 2. Title: ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\\nID: 3. Title: Abstractive Text Summarization by Incorporating Reader Comments\\nID: 4. Title: Generating Wikipedia by Summarizing Long Sequences\\nID: 5. Title: Deep Recurrent Generative Decoder for Abstractive Text Summarization\\nID: 6. Title: SentBS: Sentence-level Beam Search for Controllable Summarization\\nID: 7. Title: ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\\nID: 8. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 9. Title: Hierarchical Neural Story Generation',\n",
       "  'ground_truth': '\\n\\nHuman evaluation can be conducted with different approaches. Some work (@CITE[8]@; @CITE[3]@; @CITE[7]@; @CITE[6]@; @CITE[2]@; @CITE[9]@; ; Kry´ nski et al., 2018) employ a Likert scale to evaluate the summaries on discrete ranges, such as from 1 to 5. Meanwhile, many others suggest comparison approaches by asking human annotators to select the best summary out of 2 or more generated summaries from different systems (@CITE[5]@; @CITE[6]@; @CITE[4]@; ; @CITE[1]@). Following this, we test LLM-based evaluators using both approaches with human-friendly instruction prompts.\\nReferences:ID: 1. Title: CTRLsum: Towards Generic Controllable Text Summarization\\nID: 2. Title: ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\\nID: 3. Title: Abstractive Text Summarization by Incorporating Reader Comments\\nID: 4. Title: Generating Wikipedia by Summarizing Long Sequences\\nID: 5. Title: Deep Recurrent Generative Decoder for Abstractive Text Summarization\\nID: 6. Title: SentBS: Sentence-level Beam Search for Controllable Summarization\\nID: 7. Title: ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\\nID: 8. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 9. Title: Hierarchical Neural Story Generation<|eot_id|>'},\n",
       " 6870: {'predict': 'Recent work has highlighted the need for LLMs to be aligned with human values (@CITE[2]@; @CITE[1]@; @CITE[3]@. However, there is a lack of clarity regarding what alignment means, and a tendency of technology providers to prescribe definitions of inherently subjective preferences and values (@CITE[2]@. In this paper, we focus on the alignment of LLMs with human values, where values are defined as the principles or standards that guide decision-making, or the fundamental principles of an individual or group (@CITE[3]@. We propose a novel approach to align LLMs with human values, by learning to generate statements that are consistent with human values in a specific domain, such as environmental conservation. We use a novel multi-task learning framework that includes a value-consistency task, where the model generates statements that are consistent with a set of human values. We also include a consistency-ranking task, where the model is trained to rank statements in terms of their consistency with human values. We evaluate our approach on a new dataset of human values and statements, and show that it outperforms a baseline LLM in terms of value-consistency.\\nReferences:ID: 1. Title: Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback\\nID: 2. Title: In conversation with Artificial Intelligence: aligning language models with human values\\nID: 3. Title: Fine-tuning language models to find agreement among humans with diverse preferences',\n",
       "  'ground_truth': '\\n\\nLarge language models (LLMs) have achieved suc- cess at a range of tasks such as question answering, summarisation, and dialogue. Hannah (@CITE[2]@ proposed a three-tiered policy framework that allows users to experience the benefits of per- sonalised alignment. Similarly, Jakob (Mökander et al., 2023) also used a three-layered approach for the LLMs which include governance audits, model audits, and application audits. Atoosa (@CITE[1]@ developed a philosophical anal- ysis of building blocks of linguistic communica- tion between conversational agents and human in- terlocutors. Bakker (@CITE[3]@ studied LLMs alignment from different perspectives, their work highlights that there is still a gap between the alignment of LLMs and humans. However, there is a need to strike the balance of providing more information to LLMs and not burdening the human experts involved unnecessarily. This observation inspires us to investigate the right amount of in- formation LLMs require from humans in order to achieve satisfactory performance.\\nReferences:ID: 1. Title: Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback\\nID: 2. Title: In conversation with Artificial Intelligence: aligning language models with human values\\nID: 3. Title: Fine-tuning language models to find agreement among humans with diverse preferences<|eot_id|>'},\n",
       " 5333: {'predict': 'Stylistic exemplars have been used to enhance the quality of generated text. For example, @CITE[8]@ use exemplars to guide the generation of summaries in a controlled manner. @CITE[6]@ retrieve exemplars to help the model understand the text and write summaries in a specific style. @CITE[1]@ use exemplars to generate stylized dialogue responses. @CITE[3]@ retrieve exemplars to guide the generation of task-oriented dialogue. @CITE[7]@ retrieve exemplars to generate paraphrases. @CITE[4]@ retrieve exemplars to simplify professional texts. @CITE[2]@ retrieve exemplars to generate expository texts. However, these methods are not designed to handle the multi-style generation task, which is the focus of our work. In contrast, our method can generate texts with multiple styles simultaneously.\\nReferences:ID: 1. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 2. Title: RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization\\nID: 3. Title: Stylized Dialogue Response Generation Using Stylized Unpaired Texts\\nID: 4. Title: A Template-guided Hybrid Pointer Network for Knowledge-basedTask-oriented Dialogue Systems\\nID: 5. Title: Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data\\nID: 6. Title: Controllable Paraphrase Generation with a Syntactic Exemplar\\nID: 7. Title: Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen\\nID: 8. Title: Expository Text Generation: Imitate, Retrieve, Paraphrase',\n",
       "  'ground_truth': '\\n\\nRecent work has studied models that leverage stylis- tic exemplars to guide stylistic choices in text gen- eration (; Wei et al., 2020). Such exemplars improve the fluency of seq2seq mod- els in various tasks, including summarization (@CITE[5]@; @CITE[8]@, machine translation (Shang et al., 2021; @CITE[2]@, dialogue generation (@CITE[6]@;, and question answering (@CITE[7]@. More relevant to text fact transfer are tasks that require strictly adhering to the style of an exemplar. @CITE[1]@ propose the task of controllable paraphrase generation, which aims to combine the semantics from one sentence and the syntax from a second sentence. @CITE[4]@ introduce “style imitation” and perform data-to-text gener- ation while strictly maintaining the style of an ex- emplar. Apart from a lack of focus on transferring factual content, these works differ from our task as they do not leverage a factual corpus. The task most similar to ours is expository text generation (ETG) (@CITE[3]@, which seeks to generate factual text from a corpus in a consistent style. However, ETG dictates that this style is learned from examples of outputs in the same domain, while text fact transfer adheres to the style of a single source text. Hence, an ETG model is domain-specific, while a single text fact trans- fer model (e.g., 0-shot ModQGA) could be used in several domains. Further, the IRP model pro- posed by @CITE[3]@ for ETG combines content planning, retrieval, and rephrasing, while ModQGA modifies a source text with question gen- eration and answering, and our model tackles the additional problem of controlling specificity (§3.3).\\nReferences:ID: 1. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 2. Title: RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization\\nID: 3. Title: Stylized Dialogue Response Generation Using Stylized Unpaired Texts\\nID: 4. Title: A Template-guided Hybrid Pointer Network for Knowledge-basedTask-oriented Dialogue Systems\\nID: 5. Title: Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data\\nID: 6. Title: Controllable Paraphrase Generation with a Syntactic Exemplar\\nID: 7. Title: Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen\\nID: 8. Title: Expository Text Generation: Imitate, Retrieve, Paraphrase<|eot_id|>'},\n",
       " 3711: {'predict': 'Sentence representation learning aims to learn a compact vector representation for each sentence. Early methods use a bag-of-words approach (Pennington et al., 2014) and learn sentence embeddings from word embeddings (@CITE[1]@). However, this approach is limited to the performance of word embeddings. Recent studies focus on learning sentence embeddings from transformer-based language models. @CITE[6]@ propose a dual encoder architecture and train the encoder with a contrastive loss. @CITE[2]@ find that the semantic information in the BERT embeddings is not fully exploited, and propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution. @CITE[3]@ propose a self-supervised contrastive learning technique, which converts MLMs into effective universal lexical and sentence encoders in 20-30 seconds. @CITE[5]@ propose a deep contrastive learning framework for unsupervised textual representations. @CITE[7]@ propose a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. @CITE[8]@ propose a prompt-based sentence embeddings method and discuss two prompt representing methods and three prompt searching methods to make BERT achieve better sentence embeddings. @CITE[4]@ propose a difference-based contrastive learning framework for learning sentence embeddings. @CITE[9]@ propose a contrastive learning framework for learning sentence embeddings by predicting the masked tokens of the sentence.\\nReferences:ID: 1. Title: Learning Distributed Representations of Sentences from Unlabelled Data\\nID: 2. Title: An efficient framework for learning sentence representations\\nID: 3. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 4. Title: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision\\nID: 5. Title: Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders\\nID: 6. Title: DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations\\nID: 7. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 8. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 9. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings',\n",
       "  'ground_truth': '\\n\\nLearning universal sentence representations has been studied extensively in prior works, roughly categorized into supervised methods that train the sentence encoder on datasets with annotations like the supervised Natural Language Inference (NLI) tasks (; @CITE[3]@) and unsupervised approaches that consider deriving sentence embeddings without annotated data, e.g., average GloVe embeddings (Pennington et al., 2014), FastSent (@CITE[4]@) and Quick-Thought (@CITE[7]@). Recent works have proposed several techniques to mitigate the anisotropy issue of PLMs, such as post-processing methods like BERT-flow (@CITE[2]@) and BERT-whitening (, as well as contrastive learning approaches that learn sentence embeddings by creating semantically close augmentations and pulling these representations to be closer than representations of random negative examples, achieving significant performance improvement (; @CITE[6]@; @CITE[1]@; @CITE[9]@; @CITE[5]@; @CITE[8]@; Zhou et al., 2022; ; ).\\nReferences:ID: 1. Title: Learning Distributed Representations of Sentences from Unlabelled Data\\nID: 2. Title: An efficient framework for learning sentence representations\\nID: 3. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 4. Title: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision\\nID: 5. Title: Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders\\nID: 6. Title: DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations\\nID: 7. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 8. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 9. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings<|eot_id|>'},\n",
       " 1768: {'predict': 'Neural reasoning algorithms are typically categorized into two types: the first is based on neural networks and the second is based on neural-symbolic approaches. Neural reasoning algorithms based on neural networks include: (1) Neural Theorem Prover (@CITE[9]@; @CITE[1]@; @CITE[11]@; @CITE[5]@; @CITE[11]@; @CITE[4]@; @CITE[7]@), which is a differentiable theorem prover that can be trained to generate proofs of logical statements. (2) Logic Tensor Networks (@CITE[8]@), which combines neural networks with first-order logic to perform knowledge graph reasoning. (3) Neural Logic Machine (@CITE[6]@), which is a neural-symbolic architecture for both inductive learning and logic reasoning. Neural reasoning algorithms based on neural-symbolic approaches include: (1) Neural-Symbolic Learning (@CITE[3]@), which uses a neural network to learn a symbolic representation of a logical formula. (2) Neural-Symbolic Reasoning (@CITE[2]@), which uses a neural network to reason over a knowledge base represented as a logical formula. (3) Deep Learning for Ontology Reasoning (@CITE[10]@), which uses a deep recursive neural network to reason over a knowledge base represented as an ontology. (4) Logic-Guided Neural Networks (@CITE[1]@), which uses a neural network to reason over a knowledge base represented as a logical formula. (5) Neural-Symbolic Reasoning with Differentiable Inference (@CITE[11]@), which uses a neural network to reason over a knowledge base represented as a logical formula and differentiable neural networks to perform inference.\\nReferences:ID: 1. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 2. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 3. Title: PRover: Proof Generation for Interpretable Reasoning over Rules\\nID: 4. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 5. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 6. Title: Exploring Neural Models for Parsing Natural Language into First-Order Logic\\nID: 7. Title: Differentiable Reasoning on Large Knowledge Bases and Natural Language\\nID: 8. Title: Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge\\nID: 9. Title: Towards a Model Theory for Distributed Representations\\nID: 10. Title: Neural Logic Machines\\nID: 11. Title: Deep Learning for Ontology Reasoning',\n",
       "  'ground_truth': '\\n\\nWhile our work is, to the best of our knowledge, the ﬁrst proposed architecture that emulates backward chaining inference over rule sets and facts expressed in natural language, there are several methods which explore this research area. @CITE[11]@ introduce the use of transformers to reason over explicitly stated rule sets expressed in natural language; their approach show that transformers are able to solve the problem with high accuracy when the neural network is trained with sufficiently deep reasoning paths, without imposing any structure on the neural reasoning. Our main contribution with respect to @CITE[11]@ consists in emulating, through a neural network, a general reasoning mechanism inspired by the backward chaining algorithm used in formal logic programming. Also, our method demonstrates better accuracy for high depth queries, even when trained only with shallow queries. @CITE[6]@ and @CITE[11]@ modify the @CITE[11]@ approach in order to generate proof together with the predicted truth value. These methods, however, require the explicit knowledge of the proof during the training phase. Furthermore, our work is substantially different from the methods that focus on an initial translation of knowledge expressed in textual form to a formal specification, with the aim of applying classic reasoning algorithms, such as the architecture proposed in @CITE[5]@ for translating text into first-order logic formulas. Our work is also different from @CITE[3]@: the authors present a neural network suitable for reasoning over relationships between two entities of a knowledge base, focusing specifically on predicting additional true facts using only vector representations of existing entities in the knowledge base. Other approaches have combined neural and symbolic reasoning methods. One notable example is the Neural Theorem Proving (NTP) presented in (Rocktäschel and @CITE[4]@. The authors propose an end-to-end differentiable prover, operating on symbolic representations, for automated completion of a knowledge base: they recursively construct neural networks to prove queries on the knowledge base by following Prolog’s backward chaining algorithm. Additionally, they introduce a differentiable unification operation between vector representations of symbols. @CITE[7]@ describes an NTP capable of jointly reasoning over knowledge bases and natural language corpora. Although the method is versatile, an explicit mapping to entities in the knowledge base is required. Other relevant methods implement forms of neural reasoning starting from a formal knowledge base, including (Serafini and d’, @CITE[9]@, and @CITE[1]@, or starting from an ontology (which usually define not just the predicates but also rules). Conversely, we focus on using transformers both as a fact look-up model (over a knowledge base expressed in natural language) and as a unification unit for transforming queries, which may require many steps of inference, into factual queries that can be answered with the fact look-up model. Some early work on simulating the first-order algorithm of unification using neural networks is presented in @CITE[2]@. The author shows how the error-correction learning algorithm can be used for the purposes of unification. However, this work considers a version of the problem where the knowledge is represented using a formal first-order logic language and uses an explicit mapping of each symbol of the language into an input vector. Similarly to our work, @CITE[10]@ approach the problem of reasoning over natural language emulating unification. They present a model combining neural networks with logic programming for solving multi-hop reasoning tasks over natural language. In the proposed approach, the authors extend a Prolog prover to use a similarity function over pretrained sentence encoders. A substantial difference with respect to our work is that @CITE[10]@ approach requires the transformation of natural language text into triples (by using co-occurrences of named entities) and then embedding representations of the symbols in a triple using an encoder. In this paper, we take inspiration from the method presented in, a recurrent cell that simulates a reasoning step, although the architecture is specially designed for performing visual reasoning given a textual query. Our goal is quite different from answering complex multi-hop questions using a corpus of documents as a virtual knowledge base as proposed in the recent work by @CITE[8]@, which requires selecting spans from paragraphs of texts. Our work can be described as the formalization of a model and a training process that leads the neural network to emulate a backward chaining inference process for answering deep queries.\\nReferences:ID: 1. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 2. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 3. Title: PRover: Proof Generation for Interpretable Reasoning over Rules\\nID: 4. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 5. Title: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\\nID: 6. Title: Exploring Neural Models for Parsing Natural Language into First-Order Logic\\nID: 7. Title: Differentiable Reasoning on Large Knowledge Bases and Natural Language\\nID: 8. Title: Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge\\nID: 9. Title: Towards a Model Theory for Distributed Representations\\nID: 10. Title: Neural Logic Machines\\nID: 11. Title: Deep Learning for Ontology Reasoning<|eot_id|>'},\n",
       " 4926: {'predict': 'In recent years, unsupervised sentence representation learning has attracted extensive attention. Traditional approaches typically use a bag-of-words representation, which is not effective in capturing semantic information (@CITE[5]@). Recently, pre-trained language models (PLMs) have achieved remarkable success in natural language processing tasks, which are typically fine-tuned on downstream tasks (@CITE[8]@; @CITE[7]@). However, the native sentence representation derived from PLMs is proved to be collapsed, resulting in poor performance on semantic textual similarity tasks (@CITE[1]@). To address this issue, various contrastive learning methods have been proposed to learn sentence representations from unlabeled texts. SimCSE (@CITE[6]@) is a simple contrastive learning framework that uses dropout as noise to predict the same sentence. However, dropout is not suitable for sentence-level contrastive learning, as it randomly removes words from the sentence. @CITE[2]@ propose ConSERT, which uses the word-level contrastive learning method to fine-tune BERT. However, ConSERT requires a large amount of unlabeled texts, which is not always available. @CITE[3]@ propose CoSERT, which uses a multi-task learning method to fine-tune BERT with unlabeled texts. However, CoSERT requires the same model to be trained on both unlabeled texts and downstream tasks, which is not efficient. @CITE[4]@ propose SPT, which uses a pre-training method to learn sentence representations from unlabeled texts. However, SPT requires the same model to be trained on both unlabeled texts and downstream tasks, which is not efficient. In contrast, our method uses a pre-trained PLM as the encoder, and a separate learner to learn sentence representations from unlabeled texts. Our method is more efficient and flexible.\\nReferences:ID: 1. Title: Distributed Representations of Sentences and Documents\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\\nID: 4. Title: Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting\\nID: 5. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 6. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 7. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 8. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings',\n",
       "  'ground_truth': '\\n\\nThe early study attempts to leverage sentence internal structure (; Hill et al., 2016; @CITE[4]@, and sentence context ( to learn the representation. Pagliardini et al. (2017) proposes Sent2Vec, a simple unsupervised method that obtains sentence embeddings by combining word vectors with n-gram embeddings. Several efforts have been made to obtain sentence representations using the embedding of some special token or averagely pooled last layer representations, due to the great success of pre-trained language models (@CITE[7]@; @CITE[3]@. However, @CITE[5]@ identified the anisotropy problem that the native embeddings from PLMs are concentrated in a small cone in the vector space. To address this issue, techniques such as BERT-flow (@CITE[1]@ and BERT-whitening (Su et al., 2021) have been proposed as post-processing methods. Recently, unsupervised sentence embeddings have utilized contrastive learning schemes to further boost the performance by different data augmentation methods, such as dropout (@CITE[2]@;, noise-based negatives (Zhou et al., 2022;, deletion-reordering-substitution (@CITE[8]@, adversarial attack (, span sampling (@CITE[2]@, entity augmentations (@CITE[6]@ and BERT layer sampling (.\\nReferences:ID: 1. Title: Distributed Representations of Sentences and Documents\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\\nID: 4. Title: Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting\\nID: 5. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 6. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 7. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 8. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings<|eot_id|>'},\n",
       " 6672: {'predict': 'The goal of AL is to maximize the performance gain of a model with the fewest number of labels. Most AL methods are based on the idea of uncertainty sampling (@CITE[5]@; ; @CITE[4]@), which has been widely studied for probabilistic models. However, for deep learning models, uncertainty sampling is not applicable because deep learning models are rarely probabilistic (@CITE[3]@). Recent works have proposed various acquisition functions for deep active learning, such as the uncertainty-based active learning (, the core-set-based active learning (, the diversity-based active learning (, the contrastive active learning (@CITE[9]@), the gradient-based active learning (@CITE[7]@), the representative-based active learning (@CITE[6]@), and the data augmentation-based active learning (@CITE[8]@). In contrast, our work focuses on the problem of active learning for deep generative models, which is a new and challenging problem. We propose a novel acquisition function based on the KL divergence between the predicted and the ground truth distributions. Our method can be applied to various tasks, such as image generation, image-to-image translation, and image-to-text translation.\\nReferences:ID: 1. Title: A Survey of Deep Active Learning\\nID: 2. Title: A Sequential Algorithm for Training Text Classifiers\\nID: 3. Title: Bayesian Active Learning for Classification and Preference Learning\\nID: 4. Title: Deep Bayesian Active Learning with Image Data\\nID: 5. Title: Active Learning for Convolutional Neural Networks: A Core-Set Approach\\nID: 6. Title: Exploring Representativeness and Informativeness for Active Learning\\nID: 7. Title: Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation\\nID: 8. Title: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds\\nID: 9. Title: Active Learning by Acquiring Contrastive Examples',\n",
       "  'ground_truth': '\\n\\nActive learning is a technique proposed to minimize the annotation cost during the labeling process (Set- tles, 2009; @CITE[7]@. A popular setting for active learning is the pool-based paradigm, which aims to select the most beneficial samples from an unlabeled data pool based on criteria including uncertainty (@CITE[8]@; @CITE[4]@; @CITE[9]@, diversity (@CITE[3]@; @CITE[6]@, and hybrid ob- jectives (; @CITE[2]@; @CITE[1]@; @CITE[5]@. The selected samples are annotated by human annotators and then added into the labeled dataset iteratively.\\nReferences:ID: 1. Title: A Survey of Deep Active Learning\\nID: 2. Title: A Sequential Algorithm for Training Text Classifiers\\nID: 3. Title: Bayesian Active Learning for Classification and Preference Learning\\nID: 4. Title: Deep Bayesian Active Learning with Image Data\\nID: 5. Title: Active Learning for Convolutional Neural Networks: A Core-Set Approach\\nID: 6. Title: Exploring Representativeness and Informativeness for Active Learning\\nID: 7. Title: Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation\\nID: 8. Title: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds\\nID: 9. Title: Active Learning by Acquiring Contrastive Examples<|eot_id|>'},\n",
       " 6709: {'predict': 'OOD detection is a fundamental task for robust AI systems. Previous studies mostly focus on image OOD detection (@CITE[4]@; @CITE[6]@; @CITE[5]@; @CITE[2]@; @CITE[3]@; @CITE[1]@). However, few studies have investigated OOD detection in text. @CITE[5]@ propose a contrastive learning approach to detect OOD text by learning a representation well-suited for novelty detection. @CITE[1]@ propose a contrastive learning method to detect OOD text by contrasting the sample with distributionally-shifted augmentations of itself. @CITE[3]@ propose a novel OOD detection method based on the detection of outliers in the latent space. @CITE[6]@ propose a generative model-based OOD detection method to detect OOD text by generating pseudo OOD samples. However, these methods are not designed for document-level OOD detection, and they require additional training and inference procedures. In this paper, we propose a simple yet effective method for document-level OOD detection. Our method is based on the observation that the semantic similarity between two documents is closely related to their word overlap. Therefore, we propose to measure the semantic similarity between two documents by computing the word overlap between them. Our method is simple and easy to implement, and it does not require additional training and inference procedures.\\nReferences:ID: 1. Title: Scaling Out-of-Distribution Detection for Real-World Settings\\nID: 2. Title: Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks\\nID: 3. Title: Types of Out-of-Distribution Texts and How to Detect Them\\nID: 4. Title: Unified Pretraining Framework for Document Understanding\\nID: 5. Title: Types of Out-of-Distribution Texts and How to Detect Them\\nID: 6. Title: CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances',\n",
       "  'ground_truth': '\\n\\nOOD detection has been extensively studied for open-world multi-class classification with natural image and text inputs, where the goal is to derive an OOD score that separates OOD from ID samples. A plethora of methods are proposed for deep neural networks, where the OOD scoring function is typically derived based on logits (without softmax scaling) (@CITE[3]@, softmax outputs (@CITE[6]@; @CITE[2]@; @CITE[4]@;, gradients (@CITE[4]@, and feature embeddings (; Fort et al., 2021; @CITE[5]@. Despite their impressive performance on natural images and texts, it is underexplored if the results are transferrable to the document domain. A recent work (@CITE[1]@ studied OOD detection for documents but only explored a limited number of models and OOD detection methods. The impacts of pre-training, fine-tuning, and spatial information remain unknown. In this work, we aim to provide a comprehensive and finer-grained analysis to shed light on the key factors for OOD robustness in the document domain.\\nReferences:ID: 1. Title: Scaling Out-of-Distribution Detection for Real-World Settings\\nID: 2. Title: Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks\\nID: 3. Title: Types of Out-of-Distribution Texts and How to Detect Them\\nID: 4. Title: Unified Pretraining Framework for Document Understanding\\nID: 5. Title: Types of Out-of-Distribution Texts and How to Detect Them\\nID: 6. Title: CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances<|eot_id|>'},\n",
       " 7435: {'predict': 'Knowledge editing is a crucial task for PLMs, which aims to correct the wrong knowledge stored in PLMs without retraining the model from scratch. Existing methods can be categorized into two groups: gradient-based methods and parameter-based methods. Gradient-based methods (@CITE[1]@; @CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[4]@; ; ; ) focus on editing the gradients of the PLMs to correct the wrong knowledge. However, they suffer from the gradient explosion problem and are sensitive to the hyper-parameters. Parameter-based methods (@CITE[1]@; ; @CITE[4]@) focus on editing the parameters of the PLMs to correct the wrong knowledge. However, they are sensitive to the parameter initialization and suffer from the overfitting problem.\\nReferences:ID: 1. Title: Editing Factual Knowledge in Language Models\\nID: 2. Title: Fast Model Editing at Scale\\nID: 3. Title: Locating and Editing Factual Associations in GPT\\nID: 4. Title: Calibrating Factual Knowledge in Pretrained Language Models\\nID: 5. Title: Memory-Based Model Editing at Scale',\n",
       "  'ground_truth': '\\n\\nEditing parametric knowledge is not as straightforward as editing knowledge graphs. For editing parametric knowledge, a natural way is to use constrained fine-tuning to update parameters based on new examples (Zhu et al., 2020). However, in PLMs, a minor parameter change could change the model’s predictions on many samples. Then, @CITE[2]@; ; @CITE[3]@ trained a Figure 4: Example of Editing Steps. Hyper-network to predict the parameter shift. Other methods like @CITE[5]@, 2023) proposed a direct editing method and achieved great results on batch editing. More recently, some methods have developed external modules for edits and do not require access to base model parameters (@CITE[1]@; @CITE[4]@. In order to apply the editing to real-world challenges better, Huang et al. (2023) proposed the Sequential Model Editing task and trained Transformer-Patcher, achieving edit the model up to a thousand times continuously.\\nReferences:ID: 1. Title: Editing Factual Knowledge in Language Models\\nID: 2. Title: Fast Model Editing at Scale\\nID: 3. Title: Locating and Editing Factual Associations in GPT\\nID: 4. Title: Calibrating Factual Knowledge in Pretrained Language Models\\nID: 5. Title: Memory-Based Model Editing at Scale<|eot_id|>'},\n",
       " 2395: {'predict': 'Traditional sentiment analysis tasks can be categorized into two main categories: aspect-level sentiment classification (AC) and opinion mining (OM). For AC, the goal is to classify the sentiment polarity of a target aspect in a sentence. Early methods mainly rely on handcrafted features, such as word embeddings (@CITE[6]@; @CITE[5]@), syntactic features (@CITE[4]@; @CITE[8]@), and semantic features (@CITE[3]@; @CITE[9]@). With the rise of deep learning, various neural network-based methods have been proposed to capture the sentiment information in the target aspect, such as the convolutional neural network (CNN) (@CITE[7]@), recurrent neural network (RNN) (@CITE[2]@), and graph neural network (GNN) (@CITE[1]@). For OM, the goal is to extract the sentiment-bearing words from a sentence, which can be further divided into two subtasks: aspect term extraction (ATE) and opinion word extraction (OWE). For ATE, the goal is to extract the opinion targets from a sentence. Early methods mainly rely on handcrafted features, such as word embeddings (@CITE[6]@; @CITE[5]@), syntactic features (@CITE[4]@; @CITE[8]@), and semantic features (@CITE[3]@; @CITE[9]@). With the rise of deep learning, various neural network-based methods have been proposed to capture the aspect information in the sentence, such as the CNN (@CITE[7]@), RNN (@CITE[2]@), and GNN (@CITE[1]@). For OWE, the goal is to extract the sentiment-bearing words for a given aspect from a sentence. Early methods mainly rely on handcrafted features, such as word embeddings (@CITE[6]@; @CITE[5]@), syntactic features (@CITE[4]@; @CITE[8]@), and semantic features (@CITE[3]@; @CITE[9]@). With the rise of deep learning, various neural network-based methods have been proposed to capture the sentiment-bearing words in the sentence, such as the CNN (@CITE[7]@), RNN (@CITE[2]@), and GNN (@CITE[1]@).\\nReferences:ID: 1. Title: Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction\\nID: 2. Title: Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\\nID: 3. Title: Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis\\nID: 4. Title: Aspect Level Sentiment Classification with Deep Memory Network\\nID: 5. Title: Deep Biaffine Attention for Neural Dependency Parsing\\nID: 6. Title: Transformation Networks for Target-Oriented Sentiment Classification\\nID: 7. Title: Attention Guided Graph Convolutional Networks for Relation Extraction\\nID: 8. Title: Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction\\nID: 9. Title: Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction',\n",
       "  'ground_truth': '\\n\\nTraditional sentiment analysis tasks are sentence-level (; Severyn and Moschitti, 2015) or document-level (@CITE[8]@; Lyu et al., 2020) oriented. In contrast, Aspect-based Sentiment Analysis (ABSA) is an aspect or entity oriented fine-grained sentiment analysis task. The most three basic subtasks are Aspect Term Extraction (ATE) (@CITE[7]@; ; ; @CITE[5]@; ;, Aspect Sentiment Classification (ASC) (; @CITE[9]@; ; ; @CITE[8]@; @CITE[6]@ and Opinion Term Extraction (OTE) (, 2013; ; @CITE[7]@. The studies solve these tasks separately and ignore the dependency between these subtasks. Therefore, some efforts devoted to couple the two subtasks and proposed effective models to jointly extract aspect-based pairs.\\nReferences:ID: 1. Title: Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction\\nID: 2. Title: Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\\nID: 3. Title: Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis\\nID: 4. Title: Aspect Level Sentiment Classification with Deep Memory Network\\nID: 5. Title: Deep Biaffine Attention for Neural Dependency Parsing\\nID: 6. Title: Transformation Networks for Target-Oriented Sentiment Classification\\nID: 7. Title: Attention Guided Graph Convolutional Networks for Relation Extraction\\nID: 8. Title: Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction\\nID: 9. Title: Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction<|eot_id|>'},\n",
       " 902: {'predict': 'Data augmentation has been widely used in NLP to improve the robustness and generalization ability of models (@CITE[4]@; @CITE[7]@; @CITE[5]@; @CITE[8]@; @CITE[1]@; @CITE[2]@; @CITE[6]@; @CITE[3]@; @CITE[3]@; @CITE[1]@; @CITE[7]@. For example, SpecAug (@CITE[7]@ performs augmentation on the filter bank coefficients of acoustic features in speech recognition. Contextual augmentation (@CITE[4]@ is a word-level augmentation method that replaces words in a sentence with words that are semantically similar. However, the performance of these methods is limited to a certain level, and it is difficult to achieve further improvement. Our method improves the performance of data augmentation by introducing a multi-task learning framework.\\nReferences:ID: 1. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 2. Title: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\\nID: 3. Title: The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages\\nID: 4. Title: FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding\\nID: 5. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 6. Title: FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding\\nID: 7. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 8. Title: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition',\n",
       "  'ground_truth': '\\n\\nData augmentation (@CITE[5]@ adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (, speech (Cui et al., 2015; @CITE[4]@, NLP (@CITE[1]@; @CITE[2]@ and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (, random deletion/swap/insertion (@CITE[2]@; @CITE[6]@, generation (@CITE[3]@, etc. Data augmentation has also been proven to be useful in the cross-lingual settings (@CITE[4]@; ; @CITE[7]@; @CITE[8]@; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available.\\nReferences:ID: 1. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 2. Title: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\\nID: 3. Title: The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages\\nID: 4. Title: FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding\\nID: 5. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 6. Title: FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding\\nID: 7. Title: DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks\\nID: 8. Title: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition<|eot_id|>'},\n",
       " 1114: {'predict': 'Data augmentation has been widely used in many computer vision and natural language processing tasks to increase the size of training data. In computer vision, data augmentation techniques such as rotation, flipping, and cropping are commonly used to enhance the robustness of the model to small perturbations in images (Geirhos et al., 2020). In natural language processing, data augmentation methods can be broadly categorized into two types: (1) character-level methods, including word insertion, deletion, substitution, and random word shuffling (@CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[3]@;, and (2) sentence-level methods, including back-translation (Sennrich et al., 2016; @CITE[1]@, paraphrasing (Zhu et al., 2019), and text generation (@CITE[3]@. Our work focuses on the character-level methods. The main advantage of character-level data augmentation is that it can be used to generate adversarial examples without requiring any knowledge of the model architecture, which makes it more practical for black-box attacks.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Adversarially Robust Generalization Requires More Data\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 5. Title: T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack',\n",
       "  'ground_truth': '\\n\\nData augmentation has been extensively studied in NLP tasks for improving the robustness (@CITE[3]@; Fadaee et al., 2017; ). Similar to adversarial attack, early works mostly try heuristic rules to revise the input data for augmentation, such as synonym replacement (), grammar induction (@CITE[1]@), word insert and delete (). With the development of text generation techniques, back translation (; @CITE[2]@) and variant autoencoder (; @CITE[4]@) are used to augment new data. Besides, a surge of works (Hou et al., 2018; @CITE[5]@; ) focus on augmentation for specific tasks with special rules or models. Although they perform well, these methods have lost the generality. In this paper, we propose a new data augmentation framework VDA that utilizes a masked language model with Gaussian noise to augment virtual examples for improving the robustness. Our VDA is agnostic to downstream tasks, model architectures, and learning strategies.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Adversarially Robust Generalization Requires More Data\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 5. Title: T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack<|eot_id|>'},\n",
       " 3832: {'predict': 'The Transformer architecture has been widely used in various NLP tasks and has achieved state-of-the-art results in many of them. However, recent studies have revealed that the embeddings produced by the Transformer architecture are anisotropic. @CITE[5]@ find that the embeddings of the same word in different contexts are not isotropic, and the self-similarity of the embeddings in upper layers is lower than that in lower layers. @CITE[6]@ find that the embeddings in BERT are anisotropic, and the sentence embeddings are also anisotropic. @CITE[7]@ show that the embeddings in GPT-2 are anisotropic, and the self-similarity of the embeddings in upper layers is lower than that in lower layers. @CITE[1]@ show that the embeddings in GPT-2 are anisotropic, and the embeddings in lower layers are more anisotropic than those in upper layers. @CITE[2]@ show that the embeddings in BERT are anisotropic, and the sentence embeddings are also anisotropic. @CITE[3]@ show that the embeddings in BERT are anisotropic, and the outliers in the embeddings are closely related to the positional embeddings. @CITE[4]@ find that the embeddings in BERT are anisotropic, and the sentence embeddings are also anisotropic. @CITE[6]@ show that the embeddings in BERT are anisotropic, and the sentence embeddings are also anisotropic. The anisotropy of the embeddings can be alleviated by methods such as clipping the embeddings, and the isotropic embeddings can achieve better performance in sentence similarity tasks.\\nReferences:ID: 1. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 2. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 3. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 4. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 5. Title: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\\nID: 6. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 7. Title: Positional Artefacts Propagate Through Masked Language Model Embeddings',\n",
       "  'ground_truth': '\\n\\nThe anisotropy found by @CITE[6]@ when training a model for Natural Language Generation tasks through likelihood maximization is produced by the combination of the Zipfian nature of natural language and the log-likelihood loss function. This representation degradation makes the most frequent tokens concentrate in a hypercone in the embedding representation space, having a more sparse space for infrequent tokens. Contrastive learning methods seem to correct anisotropy to some extent as well, in addition to semantic isometry, which could make us think that the anisotropy was, in fact, part of the problem. However, (@CITE[4]@ realized through a series of experiments that there are certain biases in the BERT model, and that high anisotropy is not always equivalent to poor semantic isometry. Although the insights by @CITE[4]@ are certainly interesting, they seem to contradict to some degree previous work like (@CITE[6]@; @CITE[3]@; @CITE[1]@). On the other hand, @CITE[2]@ and Kovaleva et al. (2021) found that a big portion of the anisotropy of BERT comes from outlier dimensions, related to positional information. @CITE[5]@ showed that, despite BERT embeddings having global anisotropy, each cluster in the embedding space is isotropic, and that this local isotropy could be enough for Transformer models to achieve their full representation power. This hypothesis is supported by recent empirical results from (@CITE[7]@). If the anisotropy comes from the existence of different clusters, and these clusters encode non-semantic information like token frequency, this can be matched with the biases described by @CITE[4]@ and the representation degeneration by @CITE[6]@. As it can be observed, there is no consensus in the literature regarding the cause of the poor performance of Transformer embeddings in semantic tasks, and there is also no consensus about the reasons for the anisotropy observed in these embedding spaces. This disagreement is amplified by the fact that there is not a standard method for evaluating anisotropy; for example, @CITE[3]@ evaluates it at the word level, while Jiang et al. (2022) consider the sentence level, by averaging the word embeddings. We therefore find that there is room for research and reflection on these aspects.\\nReferences:ID: 1. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 2. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 3. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 4. Title: Representation Degeneration Problem in Training Natural Language Generation Models\\nID: 5. Title: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\\nID: 6. Title: On the Sentence Embeddings from Pre-trained Language Models\\nID: 7. Title: Positional Artefacts Propagate Through Masked Language Model Embeddings<|eot_id|>'},\n",
       " 3477: {'predict': 'Several datasets have been proposed for the KPG task. Most of them were built on academic papers and news articles. For example, @CITE[4]@; @CITE[4]@; ; @CITE[2]@; @CITE[3]@; @CITE[5]@; @CITE[1]@. The datasets are mainly focused on news articles and academic papers. Recently, @CITE[4]@ proposed KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. However, these datasets are not suitable for the KPG task because they are based on news articles and academic papers, and the keyphrases are not annotated at the document level. To the best of our knowledge, our dataset is the first one to provide keyphrases for each document in the open domain.\\nReferences:ID: 1. Title: KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents\\nID: 2. Title: KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents\\nID: 3. Title: Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization\\nID: 4. Title: Open Domain Web Keyphrase Extraction Beyond Language Modeling\\nID: 5. Title: Deep Keyphrase Generation',\n",
       "  'ground_truth': '\\n\\nThere have been numerous efforts in the direction of curating large-scale datasets for either keyphrase extraction or keyphrase generation. In the non-scholarly domain, the datasets such as KPTimes (), JPTimes (), KPCrowd (@CITE[3]@), DUC-2001 () were curated from news articles, and OpenKP () was mined from webpages from a search engine. In the scholarly domain, there has been ongoing research with the datasets such as Krapivin (@CITE[5]@), Inspec (@CITE[4]@), SemEval (), NUS (Nguyen and Kan, 2007), KP20k (@CITE[1]@), and OAGK (@CITE[2]@).\\nReferences:ID: 1. Title: KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents\\nID: 2. Title: KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents\\nID: 3. Title: Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization\\nID: 4. Title: Open Domain Web Keyphrase Extraction Beyond Language Modeling\\nID: 5. Title: Deep Keyphrase Generation<|eot_id|>'},\n",
       " 4041: {'predict': 'Autoregressive language models generate text in a left-to-right manner. They have been widely adopted in various applications such as machine translation (@CITE[3]@; Bahdanau et al., 2015), dialogue generation (Vinyals et al., 2015), and language modeling (Radford et al., 2018;. The autoregressive language model is usually composed of an encoder and a decoder. The encoder encodes the input sequence into a fixed-size vector, and the decoder generates the output sequence one token at a time. The autoregressive model is trained by maximizing the likelihood of the output sequence given the input sequence. The Transformer architecture (@CITE[1]@) is a popular architecture for autoregressive language models. It is based on self-attention mechanism and does not use recurrence or convolutional neural network. The Transformer architecture has been widely adopted in various applications such as machine translation (@CITE[2]@; @CITE[3]@) and language modeling (@CITE[1]@).\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Sequence to Sequence Learning with Neural Networks\\nID: 3. Title: Neural Machine Translation by Jointly Learning to Align and Translate',\n",
       "  'ground_truth': '\\n\\nLanguage generation is mostly based on the autoregressive language modeling methodology. The generation of one word is conditioned on previously generated words, Qθ(xt|x<t), and the final probability of the sequence x is the product of probabilities of each step, Qθ(x) = Q t Qθ(xt|x<t). Early works build n-gram neural LMs (Bengio et al., 2000) and then RNN-based LMs (, and now Transformers (@CITE[3]@ have become the dominant architecture. Language generation models have either a decoder-only ( or an encoder-decoder architecture (;. In this work, we focus on decoder-only LMs. In recent years, many large-scale pre-trained decoder-only LMs have been introduced (@CITE[2]@; Brown et al., 2020; ; @CITE[1]@. They can be finetuned for downstream tasks and even perform surprisingly well in a zero-shot or few-shot manner. Despite the impressive performance, language degeneration is one of the key issues that remain to be solved.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: Sequence to Sequence Learning with Neural Networks\\nID: 3. Title: Neural Machine Translation by Jointly Learning to Align and Translate<|eot_id|>'},\n",
       " 3634: {'predict': 'Prior to our work, few-shot benchmarking in NLP has been mostly focused on the language understanding (LU) tasks (e.g., GLUE benchmark, SuperGLUE). However, these benchmarks are limited in terms of the number of tasks, the quality of the few-shot training data, and the lack of diverse evaluation settings. For example, True Few-Shot Learning with Language Models (TFL) (@CITE[2]@) only contains 4 tasks and uses a single, arbitrary split of the training data. CrossFit (@CITE[4]@) is a more comprehensive benchmark that contains 160 diverse tasks, but it only uses a single few-shot training example per task. FLEX (@CITE[3]@) is a more comprehensive benchmark that contains 10 tasks and 4 evaluation settings. However, it only uses a single few-shot training example per task. Adversarial GLUE (@CITE[1]@) is a benchmark for evaluating the robustness of language models to adversarial attacks, but it only contains 9 tasks. In contrast, our benchmark contains 25 tasks, with 4 tasks in the original SuperGLUE benchmark, and 21 tasks that are new to the field of few-shot benchmarking. We also provide a more comprehensive evaluation setup, with 6 different few-shot training settings, 4 different evaluation settings, and 4 different few-shot training examples per task.\\nReferences:ID: 1. Title: True Few-Shot Learning with Language Models\\nID: 2. Title: CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP\\nID: 3. Title: FLEX: Unifying Evaluation for Few-Shot NLP\\nID: 4. Title: Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models',\n",
       "  'ground_truth': '\\n\\nResearch in few-shot benchmarking has evolved in several directions. Schick and Schütze (2021) create FewGLUE by sampling small fixed-sized training datasets from SuperGLUE. Variance w.r.t to training dataset size and sampling strategy is not reported. Later works overcome these issues by exploring evaluation strategies by K-fold cross-validation (@CITE[3]@), bagging, and multi-splits, introduced in FewNLU (Zheng et al., 2022). Additionally, FewNLU explores correlations between performance on development and test sets and stability w.r.t. the number of runs. CrossFit (@CITE[1]@) studies cross-task generalization by unifying task formats and splitting tasks into training, development, and test sets. FLEX (@CITE[4]@) covers the best practices and provides a unified interface for different types of transfer and varying shot sizes. Finally, to the best of our knowledge, the only non-English dataset for few-shot benchmarking is Few-CLUE in Chinese (@CITE[2]@). TAPE is the first few-shot benchmark for Russian, which introduces variations at the data level by creating adversarial test sets.\\nReferences:ID: 1. Title: True Few-Shot Learning with Language Models\\nID: 2. Title: CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP\\nID: 3. Title: FLEX: Unifying Evaluation for Few-Shot NLP\\nID: 4. Title: Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models<|eot_id|>'},\n",
       " 5417: {'predict': 'Existing factchecking systems often require a specific claim as input. For example, @CITE[1]@ and @CITE[2]@ require users to specify a specific claim to verify. @CITE[4]@ and @CITE[3]@ require users to specify a specific claim to generate evidence for. Our FACTSCORE framework can be used in conjunction with these systems to evaluate the factual precision of their output. For example, after generating a claim and evidence for a given query, FACTSCORE can be used to evaluate the factual precision of the generated claim and evidence. FACTSCORE can also be used to evaluate the factual precision of long-form text generation models, which have become increasingly popular. FACTSCORE can be used to evaluate the factual precision of the generated text, and can also be used to evaluate the factual precision of the generated claims and evidence.\\nReferences:ID: 1. Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\\nID: 2. Title: Evaluating Verifiability in Generative Search Engines\\nID: 3. Title: Measuring Attribution in Natural Language Generation Models\\nID: 4. Title: Generating Scientific Claims for Zero-Shot Scientific Fact Checking',\n",
       "  'ground_truth': '\\n\\nWe introduce FACTSCORE, a new evaluation of an LM that considers the factual precision of atomic facts generated by the LM. We perform human evaluations to calculate FACTSCOREs of the state-of-the-art LMs (Section 3.3) and discuss results (Section 3.4). FACTSCORE allows rigorous and fine-grained evaluation of factual precision, but is time-consuming and costly, motivating automatic evaluation in Section 4. Definition FACTSCORE is based on two key ideas. Key idea 1: Atomic fact as a unit. Long-form text consists of many pieces of information that can each be either true or false. Prior work has explored using a sentence as a unit; however, even a single sentence is a mix of supported and unsupported facts, e.g., in 40% of the cases with ChatGPT. Previous and concurrent work either (1) defines an additional label of partial support (@CITE[4]@; whose definition may be subjective and can lead to low agreement, or (2) takes the strictest definition of support that requires every piece of information to be supported (@CITE[1]@; @CITE[3]@, which ignores the partial support cases, e.g., assigning 0.0 to both generations in Figure 1 even though the first generation is considerably more accurate than the second. In this paper, we define an atomic fact as a short sentence conveying one piece of information (examples in Figure 1), similar to summarization content units (@CITE[2]@. An atomic fact is a more fundamental unit than a sentence for a piece of information and provides a more fine-grained evaluation, e.g., in Figure 1, rating the first generation higher than the second. Key Idea 2: Factual precision as a function of a given knowledge source. Prior work often considers factual precision as a single global truth (Manakul et al., 2023). In contrast, we adopt a perspective that the truthfulness of a statement should depend on a particular knowledge source that end users consider to be trustworthy and reliable. Therefore, instead of whether an atomic fact is globally true or false, we consider whether it is supported by a given source of knowledge. This has been used in the fact verification literature ( where conflict of information between different sources is relatively common. Definition. Let M be a language model to be evaluated, X be a set of prompts, and C be a knowledge source. Consider a response y = Mx for x ∈X and Ay, a list of atomic facts in y. A FACTSCORE of M is defined as follows. f(y) = I[a is supported by C], FACTSCORE(M) = Ex∈X[f(Mx)|Mx responds]. Mx responds means M did not abstain from responding to the prompt x. This definition assumes the following: 1. Whether or not an atomic fact is supported by C is undebatable. 2. Every atomic fact in Ay has an equal weight of importance, following Krishna et al. (2023). 3. Pieces of information in C do not conflict or overlap with each other. In the rest of the paper, we propose to use people biographies as X and Wikipedia as C because they satisfy these assumptions to a reasonable degree (Section 3.3). We discuss in which cases these assumptions hold or may not hold in more detail in the Limitation section. FACTSCORE considers precision but not recall, e.g., a model that abstains from answering too often or generates text with fewer facts may have a higher FACTSCORE, even if these are not desired. We leave the evaluation of factual recall for future work (more discussion in the Limitation section). Studied LMs We evaluate three LMs (referred to as LMSUBJ, subject): InstructGPT (text-davinci-003, updated from Ouyang et al.\\nReferences:ID: 1. Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories\\nID: 2. Title: Evaluating Verifiability in Generative Search Engines\\nID: 3. Title: Measuring Attribution in Natural Language Generation Models\\nID: 4. Title: Generating Scientific Claims for Zero-Shot Scientific Fact Checking<|eot_id|>'},\n",
       " 6437: {'predict': 'Speech translation has attracted much attention in recent years. @CITE[3]@ propose a consecutive decoding framework to decouple the speech recognition and machine translation tasks, and @CITE[4]@ propose a triple supervision framework to improve the performance of speech translation. @CITE[5]@ propose a knowledge distillation framework to improve the performance of speech translation. @CITE[1]@ propose a pre-training framework to align the representations of speech and text, and @CITE[2]@ propose a multi-task learning framework to improve the performance of speech translation. @CITE[2]@ propose a multi-task learning framework to improve the performance of speech translation. @CITE[1]@ propose a multi-task learning framework to improve the performance of speech translation. @CITE[5]@ propose a multi-task learning framework to improve the performance of speech translation. @CITE[3]@ propose a multi-task learning framework to improve the performance of speech translation.\\nReferences:ID: 1. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 2. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 3. Title: End-to-End Speech Translation with Knowledge Distillation\\nID: 4. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 5. Title: SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training',\n",
       "  'ground_truth': '\\n\\nWithout using textual tran- scriptions during inference or training, translating audio directly into the target language is very meaningful for languages that do not have a written form. Bérard et al. (2016) first proposed an end-to-end encoder-decoder architecture for such direct speech-to-text translation. Later, novel models (Di Gangi et al., 2019b; @CITE[1]@,b; Zheng et al., 2021) and training techniques, such as multi-task learning (@CITE[3]@; ; @CITE[5]@), knowledge distillation (@CITE[2]@; @CITE[5]@), and pre-training methods (Zheng et al., 2021; @CITE[4]@), were developed to improve end-to-end performance. However, these training methods often rely on the use of source text or knowledge from the pre-trained models. Without using transcripts or pretraining, proposed the parameterized distance penalty to better model speech locality in the self-attention structure and provided results on ST benchmarks covering 23 languages.\\nReferences:ID: 1. Title: Consecutive Decoding for Speech-to-text Translation\\nID: 2. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 3. Title: End-to-End Speech Translation with Knowledge Distillation\\nID: 4. Title: \"Listen, Understand and Translate\": Triple Supervision Decouples End-to-end Speech-to-text Translation\\nID: 5. Title: SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training<|eot_id|>'},\n",
       " 3117: {'predict': 'Extractive QA is a task of identifying the span in the given context passage that answers a question. SQuAD (@CITE[1]@) and NewsQA (@CITE[4]@) are two of the most popular datasets for this task. In addition to these datasets, HotpotQA (@CITE[5]@), TriviaQA (@CITE[3]@), and QuAC (@CITE[2]@) are more challenging ones that require more reasoning and context understanding to answer the questions. Our proposed method can be applied to all of these datasets.\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: NewsQA: A Machine Comprehension Dataset\\nID: 3. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 4. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 5. Title: QuAC : Question Answering in Context',\n",
       "  'ground_truth': '\\n\\nMuch of the existing work on question answering, including reading comprehension (, 2018; @CITE[2]@; @CITE[3]@, open-domain QA (@CITE[4]@; and dialog-based QA (@CITE[5]@, assumes that questions have unique answers. Min et al. (2020) relax this assumption and propose a task that aims at identifying all possible short answers to the ambiguous subset of the open-domain version of the NQ dataset, denoted NQ-OPEN (@CITE[4]@; @CITE[4]@. The AMBIGQA dataset constructed by @CITE[1]@ serves as a building block of the present work...\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: NewsQA: A Machine Comprehension Dataset\\nID: 3. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 4. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 5. Title: QuAC : Question Answering in Context<|eot_id|>'},\n",
       " 2809: {'predict': 'Dialogue safety is an emerging research area, focusing on ensuring safety in conversations. The work of @CITE[3]@ is closely related to our work as they investigate the use of adversarial examples to evaluate the robustness of a chatbot. However, they only consider the safety of the chatbot itself, whereas our work considers both the safety of the chatbot and the user. Additionally, they do not consider the impact of the chatbot on the user, whereas we do. Furthermore, they do not consider the safety of the conversation, whereas we do. Other work on dialogue safety includes the use of adversarial examples to evaluate the safety of chatbots (@CITE[1]@; @CITE[4]@; @CITE[5]@; ; @CITE[2]@; @CITE[2]@; @CITE[2]@. However, these works do not consider the impact of the chatbot on the user and do not consider the safety of the conversation.\\nReferences:ID: 1. Title: Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP\\nID: 2. Title: Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack\\nID: 3. Title: Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations\\nID: 4. Title: The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity\\nID: 5. Title: ProsocialDialog: A Prosocial Backbone for Conversational Agents',\n",
       "  'ground_truth': '\\n\\nMany datasets and methods have been proposed to make dialog systems safer and conform to norms. This includes avoiding bias (@CITE[4]@) or hateful/offensive speech (@CITE[2]@; ). In previous work (@CITE[3]@), we collected a dataset of users asking versions of \"are you a robot?\". This tries to help build tools to avoid explicit anthropomorphic deception (the machine is explicitly asked, but does not confirm it is non-human), while here we focus on implicit deception. Other work has also tried to collect corrected socially problematic machine-written utterances (@CITE[5]@) or broad characterizations of ethics (Hendrycks et al., 2020). Datasets have been built into automated tools for approximating the safety of a system. For example, @CITE[1]@ made a suite of measurements of systems\\' safety. They discussed an \"Imposter Effect\" where models give off false impressions of identity, but they do not produce metrics for this effect, in part due to limited available datasets. Robots-Dont-Cry adds to available safety/norm-setting datasets. A related concept is Cohen et al.\\'s (2022) evaluation of \"role consistency\" for their large dialog system. They evaluated two agents (e.g. an agent that pretended to be Mt. Everest), finding about 90% consistency. This is similar to our measurements, though we focus more on anthropomorphism and characterizing data sources.\\nReferences:ID: 1. Title: Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP\\nID: 2. Title: Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack\\nID: 3. Title: Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations\\nID: 4. Title: The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity\\nID: 5. Title: ProsocialDialog: A Prosocial Backbone for Conversational Agents<|eot_id|>'},\n",
       " 5442: {'predict': \"Documenting training data has become a pressing concern in the field of NLP. Several works have documented the origin of datasets (@CITE[1]@; @CITE[3]@, the quality of the data (@CITE[4]@; @CITE[5]@; @CITE[2]@, and the dynamics of the data during training (@CITE[2]@. However, these efforts have primarily focused on the quality of the data, without considering the impact of the data on the model. Our work is the first to investigate the role of training data in the model's behavior, and to provide recommendations for documenting training data. We also identify the importance of documenting the training data for large-scale language models, and highlight the need for a standardized documentation framework.\\nReferences:ID: 1. Title: Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\nID: 4. Title: Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\\nID: 5. Title: Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\",\n",
       "  'ground_truth': '\\n\\nTraining on large text corpora such as BookCorpus (@CITE[5]@, C4 (@CITE[3]@ and the Pile (@CITE[4]@ has been instrumental in extending the capability of large language models. Yet, in contrast to smaller datasets, these large corpora are less carefully curated. Besides a few attempts at documenting large datasets (e.g., C4; @CITE[1]@ or laying out diagnostic techniques for their quality (@CITE[2]@, these corpora and their use in large models is less understood. Our focus on books in this work is an attempt to empirically map the information about books that is present in these models.\\nReferences:ID: 1. Title: Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: The Pile: An 800GB Dataset of Diverse Text for Language Modeling\\nID: 4. Title: Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\\nID: 5. Title: Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics<|eot_id|>'},\n",
       " 5637: {'predict': 'The label-aware classification task has been studied extensively in the field of text classification. In this task, the model is trained to predict the class labels and their relations with the input text. In the zero-shot setting, the model is expected to generalize to unseen classes. Several methods have been proposed for this task, including the use of semantic class representations (@CITE[5]@; ; @CITE[3]@; @CITE[3]@; @CITE[6]@; @CITE[6]@; @CITE[2]@; @CITE[1]@; @CITE[4]@. In the multi-label setting, the model is trained to predict the label set of the input text. In the zero-shot setting, the model is expected to generalize to unseen labels. Several methods have been proposed for this task, including the use of label descriptions (@CITE[5]@; @CITE[6]@; @CITE[2]@; @CITE[1]@; @CITE[4]@ and the use of label graphs (@CITE[5]@; @CITE[6]@; @CITE[2]@; @CITE[1]@; @CITE[4]@.\\nReferences:ID: 1. Title: Deep Extreme Multi-label Learning\\nID: 2. Title: GILE: A Generalized Input-Label Embedding for Text Classification\\nID: 3. Title: Train Once, Test Anywhere: Zero-Shot Learning for Text Classification\\nID: 4. Title: Zero-Shot Learning with Common Sense Knowledge Graphs\\nID: 5. Title: Zero-Shot Learning with Common Sense Knowledge Graphs\\nID: 6. Title: Reading Wikipedia to Answer Open-Domain Questions',\n",
       "  'ground_truth': '\\n\\nLabel-aware classification techniques enhance classifiers by incorporating additional label information alongside standard input features. These methods use various types of label information, including label hierarchy (, label dependency or correlation (@CITE[4]@; ;, or label semantic information. The datasets in this paper lack label hierarchies, and label correlation is not practical in zero- and few-shot settings due to the need for data to learn the correlations. However, label semantic information is useful for zero- and few-shot applications. In the absence of labeled examples, models require some sort of information to scale to new classes. Early approaches gravitated toward representation-based or late interaction models and usually assumed short label descriptions (. Recent work (Schick and Schütze, 2021; @CITE[2]@; @CITE[6]@; @CITE[3]@; @CITE[5]@ has shown the promise of using textual entailment as a format for data-poor NLP tasks. These methods convert NLP tasks into entailment problems then leverage trained entailment models. This approach has yet to be explored for datasets with long class descriptions. Other label-aware classification methods use combinations of different types of label information, such as commonsense knowledge graphs for labels (@CITE[3]@; heterogeneous label graphs (@CITE[1]@; ; and word embeddings, class descriptions, class hierarchy, and knowledge graphs (.\\nReferences:ID: 1. Title: Deep Extreme Multi-label Learning\\nID: 2. Title: GILE: A Generalized Input-Label Embedding for Text Classification\\nID: 3. Title: Train Once, Test Anywhere: Zero-Shot Learning for Text Classification\\nID: 4. Title: Zero-Shot Learning with Common Sense Knowledge Graphs\\nID: 5. Title: Zero-Shot Learning with Common Sense Knowledge Graphs\\nID: 6. Title: Reading Wikipedia to Answer Open-Domain Questions<|eot_id|>'},\n",
       " 2897: {'predict': 'In graph-based summarization, a graph is constructed to model the relationships among sentences. LexRank (@CITE[1]@) is a classic graph-based summarization model that leverages the eigenvector centrality to measure the importance of sentences. Later, researchers have proposed to construct a graph based on sentence embeddings to capture the inter-sentence relationships (@CITE[3]@; @CITE[2]@). Some works (@CITE[4]@; @CITE[2]@) have leveraged the graph neural networks (GNNs) to capture the inter-sentence relationships. However, they only focus on the inter-sentence relationships and ignore the intra-sentence relationships. To address this issue, @CITE[4]@ has proposed to model the intra-sentence relationships by using hypergraphs. Hypergraph attention networks (HyperGAT) (@CITE[2]@) has also been proposed to capture the inter-sentence relationships by using hypergraphs. However, they only focus on the inter-sentence relationships and ignore the intra-sentence relationships. To capture both the intra- and inter-sentence relationships, @CITE[4]@ has proposed to construct a hypergraph to model the relationships among words and sentences. Recently, some works (@CITE[2]@; @CITE[4]@) have leveraged the graph attention networks (GAT) to capture the relationships among sentences. However, they only focus on the inter-sentence relationships and ignore the intra-sentence relationships. To capture both the intra- and inter-sentence relationships, @CITE[4]@ has proposed to construct a hypergraph to model the relationships among words and sentences. However, they have only focused on the extractive summarization. To the best of our knowledge, we are the first to propose to leverage the hypergraph attention networks to perform abstractive summarization.\\nReferences:ID: 1. Title: LexRank: Graph-based Lexical Centrality as Salience in Text Summarization\\nID: 2. Title: Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks\\nID: 3. Title: Be More with Less: Hypergraph Attention Networks for Inductive Text Classification\\nID: 4. Title: Multiplex Graph Neural Network for Extractive Text Summarization',\n",
       "  'ground_truth': '\\n\\nGraph-based models have been exploited for ex- tractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences (; Zheng and La- pata, 2019; Dong et al., 2020). Researchers also explore supervised graph neural networks for sum- marization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. @CITE[2]@ proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. @CITE[4]@ leveraged topical information by building topic- sentence graphs. Recently, @CITE[3]@ pro- posed to construct word-document heterogeneous graphs and use word nodes as the intermediary be- tween sentences. @CITE[1]@ proposed to use multiplex graph to consider different sentence relations. Our paper follows this line of work on developing novel graph neural networks for sin- gle document extractive summarization. The main difference is that we construct a hypergraph from Figure 2: (a) The overall architecture of HEGEL. (b) Two-phase message passing in hypergraph transformer layer a document that could capture high-order cross- sentence relations instead of pairwise relations, and fuse different types of sentence dependencies, in- cluding section structure, latent topics, and key- words coreference.\\nReferences:ID: 1. Title: LexRank: Graph-based Lexical Centrality as Salience in Text Summarization\\nID: 2. Title: Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks\\nID: 3. Title: Be More with Less: Hypergraph Attention Networks for Inductive Text Classification\\nID: 4. Title: Multiplex Graph Neural Network for Extractive Text Summarization<|eot_id|>'},\n",
       " 7383: {'predict': 'Formality analysis has been a long-standing research topic in natural language processing. There are many approaches to classify the formality of text, including statistical approaches (@CITE[8]@; @CITE[5]@; ; @CITE[6]@ and neural approaches (@CITE[4]@; @CITE[3]@; @CITE[7]@; @CITE[2]@. However, the majority of the existing work focuses on English and there is a lack of large-scale formality-annotated datasets for other languages. This is the reason why we propose to build a multilingual formality dataset. For example, @CITE[1]@ propose a dataset for formality transfer from English to other languages, which is useful for formality transfer but not for formality classification. There are also some works on formality-controlled machine translation (MT) (; @CITE[2]@, which is a special case of formality classification. However, the existing datasets for formality-controlled MT are either small-scale or only contain English source sentences, which are not suitable for multilingual formality classification.\\nReferences:ID: 1. Title: Detecting Text Formality: A Study of Text Classification Approaches\\nID: 2. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 3. Title: Politeness Transfer: A Tag and Generate Approach\\nID: 4. Title: Improving Formality Style Transfer with Context-Aware Rule Injection\\nID: 5. Title: Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer\\nID: 6. Title: Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings\\nID: 7. Title: Controlling Neural Machine Translation Formality with Synthetic Supervision\\nID: 8. Title: CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality',\n",
       "  'ground_truth': '\\n\\nPrevious work in formality analysis has focused on formality classification (@CITE[8]@; @CITE[7]@; ; ), formality style transfer in English (;, 2020; ; @CITE[1]@; @CITE[4]@; @CITE[3]@), and in the multilingual setting (Korotkova et al., 2019; Briakou et al., 2021b; ). Formality-sensitive machine translation to control the generation of machine translation models to target formality has received attention in recent years (Sennrich et al., 2016; Niu et al., 2017; Feely et al., 2019; ; @CITE[5]@; ) and benchmark MT datasets and models have been published (; Rippeth et al., 2022). Recently, several datasets with formality annotations have been introduced in English (@CITE[2]@; ; ). XFORMAL (Briakou et al., 2021b), TAOCD (@CITE[6]@) and InFormal () extended formality style transfer to the multilingual setting. In the following sections, we describe our experiments and results for different languages.\\nReferences:ID: 1. Title: Detecting Text Formality: A Study of Text Classification Approaches\\nID: 2. Title: Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer\\nID: 3. Title: Politeness Transfer: A Tag and Generate Approach\\nID: 4. Title: Improving Formality Style Transfer with Context-Aware Rule Injection\\nID: 5. Title: Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer\\nID: 6. Title: Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings\\nID: 7. Title: Controlling Neural Machine Translation Formality with Synthetic Supervision\\nID: 8. Title: CoCoA-MT: A Dataset and Benchmark for Contrastive Controlled MT with Application to Formality<|eot_id|>'},\n",
       " 6106: {'predict': 'FOD (@CITE[1]@; @CITE[9]@; @CITE[4]@; @CITE[7]@; @CITE[3]@; @CITE[5]@; @CITE[2]@; @CITE[6]@) aims to learn cross-modal alignment from millions of image-text pairs. Most FOD methods (@CITE[8]@; @CITE[8]@; @CITE[6]@) focus on the alignment between image features and textual words, which is usually achieved by cross-modal attention. However, the alignment between image features and textual words is not always consistent. For example, a person in an image can be described by multiple words. Therefore, it is not enough to only consider the alignment between image features and textual words. In this paper, we propose a novel FOD method which considers the alignment between image features and textual words as well as the alignment between image features and textual sentences. Our method first constructs a visual dictionary and textual dictionary to represent the semantics of images and texts, and then uses the constructed dictionaries to align image features with textual words and textual sentences. In addition, we propose a novel pre-training task to align image features with textual words and textual sentences, which is based on masked language modeling. The alignment between image features and textual words and textual sentences is achieved by the alignment between image features and textual words, and the alignment between image features and textual words is achieved by the alignment between image features and textual words and textual sentences. Our method is not only consistent with the alignment between image features and textual words, but also consistent with the alignment between image features and textual sentences.\\nReferences:ID: 1. Title: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers\\nID: 2. Title: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\\nID: 3. Title: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\\nID: 4. Title: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\\nID: 5. Title: Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts\\nID: 6. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 7. Title: Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\\nID: 8. Title: Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\\nID: 9. Title: Pre-Training with Whole Word Masking for Chinese BERT',\n",
       "  'ground_truth': '\\n\\nThe text encoder can be switched between fusion-based pattern and fusion-free pattern. We utilize three complementary learning strategies to jointly train FOD: Fusion-free learning with Image-Text Contrastive learning (ITC), Fusion-based learning with Image-Text Matching (ITM) and Mask Language Modeling (MLM), and Cross-modal Knowledge Transfer with Image-to-Multimodal (I2M) and Text-to-Multimodal (T2M) learning. 2020, 2021b; ; Zhang et al., 2021;, 2021; @CITE[4]@; ; @CITE[5]@; @CITE[6]@; @CITE[1]@; @CITE[3]@. These models perform well on vision-language understanding tasks due to the ability of capturing deep cross-modal features. However, for vision-language retrieval tasks, the fusion-based methods need to encode all the possible image-text pairs to find the most relevant candidate, resulting in extremely high time. The second category, dual-based model, utilizes a visual encoder and a text encoder to separately encode images and text, while the interaction between images and text is modeled by cosine similarity or linear projection (; @CITE[8]@; @CITE[9]@. Although dual-based models are effective for retrieval tasks since features can be pre-computed and cached offline, the shallow interaction is insufficient to tackle the vision-language understanding tasks that require complex VL reasoning. Besides, training a dual-based model often necessitates a large number of image-text pairs (e.g. 300M for Filip (@CITE[9]@ and 1.8 B for ALIGN (@CITE[8]@). Recently, some researchers have devoted themselves to investigating a unified model that is well-performed on vision-language understanding tasks while maintaining the efficiency towards retrieval tasks (@CITE[7]@; @CITE[2]@; ; @CITE[3]@;. To achieve this, one line of the works leverage knowledge distillation, in which a fusion-encoder model is pre-trained as a teacher model to guide the training of a dual-encoder model (@CITE[7]@, but the performance is inevitably limited by the teacher model. Other efforts attempt to train a modality-agnostic encoder with shared parameters, which can be used as either a fusion encoder or a dual encoder (; @CITE[3]@. Despite the benefits of modeling all the modalities into a single encoder, it is hard to fully train such a huge model and a large number of training samples in different modalities are required. Different from these methods, we incorporate a detachable cross-modal fusion module into a dual-encoder architecture, which can easily remove the fusion module in inference and switch to a fusion-free model. More importantly, our model does not rely on teacher models or massive data in other modalities.\\nReferences:ID: 1. Title: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers\\nID: 2. Title: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\\nID: 3. Title: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\\nID: 4. Title: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\\nID: 5. Title: Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts\\nID: 6. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 7. Title: Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\\nID: 8. Title: Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\\nID: 9. Title: Pre-Training with Whole Word Masking for Chinese BERT<|eot_id|>'},\n",
       " 688: {'predict': 'QFS has recently received increasing attention. Most of the existing works are based on the extractive methods, which select the most relevant sentences from the documents to form a summary. For example, @CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[6]@; @CITE[1]@; @CITE[7]@; @CITE[3]@; ; @CITE[1]@ select the most relevant sentences from the documents to form a summary. However, the extractive methods are unable to capture the semantic relationships between sentences and produce summaries that are less coherent. To address this problem, some researchers have proposed to use abstractive methods, which can generate a summary that is more coherent and fluent. For example, @CITE[3]@; ; @CITE[6]@; @CITE[1]@; @CITE[5]@; @CITE[7]@; ; @CITE[1]@; ; @CITE[4]@ use the seq2seq model to generate a summary. However, these models are not able to capture the semantic relationships between sentences and produce summaries that are less coherent.\\nReferences:ID: 1. Title: CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management\\nID: 2. Title: Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management\\nID: 5. Title: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\\nID: 6. Title: Teaching Machines to Read and Comprehend\\nID: 7. Title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model',\n",
       "  'ground_truth': '\\n\\nMore recently @CITE[5]@ propose a coarse-to-fine framework that leverages distant supervision from question answering to extract summary-worthy content. Abstractive QFS has received significantly less attention. This is due to generation models being particularly data-hungry (@CITE[3]@; @CITE[5]@) and the scarcity of QFS training data. The increasing availability of pretrained models has prompted the development of pipeline-style frameworks for QFS which use resources from a wider range of NLP tasks. For example, @CITE[5]@ fine-tune BART (@CITE[2]@) on CNN/DailyMail (@CITE[1]@), a single-document summarization dataset, and generate abstracts for QFS by iteratively summarizing paragraphs to a budget. They learn a query model for paragraph selection based on a plethora of QA and machine reading datasets (@CITE[6]@; Rajpurkar et al., 2016). Similarly, @CITE[7]@ fine-tune BERTSUM on CNN/DailyMail, and propose a three-stage system which uses supervision from QFS data (typically reserved for evaluation) and related QA and paraphrase identification tasks. We also focus on abstractive QFS, however, we do not assume access to any additional training resources over and above generic summarization datasets, even for query modeling. Moreover, our system is able to generate long QFS abstracts all at once, instead of iteratively creating bullet-style summaries which often lack coherence.\\nReferences:ID: 1. Title: CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management\\nID: 2. Title: Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization\\nID: 3. Title: Hierarchical Transformers for Multi-Document Summarization\\nID: 4. Title: CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management\\nID: 5. Title: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\\nID: 6. Title: Teaching Machines to Read and Comprehend\\nID: 7. Title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model<|eot_id|>'},\n",
       " 3916: {'predict': 'Other editing-based work has been proposed for text generation. @CITE[4]@ propose a sequence tagging approach to predict edit operations for text generation. @CITE[8]@ propose a model to learn continuous latent space of vector representations to capture the underlying semantic information of the document editing process. @CITE[1]@ propose a flexible text editing approach for generation, which decomposes the text editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input. @CITE[3]@ propose a unified framework to convert all text-based language problems into a text-to-text format. @CITE[6]@ propose a method to edit the input text by gradually changing it into a target text, which is trained by maximizing the likelihood of the target text. @CITE[5]@ propose a method to generate sentences conditioned on a set of keywords by iteratively refining the sentence through a Markov chain Monte Carlo (MCMC) algorithm. @CITE[2]@ propose a framework to unsupervised text generation by learning from search, which alternates between search and learning to generate high-quality sentences. @CITE[7]@ propose a method to generate text by iteratively refining the text through a Markov chain Monte Carlo (MCMC) algorithm.\\nReferences:ID: 1. Title: Encode, Tag, Realize: High-Precision Text Editing\\nID: 2. Title: Variational Inference for Learning Representations of Natural Language Edits\\nID: 3. Title: Felix: Flexible Text Editing Through Tagging and Insertion\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\\nID: 6. Title: Denoising Diffusion Probabilistic Models\\nID: 7. Title: CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling\\nID: 8. Title: Unsupervised Text Generation by Learning from Search',\n",
       "  'ground_truth': '\\n\\nOther work on editing has included editing for sentence fusion (@CITE[4]@, in which one can perform minimal operations to join two sentences together grammatically. Furthermore, with text style transfer in which the difference between sentences in different styles (e.g. positive vs negative) can be relatively minimal (@CITE[7]@; @CITE[8]@. Furthermore, Faltings et al. (2021) explored natural language conditioned editing as a means for controllable text generation using a T5-based (@CITE[5]@ sequence to sequence model. Also related to our work is text morphing (Huang et al., 2018), in which they look at an edit-based model to interpolate between two sentences. We also note that diffusion models (@CITE[1]@; @CITE[3]@ can be formulated as a flavor of editing models, where the model learns to iteratively edit some representation of information in order to construct a final version of said representation. Editing-based lexically-constrained generation has been explored by (@CITE[2]@ and @CITE[6]@ propose a search-based method for improving conditional text generation by way of heuristics — an approach similar in spirit to editing, however often directed towards a specific task (which benefits from specific constraints), rather than inherently modeling sequential editing processes. We also note work...\\nReferences:ID: 1. Title: Encode, Tag, Realize: High-Precision Text Editing\\nID: 2. Title: Variational Inference for Learning Representations of Natural Language Edits\\nID: 3. Title: Felix: Flexible Text Editing Through Tagging and Insertion\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\\nID: 6. Title: Denoising Diffusion Probabilistic Models\\nID: 7. Title: CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling\\nID: 8. Title: Unsupervised Text Generation by Learning from Search<|eot_id|>'},\n",
       " 6682: {'predict': 'In the context of grammar learning, independence assumptions have been studied in various ways. In statistical parsing, @CITE[1]@ first proposed a generative model of lexicalised context-free grammar where words struggle to modify each other. @CITE[3]@ and @CITE[2]@ further proposed a generative model of lexicalised context-free grammar where words fluctuate randomly in their selectional preferences. @CITE[5]@ proposed a generative model of lexicalised context-free grammar where the speaker fleshes out each word’s syntactic and conceptual structure without regard to the implications for the hearer. In the context of neural grammar learning, @CITE[4]@ proposed a neural parameterization of quasi-synchronous grammar which enables parameter sharing over the combinatorial space of derivation rules. @CITE[6]@ proposed a neural parameterization of quasi-synchronous grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering. @CITE[1]@ proposed a neural parameterization of lexicalized PCFGs without making implausible independence assumptions. In the context of grammar induction, @CITE[5]@ proposed a generative model of mildly context-sensitive grammar where words struggle to modify each other. @CITE[2]@ proposed a generative model of mildly context-sensitive grammar where words fluctuate randomly in their selectional preferences. @CITE[4]@ proposed a generative model of mildly context-sensitive grammar where the speaker fleshes out each word’s syntactic and conceptual structure without regard to the implications for the hearer. In this paper, we propose a generative model of mildly context-sensitive grammar where words fluctuate randomly in their selectional preferences. We show that our model performs better than the state-of-the-art grammar induction model in terms of unsupervised parsing performance.\\nReferences:ID: 1. Title: Three New Probabilistic Models for Dependency Parsing: An Exploration\\nID: 2. Title: Three Generative, Lexicalised Models for Statistical Parsing\\nID: 3. Title: Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars\\nID: 4. Title: Sequence-to-Sequence Learning with Latent Neural Grammars\\nID: 5. Title: Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints\\nID: 6. Title: Neural Bi-Lexicalized PCFG Induction',\n",
       "  'ground_truth': '\\n\\nIndependence assumptions are frequently made in grammar learning for tractability and scalability. Simple PCFGs assume independent generation of left and right children, thus resembling split-head dependency grammars (@CITE[1]@; @CITE[3]@; ; @CITE[6]@;. We have shown that trading expressiveness (of grammar formalism) for scalability is beneficial, and this idea could be applied to other complex grammar formalism of high parsing complexity, such as mildly context-sensitive grammars (@CITE[4]@, synchronized grammars (@CITE[2]@; ; Friedman et al., 2022; and lexicalized grammars (Zhu et al., 2020; @CITE[5]@.\\nReferences:ID: 1. Title: Three New Probabilistic Models for Dependency Parsing: An Exploration\\nID: 2. Title: Three Generative, Lexicalised Models for Statistical Parsing\\nID: 3. Title: Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars\\nID: 4. Title: Sequence-to-Sequence Learning with Latent Neural Grammars\\nID: 5. Title: Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints\\nID: 6. Title: Neural Bi-Lexicalized PCFG Induction<|eot_id|>'},\n",
       " 7027: {'predict': 'Existing works for cross-domain few-shot NER can be divided into two categories: meta-learning and non-meta-learning. Meta-learning methods (@CITE[4]@; @CITE[1]@; @CITE[3]@; @CITE[2]@; @CITE[2]@; @CITE[2]@; ; @CITE[2]@) train a shared model to learn to adapt to new domains with a few examples. Non-meta-learning methods (@CITE[2]@; @CITE[2]@; @CITE[2]@) learn the target domain model from the source domain model with a few examples. Our work belongs to the non-meta-learning category. The most relevant work to ours is (@CITE[2]@). They use a pre-trained BERT-based model as the source domain model and train a new model for the target domain with a few examples. The key difference between their work and ours is that they use the same model architecture for the source and target domains, while we use a different architecture for the source and target domains. We use a pre-trained BERT-based model for the source domain and a pre-trained T5-based model for the target domain, which is more suitable for the few-shot setting. Besides, we introduce a new contrastive learning method to learn the target domain model, which is more effective than their method.\\nReferences:ID: 1. Title: Prototypical Networks for Few-shot Learning\\nID: 2. Title: Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network\\nID: 3. Title: Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning\\nID: 4. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning',\n",
       "  'ground_truth': '\\n\\nRecently, few-shot NER tasks have attracted a lot of researches. For example, @CITE[1]@; @CITE[3]@; applied prototype networks to the few-shot NER task, while proposed StructShot, a model that learns class-specific features and extends intermediate representations to new domains. @CITE[2]@ proposed Container, a model based on contrastive learning to learn the relationship between entities of different categories for better understanding of new unseen classes. @CITE[4]@ devises two prompting mechanisms for better training data generation, which is heavily influenced by the prompt strategies and needs heavy computation. However, these methods suffer from serious label inconsistency issues in cross-domain scenarios. The knowledge they learned in the source domain cannot be directly applied to the target domain. Moreover, due to the complexity of NER tasks, these works need to design complex learning strategies to be applied to few-shot NER tasks. Unlike previous works, our work focuses on the knowledge of entity internal structure. As far as we know, we are the first to introduce entity structure and label information in data augmentation to solve cross-domain few-shot NER tasks, which may inspire the following exploration to study internal structure knowledge of entities.\\nReferences:ID: 1. Title: Prototypical Networks for Few-shot Learning\\nID: 2. Title: Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network\\nID: 3. Title: Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning\\nID: 4. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning<|eot_id|>'},\n",
       " 3780: {'predict': 'MSA has been studied for a long time. The majority of the existing MSA models are based on deep neural networks. These models can be divided into two categories: early fusion and late fusion. Early fusion methods (@CITE[3]@; @CITE[4]@; @CITE[5]@) fuse multimodal data into a single representation, which is then fed into a single classifier. Late fusion methods (@CITE[1]@; @CITE[2]@; @CITE[6]@) fuse multimodal data into a set of separate representations, which are then fed into separate classifiers. Recently, there are some works that fuse multimodal data in a memory. For example, @CITE[6]@; @CITE[2]@; @CITE[1]@ propose a memory fusion network to fuse multimodal data in a memory. However, these methods only consider the interactions between the modalities and do not consider the interactions within the modalities. In this paper, we propose a novel framework that fuses multimodal data in a memory and considers the interactions within and between the modalities. Our method is also different from the existing works in that we use a memory to store the multimodal information and use a set of classifiers to predict the sentiment of each modality. In addition, we use a novel multi-view memory to fuse multimodal data in a memory and a set of classifiers to predict the sentiment of each modality. Furthermore, we propose a novel method to fuse multimodal data in a memory and a set of classifiers to predict the sentiment of each modality.\\nReferences:ID: 1. Title: Learning Factorized Multimodal Representations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Multi-attention Recurrent Network for Human Communication Comprehension\\nID: 4. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 5. Title: Multi-attention Recurrent Network for Human Communication Comprehension\\nID: 6. Title: Memory Fusion Network for Multi-view Sequential Learning',\n",
       "  'ground_truth': '\\n\\nMSA focus on integrating textual, acoustic and visual modalities to comprehend varied human sentiment (. Previous research mainly comprises of two steps: uni-modal representation learning and multimodal fusion. For uni-modal representation, factorizes them into two independent sets while Hazarika et al. (2020) projects them into two distinct subspaces. Large pre-trained Transformer-based language models such as BERT have shown great performance improvement on downstream NLP tasks (@CITE[3]@. However, for acoustic and visual modalities, the features are extracted by CMU-MultimodalSDK with a vague description of feature and backbone selection(@CITE[1]@ in MSA task. We argue that without powerful pre-trained language tokenizer to extract features as textual modality does, the inherent noise of acoustic and visual features may disturb the inference of sentiment. Different from direct processing the uni-modal features, we present uni-modal features coding to learn robust acoustic and visual representations. For multimodal fusion, @CITE[2]@; @CITE[4]@; @CITE[1]@ present early fusion at the feature level while @CITE[2]@; @CITE[6]@; @CITE[5]@ adopt late fusion at the decision level. However, the former methods limit capabilities in modeling cross-modal dynamics due to the inconsistent space of different modalities while the later methods suffer from neglecting modality-specific information with the absence of low-level feature process. To avoid the respective issues from the two methods, Tsai et al. (2019a,b); propose hybrid fusion which perform multimodal fusion at both input and output level. Guided by this thought, we construct a cross-modal predictive network to process representations from different modalities at early and late fusion stage, which effectively exploit the intra- and inter-modality dynamics in a prediction manner.\\nReferences:ID: 1. Title: Learning Factorized Multimodal Representations\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Multi-attention Recurrent Network for Human Communication Comprehension\\nID: 4. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 5. Title: Multi-attention Recurrent Network for Human Communication Comprehension\\nID: 6. Title: Memory Fusion Network for Multi-view Sequential Learning<|eot_id|>'},\n",
       " 1480: {'predict': \"The most common approaches for unsupervised sentence embedding learning are based on the skip-gram architecture (@CITE[4]@. The skip-gram architecture is used in the original SkipThought model (@CITE[1]@, which learns distributed representations of sentences from unlabelled data by predicting a word given its context. The SkipThought model is further improved by the FastSent model (@CITE[3]@, which adds a new objective to predict the discourse coherence of a paragraph. Paragraph Vectors (PV) (@CITE[2]@ is another popular unsupervised sentence embedding method, which predicts the words of a document given the document’s representation. The Word Mover’s Embedding (WME) (@CITE[4]@ method is another popular unsupervised sentence embedding method, which uses a word mover’s distance to learn the document’s representation. In addition to these methods, there are also unsupervised sentence embedding methods that use the information from pre-trained word embeddings. For example, the SIF method ( uses the information from pre-trained word embeddings to learn the sentence embeddings. The SIF method is further improved by the SIF++ method (, which uses a combination of the information from pre-trained word embeddings and the information from the surrounding words to learn the sentence embeddings. The SIF++ method is further improved by the SIF++T method (, which uses a combination of the information from pre-trained word embeddings, the information from the surrounding words, and the information from the sentence structure to learn the sentence embeddings. The SIF++T method is further improved by the SIF++T2 method (, which uses a combination of the information from pre-trained word embeddings, the information from the surrounding words, the information from the sentence structure, and the information from the sentence context to learn the sentence embeddings. In this paper, we use the SIF++T2 method to learn the sentence embeddings, because it is the best unsupervised sentence embedding method that we have tried.\\nReferences:ID: 1. Title: Learning Distributed Representations of Sentences from Unlabelled Data\\nID: 2. Title: Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning\\nID: 3. Title: Distributed Representations of Sentences and Documents\\nID: 4. Title: Word Mover's Embedding: From Word2Vec to Document Embedding\",\n",
       "  'ground_truth': \"\\n\\nSome are capable of exploring the relations among sub-sentences, such as skip-thoughts (@CITE[4]@), FastSent (@CITE[2]@), quick-thoughts () and DiscSent (. These methods assume that adjacent sentences always have similar semantics. However, not every corpus is long enough, perfectly ordered or coherent enough to fulfill that assumption, which limits their applicable domains. Other unsupervised methods merely focus on the internal structures within each sentence, such as paragraph-vectors (@CITE[1]@), Doc2VecC (@CITE[3]@), Sent2Vec (Pagliardini et al., 2018; Gupta et al., 2019), WMD (), GEM () and IS-BERT (Zhang et al., 2020). In general, those unsupervised approaches optimize objectives based on assumptions, which limits their embeddings from being adapted towards different applications.\\nReferences:ID: 1. Title: Learning Distributed Representations of Sentences from Unlabelled Data\\nID: 2. Title: Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning\\nID: 3. Title: Distributed Representations of Sentences and Documents\\nID: 4. Title: Word Mover's Embedding: From Word2Vec to Document Embedding<|eot_id|>\"},\n",
       " 1935: {'predict': 'Analyzing the knowledge encoded in pretrained LMs has been a topic of interest in recent years. In the context of language understanding, @CITE[5]@ found that BERT encodes knowledge about linguistic information in the expected order, and can revise lower-level decisions based on higher-level representations. Similarly, @CITE[3]@ found that BERT captures information about negation in a similar way to humans. In the context of grounded commonsense, @CITE[2]@ found that pretrained LMs struggle to reason about physical commonsense, while @CITE[4]@ found that pretrained LMs struggle to reason about physical commonsense. In this work, we focus on the knowledge that pretrained LMs have about the world. We find that pretrained LMs encode knowledge about the world in a way that is similar to humans, but also find that pretrained LMs are limited in their ability to reason about the world. In particular, we find that pretrained LMs struggle to reason about commonsense that involves the manipulation of objects. This is in contrast to previous work, which found that pretrained LMs are good at commonsense reasoning that does not involve the manipulation of objects (e.g. @CITE[1]@). Our results suggest that pretrained LMs may be good at commonsense reasoning in domains that do not involve the manipulation of objects, but may struggle in domains that do involve the manipulation of objects.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 3. Title: What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models\\nID: 4. Title: SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\\nID: 5. Title: PIQA: Reasoning about Physical Commonsense in Natural Language',\n",
       "  'ground_truth': '\\n\\nThere has been substantial prior work on analyzing pretrained LMs and the linguistic properties of their representations, looking, e.g., at syntactic parse structure (@CITE[4]@; Linzen et al., 2016), semantic structure such as semantic roles and coreference (@CITE[2]@, lexical semantics (; Vuli´ c et al., 2020), and lexical composition (. Particularly relevant to our studies is prior work which has explored how well text-only models capture commonsense knowledge about the physical world via intrinsic (@CITE[1]@; @CITE[3]@ and extrinsic (@CITE[5]@, 2019; measures.\\nReferences:ID: 1. Title: BERT Rediscovers the Classical NLP Pipeline\\nID: 2. Title: Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\\nID: 3. Title: What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models\\nID: 4. Title: SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\\nID: 5. Title: PIQA: Reasoning about Physical Commonsense in Natural Language<|eot_id|>'},\n",
       " 4444: {'predict': 'Controllable text generation has been studied in various forms, such as style transfer (Prabhumoye et al., 2018; @CITE[7]@), story generation (@CITE[8]@), text simplification (@CITE[5]@), and data augmentation (@CITE[3]@; @CITE[6]@). The work most related to ours is Tailor (@CITE[1]@), which uses a control code to steer generation. Our work differs from Tailor in that we focus on controlling the length of the generated text, which is not considered in Tailor. Besides, we show that our method can be applied to various text generation tasks, including text classification, named entity recognition, and relation extraction, while Tailor only focuses on text classification. There are also some other works that focus on controlling the length of the generated text. For example, @CITE[4]@ propose a reinforcement learning framework to generate text with specific length. However, they require the length of the generated text to be within a small range, which limits their application to text classification. In contrast, we can control the length of the generated text to be arbitrary, and we can generate text of any length for any task. @CITE[2]@ propose a planning and iterative refinement framework to control the length of the generated text. However, their method is not applicable to tasks that require the generation of multiple sentences. In contrast, we can generate multiple sentences of arbitrary length for any task. Besides, their method requires a large amount of training data and human annotations, while our method only requires a small amount of training data.\\nReferences:ID: 1. Title: A Review of Human Evaluation for Style Transfer\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: Hierarchical Neural Story Generation\\nID: 4. Title: Controllable Sentence Simplification\\nID: 5. Title: Tailor: Generating and Perturbing Text with Semantic Controls\\nID: 6. Title: Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 8. Title: PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation',\n",
       "  'ground_truth': '\\n\\nControllable text generation approaches using pre-trained language models (PLMs) typically fall into four categories: (i) prompt-based methods that either construct templates for PLMs to complete (@CITE[8]@; Schick and Schütze, 2021a,b) or finetune a task-specific layer to guide the generation (@CITE[4]@; @CITE[6]@, (ii) finetuning methods that either use labelled data prepended with controlled attributes (Ziems et al., 2022; @CITE[2]@; @CITE[3]@; @CITE[7]@ or define a task-specific reward function using reinforcement learning (Ziegler et al., 2019;, (iii) post-processing methods that train discriminator models to guide the generation towards a specific criterion during decoding (@CITE[5]@; @CITE[1]@; @CITE[8]@, and (iv) pretraining methods that pre-train PLMs from the start with different control tokens prepended to the input (. In our work, we experiment with prompt-based and finetuning methods.\\nReferences:ID: 1. Title: A Review of Human Evaluation for Style Transfer\\nID: 2. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 3. Title: Hierarchical Neural Story Generation\\nID: 4. Title: Controllable Sentence Simplification\\nID: 5. Title: Tailor: Generating and Perturbing Text with Semantic Controls\\nID: 6. Title: Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation\\nID: 7. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 8. Title: PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation<|eot_id|>'},\n",
       " 1998: {'predict': 'Active learning (AL) is a widely used technique to reduce the cost of human annotation in NLP tasks (Settles and @CITE[1]@. In AL, a small set of examples are selected to be labeled by humans, and the remaining unlabeled data are used for training a model. In the past, most of the AL methods are based on the uncertainty principle, which selects the most uncertain samples to be labeled (Roy et al., 2012; @CITE[4]@. Recently, several works have proposed to select the most diverse samples to be labeled (@CITE[3]@; @CITE[2]@. However, it is challenging to find the best AL strategy for a specific task, because the performance of AL methods can vary greatly with the different models, datasets, and acquisition functions (@CITE[2]@. In this work, we propose to select the most similar samples to be labeled, which is different from the previous works. Our method is based on the idea that the most similar samples to the labeled data can be used to improve the performance of the model, which is supported by the results of the experiments.\\nReferences:ID: 1. Title: Deep Active Learning for Named Entity Recognition\\nID: 2. Title: Practical Obstacles to Deploying Active Learning\\nID: 3. Title: Active Learning by Acquiring Contrastive Examples\\nID: 4. Title: Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study',\n",
       "  'ground_truth': '\\n\\nAL has seen many usage scenarios in the Natural Language Processing (NLP) field (; @CITE[3]@; ; @CITE[1]@). The perspective of AL is that if a model is allowed to select the data from which it will learn the most, it will achieve comparable (or better) performance with less training instances (@CITE[2]@, and at the same time addressing the costly labeling process with a human annotator. A popular scenario is pool-based active learning (@CITE[4]@;, 2012), which assumes a small set of labeled data L and a large pool of unlabeled data U. Most AL algorithms start similarly: a model is fit to L to get access to Pθ(y | x), then apply a query strategy to get the best scored instance from U, label this instance and add it to L in an iterative process.\\nReferences:ID: 1. Title: Deep Active Learning for Named Entity Recognition\\nID: 2. Title: Practical Obstacles to Deploying Active Learning\\nID: 3. Title: Active Learning by Acquiring Contrastive Examples\\nID: 4. Title: Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study<|eot_id|>'},\n",
       " 5758: {'predict': 'In recent years, generative methods have become the mainstream approach to solve AVE tasks. @CITE[1]@, @CITE[2]@, and @CITE[4]@ are among the first to explore the effectiveness of generative models on AVE tasks. They formulate the AVE task as a generation problem and use a sequence-to-sequence model to generate the attribute-value pairs. Later, @CITE[3]@ propose to solve AVE tasks by using a sequence-to-sequence model to generate the attribute-value pairs. They use a sequence-to-sequence model to generate the attribute-value pairs. The model is further extended with the capability to extract attribute values across multiple product categories with a single model. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. However, these models are trained with the supervision of the attribute-value pairs, which are usually expensive to obtain. In contrast, our method does not require any labeled data during the training phase. Additionally, these methods cannot handle the attribute values that are not in the training data. In contrast, our method can generate the attribute values that are not in the training data.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Enabling Language Models to Fill in the Blanks\\nID: 3. Title: Exploring Generative Models for Joint Attribute Value Extraction from Product Titles\\nID: 4. Title: PAM: Understanding Product Images in Cross Product Category Attribute Extraction',\n",
       "  'ground_truth': '\\n\\nMotivated by successful works such as T5 ( that use generative techniques as a unified solution for various NLP tasks including text classification and slot filling, several researchers have also formulated AVE as a text generation task. @CITE[3]@ combine the context and an attribute with its value masked as blank, and utilize Infilling by Language Modeling (ILM) ( to generate the missing span as the prediction of the value. propose two generative paradigms: word sequence-based and positional sequence-based to tackle the AVE task, which jointly generate the values and positions in the text with their corresponding attribute names one by one. But generating values and their attributes by positions still needs the attribute values to appear explicitly in the product titles or descriptions. And both the two methods above have not considered multi-modal information of the products. @CITE[2]@ unify vision and language tasks using a multi-modal text generation framework. And analogously in the field of AVE, @CITE[4]@ tackle the problem by a seq2seq model called PAM that combines the product texts, Optical Character Recognition (OCR) tokens and visual objects detected in the product image in the encoder, and the decoded tokens are selected from the above inputs as well as a dynamic vocabulary of values. However, PAM just uses detected OCR tokens and partial objects in an image by pretrained models, which could lose some visual information that is unimportant to the pretraining task but useful for AVE. Besides, it only focuses on value extraction for given attributes, without considering which attributes should appear in the outputs. In addition, @CITE[1]@ propose MMT4, a transformer framework with multi modality to improve the performance of generative models in e-commerce. It is also a vision-projected sequence- to-sequence architecture with image feature vectors processed by ViT or CNN. Compared to MMT4, our DEFLATE incorporates a discriminative model to enhance the quality of generated samples, and additionally utilizes a more lightweight image encoding scheme with less information loss of images than ViT and CNN.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Enabling Language Models to Fill in the Blanks\\nID: 3. Title: Exploring Generative Models for Joint Attribute Value Extraction from Product Titles\\nID: 4. Title: PAM: Understanding Product Images in Cross Product Category Attribute Extraction<|eot_id|>'},\n",
       " 7247: {'predict': 'The detection of hate speech is a long-standing challenge in NLP. Previous studies have mainly focused on the detection of explicit hate speech, where the hate speech is directly expressed in the text (@CITE[1]@; @CITE[4]@; @CITE[5]@; @CITE[6]@. However, recent studies have shown that implicit hate speech is also prevalent online, where hate speech is expressed indirectly through coded language (@CITE[2]@; @CITE[3]@; @CITE[5]@; @CITE[6]@. For example, the text \"I hate when people use their phones in public\" may be considered as implicit hate speech towards people with disabilities who use phones in public. In this work, we consider the detection of implicit hate speech as a more challenging task compared to explicit hate speech detection. To the best of our knowledge, there is no prior work that focuses on the detection of implicit hate speech in social media.\\nReferences:ID: 1. Title: Towards generalisable hate speech detection: a review on obstacles and solutions\\nID: 2. Title: Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions\\nID: 3. Title: Towards generalisable hate speech detection: a review on obstacles and solutions\\nID: 4. Title: Latent Hatred: A Benchmark for Understanding Implicit Hate Speech\\nID: 5. Title: Survey of Hallucination in Natural Language Generation\\nID: 6. Title: Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions',\n",
       "  'ground_truth': '\\n\\nHate speech has significant societal impact. Automatic hate speech detection is essential in regulating large online communities such as social media (@CITE[1]@. However, there are significant issues with dataset quality (@CITE[4]@; @CITE[3]@; @CITE[1]@ due to the inherent subjectivity of hate speech (Sachdeva et al., 2022; @CITE[6]@. Hate speech is also highly complex, and various works choose to focus on one component. target implicit hate speech, and @CITE[5]@ demonstrate that context can often change the degree of hate in a statement. There are several strategies to mitigate intersubjectivity, forming three approximate groups: task definition, model training, and data pre-processing. For task definition, the research introduced in @CITE[4]@; Zufall et al. (2022) is the only work to our knowledge that defines a detailed annotation framework in collaboration with legal experts and social scientists. Zufall et al. (2022) propose a framework around the EU’s definition of hate speech, which is the minimum standard in Europe. Modelling strategies like contrastive learning (@CITE[2]@ reduce variance in the embeddings, and few-shot methods reduce requirements for training data (. Finally, Sachdeva et al. (2022) propose a data-level de-biasing method to reduce biases between annotators based on their demographics. In contrast, our work attempts to reconcile multiple legal definitions of hate speech within one jurisdiction.\\nReferences:ID: 1. Title: Towards generalisable hate speech detection: a review on obstacles and solutions\\nID: 2. Title: Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions\\nID: 3. Title: Towards generalisable hate speech detection: a review on obstacles and solutions\\nID: 4. Title: Latent Hatred: A Benchmark for Understanding Implicit Hate Speech\\nID: 5. Title: Survey of Hallucination in Natural Language Generation\\nID: 6. Title: Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions<|eot_id|>'},\n",
       " 1960: {'predict': 'Pragmatic paraphrases are a natural extension of the paraphrase problem. They are semantically equivalent utterances that are not necessarily syntactically identical, and may be more relevant to a task than the traditional paraphrases. In the context of dialogue systems, they can be used to express the same meaning as a user’s utterance, but in a way that is more natural, idiomatic, or concise. For example, instead of saying “I am going to the store”, a user might say “I am off to the store”, and a pragmatic paraphrase would be a more idiomatic way of saying the same thing, such as “I am headed to the store”. @CITE[3]@ collect pragmatic paraphrases from Reddit and use them to train a model that can predict the meaning of a question based on the answer. @CITE[2]@ collect a large dataset of pragmatic paraphrases from Twitter and show that they can be used to improve the performance of a dialogue system. @CITE[1]@ collect a dataset of pragmatic paraphrases from a crowdsourcing task, and use them to train a model that can predict the meaning of a sentence based on the context. However, these datasets are not suitable for our task, because they do not contain a pragmatic paraphrase for every utterance in the dialogue. In contrast, our dataset contains a pragmatic paraphrase for every utterance in the dialogue, which makes it suitable for training a model to predict the pragmatic paraphrase of an utterance based on the dialogue history.\\nReferences:ID: 1. Title: \"I\\'d rather just go to bed\": Understanding Indirect Answers\\nID: 2. Title: A Continuously Growing Dataset of Sentential Paraphrases\\nID: 3. Title: A large annotated corpus for learning natural language inference',\n",
       "  'ground_truth': '\\n\\nDespite its importance in understanding users’ intentions, the pragmatic paraphrases have been overlooked. Only a few recent studies have focused on pragmatic paraphrases to advance the understanding of users’ intentions. @CITE[2]@ proposed a rule-based approach to automatically construct a corpus consisting of pairs of indirect and direct utterances. They demonstrated that the neural conversation model could accurately extract utterances with opposing directness. Because of their rule-based approach, patterns of indirect/direct utterances in their corpus are limited. used crowdsourcing to build a corpus comprising indirect answers to Yes/No questions, annotating whether the answers were Yes or No. This corpus provides natural answers written by crowdsourcing workers; however, it is limited to context-free Yes/No questions. In contrast to these studies, DIRECT provides natural utterances written by humans with rich dialogue histories. Furthermore, it covers various types of utterances. While there several paraphrase corpora (; @CITE[1]@), all have focused on context-free paraphrases. Hence, none provide pragmatic paraphrases that emerge in contexts. Corpora for natural language inference are also relevant to our study (Giampiccolo et al., 2007; ; @CITE[3]@). Similar to the paraphrase corpora, they do not provide contexts. This means that these corpora rely on world knowledge to determine whether a text entails a hypothesis. In contrast, context is a crucial element in determining paraphrasal relationships in pragmatic paraphrases. Our DIRECT is the first corpus that provides large-scale pragmatic paraphrases. It would be a valuable resource also for research on paraphrase identification and generation to make a step forward from literal paraphrases.\\nReferences:ID: 1. Title: \"I\\'d rather just go to bed\": Understanding Indirect Answers\\nID: 2. Title: A Continuously Growing Dataset of Sentential Paraphrases\\nID: 3. Title: A large annotated corpus for learning natural language inference<|eot_id|>'},\n",
       " 2757: {'predict': \"A common approach is to explicitly model the bias and train a debiasing model to reduce its reliance on the bias. This can be done by training a bias-only model (@CITE[5]@; @CITE[5]@; @CITE[5]@, or by training a debiasing model that focuses on hard examples (@CITE[1]@; @CITE[8]@; @CITE[8]@; @CITE[5]@; @CITE[2]@; @CITE[7]@; @CITE[6]@; @CITE[9]@; @CITE[4]@. Another approach is to train a model that is robust to the bias without explicitly modeling it (@CITE[1]@; @CITE[3]@; @CITE[6]@; @CITE[9]@. However, these methods rely on knowledge of the bias, which is often unavailable in practice. Instead, we train a model to ignore the bias by learning from the mistakes of a biased model. This is similar to the approach of (@CITE[9]@, but we do not need to know the bias, and we show that this approach is effective for many types of biases.\\nReferences:ID: 1. Title: Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\\nID: 2. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 3. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 4. Title: Focal Loss for Dense Object Detection\\nID: 5. Title: Towards Debiasing Fact Verification Models\\nID: 6. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 7. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 8. Title: Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\\nID: 9. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them\",\n",
       "  'ground_truth': \"\\n\\nEarlier debiasing techniques involve reweighting the cross-entropy losses of bias-sensitive samples. Samples that can be handled effectively by a bias model are considered bias-sensitive. In contrast to the traditional training paradigm where each training example has the same importance, reweighting assigns different weights to examples based on their sensitivity to biases. A common way to reweight is to create a bias model trained on hand-crafted features () and assign a weight to each training sample using the probability of the correct label from the bias model. @CITE[5]@ extended this idea by combining scores from multiple bias-only models. In addition, @CITE[5]@ introduced a variant of focal loss (@CITE[4]@) to leverage predictions from the bias model. Alternatively, @CITE[2]@ used n-grams co-occurrence to calculate the reweighting scores. However, recent research has been shifting toward acquiring reweighting scores without using domain knowledge and hand-crafted features (@CITE[3]@; @CITE[7]@). Another prevailing approach is model ensembling. reinterpreted Product-of-Experts (PoE) (@CITE[9]@) for NLU debiasing. They used a bias model to create an ensemble with the main model with the aim that the main model learns all the information except the bias. The ensemble decreases the training loss for a sample that the bias model correctly predicts when updating the main model. @CITE[8]@ proposed a domain-knowledge-free approach where the bias model was a neural network with small parameters. @CITE[1]@ proposed a self-distillation framework, confidence regularization (Conf-Reg), that used a bias model to produce confident scaling scores to calibrate the confidence of the model’s predictions. Alternatively, @CITE[6]@ calculated the scaling scores from the local mutual information and integrated gradient explanation. Although these methods have successfully improved the robustness on the challenge sets, they can only be used to train new models. As a result, we cannot apply it to the existing models without retraining.\\nReferences:ID: 1. Title: Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\\nID: 2. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 3. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 4. Title: Focal Loss for Dense Object Detection\\nID: 5. Title: Towards Debiasing Fact Verification Models\\nID: 6. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 7. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 8. Title: Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\\nID: 9. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them<|eot_id|>\"},\n",
       " 7148: {'predict': 'Tokenization is a crucial step in the pre-processing of natural language data, and the choice of tokenization approach can significantly impact model performance. Recent work has shown that the choice of tokenization can affect model generalization (@CITE[1]@; @CITE[4]@; @CITE[2]@; @CITE[7]@; @CITE[6]@; @CITE[3]@. For example, @CITE[1]@ show that unigram tokenization can lead to better performance on downstream tasks compared to BPE, and @CITE[4]@ show that subword regularization can improve the robustness of NMT models. In the context of semantic parsing, @CITE[2]@ show that the choice of tokenization can affect the performance of the model on out-of-domain examples, and @CITE[7]@ show that the choice of tokenization can affect the performance of the model on low-resource languages. In contrast to these studies, we focus on the impact of tokenization on the performance of a model on in-domain examples, and show that the choice of tokenization can significantly affect the performance of the model on the task of code generation. In addition, we show that the choice of tokenization can significantly affect the performance of the model on code generation tasks that require numerical reasoning. We also show that the choice of tokenization can affect the performance of the model on code generation tasks that require the generation of multiple tokens, and that the choice of tokenization can affect the performance of the model on code generation tasks that require the generation of code in multiple languages.\\nReferences:ID: 1. Title: Byte Pair Encoding is Suboptimal for Language Model Pretraining\\nID: 2. Title: Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\\nID: 3. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 4. Title: CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing\\nID: 5. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 6. Title: CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing\\nID: 7. Title: Generating Sequences With Recurrent Neural Networks',\n",
       "  'ground_truth': '\\n\\nRecent research has identified these tokenization approaches as sources of poor model generalization. For example, BPE has been shown to produce less ideal morphological segmentation (@CITE[1]@; Provilkov et al., 2020), and the same text can be tokenized differently when different BPE rules are applied (). Existing approaches either modify model training by using stochastic inputs or by modifying how BPE segmentation is done to improve model robustness and generalization (Provilkov et al., 2020; ; @CITE[7]@, inter alia). In contrast, we propose a simple and deterministic approach that does not rely on altering the segmentation approach in any way, so that the original training strategy can be preserved. Besides generic approaches, researchers have also investigated domain-specific approaches to improve segmentation on specific texts (@CITE[3]@; @CITE[4]@; @CITE[2]@). @CITE[3]@ and @CITE[4]@ propose approaches that aim at overcoming the inconsistency of tokenizing numbers and enhancing the model’s numerical reasoning ability. @CITE[2]@ use “space-joined tokens” to resolve many string-matching anomalies after tokenization that lead to unfair evaluation in semantic parsing. We instead look at inconsistency from a more general perspective by making our method applicable to any tasks with an extractive nature. In addition to subword segmentation methods, another line of research focuses on character- and byte-level modeling as procedures that are free of tokens (; ; @CITE[6]@; ). While tokenization-free methods show potential to surpass subword segmentation approaches, we remain focused on the consistency issue of subword tokenizations, as most prevalent state-of-the-art models rely on subword tokenizations (; @CITE[5]@).\\nReferences:ID: 1. Title: Byte Pair Encoding is Suboptimal for Language Model Pretraining\\nID: 2. Title: Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\\nID: 3. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 4. Title: CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing\\nID: 5. Title: Injecting Numerical Reasoning Skills into Language Models\\nID: 6. Title: CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing\\nID: 7. Title: Generating Sequences With Recurrent Neural Networks<|eot_id|>'},\n",
       " 1861: {'predict': 'Contrastive learning is a self-supervised learning technique that aims to learn better representations by pulling positive samples together and pushing negative samples apart (@CITE[4]@; @CITE[1]@; @CITE[3]@; @CITE[2]@. Recently, contrastive learning has been successfully applied to sentence representations (@CITE[5]@; @CITE[2]@. It can be combined with various pre-trained language models to achieve better performance on downstream tasks. For example, @CITE[5]@ propose a contrastive learning framework to learn sentence representations from unlabeled text. @CITE[2]@ propose a supervised contrastive learning framework to learn sentence representations from labeled text. However, the above works focus on learning sentence representations for downstream tasks. In this work, we aim to learn sentence representations for dialogue systems. We propose a contrastive learning framework to learn sentence representations for dialogue systems, which is a new direction.\\nReferences:ID: 1. Title: A Discrete CVAE for Response Generation on Short-Text Conversation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning\\nID: 5. Title: SentEval: An Evaluation Toolkit for Universal Sentence Representations',\n",
       "  'ground_truth': '\\n\\nIn the past few years, many unsupervised feature extraction algorithms have emerged, for instance, variational autoencoder (@CITE[2]@; @CITE[5]@; ), generalized language models (; ). All the above methods obtain the feature of the input by reconstructing the original input or predicting masked words and so on, which do not take the relationships between the inputs into consideration. Therefore, contrastive learning, whose loss is designed to narrow down the distance between features of similar inputs and to enlarge the distance of dissimilar inputs, has been proposed and achieved great success in both unsupervised (@CITE[1]@) and supervised (@CITE[4]@) image feature extraction. There are also some works trying to apply contrastive learning into the natural language processing domain. For instance, Iter et al. (2020) propose a pretraining method for sentence representation that employs contrastive learning to improve the quality of discourse-level representations. @CITE[3]@ utilized it to pretrain the transformer and obtains state-of-the-art results on SentEval (). All of the above successes spur us to test whether contrastive learning is helpful on EGPG.\\nReferences:ID: 1. Title: A Discrete CVAE for Response Generation on Short-Text Conversation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning\\nID: 5. Title: SentEval: An Evaluation Toolkit for Universal Sentence Representations<|eot_id|>'},\n",
       " 7404: {'predict': 'Efficient Transformer architectures have been proposed to reduce the quadratic time and space complexity of the attention mechanism. @CITE[1]@ proposed Random Feature Attention (RFA) to approximate the softmax attention with random features, which is competitive in terms of both accuracy and efficiency on three long text classification datasets. @CITE[5]@ proposed ETC to address the two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. They introduced a novel global-local attention mechanism between global tokens and regular input tokens, which allows the model to scale to longer inputs. @CITE[3]@ proposed BigBird to reduce the quadratic dependency of the full attention mechanism to linear. They also proposed a novel sparse attention mechanism that reduces the quadratic dependency to linear. @CITE[4]@ proposed Performers to approximate softmax full-rank attention with provable accuracy, but using only linear space and time complexity. They used a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+) to approximate softmax attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We will use Performers as a baseline in our experiments.\\nReferences:ID: 1. Title: Random Feature Attention\\nID: 2. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 3. Title: Big Bird: Transformers for Longer Sequences\\nID: 4. Title: Big Bird: Transformers for Longer Sequences\\nID: 5. Title: Rethinking Attention with Performers',\n",
       "  'ground_truth': '\\n\\nA long list of works has been proposed to reduce the complexity of the attention layers of transformers. The simplest paradigm is to restrict each token’s attending context to a subset of the whole sequences, e.g., Reformer (Kitaev et al., 2020) and the Routing transformer (@CITE[1]@ proposes hashing or clustering based attention, where each token only attends to tokens of a single bucket/cluster. Our model architecture is influenced by previous work like Longformer (@CITE[4]@, BigBird (@CITE[2]@ and ETC (@CITE[2]@ that demonstrate strong downstream performance. These models assume strong locality bias in language data and restrict each token’s attending context to nearby tokens. In contrast, we augment the block attention with pooling layers and study the effect of additional pretraining on long sequences. Other approaches tackling the efficiency bottleneck includes kernel-based (@CITE[5]@; @CITE[1]@ and low-rank approximation (@CITE[3]@ of the N×N attention matrix. However, in contrast to local attention transformers, the effectiveness of these approximation approaches is yet to be validated in downstream tasks.\\nReferences:ID: 1. Title: Random Feature Attention\\nID: 2. Title: ETC: Encoding Long and Structured Inputs in Transformers\\nID: 3. Title: Big Bird: Transformers for Longer Sequences\\nID: 4. Title: Big Bird: Transformers for Longer Sequences\\nID: 5. Title: Rethinking Attention with Performers<|eot_id|>'},\n",
       " 6865: {'predict': 'Fine-grained entity typing (FET) aims to assign a specific type to an entity mention in a given context. The task has been widely studied in the field of Natural Language Processing (NLP) (@CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[8]@; @CITE[7]@; @CITE[6]@. The task is typically performed in a supervised setting, where the model is trained on a large dataset of labeled entity mentions. However, the task is challenging since the datasets are usually small, and the model has to learn the complex relationship between the entity mention and the context. To alleviate the data scarcity issue, distantly supervised learning is commonly used (@CITE[3]@; @CITE[4]@. In distantly supervised learning, the model is trained on a large dataset of weakly labeled entity mentions. The labels are automatically generated based on a predefined hierarchy of entity types. However, the automatically generated labels are noisy, and the model has to learn to correct the noisy labels. In this paper, we propose a novel approach to automatically correct the noisy labels. Our approach is based on the idea of a model that can learn to distinguish between noisy and clean labels. We use a simple model to estimate the posterior probability of a label being positive or negative. Based on the posterior probability, we can identify the potentially noisy labels and then relabel candidate noisy labels by training a robust model over the remaining clean labels. We use the distantly supervised learning framework to evaluate our method. The results show that our method can achieve state-of-the-art performance on the FET task.\\nReferences:ID: 1. Title: Deep Residual Learning for Image Recognition\\nID: 2. Title: Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings\\nID: 3. Title: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss\\nID: 4. Title: Learning to Denoise Distantly-Labeled Data for Entity Typing\\nID: 5. Title: Automatic Noisy Label Correction for Fine-Grained Entity Typing\\nID: 6. Title: Automatic Noisy Label Correction for Fine-Grained Entity Typing\\nID: 7. Title: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 9. Title: Multimodal Entity Linking for Tweets',\n",
       "  'ground_truth': '\\n\\nTwo major challenges of FGET have been extensively studied by researchers. One challenge is that distant supervision introduces a significant amount of noise to FGET. Some researches divide the dataset into clean set and noisy set, and model them separately (@CITE[1]@; @CITE[5]@; @CITE[4]@. @CITE[9]@ proposed a two-stage denoising method including filtering and relabeling function. proposed a probabilistic automatic relabeling method with pseudo-truth label distribution estimation. @CITE[6]@ proposed a method to correct and identify noisy labels. @CITE[6]@ tried to mitigate the effect of noise by feature clustering and loss correction. Another challenge in FGET is label hierarchy and interdependency. @CITE[4]@ introduced hierarchical loss normalization to deal with type hierarchy. @CITE[2]@ proposed a hybrid classification model to utilize type hierarchy. @CITE[3]@ proposed a novel model with a multi-level learning-to-rank loss and a coarse-to-fine decoder. @CITE[7]@ proposed a label reasoning network to capture extrinsic and intrinsic dependencies. Zuo et al. (2022) modeled type hierarchy by hierarchical contrastive strategy. Moreover, some works attempted to model FGET in hyperbolic space (López et al., 2019) or box space (@CITE[8]@ instead of traditional vector space. However, images often co-occur with text in many real-world scenarios, yet no one has investigated the impact of images on FGET. Therefore, we introduce images to FGET and propose a new task called multimodal fine-grained entity typing, and then study the effect of images on FGET.\\nReferences:ID: 1. Title: Deep Residual Learning for Image Recognition\\nID: 2. Title: Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings\\nID: 3. Title: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss\\nID: 4. Title: Learning to Denoise Distantly-Labeled Data for Entity Typing\\nID: 5. Title: Automatic Noisy Label Correction for Fine-Grained Entity Typing\\nID: 6. Title: Automatic Noisy Label Correction for Fine-Grained Entity Typing\\nID: 7. Title: Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 9. Title: Multimodal Entity Linking for Tweets<|eot_id|>'},\n",
       " 7305: {'predict': 'Reinforcement learning (RL) is an effective way to optimize the generation process. Recently, RL has been applied to various NLP tasks, such as machine translation (@CITE[8]@; @CITE[5]@; @CITE[7]@), dialogue (@CITE[1]@; @CITE[2]@), and semantic parsing (@CITE[4]@). However, the performance of RL is highly dependent on the reward design (@CITE[6]@). A well-designed reward function can significantly improve the performance of RL. However, designing a reward function is challenging in many NLP tasks, especially in text generation. As a result, a large amount of works (@CITE[3]@) focus on designing a reward function that can be easily estimated by a proxy task. However, these reward functions are often not aligned with the human preference. In this work, we focus on learning from human feedback. We design a reward function that can be easily estimated by the human feedback. Furthermore, we propose a novel algorithm that can learn from the human feedback.\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: ALP-KD: Attention-Based Layer Projection for Knowledge Distillation\\nID: 3. Title: End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient\\nID: 4. Title: Human-centric Dialog Training via Offline Reinforcement Learning\\nID: 5. Title: Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\\nID: 6. Title: Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback\\nID: 7. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 8. Title: Text Generation by Learning from Demonstrations',\n",
       "  'ground_truth': '\\n\\nRecently, reinforcement learning (RL) has gained significant attention in the field of language modeling. This approach has been successfully applied to various language tasks, including summarization (Paulus et al., 2018; @CITE[2]@; Stiennon et al., 2020; @CITE[5]@, dialogue systems (@CITE[6]@; @CITE[8]@;, machine translation (Bahdanau et al., 2017; @CITE[1]@; @CITE[4]@, semantic parsing (, story generation (@CITE[7]@, and question generation (@CITE[3]@. Concurrently, there has been a growing interest in using RL to align language models with human preferences across a wide range of language tasks. For example, Ouyang et al. (2022) employed [CLS].\\nReferences:ID: 1. Title: Neural Network Acceptability Judgments\\nID: 2. Title: ALP-KD: Attention-Based Layer Projection for Knowledge Distillation\\nID: 3. Title: End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient\\nID: 4. Title: Human-centric Dialog Training via Offline Reinforcement Learning\\nID: 5. Title: Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\\nID: 6. Title: Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback\\nID: 7. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 8. Title: Text Generation by Learning from Demonstrations<|eot_id|>'},\n",
       " 1539: {'predict': 'Large-scale Eventuality Knowledge Graphs (KGs) have attracted a lot of attention in recent years. Among them, ASER ( is a typical one that covers 15 relation types and 194M unique eventualities. ASER is constructed by extracting events from unstructured textual data. However, it has some limitations. First, the extracted events are not enough. For example, only 64M edges are extracted from 11B tokens. Second, the extracted events are not diverse enough. For example, only 15 relation types are covered. Finally, the extracted events are not enough in detail. For example, only a few sub-events are extracted from a single event. To overcome these limitations, some recent works (@CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[2]@) focus on building large-scale Eventuality KGs from various sources. They extract events from a large-scale corpus, such as Wikipedia, Reddit, etc. However, they have some limitations. First, they only extract events from a single source, which is not enough to cover all kinds of events. Second, they only extract events with a single relation, which is not enough to cover all kinds of events. Finally, they only extract events with a single sub-event, which is not enough to cover all kinds of events. To overcome these limitations, we propose to build large-scale Eventuality KGs by extracting events from multiple sources and multiple relations.\\nReferences:ID: 1. Title: ASER: A Large-scale Eventuality Knowledge Graph\\nID: 2. Title: Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering\\nID: 3. Title: K-BERT: Enabling Language Representation with Knowledge Graph\\nID: 4. Title: COMET: Commonsense Transformers for Automatic Knowledge Graph Construction\\nID: 5. Title: Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering',\n",
       "  'ground_truth': '\\n\\nWith recent developments of large-scale eventuality knowledge graphs (KG) (e.g., ASER (@CITE[1]@ and ATOMIC (@CITE[4]@), an effective remedy is to adopt “retrieval and integration” schema and integrate the inferential facts retrieved from the KG for script reasoning (@CITE[5]@. This paradigm is proven effective in both entity-centric and concept-centric tasks, such as relation extraction (Zhang et al., 2019), named entity recognition ( and commonsense reasoning (@CITE[3]@; @CITE[2]@, etc. However, this paradigm is not that compatible with event-centric script reasoning since script reasoning focuses more on the inferential relation between consecutive events in a script rather than the triple facts with exact event matching.\\nReferences:ID: 1. Title: ASER: A Large-scale Eventuality Knowledge Graph\\nID: 2. Title: Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering\\nID: 3. Title: K-BERT: Enabling Language Representation with Knowledge Graph\\nID: 4. Title: COMET: Commonsense Transformers for Automatic Knowledge Graph Construction\\nID: 5. Title: Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering<|eot_id|>'},\n",
       " 6703: {'predict': 'CoT prompting has been widely used to solve complex multi-step reasoning tasks such as numerical reasoning (@CITE[7]@; @CITE[2]@; @CITE[4]@; @CITE[6]@; @CITE[1]@; @CITE[3]@; @CITE[5]@. The main idea is to use LLMs to generate a reasoning chain, which is composed of a series of reasoning steps, to solve a task. For example, to solve a math problem, a CoT prompt can be generated by combining the input math problem with a reasoning chain of math equations. However, most CoT prompting methods are limited to solve problems that can be solved with a fixed reasoning chain, which is not applicable to tasks that require a flexible and dynamic reasoning chain. In this paper, we propose a CoT prompting method that can generate a dynamic reasoning chain to solve complex multi-step reasoning tasks.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks\\nID: 3. Title: Complexity-Based Prompting for Multi-Step Reasoning\\nID: 4. Title: Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\\nID: 5. Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\\nID: 6. Title: Tool Learning with Foundation Models\\nID: 7. Title: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models',\n",
       "  'ground_truth': '\\n\\nBy providing a few in-context examples (or demonstrations) which contain chain-of-thoughts, CoT prompting can encourage LLMs to output intermediate reasoning steps before predicting the final answer (@CITE[1]@)... Existing CoT studies mainly focus on two directions. (1) Improving the quality of CoT demonstrations. For instance, selecting better in-context examples for CoT prompting according to the question diversity (, the solution complexity (@CITE[4]@, or the example similarity (Rubin et al., 2022). (2) Exploring new representations of CoT reasoning steps. Beside the typical natural language format, researchers also proposed chain-of-thoughts in other formats. For instance, program-of-thoughts (@CITE[7]@, tree-of-thoughts (@CITE[5]@, and graph-of-thoughts (@CITE[6]@. Among them, the CoT in program languages has emerged as a powerful approach for LLMs to invoking external tools (@CITE[2]@. Recently, @CITE[3]@ proposed the Chameleon framework that augments LLMs with various tools like search engines and Python executors. We treat it as a contemporary work of our paper and list its results in the Appendix D.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks\\nID: 3. Title: Complexity-Based Prompting for Multi-Step Reasoning\\nID: 4. Title: Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\\nID: 5. Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\\nID: 6. Title: Tool Learning with Foundation Models\\nID: 7. Title: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models<|eot_id|>'},\n",
       " 2559: {'predict': 'Discrete speech representation learning has been explored in recent years. @CITE[3]@ propose a discrete VQ-VAE (VQ-VAE) for learning discrete speech representations, which are used for phoneme recognition and speech denoising tasks. @CITE[1]@ propose to learn discrete phonetic representations from raw speech and text data using a VQ-VAE. @CITE[2]@ propose a discrete VQ-VAE model that learns phonetic representations from raw speech and uses them for speech recognition and text-to-speech tasks. @CITE[5]@ propose to learn discrete phonetic representations from raw speech using a BERT-like architecture and use them for speech recognition and text-to-speech tasks. @CITE[4]@ propose a discrete VQ-VAE model that learns phonetic representations from raw speech and text data and use them for speech-to-text and text-to-speech tasks. @CITE[6]@ propose to learn discrete phonetic representations from raw speech using a VQ-VAE and use them for direct speech-to-speech translation. However, these models require a pre-defined phonetic lexicon to map the discrete representations to phonetic units, which limits their applicability in unsupervised settings.\\nReferences:ID: 1. Title: fairseq: A Fast, Extensible Toolkit for Sequence Modeling\\nID: 2. Title: Neural Discrete Representation Learning\\nID: 3. Title: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\\nID: 6. Title: Direct speech-to-speech translation with discrete units',\n",
       "  'ground_truth': '\\n\\nDiscretizing continuous speech signals for speech representation learning has drawn substantial attention. Vq-wav2vec (@CITE[3]@) and wav2vec 2.0 () attempt at discretizing speech signals into quantized units from a learnable code-book (van den ). PBERT (@CITE[2]@) instead uses phonemes as the discrete targets in a semi-supervised setting. SemFace (@CITE[4]@) proposes to use language-independent vector quantized units as the semantic interface of encoder pre-training and decoder pre-training. Inspired by the masked language model in BERT (@CITE[6]@), HuBERT (@CITE[5]@) first introduces the masked speech prediction of hidden units to pre-train a universal speech model. Particularly, the hidden units can be clustered from log Mel-filterbank features or the hidden states of the previous pre-trained model. Recently, some studies explore leveraging the discrete hidden units to build speech-to-speech translation systems (@CITE[1]@,b), which first convert source speech into target units, then generate the target waveform from predicted units. However, our goal in this paper is to jointly pre-train speech and text with the hidden units as the intermediate bridge.\\nReferences:ID: 1. Title: fairseq: A Fast, Extensible Toolkit for Sequence Modeling\\nID: 2. Title: Neural Discrete Representation Learning\\nID: 3. Title: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\\nID: 6. Title: Direct speech-to-speech translation with discrete units<|eot_id|>'},\n",
       " 3963: {'predict': 'Prompt-based learning is a new paradigm in NLP, which uses the pre-trained language models to directly perform prediction tasks by filling the slots in the prompt templates. The template can be manually designed or automatically generated (@CITE[2]@; @CITE[3]@). The prompt-based learning has achieved great success in few-shot learning (@CITE[1]@; Schick and Schütze, 2021; Zhou et al., 2022; Wang et al., 2022). However, prompt-based learning is sensitive to the design of prompt templates, which requires a lot of manual efforts and expertise. The prompt templates are also not task-specific and cannot be directly used for different tasks. To solve these problems, we propose a task-adaptive prompt learning method to automatically generate task-specific prompt templates for each task. The generated prompts can be directly used for different tasks and can also be further fine-tuned to achieve better performance.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models',\n",
       "  'ground_truth': '\\n\\nIn their extensive review, @CITE[1]@ have analyzed the prompt-based learning paradigm, exploring different verbalization techniques used to input text to PTLMs, as a key point to reach SoTA results in few and zero-shot learning scenarios. The currently under research question is: what kind of verbalizations work better? Here, two different trends arise: a) automatically searched prompts (@CITE[3]@; @CITE[2]@; Li and Liang) and b) handcrafted prompts (Schick and Schütze, 2021, 2022). The main drawback of the first one is the necessity of additional training and computational resources to find the best prompt, and the second’s major issue is the necessity of manual effort (Logan; ). A third option is however possible: null prompts () where the mask token is simply added to the input sentence. Currently, no consensus has been reached on which kind of verbalizations work best, and, while authors such as Schick and Schütze (2022) obtain the best results in a variety of NLP tasks with handcrafted verbalizations, others (; ) defend the advantages of short or even null prompts while still achieving competitive results. @CITE[2]@ found different behavior for their Ptuning-v2 method depending on the task: simple classification tasks prefer shorter prompts, while hard sequence labeling tasks prefer longer ones. Other open questions about prompting rely on the selection of the label to verbalize the mask and the order in which the mask and input are provided. Labels given in benchmark datasets are often multi-word or rare expressions consisting of more than one token, however, the mask needs to be filled by just one token (Schick and Schütze, 2022) thus there is a need to select the label either automatically or manually. The order in which input and mask are entered is also under current research (Mahabadi et al., 2022). Previous comparisons of different prompting techniques have been mostly applied to highly context-dependent NLP tasks such as sentiment analysis, subjectivity, classification, question classification, natural language inference, question answering, word sense disambiguation or paraphrasing (; Schick and Schütze, 2022; ) where the input example already consists of a well-formed sentence. Yet, other NLP tasks that are less context-sensitive such as LRC, Relation Extraction, or Lexical Entailment, have received little or no attention so far in prompt comparison studies.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models<|eot_id|>'},\n",
       " 7368: {'predict': 'Automatic consistency evaluation models are designed to automatically identify factual inconsistencies between the generated summary and the original document. The most common approach is to use a Natural Language Inference (NLI) model to evaluate whether the summary entails the document. The inconsistency detection task can be considered as a binary NLI task, where the summary is the hypothesis, and the document is the premise. @CITE[4]@ proposed a method to segment the document into sentence units and aggregate the scores between pairs of sentences. @CITE[5]@ proposed a method to decompose the document and the summary into structured meaning representations (MRs) and encode such graphs using a graph encoder. Other approaches use different types of models, such as question answering (QA) models (@CITE[1]@; @CITE[2]@), semantic role labeling (SRL) models (@CITE[3]@), and text classification models (@CITE[6]@). Most of the existing approaches focus on evaluating the factual consistency of the generated summary at the document level. However, there is no work that evaluates the consistency of the generated summary at the sentence level.\\nReferences:ID: 1. Title: SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\\nID: 2. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\\nID: 3. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 4. Title: Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization\\nID: 5. Title: Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization\\nID: 6. Title: Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization',\n",
       "  'ground_truth': '\\n\\nAutomatic consistency evaluation models can be roughly classified into entailment-based and QA-based methods. Entailment-based metrics (Kryscinski et al., 2020; @CITE[5]@; @CITE[2]@ train classification models to predict if the summary is entailed by the source document. Meanwhile, QA-based metrics (@CITE[3]@; @CITE[4]@; @CITE[6]@ generate questions based on the input summary and document, then apply QA models to answer the questions and compare the answers to calculate a faithfulness score. propose a multi-label classification model grounded on semantic role labeling to predict the types of faithfulness error in a summary. @CITE[1]@ evaluate effective faithfulness of summarization systems with a faithfulness-abstractiveness trade-off curve. evaluate and analyze the faithfulness of pre-trained summarization models on dynamically evolving data.\\nReferences:ID: 1. Title: SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\\nID: 2. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\\nID: 3. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\\nID: 4. Title: Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization\\nID: 5. Title: Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization\\nID: 6. Title: Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization<|eot_id|>'},\n",
       " 892: {'predict': 'Distance-based models are based on the idea that the distance between entities and relations in the embedding space should be minimized. The most common distance metric is the Euclidean distance, and the most popular distance-based models are TransE (Bordes et al., 2013), TransH (, and RotatE (@CITE[3]@). TransE is a translation-based model, which maps the source and target entities to a vector space and models the relation as a translation vector. TransH is a tensor-based model, which maps entities to a hyperbolic space and models the relation as a translation vector in this space. RotatE is a rotation-based model, which maps entities to a complex vector space and models the relation as a rotation vector. TransD (@CITE[1]@) and TransR (@CITE[2]@) are variants of TransE, which use different distance metrics and learn the translation vector separately for each relation. TransE, TransH, and RotatE are all based on the Euclidean distance, which is not suitable for graph-structured data. In contrast, HyperSAGE ( is a hyperbolic distance-based model that uses the hyperbolic distance, which is more suitable for graph-structured data. HyperSAGE is a variant of TransH, which uses the hyperbolic distance instead of the Euclidean distance. HyperSAGE is a graph neural network-based model, which uses the hyperbolic distance as a scoring function in the graph neural network. In contrast, our model uses the hyperbolic distance as a scoring function in a knowledge graph embedding model.\\nReferences:ID: 1. Title: Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding\\nID: 2. Title: Open Graph Benchmark: Datasets for Machine Learning on Graphs\\nID: 3. Title: RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space',\n",
       "  'ground_truth': '\\n\\nDistance based models measure plausibility of fact triples as distance between entities. TransE interprets relation as a translation vector r so that entities can be connected, i.e., h + r ≈t. TransE is efficient, though cannot model symmetry relations and have difficulty in modeling complex relations. Several models are proposed for improving TransE to deal with complex relations, including TransH, TransR, TransD, TranSparse (@CITE[1]@ and so on. All these methods project the entities to relation specific hyperplanes or spaces first, then translate projected entities with relation vectors. By projecting entities to different spaces or hyperplanes, the ability to handle complex relations is improved. However, with the added projection parameters, these models are unable to encode inverse and composition relations. The recent state-of-the-art, RotatE, which can encode symmetry/antisymmetry, inverse and composition relation patterns, utilizes rotation based translational method in a complex space. Although expressiveness for different relation patterns, complex relations remain challenging. GC-OTE (@CITE[3]@ proposes to improve complex relation modeling ability of RotatE by introducing graph context to entity embedding. However, the calculation of graph contexts for head and tail entities is time-consuming, which is inefficient for large-scale knowledge graphs, e.g. ogbl-wikikg (@CITE[3]@). Another related work is SE (, which utilizes two separate relation matrices to project head and tail entities. As pointed out by (), this model is not able to encode symmetry/antisymmetry, inverse, and composition.\\nReferences:ID: 1. Title: Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding\\nID: 2. Title: Open Graph Benchmark: Datasets for Machine Learning on Graphs\\nID: 3. Title: RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space<|eot_id|>'},\n",
       " 3702: {'predict': 'Robustness tuning is a method to improve the robustness of a model during the training process. There are two main categories: (1) training-time robustness tuning, which involves adding regularization terms to the objective function to improve the robustness of the model. For example, @CITE[3]@ propose to add the regularization term of the Jacobian matrix to improve the robustness of the model. @CITE[5]@ propose to add the regularization term of the information bottleneck to improve the robustness of the model. @CITE[2]@ propose to add the regularization term of the f-divergence to improve the robustness of the model. (2) inference-time robustness tuning, which involves modifying the model architecture or adding modules to the model to improve the robustness of the model. For example, @CITE[6]@ propose to add a soft prompt to the model to improve the robustness of the model. @CITE[4]@ propose to add adapters to the model to improve the robustness of the model. @CITE[1]@ propose to add a prefix to the model to improve the robustness of the model.\\nReferences:ID: 1. Title: Towards Deep Learning Models Resistant to Adversarial Attacks\\nID: 2. Title: InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\\nID: 3. Title: Consistency Regularization for Cross-Lingual Fine-Tuning\\nID: 4. Title: Posterior Differential Regularization with f-divergence for Improving Model Robustness\\nID: 5. Title: SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\\nID: 6. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 7. Title: On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation',\n",
       "  'ground_truth': '\\n\\nRobustness tuning (RT) is another family of methods that tries to train a more generalized model pre-deployment at training time instead of test time. Adversarial training is a well- studied method to enhance model robustness (Miy- ato et al., 2017; @CITE[6]@; @CITE[3]@; @CITE[2]@). Some work also uses regu- larization to improve model generalization (Wang et al., 2021c; @CITE[1]@; ; @CITE[4]@). Prompt tuning (@CITE[7]@ and adapter-based tuning (@CITE[5]@ can also enhance model generalization to unseen test distributions.\\nReferences:ID: 1. Title: Towards Deep Learning Models Resistant to Adversarial Attacks\\nID: 2. Title: InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\\nID: 3. Title: Consistency Regularization for Cross-Lingual Fine-Tuning\\nID: 4. Title: Posterior Differential Regularization with f-divergence for Improving Model Robustness\\nID: 5. Title: SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\\nID: 6. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 7. Title: On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation<|eot_id|>'},\n",
       " 7294: {'predict': 'Existing duplication detection studies can be broadly classified into two categories: (i) unsupervised approaches and (ii) supervised approaches. Unsupervised approaches rely on the assumption that similar texts should have similar representations, and thus, the task is formulated as a clustering problem (Faruqui et al., 2015; ; @CITE[4]@; @CITE[2]@; ; @CITE[1]@. These approaches are simple and can be used for real-time detection, but they are sensitive to the choice of the clustering algorithm and often produce suboptimal results. Supervised approaches, on the other hand, train a classifier to differentiate between duplicate and non-duplicate pairs, which are often annotated by human annotators. These approaches are more accurate than unsupervised approaches but require annotated data and are computationally expensive. In addition, supervised approaches have been shown to suffer from biases in the annotated data (@CITE[3]@; @CITE[1]@. In this paper, we propose a novel unsupervised approach that does not rely on clustering algorithms and is less sensitive to the choice of hyperparameters. Our approach is also more efficient than supervised approaches and does not suffer from biases in the annotated data.\\nReferences:ID: 1. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 2. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 3. Title: Pointwise Paraphrase Appraisal is Potentially Problematic\\nID: 4. Title: Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces',\n",
       "  'ground_truth': '\\n\\nExisting duplication detection studies mainly build text-pair classifiers based on bi-encoders or cross-encoders. Bi-encoders (Mueller and Thyagarajan, 2016; ; ; @CITE[3]@) encode each question individually while cross-encoders (Devlin et al., 2019; Liang et al., 2019; @CITE[4]@; @CITE[2]@) use cross-attention between questions or self-attention by concatenating multiple questions. Despite the popularity of cross-encoders, they typically have high computational cost () and inconsistent performance (@CITE[1]@). In this work, we focus on bi-encoders, which are more computationally efficient and have shown consistent performance in duplicate question detection. However, the above-mentioned methods are generally generic text-pair matching and do not consider intents in question, which can lead to sub-optimal results in deduplicating datasets for instruction tuning. Therefore, we incorporate intent features in Intent-DQD to detect duplicate questions. Unlike existing research on intents (; ;, 2022) which typically rely on human annotation, our proposed method is annotation-free, which leverages characteristics of CQA forums and automatically mine labels for intents.\\nReferences:ID: 1. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 2. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 3. Title: Pointwise Paraphrase Appraisal is Potentially Problematic\\nID: 4. Title: Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces<|eot_id|>'},\n",
       " 6986: {'predict': 'Temporal pre-trained models are a type of contextual language model that can be fine-tuned on the task of temporal text classification (@CITE[1]@. These models have been studied in the context of social media data and crisis events (@CITE[5]@; @CITE[6]@; @CITE[4]@. Temporal pre-trained models are also used for translation tasks, including the translation of ancient texts (@CITE[2]@ and for predicting the temporal information of texts (@CITE[3]@. There have been few studies on temporal pre-trained models for the task of sentiment analysis. @CITE[4]@ have shown that the performance of BERT-based models deteriorates with time for sentiment analysis tasks. @CITE[6]@ have proposed a method for improving the temporal generalization of pre-trained language models with lexical semantic change, and have shown that this method can be applied to the task of sentiment analysis. However, these studies do not provide a comprehensive comparison of the performance of different temporal pre-trained models for sentiment analysis tasks.\\nReferences:ID: 1. Title: Temporal Effects on Pre-trained Models for Language Processing Tasks\\nID: 2. Title: Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media\\nID: 3. Title: The challenges of temporal alignment on Twitter during crises\\nID: 4. Title: Time-Aware Ancient Chinese Text Translation and Inference\\nID: 5. Title: Time Masking for Temporal Language Models\\nID: 6. Title: Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change',\n",
       "  'ground_truth': '\\n\\nOn the other hand, research on the temporal pre-trained models shows that encoding the temporal information in language modeling is beneficial in both upstream and downstream tasks (@CITE[5]@; Röttger and @CITE[3]@). Efforts have been made to build models that can learn temporal information, such as incorporating timestamps of texts into the input of the model (Pokrywka and Grali´nski, 2022; @CITE[4]@), prefixing the input with a temporal representation (@CITE[2]@; @CITE[6]@), or learning the masked temporal information (@CITE[1]@; ). However, there is still insufficient discussion of document modeling without explicit temporal mentions.\\nReferences:ID: 1. Title: Temporal Effects on Pre-trained Models for Language Processing Tasks\\nID: 2. Title: Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media\\nID: 3. Title: The challenges of temporal alignment on Twitter during crises\\nID: 4. Title: Time-Aware Ancient Chinese Text Translation and Inference\\nID: 5. Title: Time Masking for Temporal Language Models\\nID: 6. Title: Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change<|eot_id|>'},\n",
       " 5288: {'predict': 'The global convolutional layer is a novel architecture that is used for modeling long-range dependencies in sequential data. It can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ordinary differential equation (ODE). The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can also be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of complex systems. It has been widely used in modeling the dynamics of physical systems, such as the dynamics of particles, fluids, and solids. The global convolutional layer can be applied to many domains, including computer vision, natural language processing, and speech recognition. The idea of global convolutional layers is to capture the long-range dependencies by learning the convolutional kernels in a continuous domain. The kernel is discretized into a matrix, which is then used to transform the input sequence. The global convolutional layer can be viewed as a discretized version of a continuous-time dynamical system, which is a type of non-linear ODE. The ODE is a powerful tool to model the dynamics of',\n",
       "  'ground_truth': '\\n\\nThe global convolution, also known as a long convolution, is a layer that applies scalar convolutions along the sequence dimension, enabling the handling of unrestricted 1-D sequences. Empirically, these layers have shown strong performance in tasks involving long-range dependencies, particularly in domains such as NLP (@CITE[6]@; @CITE[4]@; @CITE[10]@), audio (@CITE[5]@), speech (@CITE[7]@), video (@CITE[2]@; ), time-series analysis (@CITE[1]@) and more. Moreover, they exhibit computational efficiency as their cost is sub-quadratic. However, to achieve SOTA results, appropriate regularization is necessary. The approach of (@CITE[9]@,a; ; @CITE[11]@) incorporates a parameterization that inherently regularizes the kernel and decoupling sequence length from parameter count. (@CITE[8]@; @CITE[3]@) utilizes an implicit parameterization learned by FFNs operating on positional encodings, while (@CITE[1]@) explicitly regularizes the convolution kernels using squash or smooth operators.\\nReferences:ID: 1. Title: Hungry Hungry Hippos: Towards Language Modeling with State Space Models\\nID: 2. Title: Diagonal State Spaces are as Effective as Structured State Spaces\\nID: 3. Title: Pretraining Without Attention\\nID: 4. Title: Diagonal State Space Augmented Transformers for Speech Recognition\\nID: 5. Title: Efficient Movie Scene Detection using State-Space Transformers\\nID: 6. Title: Selective Structured State-Spaces for Long-Form Video Understanding\\nID: 7. Title: Simple Hardware-Efficient Long Convolutions for Sequence Modeling\\nID: 8. Title: Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\\nID: 9. Title: Global memory transformer for processing long documents\\nID: 10. Title: CKConv: Continuous Kernel Convolution For Sequential Data\\nID: 11. Title: Hyena Hierarchy: Towards Larger Convolutional Language Models<|eot_id|>'},\n",
       " 729: {'predict': 'KD in BERT compression is a popular approach to compressing BERT models while maintaining the performance. Some works (@CITE[6]@; @CITE[2]@; @CITE[3]@; @CITE[8]@; @CITE[4]@; @CITE[7]@; @CITE[5]@) use KD to compress BERT models by distilling the intermediate layers of a teacher model into a student model. For example, @CITE[6]@ proposed a patient knowledge distillation approach to compress BERT models by learning from multiple intermediate layers of the teacher model. @CITE[2]@ proposed a dynamic BERT model, which can flexibly adjust the size and latency by selecting adaptive width and depth. @CITE[3]@ proposed a MobileBERT model, which is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. @CITE[8]@ proposed an attention-based layer projection for knowledge distillation. @CITE[4]@ proposed a combinatorial technique which relies on attention to fuse teacher-side information and take each layer’s significance into consideration. @CITE[7]@ proposed a multi-head self-attention relation distillation for compressing pretrained transformers. @CITE[5]@ proposed a unified distillation framework to distill the intermediate layers of a teacher model into a student model. However, the above methods only consider the performance of the compressed model on the downstream tasks, but ignore the interpretability of the compressed model. In this paper, we propose a unified framework to compress BERT models and improve the interpretability of the compressed models.\\nReferences:ID: 1. Title: Patient Knowledge Distillation for BERT Model Compression\\nID: 2. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 3. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 4. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 5. Title: ALP-KD: Attention-Based Layer Projection for Knowledge Distillation\\nID: 6. Title: Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers\\nID: 7. Title: MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers\\nID: 8. Title: Attention is not not Explanation',\n",
       "  'ground_truth': '\\n\\nKD is widely studied in BERT compression. In addition to distilling the teacher’s predictions as in Hinton et al. (2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (@CITE[6]@; @CITE[7]@; ) and the self-attention distributions (; @CITE[6]@). Typically, the knowledge is extensively distilled in a layer-wise manner. To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (; @CITE[5]@) or KD on Transformer-based NMT models (@CITE[7]@). In contrast to these studies that attempt to increase the amount knowledge, we study BERT KD from the compression point of view. Similar idea can be found in MiniLMs (@CITE[6]@), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (@CITE[8]@; ; Brunner et al., 2020; ) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation.\\nReferences:ID: 1. Title: Patient Knowledge Distillation for BERT Model Compression\\nID: 2. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 3. Title: DynaBERT: Dynamic BERT with Adaptive Width and Depth\\nID: 4. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 5. Title: ALP-KD: Attention-Based Layer Projection for Knowledge Distillation\\nID: 6. Title: Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers\\nID: 7. Title: MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers\\nID: 8. Title: Attention is not not Explanation<|eot_id|>'},\n",
       " 4293: {'predict': 'Contrastive learning has been recently applied to improve faithfulness. @CITE[8]@ propose a contrastive learning formulation that leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. @CITE[5]@ propose a contrastive learning framework to train summarization systems that are better at distinguishing between the generated summaries and the erroneous summaries. @CITE[1]@ propose a contrastive learning formulation that leverages the contrastive learning between positive and negative samples to train a dialogue summarization system that is better at distinguishing between the generated summaries and the erroneous summaries. @CITE[3]@ propose a contrastive learning framework that leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train a summarization system that is better at distinguishing between them. @CITE[6]@ propose a contrastive learning framework that leverages both positive and negative training data to train a summarization system that is better at distinguishing between them. @CITE[2]@ propose a contrastive learning framework that leverages both positive and negative training data to train a summarization system that is better at distinguishing between them. @CITE[4]@ propose a contrastive learning framework that leverages both positive and negative training data to train a summarization system that is better at distinguishing between them. @CITE[7]@ propose a contrastive learning framework that leverages both positive and negative training data to train a summarization system that is better at distinguishing between them.\\nReferences:ID: 1. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 2. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 3. Title: CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization\\nID: 4. Title: Factual Error Correction for Abstractive Summarization Models\\nID: 5. Title: Entity-level Factual Consistency of Abstractive Text Summarization\\nID: 6. Title: Enhancing Factual Consistency of Abstractive Summarization\\nID: 7. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 8. Title: Factual Error Correction for Abstractive Summarization Models',\n",
       "  'ground_truth': '\\n\\nContrastive learning for faithfulness has been applied to fine-tuning (@CITE[1]@; @CITE[6]@; @CITE[4]@), post-hoc editing (@CITE[8]@; @CITE[5]@), re-ranking (@CITE[7]@), and evaluation (@CITE[2]@; @CITE[8]@; @CITE[3]@). This line of research has largely focused on the methods used to generate synthetic errors for negative contrast sets: i.e., by directly mimicking errors observed during human evaluation (@CITE[6]@), entity swapping (@CITE[4]@), language model infilling (@CITE[4]@), or using unfaithful system outputs (@CITE[1]@).\\nReferences:ID: 1. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 2. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 3. Title: CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization\\nID: 4. Title: Factual Error Correction for Abstractive Summarization Models\\nID: 5. Title: Entity-level Factual Consistency of Abstractive Text Summarization\\nID: 6. Title: Enhancing Factual Consistency of Abstractive Summarization\\nID: 7. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 8. Title: Factual Error Correction for Abstractive Summarization Models<|eot_id|>'},\n",
       " 5056: {'predict': 'Explainable deep learning in NLP has been a topic of increasing interest in recent years. @CITE[5]@ propose to explain the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. @CITE[7]@ assign each feature an importance value for a particular prediction. @CITE[9]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[8]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[10]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[2]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[1]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[6]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[4]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[3]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[10]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[11]@ propose to explain the predictions of a model by identifying the set of input features that are most important for the prediction. @CITE[',\n",
       "  'ground_truth': '\\n\\nExplainable Deep Learning in Natural Language Processing (NLP) can be decomposed in two categories: representational and practical. Representational XAI in NLP focuses on understanding the underlying structure of the representations. For instance, studies have shown that Transformer-based architectures develop abstract symbolic, or compositional, representations (@CITE[2]@; Li et al., 2022b). Similarly, it has been shown that conceptual knowledge can have sparse representations, which can thus be located and edited to induce different predictions (@CITE[9]@. Practical methods can analyze the outputs of the models, for example when perturbing the inputs (@CITE[10]@; ; Lundberg and @CITE[7]@. Recently, practical XAI in NLP focuses on prompting to increase explainability. For instance, chain-of-thought prompting is a method that implements a sequence of interposed NLP steps leading to a final answer (@CITE[1]@; @CITE[1]@; @CITE[4]@; Lyu et al., 2023). This method has the advantage to provide insights on the logical reasoning steps behind a model’s behavior, and thus allows to understand (at a higher level) the predictive success or failure of LLM (@CITE[1]@; @CITE[6]@; @CITE[1]@. Although advances in XAI within the NLP domain have offered interesting insights on the underlying mechanisms and representational structures of LLM, there has been a recent push to solely focus on interpretable models for high-stakes decisions (@CITE[11]@. This push is motivated by dramatic errors made by these models in real life situations, such as assessing criminal risk at large scale (@CITE[3]@ or incorrectly denying bail for criminals (@CITE[8]@. The main rationale behind this idea holds in that there will always be a certain level of error (or information loss) associated to the explanations of black-box models. Indeed, these explanations can, by definition, only partially incorporate information of the model’s reasoning process. Following Rudin (2019), we make the difference between an explainable black-box model and an interpretable white-box model. Explainability relies on algorithms aiming to explain the model predictions by showing cues to the user like LIME, or other ad-hoc methods (@CITE[5]@; @CITE[5]@. Interpretability relies on the possibility to know exactly why the model is making a prediction because they are inherent to the prediction and faithful to what the model actually computes. However, methods like CoT which should be interpretable (because outputting explanations with their predictions) have not always shown to give faithful explanations (. It is also arguable that our method relies on learned representations from a BERT, which decreases its overall interpretability.\\nReferences:ID: 1. Title: Unit Testing for Concepts in Neural Networks\\nID: 2. Title: Locating and Editing Factual Associations in GPT\\nID: 3. Title: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\\nID: 4. Title: A Unified Approach to Interpreting Model Predictions\\nID: 5. Title: Scaling Instruction-Finetuned Language Models\\nID: 6. Title: Scaling Instruction-Finetuned Language Models\\nID: 7. Title: Explicit Planning Helps Language Models in Logical Reasoning\\nID: 8. Title: Scaling Instruction-Finetuned Language Models\\nID: 9. Title: Active Prompting with Chain-of-Thought for Large Language Models\\nID: 10. Title: Scaling Instruction-Finetuned Language Models\\nID: 11. Title: Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead<|eot_id|>'},\n",
       " 1105: {'predict': 'Inspired by the instance discrimination task in supervised learning, self-supervised instance discrimination aims to distinguish between positive instances and their negative samples. The positive samples are generated by data augmentation, such as flipping, rotating, or cropping, while the negative samples are randomly selected from the dataset. The instance discrimination loss is formulated as the binary cross-entropy loss, and it is widely used in vision and NLP. In the vision domain, MoCo (, SimCLR (@CITE[4]@, and BYOL (@CITE[1]@ have been proposed for self-supervised learning. In the NLP domain, instance discrimination has been used in pre-training language models, such as ELECTRA (@CITE[2]@, COCO-LM (@CITE[3]@, and DINO (@CITE[1]@.\\nReferences:ID: 1. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\\nID: 4. Title: Momentum Contrast for Unsupervised Visual Representation Learning',\n",
       "  'ground_truth': '\\n\\nAnother relevant line of work is self-supervised con- trastive learning, which essentially solves an in- stance discrimination task that targets at discrim- inating each positive pair from all negative pairs within each batch of data (Oord et al., 2018; Bach- man et al., 2019; @CITE[4]@;. Owing to their notable successes, self-supervised instance discrimination has become a prominent pre-training strategy for providing effective repre- sentations for a wide range of downstream tasks. While recent successes are primarily driven by the computer vision domain, there is an increasing interest in leveraging variant instance discrimina- tion tasks to support Pretraining Language Models (PLMs) (@CITE[1]@; @CITE[3]@; @CITE[4]@; @CITE[2]@. Our proposal can be seen as complementary to this stream of work, considering that the instance dis- crimination learning based PLMs provide a good foundation for PairSupCon to further enhance the sentence representation quality by further learning on NLI.\\nReferences:ID: 1. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\\nID: 4. Title: Momentum Contrast for Unsupervised Visual Representation Learning<|eot_id|>'},\n",
       " 4748: {'predict': 'Video multimodal fusion has been extensively studied in recent years. For example, @CITE[2]@ proposed a tensor fusion network to fuse multimodal representations for sentiment analysis. @CITE[3]@ introduced low-rank multimodal fusion to improve efficiency. @CITE[4]@ proposed a video multimodal fusion network that uses a multi-layer fusion module to fuse multimodal features. @CITE[1]@ proposed a multi-modal fusion network with a self-attention mechanism to fuse multimodal features. However, these methods are not suitable for our task since they are designed for fixed-size inputs and are not robust to the long-tail problem. In contrast, our method can fuse multimodal features in a flexible way and is robust to the long-tail problem. In addition, our method is more efficient than previous methods since it does not require to transform input into tensor. In this paper, we propose a new method that uses a self-attention mechanism to fuse multimodal features and is robust to the long-tail problem.\\nReferences:ID: 1. Title: Tensor Fusion Network for Multimodal Sentiment Analysis\\nID: 2. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 3. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 4. Title: Attention Is All You Need',\n",
       "  'ground_truth': '\\n\\nVideo multimodal fusion aims to join and comprehend information from two or more modalities in videos to make a comprehensive prediction. Early fusion model adopted simple network architectures. @CITE[1]@; @CITE[4]@ fuse features by matrix operations; and @CITE[4]@ designed a LSTM-based model to capture both temporal and inter-modal interactions for better fusion. More recently, models influenced by prevalence of Transformer (@CITE[2]@ have emerged constantly: Zhang et al. (2019) injected visual information in the decoder of Transformer by cross attention mechanism to do multimodal translation task; proposed a text-centric multimodal fusion shared private framework for multimodal fusion, which consists of the cross-modal prediction and sentiment regression parts. And now vision-and-language pre-training has become a promising practice to tackle video multimodal fusion tasks. (@CITE[3]@ firstly extend the Transformer structure to video-language pre-training and used three pre-training tasks: masked language prediction, video text matching, masked video prediction. In contrast to existing works, we focus on the fundamental characteristic of video: audio and visual inputs in video are redundant and noisy (Nagrani et al., 2021) so we aim to remove noise and redundancy while preserving critical information.\\nReferences:ID: 1. Title: Tensor Fusion Network for Multimodal Sentiment Analysis\\nID: 2. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 3. Title: Efficient Low-rank Multimodal Fusion with Modality-Specific Factors\\nID: 4. Title: Attention Is All You Need<|eot_id|>'},\n",
       " 636: {'predict': 'Adversarial learning is a popular technique to train models to be robust against the generated adversarial samples. In the field of IR, adversarial learning has been used to improve the retrieval performance. @CITE[4]@ proposed a unified generative and discriminative model for IR, which is trained in a minimax game fashion. The generative model is trained to generate the adversarial samples to attack the discriminative model. The discriminative model is trained to distinguish the adversarial samples from the normal samples. @CITE[3]@ proposed a method to incorporate external knowledge into machine reading comprehension for generative question answering. @CITE[1]@ proposed a semi-supervised learning method for image captioning. @CITE[2]@ proposed an adversarial sampling and training framework to learn ad-hoc retrieval models with implicit feedback. @CITE[5]@ proposed an adversarial training method for generative question answering. @CITE[6]@ proposed a generative adversarial network for adversarial training in natural language inference. @CITE[6]@ proposed an adversarial training method for text classification.\\nReferences:ID: 1. Title: InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\\nID: 2. Title: IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models\\nID: 3. Title: Incorporating External Knowledge into Machine Reading for Generative Question Answering\\nID: 4. Title: Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach\\nID: 5. Title: Adversarial Sampling and Training for Semi-Supervised Information Retrieval\\nID: 6. Title: IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models',\n",
       "  'ground_truth': '\\n\\nAdversarial learning has been widely used in areas such as computer vision (@CITE[3]@; ; Radford et al., 2015; Arjovsky et al., 2017), text generation (; @CITE[6]@; @CITE[4]@; @CITE[1]@; ; @CITE[2]@, relation extraction (@CITE[5]@; Qin et al., 2018), question answering (;, etc. We proposed to apply adversarial learning to code retrieval, because they have effectively improved cross-domain task performances and helped generate useful training data, We adapted the method from for the bi-modal QC scenario. As future work, adversarial learning for QC can be generalized to other settings with different base neural models ( or with more complex adversarial learning methods, such as adding perturbed noises ( or generating adversarial sequences (;. Our method differs from most adversarial learning work in that the discriminator (QC model) does not see all generated samples as equally negative.\\nReferences:ID: 1. Title: InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\\nID: 2. Title: IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models\\nID: 3. Title: Incorporating External Knowledge into Machine Reading for Generative Question Answering\\nID: 4. Title: Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach\\nID: 5. Title: Adversarial Sampling and Training for Semi-Supervised Information Retrieval\\nID: 6. Title: IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models<|eot_id|>'},\n",
       " 2154: {'predict': 'To deal with the limited information challenge, there are several lines of research. Some methods use the source domain information to train a model that can be adapted to the target domain. For example, @CITE[7]@ use a prototypical network to learn a model that can be adapted to the target domain. @CITE[11]@ use a multi-task learning method to learn a model that can be adapted to the target domain. @CITE[3]@ use a meta-learning method to learn a model that can be adapted to the target domain. Some methods use the target domain information to train a model that can be adapted to the target domain. For example, @CITE[12]@ use a nearest neighbor classifier to train a model that can be adapted to the target domain. @CITE[5]@ use a contrastive learning method to train a model that can be adapted to the target domain. Some methods use both the source and target domain information to train a model that can be adapted to the target domain. For example, @CITE[8]@ use a multi-task learning method to train a model that can be adapted to the target domain. @CITE[4]@ use a meta-learning method to train a model that can be adapted to the target domain. Some methods use a weakly labeled dataset to train a model that can be adapted to the target domain. For example, @CITE[2]@ use a weakly labeled dataset to train a model that can be adapted to the target domain. Some methods use a weakly labeled dataset to train a model that can be adapted to the target domain. For example, @CITE[9]@ use a weakly labeled dataset to train a model that can be adapted to the target domain. Some methods use a weakly labeled dataset to train a model that can be adapted to the target domain. For example, @CITE[10]@ use a weakly labeled dataset to train a model that can be adapted to the target domain. Some methods use a weakly labeled dataset to train a model that can be adapted to the target domain. For example, @CITE[6]@ use a weakly labeled dataset to train a model that can be adapted to the target domain.\\nReferences:ID: 1. Title: Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition\\nID: 2. Title: Template-Based Named Entity Recognition Using BART\\nID: 3. Title: Template-Based Named Entity Recognition Using BART\\nID: 4. Title: Template-Based Named Entity Recognition Using BART\\nID: 5. Title: Template-Based Named Entity Recognition Using BART\\nID: 6. Title: Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER\\nID: 7. Title: Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER\\nID: 8. Title: Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network\\nID: 9. Title: Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning\\nID: 10. Title: Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition\\nID: 11. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 12. Title: Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data',\n",
       "  'ground_truth': '\\n\\nCurrent FS-NER studies mostly focus on leveraging external knowledge, many knowledge resources are used: 1) PLMs. Early FS-NER studies (@CITE[10]@; mainly use PLMs for better encoding. And prompt-based NER formulation is proposed to exploit the PLMs’ knowledge more effectively (; ; ; ; @CITE[11]@; ; ; ;. 2) Existing annotation datasets. These studies (@CITE[10]@; @CITE[12]@; ; Li et al., 2020a,b; @CITE[10]@; @CITE[12]@ focus on reusing annotations in existing datasets, and the annotations can be used to pre-train NER models. 3) Distantly annotated datasets. Some works (; @CITE[8]@; @CITE[7]@ try to automatically construct NER datasets via distant supervision, but which often suffer from the partially-labeled (; ; @CITE[11]@ and noise label (@CITE[5]@; @CITE[11]@;,a) problem.\\nReferences:ID: 1. Title: Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition\\nID: 2. Title: Template-Based Named Entity Recognition Using BART\\nID: 3. Title: Template-Based Named Entity Recognition Using BART\\nID: 4. Title: Template-Based Named Entity Recognition Using BART\\nID: 5. Title: Template-Based Named Entity Recognition Using BART\\nID: 6. Title: Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER\\nID: 7. Title: Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER\\nID: 8. Title: Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network\\nID: 9. Title: Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning\\nID: 10. Title: Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition\\nID: 11. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 12. Title: Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data<|eot_id|>'},\n",
       " 1941: {'predict': \"Pre-training has been a powerful tool in NLP, which enables models to learn universal language representations. BERT (@CITE[1]@) is a typical example, which achieves great success in many NLP tasks by pre-training on large-scale unlabeled corpora. To adapt BERT to dialogue domain, TOD-BERT (@CITE[2]@) is proposed by incorporating user and system tokens into the masked language modeling and adding a contrastive objective to simulate the response selection task. The pre-trained model is then fine-tuned on downstream tasks. However, TOD-BERT is pre-trained on a small-scale dataset, which may not cover enough dialogue styles. To overcome this issue, @CITE[3]@ propose domain-adaptive pre-training (DAPT), which pre-trains the model on the target domain and then fine-tunes it on downstream tasks. In this paper, we propose a pre-training framework, which not only incorporates the dialogue structure but also learns the dialogue styles by pre-training on large-scale unlabeled dialogue corpora.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue\\nID: 3. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\",\n",
       "  'ground_truth': \"\\n\\nUnsupervised or self-supervised pre-training have achieved remarkable successes in natural language processing (@CITE[2]@; ; ; @CITE[1]@; Brown et al., 2020). However, these models are pre-trained on a very large general domain corpus, e.g. Wikipedia, and may limit their performance on a specific task due to distribution shift (; ; @CITE[3]@. To better handle aforementioned issue, domain-adaptive pre-training (DAPT) by continuing pre-training of existing language models, e.g. BERT and RoBERTa, on a large corpus of unlabeled domain-specific text data has been proposed and achieved great successes in specific domains (@CITE[3]@; ;. proposed BioBERT by continuing pre-training of BERT on biomedical domain corpus and outperformed BERT in biomedical text mining significantly. Following a similar idea, proposed ToD-BERT by continuing pre-training of BERT on nine dialogue datasets for NLU tasks in task-oriented dialogue systems and achieved great successes in various few-shot NLU tasks in dialogue domain. @CITE[3]@ took one step further and continued pre-training of language models on a much smaller amount of unlabeled data but drawn from the same distribution for a given task (TAPT), which not only can achieve competitive results with DAPT but also is complementary with it.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue\\nID: 3. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks<|eot_id|>\"},\n",
       " 685: {'predict': 'Language-and-Vision Datasets. The visual question answering dataset (VQA) (@CITE[6]@) has been the most widely used dataset for evaluating the joint understanding of visual and textual information. Several VQA datasets have been proposed to improve the VQA task, such as the balanced VQA dataset (@CITE[5]@), the Visual Question Answering 2.0 dataset (, the Visual Genome dataset (, the Visual7W dataset (@CITE[2]@), the GQA dataset (@CITE[9]@), the CLEVR dataset (, the GQA dataset (, the Visual7W dataset (@CITE[2]@), the VCR dataset (@CITE[8]@), and the NLVR dataset (@CITE[1]@). Recently, the VQA dataset has been extended to the task of visual commonsense reasoning, which aims to understand the commonsense knowledge behind the visual scenes. The VCR dataset (@CITE[8]@) is a large-scale dataset for visual commonsense reasoning, which contains 290k multiple-choice questions derived from 110k movie scenes. The NLVR dataset (@CITE[1]@) is another dataset for visual commonsense reasoning, which contains 12,014 natural language descriptions paired with 6,447 image pairs. The Visually Grounded Textual Inferences dataset (VGTI) (@CITE[4]@) is a new dataset for visual commonsense reasoning, which contains 107,292 natural language captions paired with image pairs. The VQA dataset has been extended to the task of visual question generation (VQG) (@CITE[3]@), which aims to generate questions for given images. The VQG dataset contains 10,000 image-question pairs. The Visual Question Generation (VQG) dataset ( is a large-scale dataset for VQG, which contains 20,000 image-question pairs. The VQG dataset is extended to the task of visual question generation and answering (VQGA) (@CITE[7]@), which aims to generate and answer questions for given images.\\nReferences:ID: 1. Title: VQA: Visual Question Answering\\nID: 2. Title: Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\\nID: 3. Title: Microsoft COCO: Common Objects in Context\\nID: 4. Title: Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\\nID: 5. Title: Multi30K: Multilingual English-German Image Descriptions\\nID: 6. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 7. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 8. Title: A Corpus for Reasoning About Natural Language Grounded in Photographs\\nID: 9. Title: From Recognition to Cognition: Visual Commonsense Reasoning',\n",
       "  'ground_truth': '\\n\\nDatasets involving images and languages cover a variety of tasks, including visual question answering (@CITE[5]@; @CITE[6]@), image caption generation (; @CITE[5]@; @CITE[6]@), visual storytelling (; ), machine translation (@CITE[5]@), visual reasoning (@CITE[7]@; @CITE[9]@; ), and visual common sense (@CITE[8]@).\\nReferences:ID: 1. Title: VQA: Visual Question Answering\\nID: 2. Title: Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\\nID: 3. Title: Microsoft COCO: Common Objects in Context\\nID: 4. Title: Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\\nID: 5. Title: Multi30K: Multilingual English-German Image Descriptions\\nID: 6. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 7. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 8. Title: A Corpus for Reasoning About Natural Language Grounded in Photographs\\nID: 9. Title: From Recognition to Cognition: Visual Commonsense Reasoning<|eot_id|>'},\n",
       " 5507: {'predict': 'In this work, we explore the retrieval of documents and sentences for information extraction. In recent years, generative retrieval has been studied in different domains. @CITE[5]@ propose a generative neural corpus indexer for document retrieval. @CITE[1]@ propose an end-to-end retrieval model that retrieves relevant document identifiers. @CITE[3]@ propose a generative model for evidence retrieval in fact verification. @CITE[4]@ propose a generative retrieval framework for question answering. @CITE[4]@ also propose a multi-task distillation approach to enhance the retrieval quality. However, these methods are not designed for IE tasks, and their generative model is not suitable for our task. In addition, these methods are not evaluated on IE tasks. @CITE[2]@ propose a generative retrieval framework for document retrieval, which is similar to our work. However, their framework is not designed for IE tasks, and their generative model is not suitable for our task.\\nReferences:ID: 1. Title: A Neural Corpus Indexer for Document Retrieval\\nID: 2. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 3. Title: GERE: Generative Evidence Retrieval for Fact Verification\\nID: 4. Title: Understanding Differential Search Index for Text Retrieval\\nID: 5. Title: Understanding Differential Search Index for Text Retrieval',\n",
       "  'ground_truth': '\\n\\nIn recent times, generative retrieval has gained significant popularity in the field of information retrieval (IR) tasks. A model-based IR approach, where docids are embedded into the model, was first introduced in (Metzler et al., 2021; Tay et al., 2022). Since then, several other models (@CITE[4]@; Zhuang et al., 2022; @CITE[5]@ incorporating a query augmentation module have been developed to enhance the performance. Moreover, the model-based IR framework has been extended to knowledge-intensive language tasks, leading to notable improvements in performance (; Bevilacqua et al., 2022; @CITE[1]@. Additionally, a series of studies by @CITE[2]@; @CITE[2]@; @CITE[3]@; @CITE[2]@ explored various methods for representing docids to retain docid semantics. In this paper, we focus on aligning the optimizing objective of generative retrieval models with document-level relevance, so as to alleviate the bias against the objective of ranking tasks.\\nReferences:ID: 1. Title: A Neural Corpus Indexer for Document Retrieval\\nID: 2. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 3. Title: GERE: Generative Evidence Retrieval for Fact Verification\\nID: 4. Title: Understanding Differential Search Index for Text Retrieval\\nID: 5. Title: Understanding Differential Search Index for Text Retrieval<|eot_id|>'},\n",
       " 3493: {'predict': 'The task of generating reasonable explanation sentences is also referred to as commonsense explanation generation (@CITE[11]@; @CITE[5]@; @CITE[7]@; @CITE[2]@; @CITE[3]@; @CITE[9]@; @CITE[4]@; @CITE[6]@; @CITE[1]@; @CITE[8]@; @CITE[10]@. In this task, the input is a statement, and the goal is to generate a sentence that provides a reasonable explanation for the statement. For example, given the statement “a dog is a mammal”, the goal is to generate a sentence such as “a dog is a mammal because it is warm-blooded”. This task requires the model to have commonsense knowledge, relational reasoning ability, and the ability to generate sentences that are coherent and reasonable. In this paper, we focus on the task of generating reasonable explanation sentences, which is a fundamental task for explainable recommendation.\\nReferences:ID: 1. Title: Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths\\nID: 2. Title: Pretrained Transformers Improve Out-of-Distribution Robustness\\nID: 3. Title: CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\\nID: 4. Title: What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge\\nID: 5. Title: Story Ending Generation with Incremental Encoding and Commonsense Knowledge\\nID: 6. Title: Personalized Transformer for Explainable Recommendation\\nID: 7. Title: Graph Collaborative Reasoning\\nID: 8. Title: Counterfactual Explainable Recommendation\\nID: 9. Title: Neural Logic Reasoning\\nID: 10. Title: Neural Collaborative Reasoning\\nID: 11. Title: Reinforcement Knowledge Graph Reasoning for Explainable Recommendation',\n",
       "  'ground_truth': '\\n\\nSome of the current works (@CITE[1]@; ) focus on adjusting models to output sentences that fit the context and commonsense knowledge. Because investigations (@CITE[11]@; ) find that current pre-trained models have disadvantages in solving tasks needing inferences steps, some of these research (; @CITE[5]@; ) propose using a knowledge graph to provide additional concepts to ensure controllable generation. Natural language explanation generation is also extensively explored in explainable recommender system research (@CITE[4]@), which aims to generate personalized and reasonable natural language sentences to explain why certain items are recommended to users. Early methods used manual templates to generate explanation sentences (@CITE[7]@), and more recent works explored neural-template generation methods (@CITE[4]@,b), personalized transformers (@CITE[9]@, 2022b), visual-enhanced explanation generation (@CITE[6]@; @CITE[10]@), explanation ranking methods (@CITE[8]@, 2021b), counterfactual explanations (, 2022), logical explanations (; @CITE[2]@, 2022a,b; Zhu et al., 2021), path-based explanations (@CITE[3]@;, 2020), as well as large language models for explanation generation such as the P5 large recommendation model (). Following previous work, we use pre-trained language models as a knowledge base to replace annotated knowledge graphs so as to provide concepts for commonsense explanation sentence generation.\\nReferences:ID: 1. Title: Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths\\nID: 2. Title: Pretrained Transformers Improve Out-of-Distribution Robustness\\nID: 3. Title: CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\\nID: 4. Title: What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge\\nID: 5. Title: Story Ending Generation with Incremental Encoding and Commonsense Knowledge\\nID: 6. Title: Personalized Transformer for Explainable Recommendation\\nID: 7. Title: Graph Collaborative Reasoning\\nID: 8. Title: Counterfactual Explainable Recommendation\\nID: 9. Title: Neural Logic Reasoning\\nID: 10. Title: Neural Collaborative Reasoning\\nID: 11. Title: Reinforcement Knowledge Graph Reasoning for Explainable Recommendation<|eot_id|>'},\n",
       " 6945: {'predict': 'Pre-trained image-language models, e.g., CLIP (@CITE[1]@), have been widely adopted for various vision-language tasks. CLIP-Event (@CITE[5]@) further enhances the alignment between vision and language by introducing event structures. MAGMA (@CITE[3]@) is a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. @CITE[7]@ proposed a novel pre-training method by training a model to predict the masked tokens in the decoder layer. @CITE[2]@ propose a method to meta-personalize a pre-trained VLM. @CITE[4]@ propose Align and Prompt, an efficient and effective video-and-language pre-training framework with better cross-modal alignment. @CITE[6]@ propose a generic framework ClipBERT that enables affordable end-to-end learning for video-and-language tasks, by employing sparse sampling. @CITE[8]@ present SlowFast networks for video recognition. @CITE[9]@ proposed a few-shot video-language learner via image and language models, which demonstrates strong performance on few-shot video-to-text tasks without the necessity of pretraining or finetuning on any video datasets. @CITE[1]@ propose a framework for video captioning, video question answering, video caption retrieval, and video future event prediction. @CITE[3]@ proposed a method for video-language pre-training by learning to predict the masked tokens in the decoder layer. @CITE[9]@ proposed a few-shot video-language learner via image and language models, which demonstrates strong performance on few-shot video-to-text tasks without the necessity of pretraining or finetuning on any video datasets. @CITE[1]@ proposed a framework for video captioning, video question answering, video caption retrieval, and video future event prediction.\\nReferences:ID: 1. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 2. Title: CLIP-Event: Connecting Text and Images with Event Structures\\nID: 3. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 4. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 5. Title: Meta-Personalizing Vision-Language Models to Find Named Instances in Video\\nID: 6. Title: Align and Prompt: Video-and-Language Pre-training with Entity Prompts\\nID: 7. Title: Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling\\nID: 8. Title: SlowFast Networks for Video Recognition\\nID: 9. Title: Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners',\n",
       "  'ground_truth': '\\n\\nIn the realm of visual and language fusion methodologies, exploratory methods, as exemplified by the multi-stream approach (Lei et al., 2018), entail the utilization of a unified encoder to merge various information streams. This method facilitates the utilization of visual and language information at diverse levels of granularity and offers straightforward scalability. However, information of the same granularity is merged into distinct streams before alignment, resulting in a relatively poor semantic consistency among these streams. In recent years, contrastive learning has been substantiated as an efficacious approach for acquiring cross-modal joint representations. The image-language pre-trained model, based on contrastive learning, aims to establish a mapping between global image features and global text features in a shared space (@CITE[1]@; ; @CITE[4]@; @CITE[8]@; ). These models consider aligned image-text pairs as positive samples and unaligned pairs as negative samples to optimize visual-language alignment. Due to the requirement of a substantial amount of training data for video-language models to perform well, recent research has focused on transferring image-language models to video-text tasks (; @CITE[2]@; @CITE[3]@). This is because there is a strong correlation between images and videos. Sparse sampling offers an effective approach to video representation (@CITE[6]@; @CITE[7]@), contrasting with the use of 3D dense features (@CITE[9]@). It facilitates the more efficient utilization of pre-trained image-language models within the realm of video processing tasks (@CITE[5]@; Fang et al., 2021; ). Our method continues along this line of methodology by ingeniously constructing a bridge from image-language to video-language through meticulous design, thereby facilitating the transfer of knowledge.\\nReferences:ID: 1. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 2. Title: CLIP-Event: Connecting Text and Images with Event Structures\\nID: 3. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 4. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 5. Title: Meta-Personalizing Vision-Language Models to Find Named Instances in Video\\nID: 6. Title: Align and Prompt: Video-and-Language Pre-training with Entity Prompts\\nID: 7. Title: Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling\\nID: 8. Title: SlowFast Networks for Video Recognition\\nID: 9. Title: Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners<|eot_id|>'},\n",
       " 4806: {'predict': 'Large language models (LLMs) have been shown to be capable of zero-shot reasoning, including symbolic reasoning (@CITE[3]@, numerical reasoning (, and commonsense reasoning (. However, the reasoning capabilities of LLMs are not limited to these tasks. In this work, we explore the reasoning capabilities of LLMs on the task of textual entailment. Recent work has demonstrated that LLMs are capable of zero-shot textual entailment (TE) (, but the reasoning process is not well understood. In this work, we provide insights into the reasoning process used by LLMs on the TE task. We demonstrate that LLMs can be used for textual entailment, and we provide a detailed analysis of the reasoning process used by LLMs on this task. We also show that LLMs are capable of multilingual reasoning, and we evaluate LLMs on the task of multilingual textual entailment. We also explore the effect of prompting on the reasoning capabilities of LLMs, and we show that prompting can improve the performance of LLMs on the task of textual entailment. Our work is related to previous work on the use of LLMs for textual entailment (, and it is also related to previous work on the use of LLMs for reasoning (. We also explore the effect of prompting on the reasoning capabilities of LLMs, and we show that prompting can improve the performance of LLMs on the task of textual entailment. Our work is also related to previous work on the use of LLMs for multilingual textual entailment (@CITE[1]@. Our work is also related to previous work on the use of LLMs for zero-shot reasoning (. We also explore the effect of prompting on the reasoning capabilities of LLMs, and we show that prompting can improve the performance of LLMs on the task of textual entailment. Our work is also related to previous work on the use of LLMs for reasoning (. We also explore the effect of prompting on the reasoning capabilities of LLMs, and we show that prompting can improve the performance of LLMs on the task of textual entailment. Our work is also related to previous work on the use of LLMs for multilingual reasoning (@CITE[2]@. Our work is also related to previous work on the use of LLMs for zero-shot reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to previous work on the use of LLMs for reasoning (. Our work is also related to',\n",
       "  'ground_truth': '\\n\\nPrompting is an emergent capability of LLMs (. At sufficiently large scale, LLMs can utilize intermediate reasoning steps to improve performance across several tasks: arithmetic, metaphor generation (Prystawski et al., 2022), and commonsense/symbolic reasoning (. @CITE[2]@ further shows that by simply adding “Let’s think step by step” to a prompt, zero-shot performance on reasoning benchmarks sees significant improvement. We focus on “Let’s think step by step,” though other prompting methods have also yielded performance increases: aggregating CoT reasoning paths using self consistency (), combining outputs from several imperfect prompts (@CITE[3]@), or breaking down prompts into less →more complex questions (@CITE[1]@. While focus on 1Code prompts evaluation found here: https://github.com/SALT-NLP/ chain-of-thought-bias reasoning strategies for LLMs have increased, our work highlights the importance of evaluating these strategies on a broader range of tasks.\\nReferences:ID: 1. Title: Large Language Models are Zero-Shot Reasoners\\nID: 2. Title: Language Models are Multilingual Chain-of-Thought Reasoners\\nID: 3. Title: Ask Me Anything: A simple strategy for prompting language models<|eot_id|>'},\n",
       " 5950: {'predict': 'Contrastive learning is a self-supervised learning method that has gained great success in learning image representations. @CITE[6]@ first proposed a contrastive learning framework for visual representation learning. MoCo (@CITE[7]@ and SimCLR (@CITE[6]@ are two representative works of contrastive learning, which both use a contrastive loss to pull positive pairs together and push negative pairs apart. @CITE[11]@ extended the contrastive learning framework to the fully-supervised setting, which is called supervised contrastive learning. ContraD (@CITE[5]@ is a contrastive learning framework for GANs. SimCSE (@CITE[9]@ is a simple contrastive learning framework for sentence representation learning, which uses dropout as a minimal data augmentation method. DiffCSE (@CITE[10]@ and ESimCSE (@CITE[8]@ are two enhanced contrastive learning frameworks for sentence representation learning, which use the repetition operation and the momentum contrast, respectively. DiffCSE ( and ESimCSE (@CITE[10]@ are two contrastive learning frameworks for graph representation learning, which use the random walk operation and the repetition operation, respectively. @CITE[3]@ extended the contrastive learning framework to the graph domain. @CITE[1]@ and @CITE[2]@ extended the contrastive learning framework to the text domain. @CITE[4]@ extended the contrastive learning framework to the multi-view domain. In this paper, we propose a contrastive learning framework for multi-modal representation learning, which uses the augmentation operation to build positive pairs and the momentum contrast to build negative pairs.\\nReferences:ID: 1. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 2. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 3. Title: Big Self-Supervised Models are Strong Semi-Supervised Learners\\nID: 4. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 5. Title: Supervised Contrastive Learning\\nID: 6. Title: Training GANs with Stronger Augmentations via Contrastive Discriminator\\nID: 7. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 8. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 9. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings\\nID: 10. Title: ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding\\nID: 11. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings',\n",
       "  'ground_truth': '\\n\\nContrastive learning, as a self-supervised learning method, improves the transforming ability of models without large-scale labeled data and becomes a popular technique in computer vision domain (@CITE[4]@; @CITE[9]@; @CITE[2]@; Grill et al., 2020; @CITE[7]@. @CITE[4]@ proposed SimCLR, applied image transformations to generate two positive samples for each image randomly, and used the normalized temperature-scaled cross-entropy loss (NT-Xent) as the training loss to make positive pair close in the representation space. With the success of contrastive learning in computer vision tasks, recent studies attempt to extend it to other domains. @CITE[8]@ extended the self-supervised approach to the fully-supervised setting, allowing models to effectively leverage label information. @CITE[11]@ proposed ContraD to incorporate a contrastive learning scheme into GAN. In natural language processing, contrastive learning is widely applied to various tasks, such as sentence embedding, text classification, information extraction, and stance detection (@CITE[6]@; @CITE[1]@; ; @CITE[5]@; ; @CITE[10]@. In neural topic modeling, contrastive learning has been used to improve the VAE-based neural topic model by adding a contrastive objective to the training loss and taking a more principled approach to creating positive and negative samples (@CITE[3]@.\\nReferences:ID: 1. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 2. Title: Momentum Contrast for Unsupervised Visual Representation Learning\\nID: 3. Title: Big Self-Supervised Models are Strong Semi-Supervised Learners\\nID: 4. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 5. Title: Supervised Contrastive Learning\\nID: 6. Title: Training GANs with Stronger Augmentations via Contrastive Discriminator\\nID: 7. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 8. Title: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer\\nID: 9. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings\\nID: 10. Title: ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding\\nID: 11. Title: DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings<|eot_id|>'},\n",
       " 3811: {'predict': 'Influence functions are a well-established concept in robust statistics, which quantify the contribution of a given data point to the prediction of a model (Koh and Li, 2017). They have been applied in many areas, such as data valuation (@CITE[14]@, data debugging (@CITE[15]@, data selection (@CITE[8]@, data augmentation (@CITE[10]@, model explanation (@CITE[11]@; @CITE[2]@, and model selection (@CITE[9]@. Influence functions have been also applied in NLP. For example, @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point. @CITE[7]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to select data points to be used as negative examples in data augmentation. @CITE[16]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[12]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to select data points to be used as positive examples in data augmentation. @CITE[5]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to select data points to be used as negative examples in data augmentation. @CITE[13]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to select data points to be used as positive examples in data augmentation. @CITE[4]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to select data points to be used as negative examples in data augmentation. @CITE[3]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[1]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[17]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[18]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[19]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[20]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[16]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[21]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[22]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[23]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[24]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[25]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[26]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[27]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[28]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[29]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[30]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[31]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[32]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[13]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[33]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[34]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[35]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[36]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[37]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[38]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[39]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[40]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[41]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[42]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[5]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[43]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given test point, and use the results to identify data points to be used as negative examples in data augmentation. @CITE[6]@ use influence functions to measure the contribution of a given data point to the prediction of a model on a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a model on a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point to the prediction of a given data point. to the prediction of a given data point to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given the prediction of a given data point. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given data point. to the prediction of a given data point. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given data. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given data point. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the other the other the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the best. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the prediction of a given. to the first time. to the first time series of a given. to the prediction of a given. to the prediction of a given. To calculate. To calculate. To calculate. To calculate. to the methods, and data point. To calculate. to the other: (it is an example of a given. To calculate. To calculate. To calculate. In: The influence is to the methods of a given. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To what the prediction of a given. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To what their. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To calculate. To how to calculate. To calculate. To calculate. To the most, 2019. To calculate. To calculate. To calculate. To the data point. To calculate. To what, 1. To do a given. To calculate. To the following a given. To;...;... what to the data point.\\nTo what.\\nTo. To, the data point. To a given. To and a general. To the data point. To 1. To the data point. To data point. To the bests of a high degree the data point. To. to the software of a general.\\na good data point the model a model. To a high. We are to the most of a heterogeneous data point the data point the first. To the prediction of a general. To our method of a heterogeneous data point out',\n",
       "  'ground_truth': \"\\n\\nInfluence functions originated in statistics in the context of analyzing linear models. @CITE[12]@ reintroduced influence functions to the ML community. While influence remains the most common method for training data attribution, other methods have also been proposed to identify “important” instances, including Shapley Values (), Fisher Kernels (@CITE[2]@), tracking instance gradients during training (@CITE[17]@; @CITE[7]@), and training surrogate linear models (@CITE[3]@). Another line of work aims to make the computation of influence more efficient (; @CITE[16]@). Influence functions have been shown to be useful for aiding training data debugging and artifact identification (Han et al., 2020; Zylberajch et al., 2021; @CITE[1]@), understanding bias in data (@CITE[19]@), robust optimization (@CITE[5]@; @CITE[6]@), active learning (@CITE[8]@), data cleaning (@CITE[13]@; Kong et al., 2022), and domain adaptation (@CITE[4]@). Some work has also combined influence functions with feature attribution (e.g., integrated gradients) to point to specific tokens within instances that were influential (@CITE[11]@; @CITE[1]@; @CITE[14]@). While these works also identify specific segments of tokens within instances, there is a fundamental difference between both the goal and the method of these prior works in comparison to the presented method. These prior efforts provided mechanisms to approximate which tokens most contribute to the influence score of a given example (that is, locating parts of the input text which, if perturbed, would greatly affect the influence of said point). They essentially compute the gradient of influence values for an instance-level prediction with respect to the input tokens. By contrast, the presented method provides a machinery to measure influence for structured, multi-part outputs. Put another way, the described method measures the influence of segments of the labels for train instances with respect to segments of predictions of a test point, rather than the entire output for a test point. Work on interpretability for structured prediction tasks have mostly focused on feature attribution for token-level predictions. Such methods have been used to characterize model behavior (; @CITE[10]@), extract rationales (@CITE[18]@), analyze internal model representations (@CITE[15]@; @CITE[9]@), and debug erroneous text generations (Strobel et al., 2018). Although rare, some structured prediction works have attempted to trace model behavior back to the training set but they invariably use influence with respect to the whole structured output (@CITE[13]@; @CITE[16]@).\\nReferences:ID: 1. Title: Data Shapley: Equitable Valuation of Data for Machine Learning\\nID: 2. Title: Interpreting Black Box Predictions using Fisher Kernels\\nID: 3. Title: Estimating Training Data Influence by Tracing Gradient Descent\\nID: 4. Title: HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks\\nID: 5. Title: Datamodels: Predicting Predictions from Training Data\\nID: 6. Title: FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging\\nID: 7. Title: Scaling Up Influence Functions\\nID: 8. Title: Combining Feature and Instance Attribution to Detect Artifacts\\nID: 9. Title: Understanding the Origins of Bias in Word Embeddings\\nID: 10. Title: Interpreting Robust Optimization via Adversarial Influence Functions\\nID: 11. Title: Understanding Goal-Oriented Active Learning via Influence Functions\\nID: 12. Title: Gradient-guided Loss Masking for Neural Machine Translation\\nID: 13. Title: The Trade-offs of Domain Adaptation for Neural Language Models\\nID: 14. Title: Understanding Black-box Predictions via Influence Functions\\nID: 15. Title: Combining Feature and Instance Attribution to Detect Artifacts\\nID: 16. Title: Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve\\nID: 17. Title: A causal framework for explaining the predictions of black-box sequence-to-sequence models\\nID: 18. Title: Rationales for Sequential Predictions\\nID: 19. Title: What Does BERT Look At? An Analysis of BERT's Attention<|eot_id|>\"},\n",
       " 2113: {'predict': 'QR is a task that reformulates a given question into a new one to better capture the underlying meaning. QR has been studied in different settings, including conversational QA (@CITE[5]@; @CITE[2]@; @CITE[9]@; @CITE[6]@), dialogue state tracking (@CITE[1]@), and question answering (@CITE[8]@; @CITE[7]@; @CITE[3]@). QR is related to question generation (@CITE[4]@; @CITE[4]@; @CITE[4]@), but QR is a more challenging task because it requires the model to understand the original question and the context, and generate a new question that is relevant to the context.\\nReferences:ID: 1. Title: Question Rewriting for Conversational Question Answering\\nID: 2. Title: Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models\\nID: 3. Title: Unsupervised Question Decomposition for Question Answering\\nID: 4. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation\\nID: 5. Title: Query Resolution for Conversational Search with Limited Supervision\\nID: 6. Title: Generative Question Refinement with Deep Reinforcement Learning in Retrieval-based QA System\\nID: 7. Title: Adversarial Multi-task Learning for Text Classification\\nID: 8. Title: Efficient parametrization of multi-domain deep neural networks\\nID: 9. Title: AdapterFusion: Non-Destructive Task Composition for Transfer Learning',\n",
       "  'ground_truth': '\\n\\nElgohary et al. (2019) created the QR dataset which rewrites a subset of the questions from QuAC (Choi et al., 2018). Based on this dataset, some recent work has studied this task and formulates QR as a text generation task with an encoder-decoder architecture (Elgohary et al., 2019; ; @CITE[7]@; @CITE[5]@; @CITE[6]@. The difficulty of answering a question given a relevant document has been studied in the question answering community (Dua et al., 2019; Wolfson et al., 2020). examine 12 reading comprehension datasets and determine what makes a question more easily answered. ; @CITE[5]@; Talmor and Berant (2018); Dong et al. (2017) study how to make a hard question more easily answered. However, there is no work to date that studies whether rewriting difficulties exist in QR and how to measure the difficulties. Some other work is similar to QR but focuses on other tasks such as dialogue tracking (@CITE[7]@; @CITE[5]@; and information retrieval (@CITE[8]@; Lin et al., 2020b;. Varying rewriting difficulties can result in multiple underlying data distributions in the QR training data. The shared-private framework has been studied to learn from training data with multiple distributions (Zhang et al., 2018; @CITE[5]@. One issue of the shared-private framework is parameter inefficiency when building private models We use adapter tuning (@CITE[9]@, 2017) to build the private models. Adapter tuning was recently proposed for adapting a pre-trained language model, e.g., BERT (Devlin et al., 2019), to downstream tasks (,c;, and its effectiveness has been verified by previous work (Bapna and Firat, 2019; @CITE[6]@; @CITE[6]@;. We are the first to apply it to reduce model parameters in the shared-private framework. How to combine the knowledge stored in multiple adapters is also important. propose adapter fusion to build an ensemble of adapters in multi-task learning. We propose sequence-level adapter fusion in our work.\\nReferences:ID: 1. Title: Question Rewriting for Conversational Question Answering\\nID: 2. Title: Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models\\nID: 3. Title: Unsupervised Question Decomposition for Question Answering\\nID: 4. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation\\nID: 5. Title: Query Resolution for Conversational Search with Limited Supervision\\nID: 6. Title: Generative Question Refinement with Deep Reinforcement Learning in Retrieval-based QA System\\nID: 7. Title: Adversarial Multi-task Learning for Text Classification\\nID: 8. Title: Efficient parametrization of multi-domain deep neural networks\\nID: 9. Title: AdapterFusion: Non-Destructive Task Composition for Transfer Learning<|eot_id|>'},\n",
       " 6453: {'predict': 'A variety of works have been proposed to mitigate the semantic drift problem in self-training. @CITE[1]@ propose to use in-domain unlabeled data to pretrain the language model and then fine-tune it with a small set of labeled data. @CITE[3]@ propose to use advanced data augmentation to generate a large number of in-domain data. @CITE[4]@ propose to use the interpolation method to generate new data. @CITE[2]@ propose to use the unlabeled data to pretrain the language model and then fine-tune it with a small set of labeled data. However, these methods are mainly designed for the text classification task. They are not suitable for the text generation task. Besides, they do not consider the problem of semantic drift in self-training. In this paper, we propose a simple yet effective method to mitigate the semantic drift problem in self-training. We use a small set of labeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled data to correct the semantic drift in the labeled data. We also propose to use the unlabeled data to correct the semantic drift in the unlabeled data. We use the unlabeled',\n",
       "  'ground_truth': \"\\n\\nSemantic drift in self-training (Curran et al., 2007; Carlson et al., 2010) occurs when spurious pseudo-labels accumulate over time and distort the distribution of labeled documents. In the context of neural networks, the research in this area is sparse. One approach is to avoid pseudo-labels altogether, and use unlabeled documents differently (; @CITE[3]@; @CITE[2]@; @CITE[1]@. Nonetheless, these alternative methods don’t necessarily compete with self-training and can co-exist with it inside a framework. To address semantic drift directly, existing approaches mainly aim for explicitly adjusting pseudo-labels. use an angular loss function to project pseudo-labels. @CITE[4]@ assume pseudo-labels evolve in a stochastic process and normalize their values. In terms of the role of pseudo-labels in self-training, our algorithm can be taken as the generalized form of the algorithm proposed by @CITE[4]@.\\nReferences:ID: 1. Title: Variational Pretraining for Semi-supervised Text Classification\\nID: 2. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 3. Title: MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification\\nID: 4. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks<|eot_id|>\"},\n",
       " 1805: {'predict': 'Previous studies on punctuation restoration mainly focus on the Transformer encoder models (@CITE[1]@; @CITE[3]@; @CITE[2]@, which have achieved significant success in many natural language processing tasks. These models are trained with a large amount of unlabeled data, which provides rich information for punctuation restoration. In this study, we compare the performances of several Transformer encoder models, including the original Transformer, BERT (@CITE[3]@, RoBERTa (@CITE[2]@, and Longformer (@CITE[3]@. The results show that BERT achieves the best performance in punctuation restoration, while Longformer performs the worst. The Longformer model is a causal transformer model that is designed for long-range dependency modeling. It has been shown to be effective in tasks such as machine translation and text classification. However, it does not perform well in punctuation restoration, which may be due to the lack of bidirectional context information.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach',\n",
       "  'ground_truth': '\\n\\nIn recent years, large, pre-trained models based on transformer architecture (@CITE[1]@ seem to perform best on a number of NLP tasks, in- cluding punctuation restoration. Perhaps the most comprehensive comparison of various transformer encoder models in the task of punctuation restora- tion is done by @CITE[2]@, where the au- thors compared a number of models based on dif- ferent variants of pre-trained BERT (, RoBERTa (@CITE[3]@ and AlBERT ( encoders as a base of their model. They’ve shown that generally larger pre-trained models were better than the smaller ones in punc- tuation restoration and that between models of the same size, generally, RoBERTa was better than both AlBERT and BERT. They also showed that XLM (cross-lingual models) variants of RoBERTa were slightly worse than English-only ones. The authors of the paper published their code and we also used it in our research.\\nReferences:ID: 1. Title: Attention Is All You Need\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach<|eot_id|>'},\n",
       " 134: {'predict': 'The simplest static KG embedding methods are TransE, ComplEx, and DistMult, which use a bilinear model, a complex model, and a dot product, respectively. These methods have been shown to be effective, but they suffer from the problem of overfitting and can be easily broken by adversarial attacks (Zhang et al., 2016; @CITE[4]@; @CITE[2]@; @CITE[1]@). To address these issues, subsequent methods, such as SimplE and ConvE, introduce non-linear transformations and convolutional layers, respectively. These methods are more robust than the original methods, but they still suffer from the problem of overfitting. Furthermore, they are difficult to interpret. In contrast, our method is easy to interpret and can be used as a component of a more complex model. Finally, there are also some static KG embedding methods that are based on tensor factorization, such as RESCAL and CP, which are also easy to interpret. However, these methods have been shown to be less effective than the methods mentioned above (@CITE[5]@; @CITE[3]@).\\nReferences:ID: 1. Title: Complex Embeddings for Simple Link Prediction\\nID: 2. Title: Embedding Entities and Relations for Learning and Inference in Knowledge Bases\\nID: 3. Title: Stochastic gradient descent on Riemannian manifolds\\nID: 4. Title: SimplE Embedding for Link Prediction in Knowledge Graphs\\nID: 5. Title: Convolutional 2D Knowledge Graph Embeddings',\n",
       "  'ground_truth': '\\n\\nEmbedding approaches for static KGs can generally be categorized into bilinear models and translational models. The bilinear approaches are equipped with a bilinear score function that represents predicates as linear transformations acting on entity embeddings (@CITE[5]@; @CITE[3]@; ; @CITE[1]@). Translational approaches measure the plausibility of a triple as the distance between the translated subject and object entity embeddings, including TransE (@CITE[2]@) and its variations (; ). Additionally, several models are based on deep learning approaches (@CITE[4]@; Schlichtkrull et al., 2018; ) that apply (graph) convolutional layers on top of embeddings and design a score function as the last layer of the neural network.\\nReferences:ID: 1. Title: Complex Embeddings for Simple Link Prediction\\nID: 2. Title: Embedding Entities and Relations for Learning and Inference in Knowledge Bases\\nID: 3. Title: Stochastic gradient descent on Riemannian manifolds\\nID: 4. Title: SimplE Embedding for Link Prediction in Knowledge Graphs\\nID: 5. Title: Convolutional 2D Knowledge Graph Embeddings<|eot_id|>'},\n",
       " 3196: {'predict': 'Dense retrieval methods have achieved remarkable success in open-domain QA (@CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[3]@; @CITE[1]@; @CITE[4]@. They learn dense representations of queries and documents through contrastive learning and retrieve relevant documents by computing the similarity between query and document representations. These methods are efficient and can be easily combined with a downstream reader model for open-domain question answering. However, they have two major limitations. First, they are prone to noise, as they retrieve documents based on their similarity to the query, without considering the semantic relevance of the retrieved documents to the question. Second, they rely on a large amount of labeled data to train the dense retriever, which is time-consuming and costly. Our method addresses these limitations by learning a robust and interpretable representation of the question, which can be used to retrieve relevant documents by computing the similarity between the question and the documents, and by leveraging the semantic relevance of the retrieved documents to the question.\\nReferences:ID: 1. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\nID: 2. Title: More Robust Dense Retrieval with Contrastive Dual Learning\\nID: 3. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval',\n",
       "  'ground_truth': '\\n\\nDense retrievers use a bi-encoder architecture to encode queries and documents and map them in an embedding space for retrieval (Karpukhin et al., 2020;,a; Lewis et al., 2020; Zhan et al., 2021; @CITE[4]@; @CITE[3]@. To learn an effective embedding space, dense retrievers are forced to maintain high-dimensional embeddings to fit training signals. The most direct way to reduce the dimension of embeddings is that retaining parts of the dimen- sions of high-dimensional embeddings (; @CITE[1]@. Some work uses the first 128 dimensions to encode both queries and documents ( or utilizes PCA to retain the primary dimensions to recover most infor- mation from the raw embeddings (@CITE[1]@. Other work (@CITE[1]@ proposes a super- vised method, which uses neural networks to com- press the high-dimensional embeddings as lower- dimensional ones. These supervised models pro- vide a better dimension reduction way than unsuper- vised models by avoiding missing too much infor- mation. To optimize the encoders, some work (@CITE[1]@ continuously trains dense retrievers with the contrastive training strategies (Karpukhin et al., 2020; @CITE[2]@.\\nReferences:ID: 1. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\nID: 2. Title: More Robust Dense Retrieval with Contrastive Dual Learning\\nID: 3. Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\\nID: 4. Title: Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval<|eot_id|>'},\n",
       " 1868: {'predict': 'Relation detection in KBQA is a challenging task as it needs to extract the relation triplets from the given question. To solve this task, researchers usually use the pipeline approaches, i.e., entity linking and relation prediction. The entity linker (@CITE[1]@; @CITE[2]@; @CITE[2]@) is used to extract the subject entity in the question, and the relation predictor (@CITE[3]@; @CITE[4]@; @CITE[5]@) is used to predict the relation of the subject entity. These two steps are usually separated. However, they are highly correlated. For example, the relation prediction is highly dependent on the entity linker. Therefore, some researchers (@CITE[4]@; @CITE[5]@) have proposed to jointly optimize the entity linker and the relation predictor. However, they still do not consider the joint optimization of the two steps.\\nReferences:ID: 1. Title: A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges\\nID: 2. Title: Simple Question Answering by Attentive Convolutional Neural Network\\nID: 3. Title: Simple Question Answering by Attentive Convolutional Neural Network\\nID: 4. Title: CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases\\nID: 5. Title: A Structured Self-attentive Sentence Embedding',\n",
       "  'ground_truth': '\\n\\nThere are two mainstream branches of relation detection in KBQA (: information retrieval-based and neural semantic parsing-based, which can be further categorized into singe-relation (one-hop) detection and multi-relation (multi-hop) detection (. For single-relation detection, most existing ap- proaches (; @CITE[5]@; @CITE[3]@; @CITE[4]@; ; embedded questions and candidate relations into the same space and calcu- lated the semantic similarity between the questions and the candidate relations in KBs. For example, ( propose an encoder-decoder based generative framework for relation detection, (@CITE[1]@ propose a end-to-end KBQA model based on Bayesian Neural Network (BNN) to estimate uncertainties arose from both model and data. For multi-relation detection, some approaches (; @CITE[3]@, 2018) incorporated a constraint detection mechanism to deal with two or three-relation detection tasks. The drawback of such methods is that the number of hops is generally strictly restricted. (@CITE[2]@ proposed UHop, an unrestricted-hop framework which dosen’t need predeﬁned maximum hop number. (Bordes et al., 2014a,b) constructed the topic- entity-centric subgraph for each question and re- trieved answers by ranking the semantic similarity between the question embedding and entity embed- ding in the subgraph. Such methods suffer from high time complexity as the whole subgraph for each question is considered. However, all the afore- mentioned approaches ignore that for some com- plex questions, there might exist multiple relation paths leading to the correct answer.\\nReferences:ID: 1. Title: A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges\\nID: 2. Title: Simple Question Answering by Attentive Convolutional Neural Network\\nID: 3. Title: Simple Question Answering by Attentive Convolutional Neural Network\\nID: 4. Title: CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases\\nID: 5. Title: A Structured Self-attentive Sentence Embedding<|eot_id|>'},\n",
       " 3034: {'predict': 'Editing the language models is an emerging field of research that has been explored from various perspectives (@CITE[6]@; @CITE[6]@; @CITE[6]@; @CITE[4]@; @CITE[6]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[5]@. Recent studies have shown that it is possible to edit the language models’ behavior by changing their prompts (@CITE[6]@; @CITE[6]@; @CITE[4]@; @CITE[5]@; @CITE[1]@. However, this approach has limitations, such as the need to retrain the model for each new prompt and the difficulty of finding the right prompts. In contrast, our work focuses on editing the language models’ behavior by changing the model’s parameters, which is more efficient and scalable.\\nReferences:ID: 1. Title: Editing Factual Knowledge in Language Models\\nID: 2. Title: Editing Factual Knowledge in Language Models\\nID: 3. Title: Editing Factual Knowledge in Language Models\\nID: 4. Title: Editable Neural Networks\\nID: 5. Title: Fast Model Editing at Scale\\nID: 6. Title: PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts',\n",
       "  'ground_truth': '\\n\\nOur HNet based approach is based on the architecture in (@CITE[4]@ which uses it to edit factual knowledge in LMs. While the architecture is similar, we use the HNet to encode task-specific instructions and is intended for controlling task-level LM behavior unlike the micro-behavior targeted in (@CITE[4]@. Similar to ours and (@CITE[4]@, Bayesian hyper networks (Krueger et al., 2018) linearizes the number of parameters for predictions by constraining the HNet outputs to scale and shift parameters. (@CITE[3]@; @CITE[6]@ propose Meta Learning approaches for editing errors in a neural network but is not directly applicable for MTIL in a zero-shot setting. MTIL: Finally, the work most closely related to this paper is the Tk-Instruct model from (@CITE[5]@ which fine-tunes a T5 model (@CITE[1]@ with instructions, which we use as the baseline. We use the same dataset and training settings as Tk-Instruct but instead use the pretrained BART model (@CITE[2]@ as it is task agnostic compared to T5. In addition, we enhance this model with meta-learning and consider significantly different training, evaluation, and model settings to test zero-shot generalization resulting in unique contributions and conclusions orthogonal to the findings in (@CITE[5]@.\\nReferences:ID: 1. Title: Editing Factual Knowledge in Language Models\\nID: 2. Title: Editing Factual Knowledge in Language Models\\nID: 3. Title: Editing Factual Knowledge in Language Models\\nID: 4. Title: Editable Neural Networks\\nID: 5. Title: Fast Model Editing at Scale\\nID: 6. Title: PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts<|eot_id|>'},\n",
       " 3357: {'predict': 'Reinforced text generation is a popular paradigm in natural language processing, which has been widely used in many tasks, such as machine translation (@CITE[2]@, code generation (@CITE[4]@; @CITE[1]@, and code annotation (@CITE[3]@. The core idea of this paradigm is to use reinforcement learning to optimize the generation objective, which can be formulated as a Markov Decision Process (MDP). In the MD, the policy is to choose the next token, and the reward function is to evaluate the generated text. The agent can be trained by policy gradient methods, such as REINFORCE (@CITE[1]@. The policy gradient methods can be divided into two categories: on-policy methods and off-policy methods. The on-policy methods, such as REINFORCE, update the policy based on the experience collected by the policy. The off-policy methods, such as PPO (@CITE[4]@, update the policy based on the experience collected by a behavior policy. The off-policy methods can be used in the scenario where the behavior policy is not the same as the policy. The code generation task is different from the natural language processing tasks, which requires the generated code to be compilable. Therefore, we use the off-policy methods to optimize the code generation objective.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: Tree-to-tree Neural Networks for Program Translation\\nID: 3. Title: CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning\\nID: 4. Title: Energy-Based Models for Code Generation under Compilability Constraints',\n",
       "  'ground_truth': '\\n\\nReinforcement learning (@CITE[2]@ has shown great success in various tasks. It focuses on how agents ought to take actions in an environment to maximize the cumulative reward, is well suited for decision-making tasks. @CITE[4]@ were among the first to apply REINFORCE algorithm (@CITE[3]@ to train recurrent neural networks on sequence generation tasks, suggesting that directly optimizing the metric used at the test phase can lead to better results. proposed a hybrid extractive-abstractive architecture with policy-based reinforcement learning. They used an extractor agent to select salient sentences and then employed an abstractor network to rewrite these extracted sentences. Wan et al. (2018); incorporated the tree structure and sequential content of code snippets and designed a deep reinforcement learning framework optimized by the metric of BLEU to improve the performance of the code summarization task. @CITE[1]@ proposed a reinforcement learning framework, which encourages the code annotation model to generate annotations that can be used for code retrieval tasks. proposed an energy-based model with an imposed constraint of generating only compilable sequences to improve compilation rates of generated code.\\nReferences:ID: 1. Title: Sequence Level Training with Recurrent Neural Networks\\nID: 2. Title: Tree-to-tree Neural Networks for Program Translation\\nID: 3. Title: CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning\\nID: 4. Title: Energy-Based Models for Code Generation under Compilability Constraints<|eot_id|>'},\n",
       " 675: {'predict': 'Contrastive learning has been widely applied in computer vision (@CITE[6]@; @CITE[5]@; @CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[4]@) and natural language processing (@CITE[7]@; @CITE[1]@; @CITE[4]@). In computer vision, contrastive learning is often used to learn the representations of images by maximizing the similarity between augmented views of the same image and minimizing the similarity between different images. In natural language processing, contrastive learning is often used to learn the representations of words or sentences by maximizing the similarity between the same words or sentences and minimizing the similarity between different words or sentences. In this paper, we propose a contrastive learning method to learn the representations of documents by maximizing the similarity between the same documents and minimizing the similarity between different documents.\\nReferences:ID: 1. Title: Exploring Simple Siamese Representation Learning\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: Bootstrap your own latent: A new approach to self-supervised Learning\\nID: 4. Title: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\\nID: 5. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 6. Title: Distributed Representations of Words and Phrases and their Compositionality\\nID: 7. Title: A Mutual Information Maximization Perspective of Language Representation Learning',\n",
       "  'ground_truth': '\\n\\nRecently, contrastive learning has been widely studied in unsupervised representation learning for vision (@CITE[4]@; ; @CITE[3]@; @CITE[2]@; ), language (; Saunshi et al., 2019; ; ; @CITE[7]@; @CITE[5]@; ), or multi-modal (; @CITE[1]@). The goal is to learn semantic representation between two views by allowing the positive sample to be similar (in semantic space) and negatives to be dissimilar semantically simultaneously. CLIP (Radford et al.) and MIL-NCE (@CITE[1]@) have demonstrated the effectiveness of learning the semantic mapping between vision and language. Previous attempts mainly exploit the InfoNCE (Oord et al., 2018) objective to maximize a lower bound of the mutual information. This paper extends the multi-modal contrastive learning between the trace in the image and captioning sentence. In the same image, they correspond to each other semantically. This motivates us to design a contrastive loss for better alignment between the trace and language.\\nReferences:ID: 1. Title: Exploring Simple Siamese Representation Learning\\nID: 2. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 3. Title: Bootstrap your own latent: A new approach to self-supervised Learning\\nID: 4. Title: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\\nID: 5. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 6. Title: Distributed Representations of Words and Phrases and their Compositionality\\nID: 7. Title: A Mutual Information Maximization Perspective of Language Representation Learning<|eot_id|>'},\n",
       " 2396: {'predict': 'Our EMC-GCN model is inspired by previous work on aspect-based sentiment analysis (@CITE[6]@; @CITE[3]@; @CITE[5]@; @CITE[4]@; @CITE[1]@. However, the existing work only focuses on extracting a part of the opinion triplets, such as (Aspect, Opinion) or (Aspect, Opinion, Sentiment), and does not consider extracting the opinion reasons. The closest work to ours is (@CITE[7]@, which proposes a two-stage framework to extract the opinion triplets. However, they only focus on extracting the opinion reasons, and do not consider extracting the opinion targets and their sentiment polarities. Our EMC-GCN model is an end-to-end framework that can extract the complete opinion triplets. We also use a unified grid tagging scheme to encode the input sentence and the extracted triplets in a unified way, and use a graph neural network to capture the inter-triplet dependencies. Compared with (@CITE[7]@, our model is more powerful and can be applied to more downstream tasks.\\nReferences:ID: 1. Title: Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\\nID: 2. Title: Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision\\nID: 3. Title: Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\\nID: 4. Title: A Unified Model for Opinion Target Extraction and Target Sentiment Prediction\\nID: 5. Title: An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis\\nID: 6. Title: Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis\\nID: 7. Title: A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis',\n",
       "  'ground_truth': '\\n\\nThis kind of work mainly has two tasks: Aspect and Opinion Term Co-Extraction (AOTE) (@CITE[2]@, 2017; ; @CITE[5]@; @CITE[3]@; and Aspect-Sentiment Pair Extraction (ASPE) (@CITE[1]@; @CITE[4]@,b; @CITE[6]@. Most recently, first proposed the ASTE task and developed a two-stage pipeline framework to couple together aspect extraction, aspect sentiment classification and opinion extraction. To further explore this task, (@CITE[7]@; Chen et al., 2021a) transformed ASTE to a machine reading comprehension problem and utilized the shared BERT encoder to obtain the triplets after multiple stages of decoding.\\nReferences:ID: 1. Title: Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\\nID: 2. Title: Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision\\nID: 3. Title: Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\\nID: 4. Title: A Unified Model for Opinion Target Extraction and Target Sentiment Prediction\\nID: 5. Title: An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis\\nID: 6. Title: Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis\\nID: 7. Title: A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis<|eot_id|>'},\n",
       " 1214: {'predict': 'Neural approaches to information retrieval have been a hot research topic in recent years. A number of neural models have been proposed to tackle ad-hoc retrieval. The first deep model for ad-hoc retrieval was proposed by @CITE[3]@, which was based on a joint matching model and used the deep neural network to learn the matching features between queries and documents. After that, many neural models have been proposed for ad-hoc retrieval. For example, @CITE[5]@ proposed a deep relevance matching model (DRMM) that used a joint deep architecture to learn the matching features between queries and documents. @CITE[4]@ proposed a contextualized embeddings for document ranking (CEDR) that used two pretrained contextualized language models (ELMo and BERT) to generate contextualized term embeddings. @CITE[5]@ proposed a passage re-ranking with BERT that used a BERT model to generate contextualized term embeddings and a matching network to learn the matching features between queries and documents. In addition, a number of neural models have been proposed for open-domain question answering. For example, @CITE[1]@ proposed a multi-step fusion model that used a BERT model to generate contextualized term embeddings and a multi-step fusion network to learn the matching features between questions and answers. @CITE[2]@ proposed a dense interaction network that used a BERT model to generate contextualized term embeddings and a dense interaction network to learn the matching features between questions and answers. @CITE[2]@ proposed a dense interaction network that used a BERT model to generate contextualized term embeddings and a dense interaction network to learn the matching features between questions and answers. In this paper, we propose a hybrid model that combines the advantages of both neural models and traditional models. The neural model is used to generate contextualized term embeddings and the traditional model is used to learn the matching features between queries and documents. The experimental results show that the proposed hybrid model outperforms the traditional model and the neural model.\\nReferences:ID: 1. Title: A Deep Relevance Matching Model for Ad-hoc Retrieval\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: CEDR: Contextualized Embeddings for Document Ranking\\nID: 4. Title: CEDR: Contextualized Embeddings for Document Ranking\\nID: 5. Title: Passage Re-ranking with BERT',\n",
       "  'ground_truth': '\\n\\nIn the last decade, two different kinds of neural approaches to IR have been defined (@CITE[2]@): the first aims at encoding queries and documents within the same vector space (Mitra et al., 2016; ; the second, instead, focuses on learning an estimator for the relevance of a document with respect to a query (@CITE[1]@. More recently, with the advent of transformer-based language models such as BERT (@CITE[4]@, contextualized representations rapidly got incorporated into retrieval models ( — which previously had relied on static embeddings only — mainly by pairing contextualized models with a binary classifier to compute a score per query-document (; @CITE[5]@ or query-sentence pair (@CITE[3]@;.\\nReferences:ID: 1. Title: A Deep Relevance Matching Model for Ad-hoc Retrieval\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: CEDR: Contextualized Embeddings for Document Ranking\\nID: 4. Title: CEDR: Contextualized Embeddings for Document Ranking\\nID: 5. Title: Passage Re-ranking with BERT<|eot_id|>'},\n",
       " 3159: {'predict': 'Weakly supervised phrase grounding methods (@CITE[5]@; @CITE[4]@; @CITE[2]@; @CITE[3]@; @CITE[1]@) learn to localize phrases in images by leveraging the image-caption pairs. These methods can be categorized into two groups. The first group is to train a phrase localization model with a weakly supervised loss function. For example, @CITE[5]@ proposes a coarse-to-fine object refinement and entity relation modeling method, which learns to localize phrases by training a self-taught regression loss for the proposal locations and a classification loss based on parsed entity relations. The second group is to train a phrase localization model with a pre-trained caption-image alignment model. For example, @CITE[3]@ proposes a novel end-to-end model that uses caption-to-image retrieval as a downstream task to guide the process of phrase localization. Our method is more similar to the second group, which trains a phrase localization model with a pre-trained caption-image alignment model. However, our method is more robust to the pre-trained caption-image alignment model, which can be trained with a large-scale image-caption dataset. Moreover, our method can learn more robust phrase localization features by using a contrastive learning method.\\nReferences:ID: 1. Title: Relation-aware Instance Refinement for Weakly Supervised Visual Grounding\\nID: 2. Title: Grounding of Textual Phrases in Images by Reconstruction\\nID: 3. Title: MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding\\nID: 4. Title: Contrastive Learning for Weakly Supervised Phrase Grounding\\nID: 5. Title: Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment',\n",
       "  'ground_truth': '\\n\\nWeakly supervised phrase grounding has received considerable attention, as the fully supervised setting requires the labeling of phrases in image captions. Previous mainstream methods can be divided into two main categories: reconstruction-based methods (@CITE[4]@; @CITE[3]@; @CITE[2]@) and contrastive-based methods (@CITE[1]@, 2021; ; ; @CITE[5]@). Our work falls into the line of contrastive-based methods. Previous contrastive-based methods typically conduct contrastive learning at the caption-image level by heuristically aggregating several phrase-object similarities to be caption-image similarities. Align2Ground (@CITE[5]@) aggregates several features of objects to get a caption-conditioned image representation, and matches it with the corresponding caption. InfoGround () defines a compatibility function to measure the compatibility between images and BERT-contextualized word representation. InfoGround uses BERT to generate hard negative examples. CCL () defines an aggregation function to compute the alignment score between a phrase and a set of objects, and generates counterfactual examples by the gradient. MAF (@CITE[1]@) uses the mean of phrase-wise maximum similarity as the caption-image similarity and performs contrastive learning at the caption-image level. These works are distinctively different in their definition of the aggregation function and the approach in which they generate positive and negative examples. None of these works use contrastive learning at the phrase-object level nor do they highlight the importance of intrinsic structure as we.\\nReferences:ID: 1. Title: Relation-aware Instance Refinement for Weakly Supervised Visual Grounding\\nID: 2. Title: Grounding of Textual Phrases in Images by Reconstruction\\nID: 3. Title: MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding\\nID: 4. Title: Contrastive Learning for Weakly Supervised Phrase Grounding\\nID: 5. Title: Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment<|eot_id|>'},\n",
       " 1926: {'predict': 'There are several datasets for question answering in context. The datasets in the table below are in the format of question-answer pairs, where the question and the answer are provided. The answer can be a short span (@CITE[7]@; @CITE[5]@; @CITE[6]@; @CITE[8]@; @CITE[4]@; @CITE[8]@; @CITE[1]@; @CITE[3]@; @CITE[2]@, a numerical value (@CITE[6]@, or a set of spans (@CITE[8]@. The question can be a natural language question (@CITE[7]@; @CITE[5]@; @CITE[6]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[1]@; @CITE[3]@, or a set of numerical queries (@CITE[6]@. The context can be a Wikipedia article (@CITE[7]@; @CITE[5]@; @CITE[6]@; @CITE[4]@; @CITE[8]@; @CITE[8]@; @CITE[1]@; @CITE[3]@, a set of Wikipedia articles (@CITE[8]@, a table and a set of Wikipedia articles (@CITE[8]@, or a set of paragraphs (@CITE[6]@. In this work, we focus on datasets in the format of question-answer pairs, where the answer is a short span. We use the following datasets: TriviaQA, QQP, and RACE. TriviaQA (@CITE[7]@ consists of trivia questions from the internet, and the context is a Wikipedia article. QQP ( is a question-answering dataset for natural language inference. RACE (@CITE[5]@ is a reading comprehension dataset, where the context is a paragraph from Wikipedia, and the question can be a natural language question or a set of numerical queries.\\nReferences:ID: 1. Title: NumNet: Machine Reading Comprehension with Numerical Reasoning\\nID: 2. Title: QuAC : Question Answering in Context\\nID: 3. Title: The Web as a Knowledge-base for Answering Complex Questions\\nID: 4. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 5. Title: R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason\\nID: 6. Title: HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\\nID: 7. Title: HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\\nID: 8. Title: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs',\n",
       "  'ground_truth': '\\n\\nThis work mainly refers to conversational QA datasets (@CITE[7]@; @CITE[4]@; Christmann et al., 2019), Multi-hop QA datasets (@CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[5]@), and discrete reasoning datasets (@CITE[8]@; @CITE[6]@). For explanation in QA, CoQA (@CITE[7]@) provides rationale to make models understandable under conversations. HotpotQA (@CITE[1]@), R4C (@CITE[2]@), and 2WikiMultiHopQA (@CITE[5]@) provide a set of evidence to support training models to learn reasoning across paragraphs. In concurrent, Dalvi et al. (2021) present a new dataset with explanations in the form of entailment trees, however, they do not consider numerical reasoning.\\nReferences:ID: 1. Title: NumNet: Machine Reading Comprehension with Numerical Reasoning\\nID: 2. Title: QuAC : Question Answering in Context\\nID: 3. Title: The Web as a Knowledge-base for Answering Complex Questions\\nID: 4. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 5. Title: R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason\\nID: 6. Title: HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\\nID: 7. Title: HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\\nID: 8. Title: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs<|eot_id|>'},\n",
       " 6897: {'predict': 'As we have discussed in the introduction, there is a large number of existing dataset search engines, which can be categorized into two main types: (1) keyword-based search engines, such as Google Dataset Search (@CITE[1]@ and Dataverse (https://dataverse.org/), and (2) query-based search engines, such as Datasearch (https://datasearch.org/) and OpenDataSearch (https://opendatasearch.org/). However, existing search engines are usually built on top of a simple keyword-based retrieval algorithm, which may lead to a low recall of relevant datasets. Recently, some researchers have proposed to utilize deep learning techniques to improve the search performance. For example, @CITE[2]@ propose to leverage the semantic information of datasets by generating a dataset description snippet, and then apply a query-based retrieval algorithm to search for relevant datasets. However, the snippet generation algorithm proposed in their work is based on the static representation of the dataset, which may not capture the dynamic semantic information of datasets. Besides, the snippet generation algorithm is usually based on the dataset metadata, which may not be able to capture the semantic information of datasets. Recently, some researchers have proposed to utilize pre-trained language models to learn the semantic representation of datasets. For example, @CITE[3]@ propose to utilize the pre-trained language model to learn the semantic representation of datasets, and then apply a dense retrieval algorithm to search for relevant datasets. However, the dense retrieval algorithm usually requires a large number of labeled data to train the model, which may be difficult to obtain in the dataset search task.\\nReferences:ID: 1. Title: Dataset search: a survey\\nID: 2. Title: Towards More Usable Dataset Search: From Query Characterization to Snippet Generation\\nID: 3. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval',\n",
       "  'ground_truth': '\\n\\nAd hoc dataset retrieval is a specialized information retrieval task that aims to find the most relevant datasets to a user’s query. The metadata of a dataset provided by its publisher typically consists of a set of fields such as title and description. While existing retrieval methods commonly rely on metadata (@CITE[1]@ which resembles a structured document, the task of dataset retrieval differs from document retrieval and has its unique properties, e.g., queries often mention geospatial and temporal entities (@CITE[3]@;, and metadata is relatively short and often incomplete (Neumaier et al., 2016). Knowledge-enhanced retrieval methods have the potential to exploit these features, but they have not been sufficiently studied for this task. Indeed, in a recent benchmarking effort (, only a number of lexical retrieval methods were implemented and evaluated. @CITE[2]@ and, a few PLM-based implementations were evaluated but were fine-tuned on a small training set risking overfitting and their reported performance might not be generalizable. Our empirical investigation significantly extends the above evaluation efforts. We systematically evaluate a range of state-of-the-art PLM-based methods for ad hoc dataset retrieval in both in-domain and out-of-domain settings. Moreover, we design and evaluate explicit knowledge-enhanced methods, which are under-studied in the literature.\\nReferences:ID: 1. Title: Dataset search: a survey\\nID: 2. Title: Towards More Usable Dataset Search: From Query Characterization to Snippet Generation\\nID: 3. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval<|eot_id|>'},\n",
       " 7043: {'predict': 'The idea of using LLMs for text-to-SQL has been explored by many recent works. @CITE[1]@ explored the performance of LLMs for text-to-SQL with in-context learning. They also showed that the performance of LLMs can be further improved with chain-of-thought prompting (@CITE[5]@). @CITE[4]@ showed that diverse demonstrations are helpful for improving in-context learning for text-to-SQL. @CITE[2]@ explored the use of LLMs for text-to-SQL with in-context learning and also showed that LLMs can be used for table-to-SQL. @CITE[3]@ explored the use of LLMs for table-to-SQL with in-context learning and also showed that LLMs can be used for text-to-SQL. @CITE[1]@ explored the use of LLMs for table-to-SQL with in-context learning and also showed that LLMs can be used for text-to-SQL. The use of LLMs for text-to-SQL has also been explored by @CITE[4]@, @CITE[3]@, @CITE[1]@, and @CITE[2]@.\\nReferences:ID: 1. Title: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings\\nID: 2. Title: Exploring Chain-of-Thought Style Prompting for Text-to-SQL\\nID: 3. Title: Diverse Demonstrations Improve In-context Compositional Generalization\\nID: 4. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 5. Title: LEVER: Learning to Verify Language-to-Code Generation with Execution',\n",
       "  'ground_truth': '\\n\\nPrevious studies have explored text-to-SQL with in-context learning from various perspectives. Rajkumar et al. (2022) and @CITE[1]@ investigated effective approaches to represent databases in prompts. Other studies have explored the incorporation of intermediate reasoning steps to enhance the performance of LLMs (; Pourreza and @CITE[2]@;. For demonstration retrieval, prior work has explored different strategies for retrieving out-of-domain examples based on similarity (; @CITE[3]@ and diversity (;. Concurrent with our work, suggested retrieving out-of-domain demonstrations based on the initial SQL predictions, which aligns with our retrieval method. Additionally, execution results have been used in the SQL generation process to verify the generated SQL queries (; @CITE[4]@; ;. These approaches often require multiple LLM calls and SQL executions to accomplish majority voting or self-correction. It is worth noting that the database execution time is a crucial consideration in realistic applications, as highlighted by @CITE[5]@. In contrast, ODIS with our proposed retrieval strategy only requires two LLM calls (one for initial SQL prediction and one for final generation) and eliminates the need for any database execution, making it a more efficient solution for text-to-SQL tasks.\\nReferences:ID: 1. Title: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings\\nID: 2. Title: Exploring Chain-of-Thought Style Prompting for Text-to-SQL\\nID: 3. Title: Diverse Demonstrations Improve In-context Compositional Generalization\\nID: 4. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 5. Title: LEVER: Learning to Verify Language-to-Code Generation with Execution<|eot_id|>'},\n",
       " 7461: {'predict': 'There are also several works that use QA extraction to generate question-answer pairs for multi-turn dialogues. For example, @CITE[1]@ extract question-answer pairs from Wikipedia to generate question-answer pairs for multi-turn dialogues. However, the QA extraction is not as good as the human QA. Therefore, it is not suitable for generating question-answer pairs for multi-turn dialogues. @CITE[2]@ and @CITE[3]@ use the QA extraction to extract the question-answer pairs from the dialogue context. However, they are not suitable for generating question-answer pairs for multi-turn dialogues because the QA extraction is not as good as the human QA. Our work differs from previous work in that we use the human QA to generate question-answer pairs for multi-turn dialogues. Our work is more suitable for generating question-answer pairs for multi-turn dialogues.\\nReferences:ID: 1. Title: Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia\\nID: 2. Title: Structured Attention for Unsupervised Dialogue Structure Induction\\nID: 3. Title: The JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service',\n",
       "  'ground_truth': '\\n\\nFor text-based QA extraction, Rajpurkar et al. (2016) proposed the dataset SQuAD 1.1, in which the 100k+ questions were created by crowdworkers on 536 Wikipedia articles. Subsequently, @CITE[3]@ created 1M+ paragraph-level the discourse tree of questions. @CITE[2]@ incorporated structured attention into a Variational Recurrent Neural Network for dialogue structure induction in an unsupervised way. introduced a new problem, extracting events from dialogue, annotated the dataset Personal Events in Dialogue Corpus, and trained a support vector machine model. Relation Extraction over Dialogue is a newly defined task by DialogRE (@CITE[1]@, which fo- tagging-based methods that perform well on 5 customer service datasets, as well as utterance and session level evaluation metrics for DialogQAE. With further analysis, we find that the extracted QA pairs characterize the dialogue structure, e.g. information seeking, clarification, barge-in, and elaboration. Extensive experiments show that the proposed models can adapt to different domains and languages and largely accelerate knowledge accumulation in the real-world dialogue platform.\\nReferences:ID: 1. Title: Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia\\nID: 2. Title: Structured Attention for Unsupervised Dialogue Structure Induction\\nID: 3. Title: The JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for E-commerce Customer Service<|eot_id|>'},\n",
       " 5873: {'predict': 'Prior work has explored retrieval with descriptions in the context of open-domain QA (@CITE[3]@; @CITE[2]@; @CITE[4]@). In this work, we use a similar approach to retrieve evidence for scientific claim verification. We also explore the use of a language model to generate a description of a scientific claim and use this description to retrieve evidence. In this work, we focus on the use of a language model to generate a description of a scientific claim for use in evidence retrieval. This approach is similar to the use of a language model to generate a description of a scientific claim for use in scientific claim verification (@CITE[5]@; @CITE[1]@). However, in this work, we use a multitask language model to generate a description of a scientific claim for use in evidence retrieval, and we do not use the generated description to verify the claim.\\nReferences:ID: 1. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Open-Domain Question Answering Goes Conversational via Question Rewriting\\nID: 4. Title: Fact or Fiction: Verifying Scientific Claims\\nID: 5. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization',\n",
       "  'ground_truth': '\\n\\nThe problem of retrieval with descriptions (e.g., TREC 2004 Robust Track; @CITE[4]@) incorporate query-dependent descriptions that describe information needs for query disambiguation (e.g., desirable documents), unlike query-independent instructions in this work. Early work shows that concatenating descriptions only marginally helps (Walker et al., 1998), while @CITE[1]@, 2020) suggests that powerful BERT encoders (@CITE[3]@ could better incorporate such rich context. Dataset Instruction Retrieval QReCC Find a Arguana Retrieval SciFact Find a MultiLexSum I want Table 1: Example instructions for Natural Questions (NQ; @CITE[5]@, QReCC (, Arguana (Wachsmuth et al., 2018), SciFact ( and MultiLexSum (@CITE[2]@. Each instruction defines and unit. The full list of instructions are in Appendix A.5.\\nReferences:ID: 1. Title: Deeper Text Understanding for IR with Contextual Neural Language Modeling\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Open-Domain Question Answering Goes Conversational via Question Rewriting\\nID: 4. Title: Fact or Fiction: Verifying Scientific Claims\\nID: 5. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization<|eot_id|>'},\n",
       " 5322: {'predict': 'Table-to-text generation is a task that generates a natural language description for a given table. Early works on this task focus on generating the most common and the most informative sentence (@CITE[2]@; @CITE[3]@. Recently, the task has been extended to generate multiple sentences (@CITE[1]@; @CITE[3]@, and the generated sentences are required to be logically faithful to the table (@CITE[1]@; @CITE[3]@. The table-to-text generation task is different from the table-to-text generation task in that the former task requires the generated sentences to be logically faithful to the table. In this paper, we propose a novel table-to-text generation task and a novel method to solve it.\\nReferences:ID: 1. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 2. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 3. Title: PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation',\n",
       "  'ground_truth': '\\n\\nAs illustrated in Table 1, existing work and datasets on table-to-text generation typically pose the problem as either a single-sentence generation task (@CITE[1]@; @CITE[3]@; ; ), or a generic summarization task (Lebret et al., 2016; @CITE[2]@; Suadaa et al., 2021). In the single-sentence generation task (@CITE[3]@; @CITE[1]@; ), the focus is on generating fluent and faithful descriptions using provided table regions as a control for text generation. Nevertheless, using table regions for controlling text generation does not align with real-world scenarios, where people refer to tabular data for information-seeking purposes. The generic table summarization tasks (Lebret et al., 2016; @CITE[2]@; Suadaa et al., 2021) aim to create concise and informative summaries based on the content of a given domain-specific table (i.e., sports or scientific). In contrast, the tables in QTSUMM cover diverse topics. Furthermore, considering the numerous data points in the table, various users may be interested in different aspects for their own information-seeking.\\nReferences:ID: 1. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 2. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 3. Title: PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation<|eot_id|>'},\n",
       " 2002: {'predict': 'Span-level pre-training methods have been proposed to enhance the understanding of texts by pre-training the model to predict spans. @CITE[2]@ proposes to pre-train the model by predicting the start and end of spans, and the span boundary representations are used to predict the content of the masked span. @CITE[1]@ proposes to pre-train the model by predicting the entity-level and phrase-level spans. @CITE[3]@ proposes to pre-train the model by predicting the span boundaries and the content of the masked span. However, all these span-level pre-training methods assume that the span is composed of several continuous tokens, and they cannot handle the span composed of non-continuous tokens. Our method can be viewed as an extension of the span-level pre-training methods, and we propose to predict the span composed of non-continuous tokens.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 3. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans',\n",
       "  'ground_truth': '\\n\\nPrevious works manifest that the introduction of span-level information in the pre-training phase can improve PrLMs\\' performance. In the first place, BERT leverages the prediction of single masked tokens as one of the pre-training objectives. Due to the use of WordPiece embeddings (), BERT is able to segment sentences into sub-word level tokens, so that the masked tokens are at the sub-token level, e.g., \"##ing\". (@CITE[2]@) shows that masking the whole word, rather than only single tokens, can further enhance the performance of BERT. Later, it is proved by (@CITE[3]@; ) that the masking of entities is also helpful for PrLMs. By randomly masking adjoining spans in pre-training, SpanBERT () can generate better representation for given texts. AMBERT (@CITE[1]@) achieves better performance than its precursors in NLU tasks by incorporating both sub-token-level and span-level tokenization in pre-training. The upper-mentioned studies all focus on introducing span-level information in pre-training. To the best of our knowledge, the introduction of span-level information in fine-tuning is still a white space to explore, which makes our approach a valuable attempt.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 3. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans<|eot_id|>'},\n",
       " 7362: {'predict': \"The idea of selective prediction has been explored for LLMs in various contexts, including classification (, question answering (@CITE[4]@; @CITE[5]@; @CITE[2]@), and generation (@CITE[3]@; @CITE[6]@; @CITE[7]@; @CITE[8]@). In medical question answering, @CITE[1]@ use a sampling-based confidence score to select answers for ambiguous questions. In this work, we use the same confidence score to guide selective prediction for medical questions. However, we focus on the clinical context and consider the question's ambiguity and the model's confidence simultaneously to select the best prediction. We also propose a novel ensemble refinement approach to further improve the model’s performance.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: Out-of-Distribution Detection and Selective Generation for Conditional Language Models\\nID: 3. Title: Prompting GPT-3 To Be Reliable\\nID: 4. Title: Language Models (Mostly) Know What They Know\\nID: 5. Title: Out-of-Distribution Detection and Selective Generation for Conditional Language Models\\nID: 6. Title: Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation\\nID: 7. Title: Selectively Answering Ambiguous Questions\\nID: 8. Title: Towards Expert-Level Medical Question Answering with Large Language Models\",\n",
       "  'ground_truth': '\\n\\nRecently, LLMs (e.g., GPT-4 (@CITE[1]@ and PaLM (Chowd- hery et al., 2022)) have achieved great success in solving various kinds of Natural Language Gener- ation (NLG) tasks. However, LLMs are still not very reliable and may generate wrong outputs when solving NLG tasks. Due to this, selective predic- tion (or sometimes called selective generation (@CITE[6]@) is critical for safely deploying LLMs in the real-world. Different from selective predic- tion for classiﬁcation tasks (e.g., Natural Language Inference (NLI) tasks) (Xin et al., 2021), selec- tive prediction for LLMs in solving NLG tasks is fundamentally different since the prediction is done auto-regressively over many steps and the possible answer set has an inﬁnite size. Recently, several work propose some uncertainty measures for LLMs, which can be used for selective predic- tion (@CITE[4]@; @CITE[8]@; Varshney et al., 2022; @CITE[6]@; @CITE[5]@. Some recent work studies selective prediction for solving question answering tasks where questions are ambiguous (@CITE[7]@; @CITE[2]@. @CITE[3]@ propose a selective pre- diction method that at inference time leverages an auxiliary model which is trained to distinguish the correct predictions of the QA model from the in- correct ones. Different from previous work, our work proposes to improve selective prediction per- formance of LLMs in solving question answering tasks by learning self-evaluation during ﬁne-tuning.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: Out-of-Distribution Detection and Selective Generation for Conditional Language Models\\nID: 3. Title: Prompting GPT-3 To Be Reliable\\nID: 4. Title: Language Models (Mostly) Know What They Know\\nID: 5. Title: Out-of-Distribution Detection and Selective Generation for Conditional Language Models\\nID: 6. Title: Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation\\nID: 7. Title: Selectively Answering Ambiguous Questions\\nID: 8. Title: Towards Expert-Level Medical Question Answering with Large Language Models<|eot_id|>'},\n",
       " 2036: {'predict': 'Many question answering datasets have been proposed to evaluate the performance of QA systems. For example, SQuAD (@CITE[1]@) and SQuAD v2 (@CITE[6]@) are two popular datasets for single-document QA, while HotpotQA (@CITE[7]@; @CITE[7]@) and Qasper (@CITE[3]@) are multi-document QA datasets. Recently, several datasets have been proposed to evaluate the performance of QA systems in real-world scenarios. For example, QASPER (@CITE[8]@) is a multi-document QA dataset for academic papers. PolicyQA (@CITE[5]@) is a multi-document QA dataset for privacy policies. IIRC (@CITE[9]@) is a multi-document QA dataset for incomplete information reading comprehension. However, these datasets are all based on existing documents and may not be able to capture the complex information needs of real-world users. In this work, we propose a dataset to evaluate the performance of QA systems in real-world scenarios.\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 3. Title: A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nID: 6. Title: PolicyQA: A Reading Comprehension Dataset for Privacy Policies\\nID: 7. Title: Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge\\nID: 8. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 9. Title: IIRC: A Dataset of Incomplete Information Reading Comprehension Questions',\n",
       "  'ground_truth': '\\n\\nhave been proposed in the past few years (, 2018; ; @CITE[6]@; Ferguson et al., 2020; ) and research on these has significantly boosted the performance of QA models. As large pretrained language models (; @CITE[5]@; @CITE[6]@; ; @CITE[7]@; @CITE[9]@) achieved better performance on traditional reading comprehension and question answering tasks, efforts have been made to make the questions more complex. Several multi-hop QA datasets were released (; ; @CITE[8]@; Welbl et al., 2018) to test models’ ability to solve complex questions.\\nReferences:ID: 1. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 2. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 3. Title: A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nID: 6. Title: PolicyQA: A Reading Comprehension Dataset for Privacy Policies\\nID: 7. Title: Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge\\nID: 8. Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\\nID: 9. Title: IIRC: A Dataset of Incomplete Information Reading Comprehension Questions<|eot_id|>'},\n",
       " 2909: {'predict': 'Aspect based sentiment analysis (ABSA) aims to extract aspect terms and predict sentiment polarity for each aspect. In this paper, we mainly focus on the task of aspect term extraction. Early studies mainly focus on the extraction of aspect terms, which are usually modeled by traditional machine learning methods (,, 2014). Recently, with the development of deep learning, researchers have proposed many effective models for ABSA, including CNN-based models (@CITE[6]@; @CITE[5]@, LSTM-based models (, GRU-based models (@CITE[7]@, and BERT-based models (@CITE[1]@; @CITE[8]@; @CITE[9]@;. To improve the performance of ABSA, many works also focus on exploiting external knowledge. For example, @CITE[2]@; @CITE[3]@; @CITE[4]@ propose to use document-level information to enhance ABSA. Besides, some works also focus on aspect-level sentiment classification. For example, @CITE[1]@ propose to use aspect category to enhance aspect-level sentiment classification. Besides, @CITE[8]@ propose to use multi-granularity alignment network to enhance aspect-level sentiment classification. However, all the above works only focus on the single-task setting. In this paper, we propose a unified framework for ABSA, which can be easily extended to multi-task setting. In addition, we also propose a multi-task learning method to enhance ABSA, which can be used in both single-task and multi-task settings.\\nReferences:ID: 1. Title: Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\\nID: 2. Title: Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\\nID: 3. Title: Exploiting Document Knowledge for Aspect-level Sentiment Classification\\nID: 4. Title: Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\\nID: 5. Title: Aspect Level Sentiment Classification with Deep Memory Network\\nID: 6. Title: Exploiting Document Knowledge for Aspect-level Sentiment Classification\\nID: 7. Title: Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification\\nID: 8. Title: An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis\\nID: 9. Title: A Unified Model for Opinion Target Extraction and Target Sentiment Prediction',\n",
       "  'ground_truth': '\\n\\nABSA is a fine-grained sentiment analysis task that consists of various subtasks, including aspect term extraction (ATE) (; ; @CITE[4]@; ; ), opinion term extraction (OTE) (@CITE[7]@; ; @CITE[8]@), aspect-level sentiment classification (ASC) (@CITE[2]@; @CITE[6]@; ;. Since these subtasks are solved individually, recent studies attempted to couple two subtasks as a compound task, such as aspect term polarity co-extraction (APCE) (; @CITE[9]@; @CITE[5]@), aspect and opinion co-extraction (Qiu et al., 2011; ; ), aspect category and sentiment classification (), and aspect-opinion pair extraction (AOPE) (@CITE[1]@; @CITE[8]@; ), and aspect-opinion pair extraction (AOPE) (; @CITE[8]@; @CITE[3]@.\\nReferences:ID: 1. Title: Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\\nID: 2. Title: Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\\nID: 3. Title: Exploiting Document Knowledge for Aspect-level Sentiment Classification\\nID: 4. Title: Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\\nID: 5. Title: Aspect Level Sentiment Classification with Deep Memory Network\\nID: 6. Title: Exploiting Document Knowledge for Aspect-level Sentiment Classification\\nID: 7. Title: Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment Classification\\nID: 8. Title: An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis\\nID: 9. Title: A Unified Model for Opinion Target Extraction and Target Sentiment Prediction<|eot_id|>'},\n",
       " 3081: {'predict': 'Recently, there has been a surge of interest in BERT-based evaluation metrics. These metrics use contextualized embeddings to measure the similarity between candidate and reference sentences. The similarity is often measured using a distance metric such as cosine similarity, Euclidean distance, or Earth Mover Distance (EMD) (@CITE[4]@; @CITE[2]@; @CITE[5]@. The BERTScore metric (@CITE[2]@ uses cosine similarity to measure token-level similarity between candidate and reference sentences. MoverScore (@CITE[5]@ uses EMD to measure the distance between the candidate and reference sentences. BaryScore (@CITE[7]@ uses the Wasserstein distance to measure the distance between candidate and reference sentences. However, these metrics assume that the reference sentence is available. Recently, @CITE[1]@ proposed a reference-free evaluation metric for machine translation, which uses contextualized embeddings to measure the similarity between a source sentence and a system translation. They found that BERT-based metrics perform poorly as semantic encoders for reference-free MT evaluation, and proposed two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. USCORE (@CITE[3]@ is a fully unsupervised evaluation metric for machine translation. It uses an unsupervised evaluation metric to mine pseudo-parallel data, which is then used to remap deficient underlying vector spaces (in an iterative manner) and to induce an unsupervised MT system. Finally, they also induce unsupervised multilingual sentence embeddings from pseudo-parallel data. In this work, we also use a reference-free approach, but we use a different method to measure the similarity between a source sentence and a system translation. We also show that our method is more effective than USCORE.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: COMET: A Neural Framework for MT Evaluation\\nID: 3. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 4. Title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\\nID: 5. Title: Automatic Text Evaluation through the Lens of Wasserstein Barycenters\\nID: 6. Title: On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation\\nID: 7. Title: USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation',\n",
       "  'ground_truth': '\\n\\nIn recent years, many strong automatic evaluation metrics based on BERT ( or its variants have been proposed. It has been shown that those BERT-based evaluation metrics correlate much better with human judgements than traditional evaluation metrics such as BLEU (Papineni et al., 2002). Popular supervised BERT-based evaluation metrics include BLEURT (Sellam et al., 2020) and COMET (@CITE[5]@, which are trained on segment-level human judgments such as DA scores in WMT datasets. Unsupervised BERT-based evaluation metrics such as BERTScore (@CITE[4]@, MoverScore (@CITE[2]@, BaryScore (@CITE[3]@, and XMoverScore (@CITE[6]@ do not use training signals, thus potentially may generalize better to unseen language pairs (. MoverScore, BaryScore, and BERTScore are reference-based evaluation metrics. In contrast, reference-free evaluation metrics directly compare system outputs to source texts. For MT, popular such metrics are Yisi-2 (@CITE[7]@, XMoverScore, and SentSim (@CITE[1]@.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: COMET: A Neural Framework for MT Evaluation\\nID: 3. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 4. Title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\\nID: 5. Title: Automatic Text Evaluation through the Lens of Wasserstein Barycenters\\nID: 6. Title: On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation\\nID: 7. Title: USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation<|eot_id|>'},\n",
       " 3118: {'predict': 'Mental disease detection is a significant research area, with many studies focusing on depression (@CITE[3]@; @CITE[2]@; @CITE[1]@; @CITE[5]@; @CITE[4]@;. In addition, there are some works that consider other mental diseases, such as anxiety (Zhang et al., 2020), post-traumatic stress disorder (PTSD) (, and suicide risk (@CITE[4]@). In contrast, our work focuses on detecting multiple mental diseases. Most of the existing works are based on text classification, which involves training a model to predict the label of a given text sample. However, these models are hard to interpret, which can hinder their deployment in real-world applications. In this work, we propose a framework that can generate explanations for its predictions, which can help improve its interpretability.\\nReferences:ID: 1. Title: SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions\\nID: 2. Title: Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences\\nID: 3. Title: Depression and Self-Harm Risk Assessment in Online Forums\\nID: 4. Title: Adapting Deep Learning Methods for Mental Health Prediction on Social Media\\nID: 5. Title: Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires',\n",
       "  'ground_truth': '\\n\\nMental Disease Detection (MDD) from social media is enabled by the users’ self-disclosure of their diagnosis or their participation in mental disease-related topics and forums. These proxy signals can be leveraged to automatically label the diagnosed diseases of users for the supervised learning of machine learning algorithms. Early research mainly focuses on the detection of depression (), and following works further extend the scope to multiple diseases (@CITE[2]@; ). Approaches for MDD can be mainly divided into two types. The first utilizes features like bag-of-words, topic modeling, and LIWC () with traditional machine learning algorithms (Shen et al., 2017; @CITE[1]@). These methods can provide word/topic-level interpretability, but they cannot leverage the temporal pattern of the posts. The second type leverages deep neural networks that can encode the posts as a sequence for better temporal modeling (@CITE[3]@; Sekuli´ c and @CITE[5]@; @CITE[4]@). However, these methods work as black-boxes and cannot provide explanations. Recent works have also revealed that both types of methods suffer from the lack of generalizability (Harrigian et al., 2020; ).\\nReferences:ID: 1. Title: SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions\\nID: 2. Title: Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences\\nID: 3. Title: Depression and Self-Harm Risk Assessment in Online Forums\\nID: 4. Title: Adapting Deep Learning Methods for Mental Health Prediction on Social Media\\nID: 5. Title: Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires<|eot_id|>'},\n",
       " 7164: {'predict': \"Large language models (LLMs) have been increasingly used in a variety of applications, including coding (@CITE[7]@; @CITE[4]@, scientific research (Zhang et al., 2023; @CITE[9]@, and education (Zhou et al., 2023; @CITE[5]@. As LLMs become more prevalent, there is a growing need to understand and evaluate their capabilities and limitations, as well as to develop strategies for improving their performance, especially in interactive settings. A line of work has been focused on the self-correction of LLMs (@CITE[8]@; @CITE[1]@; @CITE[12]@; @CITE[11]@; @CITE[6]@; @CITE[14]@, 2023; @CITE[3]@; @CITE[2]@; @CITE[10]@, 2023; @CITE[13]@; @CITE[13]@. @CITE[8]@ first propose to use self-correction to improve the factual correctness of LLMs. @CITE[1]@ introduce self-correction in the context of arithmetic reasoning. @CITE[6]@ study the self-correction of LLMs in the context of commonsense reasoning. @CITE[12]@ find that LLMs are prone to hallucinations in their self-correction process. @CITE[11]@ show that LLMs are unable to self-correct when the initial answer is incorrect. @CITE[14]@ propose to use self-correction to improve the reasoning ability of LLMs. @CITE[13]@ find that LLMs are not able to self-correct in the context of arithmetic reasoning. @CITE[2]@ propose to use self-correction to improve the performance of LLMs in the context of programming. @CITE[10]@ find that LLMs are prone to hallucinations in their self-correction process. @CITE[13]@ propose to use self-correction to improve the performance of LLMs in the context of commonsense reasoning. @CITE[3]@ find that LLMs are prone to hallucinations in their self-correction process. @CITE[2]@ propose to use self-correction to improve the performance of LLMs in the context of commonsense reasoning. @CITE[10]@ find that LLMs are prone to hallucinations in their self-correction process. In this paper, we propose to use interactive testing to evaluate and improve the reasoning ability of LLMs. We conduct extensive experiments to show that our method can significantly improve the reasoning ability of LLMs.\\nReferences:ID: 1. Title: Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs\\nID: 2. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\nID: 3. Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\\nID: 4. Title: Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\\nID: 5. Title: Language Models (Mostly) Know What They Know\\nID: 6. Title: Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\\nID: 7. Title: Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\\nID: 8. Title: Large Language Models Can Be Easily Distracted by Irrelevant Context\\nID: 9. Title: Large Language Models Cannot Self-Correct Reasoning Yet\\nID: 10. Title: The Capacity for Moral Self-Correction in Large Language Models\\nID: 11. Title: Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4\\nID: 12. Title: Teaching Large Language Models to Self-Debug\\nID: 13. Title: Improving Factuality and Reasoning in Language Models through Multiagent Debate\\nID: 14. Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\",\n",
       "  'ground_truth': \"\\n\\n... test LLMs interactively in a qualitative fashion utilizing human creativity. Cohn and @CITE[1]@ focuses on spatial commonsense reasoning on a set of conversational LLMs, and shares some of our findings such as the model could contradict itself and apologize with wrong reasons, which displays fundamental misunderstandings and lack of reasoning. @CITE[8]@ tests an early version of GPT-4 on a wide range of tasks such as coding, multimodal composition and math, where GPT-4 demonstrates superior capabilities. Our work makes efforts on characterizing desired properties toward a more systematic evaluation framework which allows quantitative evaluation of LLM reasoning without human subjectivity. LLMs could be influenced by contextual perturbations or biases. @CITE[5]@ injects irrelevant sentences into the context of math questions and finds that LLMs could be easily distracted by them. finds that LLMs’ responses could be heavily influenced by answer bias in the context. @CITE[7]@ finds that models trained via human feedback could exhibit sycophancy and tailor responses only to look more preferable to humans. @CITE[9]@ shows that LLMs could be convinced by wrong facts that conflict with their knowledge. Our proposed setting could be regarded as adding bias from the user into the conversation contexts, but differs from in that we only inject bias during the interaction phase between the model and the user, and do not bias the model’s own solution. LLMs can improve via feedback. Prior work shows that LLMs can improve their responses via feedback (@CITE[2]@; @CITE[4]@; @CITE[11]@; @CITE[12]@; @CITE[14]@; ; @CITE[3]@; Kim et al., 2023; @CITE[6]@; @CITE[5]@; @CITE[10]@; @CITE[13]@. Our work tests the dual direction on LLMs’ behaviors under invalid feedback, which we believe is an important step toward better understanding and interpreting the model performance and make future improvements.\\nReferences:ID: 1. Title: Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs\\nID: 2. Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\nID: 3. Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\\nID: 4. Title: Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\\nID: 5. Title: Language Models (Mostly) Know What They Know\\nID: 6. Title: Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\\nID: 7. Title: Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting\\nID: 8. Title: Large Language Models Can Be Easily Distracted by Irrelevant Context\\nID: 9. Title: Large Language Models Cannot Self-Correct Reasoning Yet\\nID: 10. Title: The Capacity for Moral Self-Correction in Large Language Models\\nID: 11. Title: Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4\\nID: 12. Title: Teaching Large Language Models to Self-Debug\\nID: 13. Title: Improving Factuality and Reasoning in Language Models through Multiagent Debate\\nID: 14. Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate<|eot_id|>\"},\n",
       " 572: {'predict': 'In recent years, abstractive summarization has become a mainstream research topic in the NLP community. Many researchers have explored the effectiveness of various neural models, including CNN-based, RNN-based, and Transformer-based models (@CITE[1]@; @CITE[2]@; @CITE[3]@. However, abstractive summarization has been criticized for its lack of faithfulness and factual correctness (@CITE[3]@; @CITE[2]@. For example, @CITE[3]@ show that 30% of the summaries generated by a state-of-the-art model are hallucinations. To address this issue, some researchers have explored the use of fact-aware models, which incorporate factual information to improve the faithfulness of the generated summaries (@CITE[1]@; @CITE[2]@. However, these models are limited to summarizing news articles, which are relatively short and factual. In contrast, the summaries generated by our model are long and narrative, which requires a more sophisticated model.\\nReferences:ID: 1. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 2. Title: Neural Text Summarization: A Critical Evaluation\\nID: 3. Title: On Faithfulness and Factuality in Abstractive Summarization',\n",
       "  'ground_truth': '\\n\\nRecent studies have suggested that abstractive summarization systems are prone to generate summaries with hallucinated facts that cannot be supported by the source document. reported that almost 30% of the outputs of a state-of-the-art system contain factual inconsistencies. An evaluation of summaries produced by recent state-of-the-art models via crowdsourcing suggested that 25% of the summaries have factual errors (@CITE[3]@. The work also showed that ROUGE scores do not correlate with factual correctness, emphasizing that ROUGE based evaluation alone is not enough for summarization task. In addition, @CITE[2]@ pointed out that current evaluation protocols correlate weakly with human judgements and do not take factual correctness into account. @CITE[1]@ conducted a large scale human evaluation on the generated summaries of various abstractive summarization systems and found substantial amounts of hallucinated content in those summaries. They also concluded that summarization models initialized with pre-trained parameters perform best on not only ROUGE, but also human judgements of faithfulness/factuality.\\nReferences:ID: 1. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 2. Title: Neural Text Summarization: A Critical Evaluation\\nID: 3. Title: On Faithfulness and Factuality in Abstractive Summarization<|eot_id|>'},\n",
       " 2516: {'predict': 'Event extraction is a long-standing task in natural language processing, which has been extensively studied in recent years. Existing approaches to this task can be broadly categorized into extractive and generative approaches. Extractive approaches extract event triggers and arguments by selecting spans from the input text (@CITE[4]@; @CITE[3]@; @CITE[1]@). Generative approaches, on the other hand, generate the event trigger and arguments in a natural language format. Early work on this task has focused on generating event triggers and arguments separately, with different models and architectures (@CITE[2]@; ). Recently, a few works have been proposed to generate event triggers and arguments jointly (@CITE[4]@; ). However, these approaches rely on the input of event triggers, which are typically predicted by a trigger detection model. In this paper, we propose an end-to-end framework that generates event triggers and arguments jointly without relying on the input of event triggers. We also propose a new task, event extraction by natural language generation, which is different from existing work on event extraction.\\nReferences:ID: 1. Title: Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding\\nID: 2. Title: Entity, Relation, and Event Extraction with Contextualized Span Representations\\nID: 3. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 4. Title: Structured Prediction as Translation between Augmented Natural Languages',\n",
       "  'ground_truth': '\\n\\nTraditionally, researchers have been investigating extractive approaches for event/information extraction. Specifically, one branch of work use B-I-O sequence labeling based approaches using CRF or structured SVM models (Björne et al., 2009; Yang and Mitchell, 2016;, and more recently with neural networks (@CITE[2]@; ). Another branch of extractive approaches includes using span enumeration (@CITE[1]@), as well as using question answering to encourage transfer between argument roles (). Recently, generation-based approaches have been proposed. Among them more generally, TANL (@CITE[3]@) proposes to use translation-based approaches for structured prediction. More specifically, it constructs decoding targets by inserting text markers and labels around entity mentions in the input sentence. To better capture cross-entity dependencies. Huang et al. (2021); ; ; @CITE[4]@ propose template-generation based approaches. They fill in the role slots in the template (e.g. Sec 2) with arguments to construct the gold sequences. As compared to TANL and template generation-based methods, our R-GQA is designed to be a QA-based generative model with a simpler generation objective. Plus, it augments the current example’s context with the most similar demonstration in the training set as prompt. It gets the best of both worlds (i.e. question answering and generative models).\\nReferences:ID: 1. Title: Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding\\nID: 2. Title: Entity, Relation, and Event Extraction with Contextualized Span Representations\\nID: 3. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 4. Title: Structured Prediction as Translation between Augmented Natural Languages<|eot_id|>'},\n",
       " 6562: {'predict': 'Prompt-based learning is a recent approach to leverage pre-trained language models (PLMs) in a few-shot setting. The idea is to formulate a task as a cloze-style question and use PLMs to predict the answer. For example, in the text classification task, the input text is transformed into a question format, and the label is replaced with a placeholder. The PLM is trained to predict the placeholder. Several studies have shown that prompt-based learning can achieve comparable performance with fine-tuning on few-shot datasets (@CITE[2]@; @CITE[3]@. However, prompt-based learning is sensitive to the design of the prompt template and the verbalizer, which requires domain-specific prior knowledge and human efforts. Several studies have been conducted to improve the design of the prompt template and the verbalizer. For example, @CITE[1]@ proposed to use a prototypical verbalizer that is built directly from training data. @CITE[1]@ proposed to incorporate external knowledge into the verbalizer. @CITE[1]@ proposed to learn a label-word mapping based on the learned representation of the PLM. However, these methods still require a large amount of training data to learn the verbalizer. In this work, we propose a new method to learn the verbalizer based on the representation of the PLM without using any training data.\\nReferences:ID: 1. Title: Improving and Simplifying Pattern Exploiting Training\\nID: 2. Title: Prototypical Verbalizer for Prompt-based Few-shot Tuning\\nID: 3. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification',\n",
       "  'ground_truth': '\\n\\nStemming from the GPT models (, 2019), the prompt-based learning has been widely discussed. The core idea of cloze-style prompt methods ( is to transform a classification problem into a cloze-style task with textual templates, and then map label words to the verbalizer. Schick and Schütze (2021) use manually defined templates and verbalizers for prompting text classification tasks. To alleviate manual efforts, @CITE[1]@ propose a mining approach for automatically searching for templates. Meanwhile, several approaches have explored the designation of verbalizers. @CITE[2]@ train prototype vectors as verbalizers by contrastive learning. @CITE[3]@ expand the label word space of the verbalizer using external knowledge, and refine the verbalizer space with the training data. However, there is no training data to refine the verbalizer under the zero-shot setting, and it is still very difficult to map the label words to the verbalizer. In addition, the cloze-style prompt methods can predict only one token label per template, which is extremely slow for inference in token-level tasks, such as ZSNER. To solve the above problems, we propose DSP to formulate ZSNER and ZSRE tasks into label discrimination tasks without build verbalizers, while all entities of the same type in a sentence are extracted using only one inference.\\nReferences:ID: 1. Title: Improving and Simplifying Pattern Exploiting Training\\nID: 2. Title: Prototypical Verbalizer for Prompt-based Few-shot Tuning\\nID: 3. Title: Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification<|eot_id|>'},\n",
       " 4556: {'predict': 'COPA ( is a widely used benchmark for commonsense causal reasoning. It is a multiple-choice question answering task where the premise is given and the options are all true. The goal is to rank the correct option. @CITE[2]@ introduced COPA and showed that neural models can achieve high performance on this task. @CITE[5]@ further showed that fine-tuning PLMs on COPA can achieve high performance. @CITE[1]@ proposed a scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. They showed that their method can produce strong baselines without fine-tuning. @CITE[3]@ showed that many improvements in COPA are not due to models learning the task, but due to their increasing ability to exploit superficial cues. They introduced Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues. @CITE[4]@ further showed that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that BERT learns the task to a certain degree when forced to. In contrast, RoBERTa does not appear to rely on superficial cues. However, recent work shows that current models still have a long way to go to truly acquire the ability of causal reasoning (@CITE[1]@; @CITE[3]@; ; @CITE[1]@.\\nReferences:ID: 1. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 2. Title: SocialIQA: Commonsense Reasoning about Social Interactions\\nID: 3. Title: Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning\\nID: 4. Title: When Choosing Plausible Alternatives, Clever Hans can be Clever\\nID: 5. Title: Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models',\n",
       "  'ground_truth': '\\n\\nSince our work is about Contextualized CCR, we first discuss related works about commonsense causal reasoning. Existing commonsense causal reasoning approaches are typically categorized under the general topic of commonsense reasoning (@CITE[4]@;. Most previous works depend on language models. Remarkable progress in CCR mainly comes from dataset augmentation, training procedure design, and external knowledge (Stali¯ unait˙ e et al., 2021; ; ; @CITE[3]@; Iter et al., 2020). Studies (; show that language models exploit superficial artifacts to achieve suspicious high performance. Causal event detection (@CITE[5]@; @CITE[5]@ forms another line of work pertinent to CCR. The task aims to detect causal relations in documents, where various methods are proposed (@CITE[2]@; ; @CITE[1]@. However, those works consider verbal (e.g., “attack”) or nominal predicates (e.g., “explosion”) as events, oversimplifying the relation detection task. In this paper, we study events expressed in free text, facing a more challenging setup but being closer to real applications.\\nReferences:ID: 1. Title: Event2Mind: Commonsense Inference on Events, Intents, and Reactions\\nID: 2. Title: SocialIQA: Commonsense Reasoning about Social Interactions\\nID: 3. Title: Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning\\nID: 4. Title: When Choosing Plausible Alternatives, Clever Hans can be Clever\\nID: 5. Title: Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models<|eot_id|>'},\n",
       " 2640: {'predict': 'Cross-lingual quality of LMs is usually improved by incorporating bilingual dictionaries (, cross-lingual word embeddings (@CITE[4]@; @CITE[3]@; @CITE[5]@; @CITE[2]@; @CITE[1]@, or aligning monolingual embedding spaces in an unsupervised way (@CITE[7]@. Another direction is to align contextualized word representations (@CITE[6]@; @CITE[7]@; @CITE[7]@;. Recently, some works (@CITE[7]@; @CITE[1]@; @CITE[7]@; ; ; propose to use cross-lingual language models to generate cross-lingual contextualized word embeddings. In this paper, we use XLM-RoBERTa to generate cross-lingual contextualized word embeddings. Our method is orthogonal to the above methods, which can be used in conjunction with them to further improve the cross-lingual quality of LMs.\\nReferences:ID: 1. Title: Exploiting Similarities among Languages for Machine Translation\\nID: 2. Title: Word Translation Without Parallel Data\\nID: 3. Title: Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing\\nID: 4. Title: Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing\\nID: 5. Title: Multilingual Alignment of Contextual Word Representations\\nID: 6. Title: Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment\\nID: 7. Title: Emerging Cross-lingual Structure in Pretrained Language Models',\n",
       "  'ground_truth': '\\n\\nTo improve the cross-lingual quality of LMs various authors proposed steps on top of model pretraining. Based on embedding mapping approaches (@CITE[3]@; @CITE[7]@ it was shown that representations of monolingual LMs can also be aligned (@CITE[4]@. In addition, mapping approaches can be applied to the representations of multilingual LMs as well for further improvements (; @CITE[6]@; Hämmerl et al., 2022). In contrast to the above approaches, @CITE[1]@ proposes a word alignment based objective function that fine-tunes the whole model in order to build more similar token representations for the aligned word pairs, while @CITE[2]@ introduced the denoising word alignment task and @CITE[5]@ relied on a contrastive alignment objective to encourage better cross-lingual performance. Others leverage cross-lingual training signals already in the model pretraining phase (Conneau and Lample, 2019; @CITE[2]@. However, the above approaches require parallel data, which is not available for many low-resource languages. In contrast, we show that LMs can be improved by using only monolingual corpora where source and target language sentences with similar contexts can be found.\\nReferences:ID: 1. Title: Exploiting Similarities among Languages for Machine Translation\\nID: 2. Title: Word Translation Without Parallel Data\\nID: 3. Title: Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing\\nID: 4. Title: Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing\\nID: 5. Title: Multilingual Alignment of Contextual Word Representations\\nID: 6. Title: Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment\\nID: 7. Title: Emerging Cross-lingual Structure in Pretrained Language Models<|eot_id|>'},\n",
       " 2380: {'predict': \"Our work is closely related to the prediction-based methods, which predict the remaining inference time of the model. @CITE[2]@ proposed DeeBERT to accelerate BERT inference by predicting the exit point. @CITE[1]@ proposed a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. @CITE[3]@ proposed FastBERT to improve the efficiency of BERT with adaptive inference time. Different from the prediction-based methods, our work focuses on the layer-wise prediction to make the model more lightweight. Besides, our method can be used in conjunction with the prediction-based methods to further accelerate the inference.\\nReferences:ID: 1. Title: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\\nID: 2. Title: BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance\\nID: 3. Title: FastBERT: a Self-distilling BERT with Adaptive Inference Time\",\n",
       "  'ground_truth': \"\\n\\nPrediction Prediction Prediction Exit? Exit? Exit? Figure 2: The overview of our system during inference. PLOT stores hidden states of the local transformer layers. The global transformer layers are enhanced with early exit mechanisms. Input-adaptive inference allows models to choose different computational paths according to the input during inference. In this way, simpler input samples usually require less calculation to make predictions. Recently, DeeBERT (@CITE[2]@ adapts confidence-based BranchyNet (Teerapittayanon et al., 2016), which uses entropy as an early-exit criterion. FastBERT (@CITE[3]@ uses self-distillation to train the branch classifiers. RightTool (Schwartz et al., 2020) leverages the same early-exit criterion as in the Shallow-Deep Network (Kaya et al., 2019), i.e., softmax scores of predictions. PABEE (@CITE[1]@ stops inference when the intermediate predictions of the internal classifiers remain unchanged consecutively.\\nReferences:ID: 1. Title: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\\nID: 2. Title: BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance\\nID: 3. Title: FastBERT: a Self-distilling BERT with Adaptive Inference Time<|eot_id|>\"},\n",
       " 6339: {'predict': 'In the field of deep learning, the backdoor attack, which can be regarded as a kind of data poisoning attack, is a new threat to deep learning systems. Backdoor attacks are performed by injecting a backdoor into a model, so that the model will misclassify any input containing a specific trigger into the target label (@CITE[3]@; @CITE[2]@; @CITE[5]@; @CITE[6]@; @CITE[4]@; @CITE[1]@; @CITE[7]@; @CITE[1]@; @CITE[3]@; @CITE[3]@. These attacks are generally considered as a black-box attack, as the adversary does not know the internal structure of the model, and can only access the input and output of the model. However, the backdoor attack is still a white-box attack in the sense that the adversary knows the trigger word in the poisoned sample. In contrast, in the backdoor attack in the context of natural language processing, the trigger word is not known to the adversary, and the adversary can only access the input and output of the model. Therefore, the backdoor attack in the context of natural language processing can be regarded as a more challenging and realistic black-box attack. Previous studies on backdoor attacks mainly focus on image classification models, while there are few studies on backdoor attacks in natural language processing. In this paper, we propose a new method of backdoor attacks in the context of natural language processing, and propose a method of defense against this attack. In addition, we conduct extensive experiments on the IMDB dataset to demonstrate the effectiveness of our method.\\nReferences:ID: 1. Title: Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning\\nID: 2. Title: A backdoor attack against LSTM-based text classification systems\\nID: 3. Title: Generating Natural Language Adversarial Examples\\nID: 4. Title: Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning\\nID: 5. Title: ONION: A Simple and Effective Defense Against Textual Backdoor Attacks\\nID: 6. Title: Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models\\nID: 7. Title: A backdoor attack against LSTM-based text classification systems',\n",
       "  'ground_truth': '\\n\\nThe concept of backdoor attacks or Trojan attacks of neural network models was first proposed in computer vision research (Gu et al., 2017; @CITE[1]@; @CITE[4]@; @CITE[2]@) and has recently caught the attention of the natural language processing community (; @CITE[3]@; ; ; ; Qi et al., 2021b; @CITE[5]@). Most of the previous work focused on backdoor attacks. BadNL () followed the design settings of BadNet (Gu et al., 2017) from the computer vision literature to study how words from the target class can be randomly inserted into the source text to serve as triggers of backdoor attacks. Li et al. (2021a) replaced the embedding of the rare words, such as ‘cf’ as input-agnostic triggers, to launch a more stable and universal attack. To make the attack more stealthy and invisible, InSent () inserted meaningful fixed short sentences as backdoor attack triggers into movie reviews. In other works, researchers studied numerous non-insertion-based backdoor attacks (@CITE[6]@,b) and model manipulation backdoor attack (@CITE[7]@,b). Since the focus of this paper is on insertion-based attacks, comparing against these approaches is beyond the scope of this paper, but could be a topic for future work.\\nReferences:ID: 1. Title: Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning\\nID: 2. Title: A backdoor attack against LSTM-based text classification systems\\nID: 3. Title: Generating Natural Language Adversarial Examples\\nID: 4. Title: Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning\\nID: 5. Title: ONION: A Simple and Effective Defense Against Textual Backdoor Attacks\\nID: 6. Title: Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models\\nID: 7. Title: A backdoor attack against LSTM-based text classification systems<|eot_id|>'},\n",
       " 4457: {'predict': 'Few-shot argument extraction is a challenging task, which has not been fully explored in previous work. Most existing work is based on supervised learning (@CITE[1]@; @CITE[4]@; @CITE[6]@; @CITE[2]@; @CITE[3]@; @CITE[5]@, where the training data is assumed to cover all the event types. In contrast, few-shot argument extraction is more challenging as the training data is limited and the model has to generalize to unseen event types. To the best of our knowledge, there is no work on few-shot argument extraction. In this work, we propose a novel few-shot argument extraction framework, which is based on contrastive learning and uses a prototypical network to learn the prototypes of the event types.\\nReferences:ID: 1. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 2. Title: FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation\\nID: 3. Title: Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\\nID: 4. Title: Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction\\nID: 5. Title: Extensively Matching for Few-shot Learning Event Detection\\nID: 6. Title: Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes',\n",
       "  'ground_truth': '\\n\\nFew-shot learning for information extraction is proposed to tackle such circumstances when only limited instances are annotated. There have been growing interests under few-shot settings for named entity recognition (@CITE[6]@; @CITE[4]@), and relation extraction (@CITE[2]@; Popovic and Färber, 2022) under single-sentence and document-level scenarios. There has also been research (@CITE[3]@; @CITE[5]@; ; ) for few-shot event extraction within single-sentence. However, to the best of our knowledge, there is no research about document-level few-shot event argument extraction until the submission date. To fill this gap, this work focuses on few-shot learning for document-level argument extraction. Instead of building a new dataset from scratch, we aim at leveraging the existing supervised dataset for reconstructing the instances by a novel N-Way-D-Doc sampling strategy, inspired by similar work (@CITE[1]@; Popovic and Färber, 2022).\\nReferences:ID: 1. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 2. Title: FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation\\nID: 3. Title: Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection\\nID: 4. Title: Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction\\nID: 5. Title: Extensively Matching for Few-shot Learning Event Detection\\nID: 6. Title: Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes<|eot_id|>'},\n",
       " 1261: {'predict': 'Long text generation has been studied in various forms of text, including document summarization (@CITE[5]@, and story generation (@CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[6]@; @CITE[2]@; @CITE[7]@. However, these tasks are mostly in the context of generating long texts from a fixed input, which is different from our task of generating long texts from a short prompt. We also note that some of these works focus on generating long texts by planning the text structure first (@CITE[3]@; @CITE[6]@; @CITE[7]@, and then filling in the text based on the plan. In contrast, we aim to generate long texts directly from a prompt, without explicitly planning the text structure. This is more similar to the task of text summarization, but we focus on generating long texts, rather than short summaries. Our work is also related to the work of generating long texts with a large vocabulary size (@CITE[1]@; @CITE[2]@. However, these works aim to generate long texts by repeating short texts, while we aim to generate long texts by generating new text.\\nReferences:ID: 1. Title: A Hierarchical Neural Autoencoder for Paragraphs and Documents\\nID: 2. Title: Long and Diverse Text Generation with Planning-based Hierarchical Variational Model\\nID: 3. Title: Long and Diverse Text Generation with Planning-based Hierarchical Variational Model\\nID: 4. Title: Hierarchical Neural Story Generation\\nID: 5. Title: Plan-And-Write: Towards Better Automatic Storytelling\\nID: 6. Title: Strategies for Structuring Story Generation\\nID: 7. Title: A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation',\n",
       "  'ground_truth': '\\n\\nPrior works endeavored to solve the incoherence issue in long text generation can be mainly cate- gorized into model structure modiﬁcations, gener- ation mechanism modiﬁcations, and prior knowl- edge injection. To model the hierarchical nature of human texts, @CITE[6]@ proposed a hierarchical RNN de- coder to learn sentence-level representations within the paragraph. @CITE[4]@ augmented the hierarchical model with multi-level latent variables, and @CITE[4]@ further incorporates a plan- ning mechanism to pre-arrange the order and the group of the input keywords. Another line of works proposed to decompose long text generation into multiple stages (@CITE[3]@; @CITE[1]@; @CITE[2]@; Tan et al., 2020) where the model ﬁrst generates a rough sketch, such as key phrases or summaries, and then expands it into the complete long text with ﬁne detail. However, the multi-step generation method is known to have the stage-level exposure bias (Tan et al., 2020), i.e., the discrepancy of middle-stage outputs during training and inference, which can ac- cumulate error through stages and impair the ﬁnal generation quality. The ﬁnal direction is to inject prior external knowledge into pre-trained language models for commonsense story generation (@CITE[5]@; @CITE[7]@. However, these methods may not be generalizable to different data genres such as ﬁctional stories and do not provide long-range guidance during text generation.\\nReferences:ID: 1. Title: A Hierarchical Neural Autoencoder for Paragraphs and Documents\\nID: 2. Title: Long and Diverse Text Generation with Planning-based Hierarchical Variational Model\\nID: 3. Title: Long and Diverse Text Generation with Planning-based Hierarchical Variational Model\\nID: 4. Title: Hierarchical Neural Story Generation\\nID: 5. Title: Plan-And-Write: Towards Better Automatic Storytelling\\nID: 6. Title: Strategies for Structuring Story Generation\\nID: 7. Title: A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation<|eot_id|>'},\n",
       " 7028: {'predict': 'Zero-shot approach in ODQA has been explored in recent years, which trains a re-ranker on a small number of labeled data to improve the performance of retrieval models. Some works (@CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[1]@; @CITE[2]@) train a re-ranker on a small number of labeled data to improve the performance of retrieval models. @CITE[2]@ and @CITE[4]@ train the re-ranker on the labeled data generated by a few-shot query expansion model. @CITE[5]@ and @CITE[3]@ train the re-ranker on the labeled data generated by a zero-shot query expansion model. @CITE[1]@ trains the re-ranker on the labeled data generated by a chain-of-thought model. Our work is different from the previous work in two aspects: 1) we train the re-ranker on the labeled data generated by a few-shot query expansion model, which can obtain more accurate labels than the zero-shot query expansion model. 2) we use the labeled data generated by the few-shot query expansion model to train the re-ranker, which can obtain more accurate labels than the labeled data generated by the chain-of-thought model.\\nReferences:ID: 1. Title: Improving Passage Retrieval with Zero-Shot Question Generation\\nID: 2. Title: Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker\\nID: 3. Title: Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering\\nID: 4. Title: Scaling Instruction-Finetuned Language Models\\nID: 5. Title: Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering',\n",
       "  'ground_truth': '\\n\\nThe advent of an LLM has shown the potential that it can be used in two stages without parameter updates. For the retrieval stage, an LLM is exploited as a re-ranker via query generation or document permutation (@CITE[1]@; @CITE[4]@; @CITE[2]@ or expanded query to diverse pseudo queries for improving the performance of supervised retrievers (@CITE[5]@; @CITE[2]@; @CITE[2]@. For the reader stage, @CITE[3]@ attempted to utilize an LLM as a zero-shot reader, addressing the irrelevant documents through a re-ranker. In this study, we focus on a fully zero-shot reader without an additional module.\\nReferences:ID: 1. Title: Improving Passage Retrieval with Zero-Shot Question Generation\\nID: 2. Title: Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker\\nID: 3. Title: Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering\\nID: 4. Title: Scaling Instruction-Finetuned Language Models\\nID: 5. Title: Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering<|eot_id|>'},\n",
       " 7152: {'predict': 'NAS has been widely applied to image classification (@CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[5]@. Recently, many works have applied NAS to the pre-trained language models (@CITE[4]@; ; @CITE[5]@. @CITE[4]@ first applied the NAS to the pre-trained language models and searched for the best architecture for the pre-trained language models. @CITE[5]@ searched for the optimal architecture for the pre-trained language models in the speech recognition task. @CITE[3]@ proposed a novel NAS method for the pre-trained language models and searched for the fast model with high performance. @CITE[5]@ decomposed the Transformer architecture into smaller components and proposed a sampling-based one-shot architecture search method to find an optimal model for inference. @CITE[4]@ proposed a novel NAS method to find the best architecture for the pre-trained language models in the machine translation task. Our work is the first to apply NAS to the pre-trained language models in the dialogue generation task.\\nReferences:ID: 1. Title: Regularized Evolution for Image Classifier Architecture Search\\nID: 2. Title: The Evolved Transformer\\nID: 3. Title: The Lottery Ticket Hypothesis for Pre-trained BERT Networks\\nID: 4. Title: DARTS-ASR: Differentiable Architecture Search for Multilingual Speech Recognition and Adaptation\\nID: 5. Title: Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition',\n",
       "  'ground_truth': '\\n\\nAfter the success of automatically searched neural networks (; ) outperforming the manually-designed architectures, researchers have also applied NAS for NLP models (@CITE[3]@; @CITE[5]@; ; Gao et al., 2022; @CITE[4]@). While most of these works focus on optimizing models for a single language, DARTS-ASR (@CITE[1]@) leverages a successful NAS method, DARTS (@CITE[2]@), to automatically search multilingual ASR model for four languages. Similarly, perform NAS to find a shared architecture for multilingual corpora. In contrast, our approach leverages NAS techniques to address negative interference by searching for language-specific tickets rather than focusing on the shared model part, as overlooked in previous works. Moreover, to the best of our knowledge, we are the first to explore zero-shot NAS methods for mPLMs.\\nReferences:ID: 1. Title: Regularized Evolution for Image Classifier Architecture Search\\nID: 2. Title: The Evolved Transformer\\nID: 3. Title: The Lottery Ticket Hypothesis for Pre-trained BERT Networks\\nID: 4. Title: DARTS-ASR: Differentiable Architecture Search for Multilingual Speech Recognition and Adaptation\\nID: 5. Title: Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition<|eot_id|>'},\n",
       " 3201: {'predict': 'Recent research on dialogue safety has primarily focused on detecting offensive language in context (@CITE[5]@; @CITE[4]@; @CITE[6]@; @CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[7]@. However, there are several limitations of these studies. For example, they are primarily focused on detecting offensive language in context and do not consider other types of safety concerns, such as privacy and misinformation. Additionally, they are primarily based on human annotated datasets and do not leverage the vast amounts of data available from the web. Finally, they primarily focus on safety in isolation, rather than considering it in conjunction with other important factors such as engagement and humanness.\\nReferences:ID: 1. Title: Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack\\nID: 2. Title: Recipes for Safety in Open-domain Chatbots\\nID: 3. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts\\nID: 4. Title: On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark\\nID: 5. Title: LaMDA: Language Models for Dialog Applications\\nID: 6. Title: Red Teaming Language Models with Language Models\\nID: 7. Title: Recipes for building an open-domain chatbot\\nID: 8. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts',\n",
       "  'ground_truth': '\\n\\nMost existing dialogue safety work has focused on detecting problematic contexts, often using binary or ternary labels. @CITE[7]@; @CITE[4]@. develop classifiers to detect when an agent agrees with toxic content. @CITE[2]@ create a suite of classifiers to assess safety concerns. @CITE[3]@ collect fine-grained context and utterance-level safety labels. Other works leverage these safety labels to make conversational agents generate better responses. @CITE[5]@; @CITE[8]@; @CITE[6]@. More recently, several works have introduced strategies to respond to problematic context with canned non-sequiturs. @CITE[1]@, control for steering away from toxicity., and apologies.. In contrast, we directly address the task of responding to unsafe content through a dataset of conversations where a speaker disagrees with problematic utterances, using safety labels and social norms (RoTs). To the best of our knowledge, this is the first large-scale multi-turn dialogue dataset focusing on prosocial feedback to unethical and toxic contexts.\\nReferences:ID: 1. Title: Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack\\nID: 2. Title: Recipes for Safety in Open-domain Chatbots\\nID: 3. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts\\nID: 4. Title: On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark\\nID: 5. Title: LaMDA: Language Models for Dialog Applications\\nID: 6. Title: Red Teaming Language Models with Language Models\\nID: 7. Title: Recipes for building an open-domain chatbot\\nID: 8. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts<|eot_id|>'},\n",
       " 7471: {'predict': 'Claim analysis is a research area that has been gaining traction in the recent years. It involves identifying claims (@CITE[4]@; @CITE[1]@; @CITE[1]@; @CITE[3]@; @CITE[5]@; ; ; ; @CITE[2]@;, evaluating their validity (, and ranking them for fact-checking ( or news summarization (Zhang et al., 2021). In this paper, we focus on the claim detection task. Claim detection is a task that involves identifying claims in a given text. @CITE[4]@; @CITE[1]@; @CITE[1]@; @CITE[3]@; @CITE[5]@; ; ; @CITE[2]@; perform claim detection in political debates, news articles, and Twitter data. In this paper, we propose a method for claim detection in product reviews. We use a combination of lexical and contextual features to identify claims in product reviews. We propose a method that uses a combination of lexical features such as word embeddings, dependency parsing, and POS tagging, and contextual features such as BERT embeddings. Our proposed method is a simple and efficient method that can be used in any domain. We also show that the performance of our method is comparable to state-of-the-art methods.\\nReferences:ID: 1. Title: What is the Essence of a Claim? Cross-Domain Claim Identification\\nID: 2. Title: LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content\\nID: 3. Title: LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content\\nID: 4. Title: ClaimRank: Detecting Check-Worthy Claims in Arabic and English\\nID: 5. Title: It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction',\n",
       "  'ground_truth': '\\n\\nPrevious work has focused on distinct aspects of claims, including claim detection (@CITE[1]@; @CITE[4]@; @CITE[4]@; Gangi Reddy et al., 2022a,b), claim check-worthiness estimation (; ; Barrón-Cedeño et al., 2018; ; @CITE[5]@; Barrón-Cedeño et al., 2020; @CITE[2]@), claim span identification (@CITE[3]@), etc. By curating the AAWD corpus, pioneered the efforts in claim detection, the foremost step in the fact-checking tasks. Following this, linguistically motivated features, including sentiment, syntax, context-free grammar, and parse trees, were frequently used (Daxenberger et al., 2017; ; ; @CITE[4]@). Recently, large language models (LLMs) have also been used for claim detection (; Barrón-Cedeño et al., 2020; @CITE[4]@; Gangi Reddy et al., 2022a,b). Most previous work on claim detection and extraction primarily concentrated on adapting to text that comes from similar distributions or topics. Moreover, it often relied on well-structured formal writing. In contrast, our objective is to develop a system that specifically addresses the challenges posed by posts in social media and aims to extract the central claim in a more simplified manner, which goes beyond extracting a text subspan in a social media post and aims at abstractive claim extraction that mimics what professional fact-checkers do. To the best of our knowledge, we are the first to address the task of claim extraction in this very practical formulation.\\nReferences:ID: 1. Title: What is the Essence of a Claim? Cross-Domain Claim Identification\\nID: 2. Title: LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content\\nID: 3. Title: LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content\\nID: 4. Title: ClaimRank: Detecting Check-Worthy Claims in Arabic and English\\nID: 5. Title: It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction<|eot_id|>'},\n",
       " 2145: {'predict': 'Several studies have evaluated the performance of interpretation methods in the past. For example, @CITE[6]@ evaluated the performance of gradient-based methods on image classification tasks, while @CITE[5]@ evaluated the performance of attention-based methods on text classification tasks. More recently, @CITE[8]@ evaluated the performance of different explanation methods on text classification tasks, and @CITE[7]@ evaluated the performance of different explanation methods on image classification tasks. However, these studies only evaluated the performance of explanation methods on specific tasks, and did not provide a general framework for evaluating explanation methods. More recently, @CITE[1]@ evaluated the performance of explanation methods by evaluating the correlation between explanation scores and model predictions, but they only evaluated explanation methods on image classification tasks and did not evaluate the performance of explanation methods on text classification tasks. In contrast, we evaluate the performance of explanation methods on both image and text classification tasks and evaluate explanation methods by evaluating the correlation between explanation scores and model predictions, as well as by evaluating the performance of explanation methods on different types of inputs.\\nReferences:ID: 1. Title: Attention is not Explanation\\nID: 2. Title: Towards better understanding of gradient-based attribution methods for Deep Neural Networks\\nID: 3. Title: Axiomatic Attribution for Deep Networks\\nID: 4. Title: Learning Important Features Through Propagating Activation Differences\\nID: 5. Title: On the Robustness of Interpretability Methods\\nID: 6. Title: The (Un)reliability of saliency methods\\nID: 7. Title: Towards A Rigorous Science of Interpretable Machine Learning\\nID: 8. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 9. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 10. Title: Evaluations and Methods for Explanation through Robustness Analysis\\nID: 11. Title: Evaluating Saliency Methods for Neural Language Models',\n",
       "  'ground_truth': '\\n\\nOne line of studies explores approaches to evaluate interpretations. Several studies propose measurements for faithfulness. A large proportion of them occlude tokens identified as important by interpretations and measure the confidence change of models (DeYoung et al., 2020; @CITE[2]@; ;. Some other works propose to evaluate the faithfulness by checking to what extent they satisfy some desired axioms (@CITE[11]@; ; @CITE[8]@. Besides, @CITE[6]@; Ghorbani et al. (2019); @CITE[11]@; @CITE[5]@ reveal limitations in interpretation faithfulness through testing the robustness of interpretations. Another group of studies measure the plausibility of interpretations, i.e., whether the explanations conform with human judgments (; Ribeiro et al., 2016), or assist humans or student models to predict model behaviors on new data (@CITE[7]@; @CITE[1]@. Note that although there exist many hybrid works that evaluate both the faithfulness and the plausibility of interpretations by combining a suite of diagnostic tests (; ; @CITE[8]@, advocate to explicitly distinguish between the two measurements. In our work, we focus on interpretation faithfulness but consider two new metrics. We apply them to the dependency parsing task. Also, notice that the stability could be regarded as an automatic input consistency test suggested by.\\nReferences:ID: 1. Title: Attention is not Explanation\\nID: 2. Title: Towards better understanding of gradient-based attribution methods for Deep Neural Networks\\nID: 3. Title: Axiomatic Attribution for Deep Networks\\nID: 4. Title: Learning Important Features Through Propagating Activation Differences\\nID: 5. Title: On the Robustness of Interpretability Methods\\nID: 6. Title: The (Un)reliability of saliency methods\\nID: 7. Title: Towards A Rigorous Science of Interpretable Machine Learning\\nID: 8. Title: ERASER: A Benchmark to Evaluate Rationalized NLP Models\\nID: 9. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 10. Title: Evaluations and Methods for Explanation through Robustness Analysis\\nID: 11. Title: Evaluating Saliency Methods for Neural Language Models<|eot_id|>'},\n",
       " 333: {'predict': 'With the increasing usage of Transformers in NLP, compressing the large-scale Transformer-based models has attracted extensive attention. One common method is pruning (@CITE[6]@; @CITE[2]@), which removes unimportant parameters and keeps the remaining ones. Pruning methods can be divided into two categories: magnitude-based pruning (@CITE[6]@; @CITE[2]@) and structured pruning (@CITE[1]@; @CITE[4]@; @CITE[5]@). The magnitude-based pruning methods prune the parameters based on the magnitude of the weights. However, this method may lead to a loss of information. The structured pruning methods prune the parameters in a structured way, such as pruning the kernel in a convolutional layer or the attention head in a Transformer layer. Another method is quantization (@CITE[3]@), which converts the weights of the model to a lower precision format, such as 8-bit integers. Quantization methods can be divided into three categories: post-training quantization, quantization-aware training, and knowledge distillation-based quantization. Post-training quantization quantizes the pre-trained model directly, which may lead to a loss of accuracy. Quantization-aware training quantizes the model during training and the gradients are approximated with the Straight-Through Estimator. Knowledge distillation-based quantization trains a student model to mimic the behavior of a teacher model.\\nReferences:ID: 1. Title: Compressing Large-Scale Transformer-Based Models: A Case Study on BERT\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 5. Title: BioBERT: a pre-trained biomedical language representation model for biomedical text mining\\nID: 6. Title: Training with Quantization Noise for Extreme Model Compression',\n",
       "  'ground_truth': '\\n\\nA wealth of work has recently been released (@CITE[5]@ on compressing Transformer-based models (; @CITE[3]@ through the pre-training of compact models (@CITE[1]@, distillation (Hinton et al., 2015; Jiao et al., 2019; @CITE[2]@, pruning (@CITE[4]@; McCarley et al., 2019; Sanh et al., 2020; Fan et al., 2020a) and quantization (@CITE[6]@; 1https://illuin-tech.github.io/ FQuAD-explorer/. Nevertheless, absolute performance is not the end goal of this study. Rather, we investigate the training process of compact models in the absence of larger ones to distillate or prune. Furthermore, Sanh et al. (2020) acknowledge the difficulty of speeding up sparse models due to the absence of specialized hardware. Therefore, from an inference speed standpoint, it is currently preferable to train compact models.\\nReferences:ID: 1. Title: Compressing Large-Scale Transformer-Based Models: A Case Study on BERT\\nID: 2. Title: Attention Is All You Need\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\\nID: 5. Title: BioBERT: a pre-trained biomedical language representation model for biomedical text mining\\nID: 6. Title: Training with Quantization Noise for Extreme Model Compression<|eot_id|>'},\n",
       " 7210: {'predict': 'Query-focused text summarization aims to generate a summary of a given document(s) in response to a query. Neural models have been widely used for this task (@CITE[4]@; @CITE[3]@; @CITE[2]@; @CITE[1]@; @CITE[6]@; @CITE[5]@. Most of the neural models for query-focused summarization are based on the encoder-decoder architecture. A typical encoder-decoder model consists of an encoder, an attention-based fusion layer, and a decoder. The encoder is usually based on a pre-trained model like BERT (@CITE[7]@ or RoBERTa (@CITE[1]@, which is fine-tuned on the summarization dataset. The decoder is usually an LSTM-based model with an attention mechanism. The attention mechanism is used to focus on the relevant parts of the document for summarization. The fusion layer is used to fuse the information from the encoder and decoder. In this paper, we focus on the fusion layer and propose a novel method to enhance the query-focused summarization.\\nReferences:ID: 1. Title: Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models\\nID: 2. Title: Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization\\nID: 3. Title: Attention Is All You Need\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Exploring Neural Models for Query-Focused Summarization\\nID: 6. Title: Diversity driven Attention Model for Query-based Abstractive Summarization\\nID: 7. Title: Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization',\n",
       "  'ground_truth': '\\n\\nQuery-focused text summarization using neural models has gained increasing attention in recent years (@CITE[1]@; @CITE[5]@). The recent success of transformer-based models (@CITE[4]@; @CITE[7]@; ; ) on generic abstractive summarization has also inspired researchers to utilize such models for query-based abstractive summarization (@CITE[2]@; ;,b, 2022), leading to state-of-the-art performance in benchmark query-based summarization datasets, such as DUC (@CITE[6]@; ), AQuaMuSe (), QMSum (Zhong et al., 2021), Debatepedia (@CITE[3]@), etc. Among the datasets mentioned above, one notable exception is the Debatepedia dataset since it requires generating summaries from a document containing argumentative text (i.e., arguments and counter-arguments). However, it has been found recently that many samples in the Debatepedia dataset are not actually query-oriented while models that are trained without considering the query relevance could achieve almost similar performance as the query-focused summarization models (@CITE[5]@). Thus, there remains a scarcity of datasets specifically tailored to generate query-focused summaries of argumentative texts. Though some studies () have attempted to generate the queries in generic summarization datasets (e.g., CNNDM ()), we find that these queries are generated by directly extracting words from the reference summaries, leading to unexpected access to the keywords in the reference summaries for the summarization models.\\nReferences:ID: 1. Title: Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models\\nID: 2. Title: Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization\\nID: 3. Title: Attention Is All You Need\\nID: 4. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 5. Title: Exploring Neural Models for Query-Focused Summarization\\nID: 6. Title: Diversity driven Attention Model for Query-based Abstractive Summarization\\nID: 7. Title: Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization<|eot_id|>'},\n",
       " 4470: {'predict': 'Mixture-of-Experts (MoE) models are a form of conditional computation, where a set of experts are conditionally activated for each input (@CITE[4]@; @CITE[6]@; @CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[3]@. In MoE models, experts are typically trained in isolation and then combined to form a single model, with a sparse gating network determining which experts to use for each input. This has led to a variety of MoE architectures, including those that learn to combine experts using a single gating network (@CITE[4]@, those that use a gating network for each input (@CITE[6]@, and those that use a combination of both (@CITE[5]@. In addition to their use in language modeling, MoE models have also been applied to a variety of other domains, including vision (@CITE[1]@, multimodal learning (@CITE[3]@, and reinforcement learning (@CITE[2]@.\\nReferences:ID: 1. Title: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\\nID: 2. Title: GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\\nID: 3. Title: Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\\nID: 4. Title: Scalable Transfer Learning with Expert Models\\nID: 5. Title: Scalable and Efficient MoE Training for Multitask Multilingual Models\\nID: 6. Title: Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts',\n",
       "  'ground_truth': '\\n\\nThe concept of Mixture-of-Experts models in machine learning dates back to the works of ; @CITE[4]@. Most recent versions were inspired by @CITE[2]@, who achieved state-of-the-art language modeling and translation results with the largest model at that time. Combined with the Transformer model, MoE models grew in popularity (; @CITE[3]@). Beyond natural language processing, MoE models showed a large success in computer vision (@CITE[5]@), speech recognition (), multi-modal learning (@CITE[1]@), and diffusion models (@CITE[6]@; Balaji et al., 2022) to name a few. For a more detailed survey of MoE models, we refer readers to Yuksel et al. (2012) and. Despite the recent successes, large MoE models require a lot of memory and the contribution (or roles) of experts is under-explored. Chen et al. (2022) showed that the contributions of experts of a pre-trained MoE model in different tasks such as MNLI, CoLA, and SQuAD are quite different. Moreover, they converted a large sparse MoE model pre-trained on a general task to a single-expert dense model by fine-tuning the most ‘professional’ expert and dropping the other experts. It demonstrates that experts do not contribute equally to the performance and some are more important than others. also studied different expert specializations such as sentinel tokens, punctuation, conjunctions and articles, and even languages. They concluded that experts in the encoder exhibit specialization, in contrast to the decoder, but not by language. According to the authors, their mechanism of token routing and load balancing prevents language specialization. train study routing mechanisms at different levels of granularity and show that task-level experts (i.e., per language) can achieve similar performance as token-level experts. However, this work assumes that the model is trained this way, while our own work attempts to prune an existing token-level MoE model at inference without re-training it.\\nReferences:ID: 1. Title: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\\nID: 2. Title: GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\\nID: 3. Title: Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\\nID: 4. Title: Scalable Transfer Learning with Expert Models\\nID: 5. Title: Scalable and Efficient MoE Training for Multitask Multilingual Models\\nID: 6. Title: Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts<|eot_id|>'},\n",
       " 6902: {'predict': 'A large body of work has examined various aspects of syntactic and semantic capabilities in language models. For example, @CITE[5]@; @CITE[3]@ show that large language models can be used to perform semantic parsing with little training data, when prompted with in-context examples. @CITE[1]@ and @CITE[4]@ also explore the use of language models to perform semantic parsing tasks, such as mapping natural language to code. @CITE[2]@ and @CITE[4]@ show that large language models can be used to perform semantic parsing tasks, such as mapping natural language to code. In contrast, we focus on the use of large language models to perform semantic parsing tasks, such as mapping natural language to AMR graphs.\\nReferences:ID: 1. Title: Constrained Language Models Yield Few-Shot Semantic Parsers\\nID: 2. Title: Few-Shot Semantic Parsing with Language Models Trained On Code\\nID: 3. Title: Graph Pre-training for AMR Parsing and Generation\\nID: 4. Title: Maximum Bayes Smatch Ensemble Distillation for AMR Parsing\\nID: 5. Title: Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing',\n",
       "  'ground_truth': '\\n\\nA large body of work has examined various aspects of syntactic and semantic capabilities in language models (c.f., indicating that LLMs show strong knowledge of syntactic structure, while semantic capabilities are more mixed. Nonetheless, LLMs have also been used for few-shot semantic parsing with some success. In particular, @CITE[3]@ and Shin and @CITE[2]@ find that few-shot learning in GPT-3 and Codex produces semantic parses that outperform baselines with comparable training sizes. These semantic parsing datasets, which focus on producing database queries in particular domains, are less complex and domain-general than AMR, but the results suggest that LLMs should contain aspects of the knowledge needed to analyze semantic structure. As for AMR, pre-trained transformer models have helped to advance the state of the art in AMR parsing, with recent AMR parsers building on the foundation of models like BART (@CITE[5]@; ; @CITE[4]@; @CITE[1]@. This indicates that pre-trained models may also pick up on representational capabilities relevant for supporting AMR. Though these prior works are suggestive that LLMs and pre-trained transformers capture certain aspects of linguistic structure, it is not clear from existing results how detailed or reliable LLMs’ ability to analyze meaning structure may be— formalisms used for prior few-shot semantic parsing are simpler and more domain-specific than AMR, and the supervised fine-tuning of BART for AMR parsing obscures the details of what original knowledge may have been contained in the pre-trained model. To achieve a clearer picture of LLMs’ ability to analyze rich semantic structure, Level 1 criteria Proportion of parse outputs that... Basic Form meet basic AMR graph format (concept nodes, edge relationships, hierarchical bracketing) Top Node correctly identify the top parse node Main Rel select the correct main event as the highest event relation, disregarding non-eventive relations like and (Fig. 1) or contrast-01 (Fig. 2) Overall Accept constitute a valid AMR representation for the sentence, regardless of match to the gold annotation (allows us to credit models if they parse the sentence reasonably but differ from gold) Level 2 criteria Proportion of parse outputs in which, for each correctly identified top-level event,... Event all arguments are present and identified as arguments of that event Event all event modifiers are present and identified as modifiers of that event Arg Mods all modifiers for arguments of that event are present and identified as modifiers of the args Extra Mods there are any modifiers added erroneously to events or arguments anywhere in the parse\\nReferences:ID: 1. Title: Constrained Language Models Yield Few-Shot Semantic Parsers\\nID: 2. Title: Few-Shot Semantic Parsing with Language Models Trained On Code\\nID: 3. Title: Graph Pre-training for AMR Parsing and Generation\\nID: 4. Title: Maximum Bayes Smatch Ensemble Distillation for AMR Parsing\\nID: 5. Title: Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing<|eot_id|>'},\n",
       " 5527: {'predict': 'GEC is a fundamental task in NLP with many applications in language learning, writing assistance, and text summarization. The conventional approach to GEC is to model it as a sequence-to-sequence task, which can be further divided into encoder-decoder (@CITE[5]@; @CITE[4]@; ; @CITE[1]@), parallel iterative edit (@CITE[3]@), and non-autoregressive (@CITE[9]@; ; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[10]@) models. These approaches can be further classified as either character-level or token-level. In this work, we focus on token-level approaches, which are more prevalent in GEC literature. For example, @CITE[2]@ use a token-level classification model to correct errors. @CITE[6]@ propose a semi-autoregressive text editing model that decomposes the generation process into three sub-tasks: tagging, re-ordering, and insertion. @CITE[9]@ propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. @CITE[10]@ propose an extended sequence tagging vocabulary for GEC, introducing specialized tags for spelling correction and morphological inflection. In this work, we propose a new token-level sequence tagging approach that is particularly effective for GEC. Our model is based on the Transformer encoder (@CITE[5]@) and is trained on synthetic data. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an $F_{0.5}$ of 65.3/66.5 on CoNLL-2014 (test) and $F_{0.5}$ of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.\\nReferences:ID: 1. Title: Neural Language Correction with Character-Based Attention\\nID: 2. Title: Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task\\nID: 3. Title: Parallel Iterative Edit Models for Local Sequence Transduction\\nID: 4. Title: Encode, Tag, Realize: High-Precision Text Editing\\nID: 5. Title: Seq2Edits: Sequence Transduction Using Span-level Edit Operations\\nID: 6. Title: Felix: Flexible Text Editing Through Tagging and Insertion\\nID: 7. Title: GECToR -- Grammatical Error Correction: Tag, Not Rewrite\\nID: 8. Title: Character Transformations for Non-Autoregressive GEC Tagging\\nID: 9. Title: EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start\\nID: 10. Title: An Extended Sequence Tagging Vocabulary for Grammatical Error Correction',\n",
       "  'ground_truth': '\\n\\nEarly efforts focused on building feature-based machine learning (ML) classifiers to fix common error types (Chodorow et al., 2007; @CITE[9]@; ; Kochmar et al., 2012; @CITE[4]@;. Such models required feature engineering and lacked the ability to correct all error types simultaneously. Reformulating GEC as a monolingual MT task alleviated these issues, first with statistical MT approaches (;, 2016) and then neural MT approaches (@CITE[6]@; @CITE[1]@; @CITE[5]@; Watson et al., 2018), with Transformer-based models being the most dominant (; Zhao et al., 2019; @CITE[8]@; ;. More recently, edit-based models have been proposed to solve GEC (@CITE[10]@; ; @CITE[7]@; @CITE[3]@; ; @CITE[2]@; ;. While Seq2Seq models generate corrections to erroneous input, edit-based models generate a sequence of corrective edit operations. Edit-based models add explainability to GEC and improve inference time efficiency. However, they generally require human engineering to define the size and scope of the edit operations (.\\nReferences:ID: 1. Title: Neural Language Correction with Character-Based Attention\\nID: 2. Title: Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task\\nID: 3. Title: Parallel Iterative Edit Models for Local Sequence Transduction\\nID: 4. Title: Encode, Tag, Realize: High-Precision Text Editing\\nID: 5. Title: Seq2Edits: Sequence Transduction Using Span-level Edit Operations\\nID: 6. Title: Felix: Flexible Text Editing Through Tagging and Insertion\\nID: 7. Title: GECToR -- Grammatical Error Correction: Tag, Not Rewrite\\nID: 8. Title: Character Transformations for Non-Autoregressive GEC Tagging\\nID: 9. Title: EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start\\nID: 10. Title: An Extended Sequence Tagging Vocabulary for Grammatical Error Correction<|eot_id|>'},\n",
       " 4371: {'predict': 'Recently, there are several works that use weak supervision to improve the dialogue generation. @CITE[1]@ proposed to use a weakly labeled dataset, where only a small portion of the data is labeled, and the rest of the data is automatically labeled by a fine-tuned GPT-2. They used a semi-supervised framework to jointly train the NLG and NLU models. @CITE[2]@ proposed to use a generative model to synthesize weak labels at scale. They employed a semi-supervised framework to jointly train the NLG and NLU models. @CITE[4]@ proposed to use a pre-trained language model as the NLU model and fine-tune it to predict the next utterance. They proposed a framework to generate the next utterance with a pre-trained language model and fine-tuned it to predict the next utterance. @CITE[3]@ proposed to use a pre-trained language model to generate a response and select the appropriate knowledge to generate a response. They proposed a framework to generate the next utterance with a pre-trained language model and select the appropriate knowledge to generate a response. However, these works are based on the assumption that the weak supervision is sufficient for training a dialogue generation model. However, the weak supervision is not sufficient for training a dialogue generation model, especially in the low-resource scenario. In this paper, we propose to use the weak supervision to improve the dialogue generation in the low-resource scenario. We propose to use the weak supervision to select the appropriate knowledge and generate the next utterance.\\nReferences:ID: 1. Title: Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 4. Title: Learning to Select Knowledge for Response Generation in Dialog Systems',\n",
       "  'ground_truth': '\\n\\nFor the dialogue generation task, even though the responses are naturally the ground truth for the model training, previous studies showed that several auxiliary tasks with weak supervision can help to improve the generation performance (@CITE[3]@. uses BERT encoders (@CITE[2]@ and a GPT-2 decoder ( for knowledge projection and prediction. They built pseudo ground truth documents by leveraging the similarity between each document and the response to weakly supervise document selection. @CITE[4]@ propose a de-noising mechanism for the knowledge tokens to be injected in a weak supervision manner. @CITE[1]@ design a weak supervision-based discriminator to capture the relations between the answer and the corresponding passage and eventually generate the questions. Informed by these studies, in our work we introduce a weak supervision to complement the generation model training. Our proposed CKL model combines context differentiation and weak supervision for response generation purposes. The CKL model differs from prior studies by (1) devising latent vectors for both context and knowledge to derive the context latent weights and knowledge latent weights for knowledge selection and response generation tasks, and (2) designing a latent weight enhanced attention operation, combined with a weak supervision technique to provide more effective use of context and knowledge for response generation.\\nReferences:ID: 1. Title: Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 4. Title: Learning to Select Knowledge for Response Generation in Dialog Systems<|eot_id|>'},\n",
       " 5168: {'predict': \"The domain adaptation of language models has been studied extensively in recent years. @CITE[1]@ first propose to perform domain-adaptive pretraining, which adapts the language model to the target domain by using domain-specific data. @CITE[2]@ propose to use task-adaptive pretraining, which adapts the language model to the target task by using task-specific data. @CITE[5]@ propose to use adaptive tokenization, which adapts the tokenization of the language model to the target domain. In addition, some studies propose to use domain-specific data to pretrain the language model. @CITE[3]@ propose to use a domain-specific corpus to pretrain the language model. @CITE[4]@ propose to use a domain-specific corpus to pretrain the language model by contrastive learning. @CITE[6]@ propose to use domain-specific data to pretrain the language model by masked autoencoder. However, these methods are not suitable for the domain adaptation of pre-trained language models, as they need to collect domain-specific data and require large computational resources.\\nReferences:ID: 1. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Efficient Domain Adaptation of Language Models via Adaptive Tokenization\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 5. Title: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference\\nID: 6. Title: Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation\",\n",
       "  'ground_truth': \"\\n\\nOne typical way for creating domain-specific language models is pre-training through Masked Language Modelling on in-domain corpora, either continuously ( or from-scratch (@CITE[4]@. An alternative is adapting the tokenizer to accommodate domain-specific vocabulary (@CITE[1]@; @CITE[3]@. For sentence embedding models specifically, domain adaptation is usually done through unsupervised training with novel objectives (@CITE[2]@; @CITE[5]@ or in-domain data generation (@CITE[6]@, mainly for the similarity or relevance estimation tasks. However, supervised sentence embedding training with general-domain data (SEPT) is always required after the unsupervised domain-specific training phase (DAPT) to achieve optimal performance (@CITE[2]@. Our proposed method is inspired by the idea of disentangling domain adaptation and the downstream relevance estimation task via PEFT in Zhan et al. (2022). In the present study, we show that PEFT can also be used to decouple DAPT and SEPT for few-shot classification tasks.\\nReferences:ID: 1. Title: Don't Stop Pretraining: Adapt Language Models to Domains and Tasks\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Efficient Domain Adaptation of Language Models via Adaptive Tokenization\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 5. Title: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference\\nID: 6. Title: Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation<|eot_id|>\"},\n",
       " 5292: {'predict': 'Stylistic text rewriting is a challenging task in NLP. Many studies have been conducted on rewriting text with different styles. Some works focus on the style transfer between positive and negative sentiments (@CITE[8]@; @CITE[3]@; @CITE[1]@; ; @CITE[7]@). Formality style transfer is also widely studied (@CITE[4]@; ; @CITE[5]@; @CITE[2]@; @CITE[6]@). In addition, some studies explore rewriting in the context of dialogue systems (@CITE[9]@; ; @CITE[10]@; @CITE[10]@; @CITE[10]@). The main difference between our work and previous studies is that we focus on rewriting a dialogue system’s response to make it more personalized to the user.\\nReferences:ID: 1. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 2. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 3. Title: Style Transfer in Text: Exploration and Evaluation\\nID: 4. Title: Deep Learning for Text Style Transfer: A Survey\\nID: 5. Title: Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization\\nID: 6. Title: Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase\\nID: 7. Title: Text Detoxification using Large Pre-trained Neural Models\\nID: 8. Title: Toward Controlled Generation of Text\\nID: 9. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 10. Title: Contextual Text Style Transfer',\n",
       "  'ground_truth': '\\n\\nDespite being introduced over ten years ago (, current methods for stylistic rewriting (e.g. @CITE[5]@; ; ; @CITE[4]@; @CITE[6]@; @CITE[1]@; @CITE[3]@; @CITE[8]@; Logacheva et al., 2022, etc.) still rely solely on parallel source-to-target sentence pairs, primarily due to a lack of datasets that include contextual information. While new models have emerged that do not require parallel data for training (@CITE[9]@; @CITE[10]@; @CITE[7]@; Hallinan et al., 2023), they also operate without contextual information. Building on some preliminary research that explored context in small custom-trained seq2seq rewriting models (@CITE[2]@; Atwell et al., 2022) and large language models for exemplar-based conversation-level rewriting (Roy et al., 2023), we extend the investigation to large language models with defined style attributes like formality, toxicity, and sentiment. Importantly, we also explore the need for context in evaluations in addition to modeling, and propose a new suite of contextualized metrics for automatic evaluation.\\nReferences:ID: 1. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 2. Title: Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach\\nID: 3. Title: Style Transfer in Text: Exploration and Evaluation\\nID: 4. Title: Deep Learning for Text Style Transfer: A Survey\\nID: 5. Title: Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization\\nID: 6. Title: Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase\\nID: 7. Title: Text Detoxification using Large Pre-trained Neural Models\\nID: 8. Title: Toward Controlled Generation of Text\\nID: 9. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 10. Title: Contextual Text Style Transfer<|eot_id|>'},\n",
       " 2266: {'predict': 'The goal of open-domain question answering (QA) is to answer questions that can be found in a large collection of documents, such as Wikipedia. Early work in open-domain QA focused on building a QA system based on Wikipedia articles (@CITE[5]@; @CITE[2]@; @CITE[4]@; @CITE[2]@. Recently, many efforts have been made to leverage pre-trained language models (PLMs) for open-domain QA. Some works fine-tune PLMs to directly answer questions (@CITE[2]@; @CITE[4]@, while others fine-tune PLMs to retrieve relevant passages and then answer questions (@CITE[5]@; @CITE[3]@; @CITE[1]@;. In this work, we focus on the latter approach and propose a novel retrieval model that can be fine-tuned with a small amount of data.\\nReferences:ID: 1. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Dense Passage Retrieval for Open-Domain Question Answering',\n",
       "  'ground_truth': '\\n\\nOpen-domain QA () is practically important, which requires a system to retrieve relevant documents to answer a given question. The task is recently gaining much attention, thanks to the development of large-scale datasets like HotpotQA, SQuAD Open (@CITE[2]@), Natural Questions Open (@CITE[3]@; @CITE[5]@), etc. Pre-trained transformer models like BERT (@CITE[5]@) have accelerated the development of neural text retrievers (@CITE[5]@; @CITE[4]@; ; @CITE[1]@; @CITE[1]@) in the retriever-reader framework (@CITE[2]@). We have investigated the effectiveness of our method in the multi-hop open-domain QA task (see Appendix B) using an existing external retriever component.\\nReferences:ID: 1. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Dense Passage Retrieval for Open-Domain Question Answering<|eot_id|>'},\n",
       " 7493: {'predict': 'Semantic parsing is a technique to generate structured semantic representation from natural language. Early work on semantic parsing focused on mapping natural language to logical forms (@CITE[1]@; ; ; @CITE[5]@; @CITE[4]@; @CITE[3]@; @CITE[6]@; @CITE[2]@. Recent work has explored semantic parsing in task-oriented dialog, where the goal is to map user utterances to structured semantic representations that can be used to perform downstream tasks such as slot filling and action selection (@CITE[7]@; @CITE[8]@; @CITE[9]@; @CITE[3]@; @CITE[3]@; @CITE[3]@; @CITE[3]@; @CITE[9]@; @CITE[8]@. Our work is also related to recent work on causal reasoning (@CITE[9]@; @CITE[3]@; @CITE[7]@; @CITE[1]@; @CITE[4]@. These works focus on generating causal sentences and reasoning about causal relationships, while our work focuses on generating causal graphs from text.\\nReferences:ID: 1. Title: Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars\\nID: 2. Title: Coarse-to-Fine Decoding for Neural Semantic Parsing\\nID: 3. Title: Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs\\nID: 4. Title: Semantic Parsing for Task Oriented Dialog using Hierarchical Representations\\nID: 5. Title: Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder\\nID: 6. Title: A survey of joint intent detection and slot-filling models in natural language understanding\\nID: 7. Title: Learning Executable Semantic Parsers for Natural Language Understanding\\nID: 8. Title: Guided Generation of Cause and Effect\\nID: 9. Title: Task-Oriented Dialogue as Dataflow Synthesis',\n",
       "  'ground_truth': '\\n\\nGenerating semantic representation from natural language is a beast with many heads (@CITE[1]@; ; @CITE[4]@;. In our work, we look at a subclass of this problem where the in- put is a task-oriented utterance and the output takes the form of API call representations. Our input format aligns us closely to work in task-oriented semantic parsing (@CITE[5]@, task-oriented dialog (@CITE[6]@;, and intent classification and slot filling (Agha- janyan et al., 2020; @CITE[3]@, while our output representation invites challenges similar to ones seen in generating executable semantic parses (@CITE[8]@;, database queries (; ; Zhang et al., 2019; @CITE[9]@; @CITE[7]@, and AWS API calls (@CITE[2]@. Our problem borrows the API-styled output, similar to @CITE[2]@, but does not rely on an entire conversation or dialog as the input medium. To this extent, we enhance the existing TOPv2 dataset (@CITE[9]@, leveraged in task-oriented set- tings, with API-styled output representations.\\nReferences:ID: 1. Title: Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars\\nID: 2. Title: Coarse-to-Fine Decoding for Neural Semantic Parsing\\nID: 3. Title: Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs\\nID: 4. Title: Semantic Parsing for Task Oriented Dialog using Hierarchical Representations\\nID: 5. Title: Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder\\nID: 6. Title: A survey of joint intent detection and slot-filling models in natural language understanding\\nID: 7. Title: Learning Executable Semantic Parsers for Natural Language Understanding\\nID: 8. Title: Guided Generation of Cause and Effect\\nID: 9. Title: Task-Oriented Dialogue as Dataflow Synthesis<|eot_id|>'},\n",
       " 6311: {'predict': 'The concept of Memory-Augmented Neural Networks (MANNs) is rooted in the idea of extending the capacity of neural networks with external memory (; @CITE[1]@; ; Sukhbaatar et al., 2015; @CITE[3]@; @CITE[8]@; ; @CITE[4]@; ; @CITE[9]@; @CITE[6]@; @CITE[7]@. Neural Turing Machines (NTMs) ( and Neural Differentiable Computers (NDCs) (Graves et al., 2014; 2016) are the first examples of MANNs. These models are designed to perform external memory operations such as reading, writing and erasing in a differentiable way. NTMs use a fixed-size external memory and a read/write head that can access any location in the memory. NDCs, instead, use a stack-based memory architecture where new items are pushed and popped from the top of the stack. The Memory Network (MN) (@CITE[1]@) is a memory-augmented neural network with a fixed-size memory and a memory controller that can perform read and write operations. The controller is a neural network that is trained to predict the memory location to read from and the value to write to the memory. The controller can be trained to perform different tasks, such as question answering, by changing the loss function. The Memory-Augmented Dialogue Management (MAD) (@CITE[3]@) is a memory-augmented neural network for dialogue management. The model uses two different memories: an external memory and a slot-value memory. The external memory is used to store the entire dialogue session, while the slot-value memory is used to store the semantic slots and their values. The MAD model is trained with a sequence-to-sequence loss function. The Memory-Augmented Language Model (MALM) ( is a memory-augmented neural language model that uses a fixed-size external memory to store the context. The model uses a memory controller that is trained to predict the memory location to read from and the value to write to the memory. The controller is trained with a language modeling loss function. The Memory-Augmented Neural Network (MANN) (@CITE[2]@) is a memory-augmented neural network that uses a fixed-size external memory to store the context. The model uses a memory controller that is trained to predict the memory location to read from and the value to write to the memory. The controller is trained with a sequence-to-sequence loss function. The Memory-Augmented Neural Network (MANN) (@CITE[5]@) is a memory-augmented neural network that uses a fixed-size external memory to store the context. The model uses a memory controller that is trained to predict the memory location to read from and the value to write to the memory. The controller is trained with a language modeling loss function.\\nReferences:ID: 1. Title: Neural Turing Machines\\nID: 2. Title: Relational recurrent neural networks\\nID: 3. Title: Memory-augmented Dialogue Management for Task-oriented Dialogue Systems\\nID: 4. Title: Learning to Remember Rare Events\\nID: 5. Title: The Referential Reader: A Recurrent Entity Network for Anaphora Resolution\\nID: 6. Title: Tracking the World State with Recurrent Entity Networks\\nID: 7. Title: Tracking the World State with Recurrent Entity Networks\\nID: 8. Title: MEMO: A Deep Network for Flexible Combination of Episodic Memories\\nID: 9. Title: Relational recurrent neural networks',\n",
       "  'ground_truth': '\\n\\nMANNs introduce models with external memory to store and access the past contents by differentiable operators. These models attempt to mimic human memory processes, bringing them closer to biological plausibility. NTM () and DNC (@CITE[6]@) are well-known memorization and reasoning models which use memory instead of original input to generate inferences. In this line of research, several papers have proposed new approaches for different NLP tasks, such as language modeling (@CITE[7]@), dialogue (@CITE[9]@), machine translation (@CITE[5]@), coreference resolution (@CITE[1]@), question answering (@CITE[3]@), and visual question answering (Xiong et al., 2016). These approaches propose new ways of storing and retrieving information (@CITE[3]@; @CITE[4]@), relational or associative mechanisms (@CITE[7]@; @CITE[8]@; @CITE[4]@), and methods to improve memory representation and prevent forgetting (@CITE[2]@; Zhang et al., 2021).\\nReferences:ID: 1. Title: Neural Turing Machines\\nID: 2. Title: Relational recurrent neural networks\\nID: 3. Title: Memory-augmented Dialogue Management for Task-oriented Dialogue Systems\\nID: 4. Title: Learning to Remember Rare Events\\nID: 5. Title: The Referential Reader: A Recurrent Entity Network for Anaphora Resolution\\nID: 6. Title: Tracking the World State with Recurrent Entity Networks\\nID: 7. Title: Tracking the World State with Recurrent Entity Networks\\nID: 8. Title: MEMO: A Deep Network for Flexible Combination of Episodic Memories\\nID: 9. Title: Relational recurrent neural networks<|eot_id|>'},\n",
       " 1271: {'predict': 'MSA is a challenging task due to the complexity of human multimodal communication. Recent approaches to MSA have been primarily based on deep learning, and can be broadly categorized into two groups. The first group focuses on multimodal feature fusion, which aims to integrate the extracted features from different modalities into a unified representation, and then apply the fused representation to the classification task. For example, TPN (@CITE[3]@) and SAL (@CITE[4]@) are two representative works in this line. The second group focuses on learning multimodal embeddings, which aims to learn a shared embedding space for different modalities, and then perform the classification task in this shared space. For example, ICCN (@CITE[2]@) and CAN (@CITE[1]@) are two representative works in this line. The main difference between the two groups is that the first group only considers the feature-level fusion, while the second group also considers the correlation between different modalities. In this paper, we propose a novel approach to MSA, which combines the advantages of the two groups and achieves better performance.\\nReferences:ID: 1. Title: Tensor Fusion Network for Multimodal Sentiment Analysis\\nID: 2. Title: Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis\\nID: 3. Title: CASCADE: Contextual Sarcasm Detection in Online Discussion Forums\\nID: 4. Title: Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis',\n",
       "  'ground_truth': '\\n\\nMSA is an NLP task that collects and tackles data from multiple resources such as acoustic, visual, and textual information to comprehend varied human emotions (. Early fusion models adopted simple network architectures, such as RNN based models (Wöllmer et al., 2013; @CITE[3]@) that capture temporal dependencies from low-level multimodal inputs, SAL-CNN (@CITE[2]@) which designed a selective additive learning procedure to improve the generalizability of trained neural networks, etc. Meanwhile, there were many trials to combine geometric measures as accessory learning goals into deep learning frameworks. For instance, ; @CITE[1]@ optimized the deep canonical correlation between modality representations for fusion and then passed the fusion result to downstream tasks. More recently, formulations influenced by novel machine learning topics have emerged constantly: @CITE[4]@ presented a deep multi-task learning framework to jointly learn sentiment polarity and emotional intensity in a multimodal background. Pham et al. (2019) proposed a method that cyclically translates between modalities to learn robust joint representations for sentiment analysis. Tsai et al. (2020) proposed a routing procedure that dynamically adjusts weights among modalities to provide interpretability for multimodal fusion. Motivated by advances in the field of domain separation, projected modality features into private and common feature spaces to capture exclusive and shared characteristics across different modalities. Yu et al. (2021) designed a multi-label training scheme that generates extra unimodal labels for each modality and concurrently trained with the main task. In this work, we build up a hierarchical MI-maximization guided model to improve the fusion outcome as well as the performance in the downstream MSA task, where MI maximization is realized not only between unimodal representations but also between fusion embeddings and unimodal representations.\\nReferences:ID: 1. Title: Tensor Fusion Network for Multimodal Sentiment Analysis\\nID: 2. Title: Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis\\nID: 3. Title: CASCADE: Contextual Sarcasm Detection in Online Discussion Forums\\nID: 4. Title: Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis<|eot_id|>'},\n",
       " 2746: {'predict': 'Conversational Question Answering (CQA) is a task that requires a model to answer questions within the context of a multi-turn dialogue. CQA has been explored in different settings, such as reading comprehension (@CITE[3]@, search (@CITE[4]@, and open-domain dialogue (@CITE[5]@. For example, CoQA (@CITE[1]@ is a popular CQA benchmark dataset that is built on top of Wikipedia articles. CQA models can be divided into two categories: (1) models that use a single network to encode the conversation history and the current question, such as BERT with History Answer Embedding (HAE) (@CITE[2]@, SDNet (@CITE[5]@, and FlowQA (@CITE[2]@; and (2) models that use a network to encode the conversation history and another network to encode the current question, such as CANDOR (@CITE[1]@, ExCorD (@CITE[5]@, and CQA-Net (@CITE[2]@. In this paper, we propose a novel framework that can be used in both categories of CQA models.\\nReferences:ID: 1. Title: QuAC : Question Answering in Context\\nID: 2. Title: BERT with History Answer Embedding for Conversational Question Answering\\nID: 3. Title: SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering\\nID: 4. Title: FlowQA: Grasping Flow in History for Conversational Machine Comprehension\\nID: 5. Title: Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering',\n",
       "  'ground_truth': '\\n\\nWith the advent of recent large-scale CQA datasets (@CITE[5]@; Reddy et al., 2019), numerous studies proposed methods to resolve the challenging task. Most works focused on developing model structures (Zhu et al., 2018; @CITE[4]@) that are specialized in the CQA task. Several works demonstrated the effectiveness of the flow mechanism also use CoQA to achieve the scores. CQA (@CITE[3]@; @CITE[1]@; @CITE[2]@). Most recently, leveraging self-contained questions () or encoding longer context (@CITE[3]@) have been shown to be effective in the task.\\nReferences:ID: 1. Title: QuAC : Question Answering in Context\\nID: 2. Title: BERT with History Answer Embedding for Conversational Question Answering\\nID: 3. Title: SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering\\nID: 4. Title: FlowQA: Grasping Flow in History for Conversational Machine Comprehension\\nID: 5. Title: Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering<|eot_id|>'},\n",
       " 520: {'predict': 'The task of REC is to locate the target object described by a referring expression. The task can be divided into two stages: language understanding and object localization. Language understanding is to recognize the target object from the referring expression, and object localization is to locate the target object from the image. Previous works have made great progress in the two stages. For example, @CITE[1]@ proposed a method that can generate unambiguous referring expressions and comprehend such expressions to infer which object is being described. @CITE[2]@ proposed a method that can localize the target object described by a referring expression. However, these methods are not suitable for the REVERIE task, because they require the referring expression to be unambiguous, and the referring expression is generated by a human. For the REVERIE task, the referring expression is generated by a robot, and the referring expression may be ambiguous. @CITE[4]@ proposed a method that can perform a wide variety of complex tasks in response to multifarious instructions from humans, but it is not suitable for the REVERIE task, because it requires a human to provide instructions. @CITE[3]@ proposed a method that can perform a wide variety of complex tasks in response to multifarious instructions from robots, but it is not suitable for the REVERIE task, because it requires a human to provide instructions. proposed a method that can perform the REVERIE task, but it is not suitable for the REVERIE task, because it requires the referring expression to be unambiguous. proposed a method that can perform the REVERIE task, but it is not suitable for the REVERIE task, because it requires the referring expression to be unambiguous. proposed a method that can perform the REVERIE task, but it is not suitable for the REVERIE task, because it requires the referring expression to be unambiguous. proposed a method that can perform the REVERIE task, but it is not suitable for the REVERIE task, because it requires the referring expression to be unambiguous.\\nReferences:ID: 1. Title: Generation and Comprehension of Unambiguous Object Descriptions\\nID: 2. Title: Natural Language Object Retrieval\\nID: 3. Title: Building Generalizable Agents with a Realistic and Rich 3D Environment\\nID: 4. Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments\\nID: 5. Title: REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments',\n",
       "  'ground_truth': '\\n\\nThe abil- ity to make connections between objects or spatial regions and the natural language expressions that describe those objects or regions, has been a focus of many studies. Given that humans regularly carry out complex symbolic-spatial reasoning, there has been much effort to improve the capability of refer- ring expression comprehension (including remote objects) in agents (@CITE[2]@; @CITE[4]@; @CITE[1]@; ; @CITE[3]@; @CITE[5]@, but such reasoning re- mains challenging for current models. Our ARRA- MON task integrates substantial usage of referring expression comprehension as a requirement, as it is necessary to the successful completion of both the navigation and assembly phases.\\nReferences:ID: 1. Title: Generation and Comprehension of Unambiguous Object Descriptions\\nID: 2. Title: Natural Language Object Retrieval\\nID: 3. Title: Building Generalizable Agents with a Realistic and Rich 3D Environment\\nID: 4. Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments\\nID: 5. Title: REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments<|eot_id|>'},\n",
       " 323: {'predict': 'Probing models’ learning from datasets is an important research direction in NLP. @CITE[5]@ first proposed to use a controlled evaluation set to analyze models’ heuristics in natural language inference. Then, @CITE[2]@ proposed a similar evaluation set for probing models’ learning from datasets in question answering. More recently, @CITE[7]@ proposed a methodology for automatically building datasets from various types of expert knowledge and introduced a challenge task to probe whether state-of-the-art QA models have general knowledge about word definitions and general taxonomic reasoning. @CITE[4]@ introduced a model-agnostic benchmark for evaluating the robustness of MRC models under four different types of adversarial attacks. @CITE[6]@ proposed systematic methods to analyze the behavior of VQA models and showed that despite recent progress, today’s VQA models are “myopic” and “stubborn”. @CITE[1]@ proposed an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD) and showed that the accuracy of sixteen published models drops from an average of 75% F1 score to 36%. @CITE[3]@ proposed universal adversarial triggers for attacking and analyzing NLP and showed that the triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%. In this paper, we use a similar methodology to analyze models’ learning from datasets in question answering. However, the main difference is that we focus on the question-answering models’ learning from datasets in the question-answering task. Besides, we use a more general way to generate adversarial examples, i.e., using the gradient-guided search over tokens. Finally, we propose a new evaluation set to probe models’ learning from datasets in question answering and show that the accuracy of the state-of-the-art models drops from 83.6% to 10.6%.\\nReferences:ID: 1. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 2. Title: Shortcut Learning in Deep Neural Networks\\nID: 3. Title: What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge\\nID: 4. Title: Benchmarking Robustness of Machine Reading Comprehension Models\\nID: 5. Title: Analyzing the Behavior of Visual Question Answering Models\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments\\nID: 7. Title: Adversarial Examples for Evaluating Reading Comprehension Systems\\nID: 8. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP',\n",
       "  'ground_truth': '\\n\\nThere is also growing interest in probing what models learn from datasets (@CITE[5]@; ; ;. Previous work in question answering has found that state-of-the-art models can get good performance on incomplete input (@CITE[8]@; @CITE[3]@;, under-rely on important words, (@CITE[4]@, and over-rely on sim- ple heuristics (Weissenborn et al., 2017;. Experiments on SQuAD in particular have shown that SQuAD models are vulnerable to adver- sarial attacks (@CITE[6]@; @CITE[7]@ and not robust to paraphrases (@CITE[1]@; @CITE[2]@. Our work continues exploring what models learn by comprehensively testing multiple QA datasets against a variety of simple but informative probes. We take inspiration from previous studies, and we make novel contributions by using BERT, a state- of-the-art model, and running several experiments against ﬁve different QA datasets to investigate the progress made in reading comprehension.\\nReferences:ID: 1. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 2. Title: Shortcut Learning in Deep Neural Networks\\nID: 3. Title: What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge\\nID: 4. Title: Benchmarking Robustness of Machine Reading Comprehension Models\\nID: 5. Title: Analyzing the Behavior of Visual Question Answering Models\\nID: 6. Title: Probing Neural Network Comprehension of Natural Language Arguments\\nID: 7. Title: Adversarial Examples for Evaluating Reading Comprehension Systems\\nID: 8. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP<|eot_id|>'},\n",
       " 1304: {'predict': 'Hallucination is a common issue in natural language generation. @CITE[4]@ propose a metric to evaluate the faithfulness of table-to-text models. @CITE[2]@ show that hallucination is a common issue in machine translation. @CITE[6]@ propose a metric to evaluate the faithfulness of image captioning models. @CITE[1]@ propose a metric to evaluate the faithfulness of abstractive summarization models. @CITE[3]@ propose a question answering evaluation framework to assess the faithfulness of abstractive summarization models. @CITE[5]@ propose a metric to evaluate the faithfulness of neural data-to-text models. While these works focus on hallucination in specific tasks, we investigate hallucination in general NLP tasks. In addition, while these works focus on evaluating hallucination, we also propose a novel approach to detect hallucination in a text. Our work is also related to the work of @CITE[3]@, who propose a question answering evaluation framework to assess the faithfulness of abstractive summarization models. However, our work is more general and can be applied to a wide range of NLP tasks.\\nReferences:ID: 1. Title: Handling Divergent Reference Texts when Evaluating Table-to-Text Generation\\nID: 2. Title: Six Challenges for Neural Machine Translation\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Object Hallucination in Image Captioning\\nID: 5. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 6. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization',\n",
       "  'ground_truth': '\\n\\nThe injection of false information is a well-known phenomena in data-to-text generation (@CITE[3]@; ; @CITE[4]@, machine translation (;, image captioning (@CITE[1]@, machine summarization (@CITE[2]@; and question answering (@CITE[6]@. In the context of dialogue systems, Dušek et al. (2018, 2020) demonstrate that state-of-the-art natural language generation (NLG) models can hallucinate by missing important entities. Few NLG models have been proposed to cope with the issue, but are often custom-made for task-oriented dialogue (Balakrishnan et al., 2019). Recently, little progress has been made for studying hallucination in open-domain dialog systems. @CITE[5]@ study hallucination in knowledge-grounded dialogue systems and introduce a the BEGIN benchmark for measuring groundedness in dialogue systems. Finally, @CITE[5]@ propose a dialogue system that is more faithful to the source knowledge by adding control tokens at training time that guide the model towards generating more objective sentences which have higher overlap with the source.\\nReferences:ID: 1. Title: Handling Divergent Reference Texts when Evaluating Table-to-Text Generation\\nID: 2. Title: Six Challenges for Neural Machine Translation\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: Object Hallucination in Image Captioning\\nID: 5. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 6. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization<|eot_id|>'},\n",
       " 1722: {'predict': 'In document generation, there are two main directions: abstractive and extractive. Abstractive generation is the task of generating a summary or a text from scratch, while extractive generation is the task of generating a text by combining the words from the input text. Recently, abstractive generation has attracted much attention and has achieved significant progress (@CITE[5]@; @CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@. However, abstractive generation is still a challenging task, especially when generating a long document. Extractive generation is also a common task in document generation, such as text summarization, data-to-text generation, etc. It is easier to generate a long document with extractive generation, since we only need to extract words from the input text to form the output. However, it is difficult to generate a high-quality text with extractive generation, since the output is limited by the input. In this paper, we propose a new method for document generation, which combines the advantages of abstractive and extractive generation. The proposed method first generates a text with abstractive generation, and then uses the generated text to guide the extractive generation. The proposed method can generate a high-quality document with a long length, which is difficult to achieve with traditional extractive or abstractive generation.\\nReferences:ID: 1. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 2. Title: Towards Coherent and Cohesive Long-form Text Generation\\nID: 3. Title: Gmail Smart Compose: Real-Time Assisted Writing\\nID: 4. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 5. Title: Text Generation with Exemplar-based Adaptive Decoding',\n",
       "  'ground_truth': '\\n\\nRecent work leverages the success of large pre-trained language models to generate long texts such as stories (@CITE[5]@, reviews (@CITE[5]@, and fake news (Zellers et al., 2019). Most end-user applications for assisting user writing, however, are confined to sentence-level generation (@CITE[3]@; @CITE[4]@; Alikaniotis and Raheja, 2019; @CITE[5]@; Faltings et al., 2021). Our work focuses on document-level writing assistance in which a document sketch is constructed from a set of similar documents. Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. We release our data and source code for dataset construction and experiments at https://github.com/ellenmellon/document_sketching.\\nReferences:ID: 1. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 2. Title: Towards Coherent and Cohesive Long-form Text Generation\\nID: 3. Title: Gmail Smart Compose: Real-Time Assisted Writing\\nID: 4. Title: Smart Reply: Automated Response Suggestion for Email\\nID: 5. Title: Text Generation with Exemplar-based Adaptive Decoding<|eot_id|>'},\n",
       " 4943: {'predict': 'Diffusion probabilistic models (DPMs) are a class of probabilistic models that are inspired by non-equilibrium thermodynamics and the forward-backward process in diffusion. @CITE[1]@ first introduced the DPMs as a method for learning generative models for image data. @CITE[6]@ further showed that DPMs can be used for text generation. DPMs are closely related to variational autoencoders (VAEs) (@CITE[2]@), which are generative models that learn a probabilistic mapping from data to a latent space. DPMs can be viewed as a VAE with a probabilistic forward process and a deterministic backward process. @CITE[6]@ showed that DPMs can be used for text generation. @CITE[3]@ introduced the multinomial DPMs for discrete data, which can be viewed as a VAE with a multinomial forward process and a deterministic backward process. @CITE[4]@ introduced the structured DPMs, which can be viewed as a VAE with a structured forward process and a deterministic backward process. @CITE[5]@ introduced the discrete-time DPMs, which can be viewed as a VAE with a discrete-time forward process and a deterministic backward process. @CITE[4]@ introduced the D3PMs, which can be viewed as a VAE with a structured forward process and a deterministic backward process. @CITE[3]@ introduced the structured DPMs, which can be viewed as a VAE with a structured forward process and a deterministic backward process. @CITE[4]@ introduced the structured DPMs, which can be viewed as a VAE with a structured forward process and a deterministic backward process. @CITE[5]@ introduced the discrete-time DPMs, which can be viewed as a VAE with a discrete-time forward process and a deterministic backward process.\\nReferences:ID: 1. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\\nID: 2. Title: Denoising Diffusion Probabilistic Models\\nID: 3. Title: Structured Denoising Diffusion Models in Discrete State-Spaces\\nID: 4. Title: Denoising Diffusion Probabilistic Models\\nID: 5. Title: Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions\\nID: 6. Title: Structured Denoising Diffusion Models in Discrete State-Spaces',\n",
       "  'ground_truth': '\\n\\nDiffusion generative models (@CITE[6]@; ; Song et al., 2020) have achieved remarkable progress in generating high-quality images (Rombach et al., 2022), audios (@CITE[3]@; Shen et al., 2023), and texts (@CITE[1]@; Hoogeboom et al., 2021). DDPM ( defines the generative process as the reverse of a particular Markovian diffusion process. By gradually corrupting an image with Gaussian noise and training a model based on U-Net to denoise it, DDPM can generate high-quality images. Multinomial diffusion (@CITE[2]@ proposes a uniform corruption process for categorical variables, which is extended by D3PMs (@CITE[1]@ to support general transition matrices, including an absorbing variant that draws close connections to masked LMs. Recent works (@CITE[4]@; @CITE[5]@ aim at conducting the Gaussian diffusion process over token embeddings so that the continuous diffusion models can be applied to discrete texts. Some works (Chen et al., 2022b) consider converting discrete tokens to bit strings and model them as real values. However, these approaches neglect the semantic relationships between tokens because the vocabulary is constructed using BPE, limiting the model performance. In this work, we introduce the diffusion generative models into the S2ST task. We conduct a thorough investigation of the unique properties of the speech modality and propose a novel diffusion paradigm decoupling the diffusion forward and backward process to bridge the discrete and continuous spaces of speech, while preserving the semantic structure of speech representations.\\nReferences:ID: 1. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\\nID: 2. Title: Denoising Diffusion Probabilistic Models\\nID: 3. Title: Structured Denoising Diffusion Models in Discrete State-Spaces\\nID: 4. Title: Denoising Diffusion Probabilistic Models\\nID: 5. Title: Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions\\nID: 6. Title: Structured Denoising Diffusion Models in Discrete State-Spaces<|eot_id|>'},\n",
       " 3568: {'predict': 'Semi-supervised learning (SSL) has been widely used to improve the performance of deep neural networks with limited labeled data. The core idea of SSL is to utilize the large-scale unlabeled data to enhance the performance of the model on the small-scale labeled data. There are two main types of SSL methods: self-supervised learning (@CITE[6]@; @CITE[4]@) and consistency training (@CITE[7]@; @CITE[1]@). Self-supervised learning aims to learn the representation of unlabeled data by designing a pretext task. Consistency training aims to make the model predictions consistent across different views of the same input. Recently, there are many SSL methods proposed in the NLP domain. For example, @CITE[2]@ proposed a tree-structured semi-supervised contrastive pre-training method for dialog understanding. @CITE[3]@ proposed a contrastive learning framework to learn the dialogue representation from the unlabeled dialog corpus. @CITE[5]@ proposed a multi-view SSL framework to learn the representations of the unlabeled data from multiple views. @CITE[1]@ proposed a self-supervised learning framework to learn the representations of the unlabeled data by using the adapter network. @CITE[1]@ proposed a R-Drop method to learn the representations of the unlabeled data by minimizing the bidirectional KL-divergence between the output distributions of two sub-models sampled by dropout.\\nReferences:ID: 1. Title: SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding\\nID: 2. Title: AdapterHub: A Framework for Adapting Transformers\\nID: 3. Title: Big Self-Supervised Models are Strong Semi-Supervised Learners\\nID: 4. Title: Semi-Supervised Learning with Ladder Networks\\nID: 5. Title: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\\nID: 6. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 7. Title: R-Drop: Regularized Dropout for Neural Networks',\n",
       "  'ground_truth': '\\n\\nSemi-Supervised Learning aims to learn from both labeled and unlabeled data (@CITE[6]@; ;. As an efficient approach for semi-supervised learning, pseudo labeling (@CITE[1]@; tries to utilize unlabeled data by predicting their labels. Some studies (@CITE[1]@; @CITE[2]@ train multiple learners and exploit disagreements among different learners. Other studies utilize self-training to generate pseudo labels for unlabeled data (@CITE[7]@; @CITE[4]@;. Consistency regularization (; ; @CITE[5]@; ; @CITE[3]@ is another popular scheme. It regularizes the model to be invariant to small perturbations on the input, hidden states, or model parameters. Mean Teacher ( is an efficient method to implement consistency regularization, where a student model and a teacher model are maintained to enforce the predictions’ consistency. This paper combines pseudo labeling with a teacher-student framework to handle semi-supervised learning in the lifelong scenarios.\\nReferences:ID: 1. Title: SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding\\nID: 2. Title: AdapterHub: A Framework for Adapting Transformers\\nID: 3. Title: Big Self-Supervised Models are Strong Semi-Supervised Learners\\nID: 4. Title: Semi-Supervised Learning with Ladder Networks\\nID: 5. Title: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\\nID: 6. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 7. Title: R-Drop: Regularized Dropout for Neural Networks<|eot_id|>'},\n",
       " 6850: {'predict': 'Prompt engineering has been widely used to enhance the performance of PLMs on various tasks. The most common method is to concatenate a prompt with the input to guide the PLM to perform the desired task (@CITE[11]@; @CITE[2]@; @CITE[3]@; @CITE[5]@; @CITE[9]@; @CITE[6]@; @CITE[10]@. This method is effective in many scenarios, but it requires a large number of labeled examples to generate a high-quality prompt. To overcome the limitation, some methods use the pre-trained prompt (@CITE[11]@; @CITE[4]@, 2021; @CITE[1]@; @CITE[7]@; @CITE[8]@; @CITE[8]@; @CITE[8]@; @CITE[8]@. These methods are effective in many scenarios, but they require the PLM to learn the task-specific knowledge from the pre-trained prompt.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: WARP: Word-level Adversarial ReProgramming\\nID: 5. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 6. Title: Open Aspect Target Sentiment Classification with Natural Language Prompts\\nID: 7. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 8. Title: Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing\\nID: 9. Title: A Unified Generative Framework for Aspect-Based Sentiment Analysis\\nID: 10. Title: KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction\\nID: 11. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System',\n",
       "  'ground_truth': '\\n\\nFueled by the birth of large-scale language models (@CITE[10]@), prompt-based learning (@CITE[5]@) for the Pre-trained Language Models has been developed to convert different downstream tasks into cloze-style mask language model objectives, achieving impressive performance in text classification (@CITE[7]@; @CITE[8]@; ; @CITE[1]@; Schick et al., 2020; Schick and Schütze, 2021), sentiment classification (@CITE[9]@; @CITE[7]@; @CITE[6]@; @CITE[2]@), named entity recognition (Cui et al., 2021), relation extraction (@CITE[11]@), question answering (; @CITE[4]@), commonsense reasoning (@CITE[3]@), etc. Orthogonal to these studies of prompt learning, our paper focuses on the comparison of different retrieval methods, where prompt learning is just employed as a backbone.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: WARP: Word-level Adversarial ReProgramming\\nID: 5. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 6. Title: Open Aspect Target Sentiment Classification with Natural Language Prompts\\nID: 7. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 8. Title: Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing\\nID: 9. Title: A Unified Generative Framework for Aspect-Based Sentiment Analysis\\nID: 10. Title: KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction\\nID: 11. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System<|eot_id|>'},\n",
       " 3530: {'predict': 'Controllable text generation has been a long-standing challenge in NLP, with recent works focusing on controlling the style, sentiment, topic, or toxicity of generated text. @CITE[6]@ propose a conditional transformer language model for controllable generation, while @CITE[4]@ introduce Plug and Play Language Models that combine a pretrained LM with one or more simple attribute classifiers. @CITE[5]@ propose GeDi, which uses a smaller LM as a generative discriminator to guide generation from a large LM. @CITE[1]@ and @CITE[2]@ propose to use a classifier to select a prefix to steer the generation. @CITE[3]@ propose a contrastive prefix method that utilizes a set of small attribute-specific vectors to steer the generation. Our approach is related to the prefix-based methods, but we use a more complex prefix network to generate the prefix. Moreover, our method can control multiple attributes simultaneously, while existing prefix-based methods only control one attribute.\\nReferences:ID: 1. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 4. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 5. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts\\nID: 6. Title: Controllable Natural Language Generation with Contrastive Prefixes',\n",
       "  'ground_truth': '\\n\\nGenerations classified as offensive can be simply discarded. However, this post-filtering strategy using classifiers is inefficient, and there may exist cases where no safe choices exist within a fixed number of generations (@CITE[5]@). In order to circumvent this limitation, recent research has focused on controlling the generation of the Transformer-based models from the source. @CITE[1]@ proposed a novel pretrained model, CTRL. CTRL achieves controllability at the expense of training a large conditional LM with 1.6 billion parameters from scratch, which is costly. Therefore, later research proposes controlling methods that do not require updating the parameters. @CITE[4]@ froze the parameters of the GPT2 but stacked an additional attribute model on top of it. It guides generation by iteratively updating the LM’s hidden representations using the gradients back-propagated from the attribute model. Instead of using updated hidden representations to guide generation, @CITE[2]@ used two conditional LMs to directly re-weight the next token probability given by the LM during generation. @CITE[6]@ also kept LM parameters frozen but optimized a small continuous task-specific vector (called a prefix) to achieve competitive results with fine-tuning on downstream NLG tasks, and @CITE[3]@ further improved the prefix-tuning method with contrastive training objectives to achieve better attribute alignment. All the aforementioned work assumes that the desired attributes are pre-selected before generation. However, in our dialogue detoxification task, the desired stance attribute depends on a hidden attribute of the input context, which leads to an additional challenge.\\nReferences:ID: 1. Title: Universal Adversarial Triggers for Attacking and Analyzing NLP\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 4. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 5. Title: Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts\\nID: 6. Title: Controllable Natural Language Generation with Contrastive Prefixes<|eot_id|>'},\n",
       " 4550: {'predict': 'Gradient-based methods have been widely used to visualize the feature importance of the input data, and many methods have been proposed to generate saliency maps based on gradients (@CITE[6]@; @CITE[4]@; @CITE[2]@; @CITE[1]@; @CITE[3]@; @CITE[5]@; @CITE[7]@; @CITE[8]@; @CITE[9]@; @CITE[1]@; @CITE[1]@; @CITE[10]@. However, @CITE[1]@ find that gradient-based methods can be easily manipulated. In this paper, we focus on the problem of data poisoning and use gradient-based methods to analyze the model behavior. In particular, we show that gradient-based methods can be used to detect the existence of backdoor patterns in the data. Our approach is inspired by the work of @CITE[9]@, who use gradient-based methods to analyze the model behavior and find that the gradients of a model are easily manipulable. They show that the gradients of a model can be easily modified by merging the layers of a target model with a Facade that overwhelms the gradients without affecting the predictions. In this paper, we use this idea to design a gradient-based method to detect the existence of backdoor patterns in the data.\\nReferences:ID: 1. Title: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\\nID: 2. Title: Investigating the influence of noise and distractors on the interpretation of neural networks\\nID: 3. Title: Axiomatic Attribution for Deep Networks\\nID: 4. Title: Visualizing and Understanding Neural Models in NLP\\nID: 5. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 6. Title: Combining Feature and Instance Attribution to Detect Artifacts\\nID: 7. Title: \"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification\\nID: 8. Title: Gradient-based Analysis of NLP Models is Manipulable\\nID: 9. Title: Sanity Checks for Saliency Maps',\n",
       "  'ground_truth': '\\n\\nGradient-based methods might consider gradient-based explanation methods as a workaround to the three issues stated above. Methods such as vanilla gradients (@CITE[7]@), GradientXInput (@CITE[8]@), and Integrated gradients () all rely on the gradients of the prediction score of the model w.r.t. the input embeddings. To convert the gradient vectors into scalar per-token importance, various reduction methods such as L1-norm (@CITE[5]@), L2-norm (Poerner et al., 2018), and mean (@CITE[1]@; @CITE[3]@) have been employed. Nonetheless, @CITE[4]@ evaluations showed that none of them is consistently better than the other. Furthermore, adversarial analysis and sanity checks both have raised doubts about gradient-based methods’ trustworthiness (@CITE[9]@; @CITE[2]@; @CITE[6]@).\\nReferences:ID: 1. Title: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\\nID: 2. Title: Investigating the influence of noise and distractors on the interpretation of neural networks\\nID: 3. Title: Axiomatic Attribution for Deep Networks\\nID: 4. Title: Visualizing and Understanding Neural Models in NLP\\nID: 5. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 6. Title: Combining Feature and Instance Attribution to Detect Artifacts\\nID: 7. Title: \"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification\\nID: 8. Title: Gradient-based Analysis of NLP Models is Manipulable\\nID: 9. Title: Sanity Checks for Saliency Maps<|eot_id|>'},\n",
       " 5769: {'predict': 'Negative sampling is a key component in contrastive learning. The most common way is to randomly sample negative samples from the dataset. However, random negative sampling can lead to a biased contrastive loss, where the model can easily fit the trivial solution of pulling all samples together (@CITE[4]@. To address this issue, @CITE[1]@ proposed to use hard negative samples, which are sampled from the dataset but are more dissimilar to the anchor than the positive samples. In the NLP field, @CITE[2]@ proposed to use data augmentation to generate positive samples, while @CITE[3]@ used prompts to construct negative samples. @CITE[4]@ proposed to use a debiased contrastive loss, which is a regularized version of the standard contrastive loss. In this work, we use a combination of random and hard negative sampling to construct negative samples, and use a debiased contrastive loss to train the model.\\nReferences:ID: 1. Title: SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples\\nID: 2. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 3. Title: Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding\\nID: 4. Title: Debiased Contrastive Learning',\n",
       "  'ground_truth': '\\n\\nIn-batch negative sampling is a common strategy in unsupervised contrastive learning, which may have limitations as we mentioned above. To fix these issues, @CITE[2]@ and Kalantidis et al. (2020) synthesize hard negatives by mixing positives with in-batch negatives. @CITE[3]@ utilize dependency parsing to create the negation of original sentences as soft negatives. Following @CITE[4]@ who use different prompt templates as positive, @CITE[1]@ derive negatives from the negation of the templates. The two methods create negatives with fixed templates and rules, thus may introduce bias. design a debiased contrastive objective that corrects the false negatives without true labels. Zhou et al. (2022) use a trained model to distinguish false negatives, which results in addition module comparing with our method.\\nReferences:ID: 1. Title: SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples\\nID: 2. Title: PromptBERT: Improving BERT Sentence Embeddings with Prompts\\nID: 3. Title: Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding\\nID: 4. Title: Debiased Contrastive Learning<|eot_id|>'},\n",
       " 2449: {'predict': 'A line of work has explored consistency as a model comprehension metric. This line of work has been motivated by the observation that models often achieve high scores on VQA benchmarks while failing on rephrased questions (@CITE[2]@; @CITE[3]@. @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[4]@ find that models often perform well on reasoning questions but poorly on the corresponding low-level perception sub-questions. @CITE[1]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets. @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[5]@ find that models are prone to exploiting biases in the question and answer distribution of VQA datasets.\\nReferences:ID: 1. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 2. Title: Cycle-Consistency for Robust Visual Question Answering\\nID: 3. Title: SQuINTing at VQA Models: Introspecting VQA Models with Sub-Questions\\nID: 4. Title: Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA\\nID: 5. Title: Measuring CLEVRness: Blackbox testing of Visual Reasoning Models',\n",
       "  'ground_truth': '\\n\\nRecent work has sought to evaluate models using consistency and other metrics (@CITE[4]@; @CITE[5]@; ; @CITE[2]@; @CITE[3]@; @CITE[1]@. These evaluations often evaluate consistency through question entailment and implication, or simply contrasting examples in the case of (@CITE[3]@. The concurrent work (@CITE[1]@ takes a unique approach, performing discrete visual perturbations while preserving the semantic integrity of a question; their work complements our own as it focuses exclusively on visual perturbations in a synthetic image setting. While we consider these previous methods important for evaluating model comprehension, they often combine question types and capabilities, changing the kind of expected answer, or evaluating consistency on a tree or set of entailed questions. Though ideally models would be consistent and robust for these more complex types of tests, our approach reveals that models can fail even on simple implications.\\nReferences:ID: 1. Title: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\\nID: 2. Title: Cycle-Consistency for Robust Visual Question Answering\\nID: 3. Title: SQuINTing at VQA Models: Introspecting VQA Models with Sub-Questions\\nID: 4. Title: Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA\\nID: 5. Title: Measuring CLEVRness: Blackbox testing of Visual Reasoning Models<|eot_id|>'},\n",
       " 5731: {'predict': 'The Graph Convolutional Network (GCN) (Kipf et al., 2016; @CITE[4]@; @CITE[2]@) is a popular neural network architecture for graph-structured data, which is based on the spectral convolution. The GCN model is defined as follows: given a graph G = (V, E, X), where V is the set of nodes, E is the set of edges, and X is the feature matrix of nodes. The GCN model is defined as follows: \\\\mathcal{H}^{l+1} = \\\\mathcal{A} \\\\mathcal{H}^{l} \\\\mathcal{H}^{l} = σ( \\\\widetilde{\\\\mathcal{D}} \\\\mathcal{H}^{l-1} W^{l} + b^{l} ) (1) where W^{l} is the learnable weight matrix, b^{l} is the learnable bias, σ is the activation function, and \\\\widetilde{\\\\mathcal{D}} is the adjacency matrix of the graph. The adjacency matrix is defined as follows: \\\\widetilde{\\\\mathcal{D}} = \\\\mathcal{D} + I, where I is the identity matrix, and \\\\mathcal{D} is the adjacency matrix of the graph. The GCN model is designed to capture the graph structure information. However, the GCN model cannot capture the information of the edge type. To solve this problem, the graph convolutional network with edge type (GCNET) is proposed in (@CITE[3]@). The GCNET model is defined as follows: \\\\mathcal{H}^{l+1} = \\\\mathcal{A} \\\\mathcal{H}^{l} \\\\mathcal{H}^{l} = σ( \\\\widetilde{\\\\mathcal{D}} \\\\mathcal{H}^{l-1} W^{l} + b^{l} ) (2) where W^{l} is the learnable weight matrix, b^{l} is the learnable bias, σ is the activation function, and \\\\widetilde{\\\\mathcal{D}} is the adjacency matrix of the graph. The adjacency matrix is defined as follows: \\\\widetilde{\\\\mathcal{D}} = \\\\mathcal{D} + I, where I is the identity matrix, and \\\\mathcal{D} is the adjacency matrix of the graph. The GCNET model is designed to capture the graph structure information and the information of the edge type. However, the GCNET model cannot capture the information of the entity type. To solve this problem, the heterogeneous graph convolutional network (HGCN) is proposed in (@CITE[1]@). The HGCN model is defined as follows: \\\\mathcal{H}^{l+1} = \\\\mathcal{A} \\\\mathcal{H}^{l} \\\\mathcal{H}^{l} = σ( \\\\widetilde{\\\\mathcal{D}} \\\\mathcal{H}^{l-1} W^{l} + b^{l} ) (3) where W^{l} is the learnable weight matrix, b^{l} is the learnable bias, σ is the activation function, and \\\\widetilde{\\\\mathcal{D}} is the adjacency matrix of the graph. The adjacency matrix is defined as follows: \\\\widetilde{\\\\mathcal{D}} = \\\\mathcal{D} + I, where I is the identity matrix, and \\\\mathcal{D} is the adjacency matrix of the graph. The HGCN model is designed to capture the information of the entity type. However, the HGCN model cannot capture the information of the relation type. To solve this problem, the relation-aware heterogeneous graph convolutional network (RAHGCN) is proposed in this paper. The RAHGCN model is defined as follows: \\\\mathcal{H}^{l+1} = \\\\mathcal{A} \\\\mathcal{H}^{l} \\\\mathcal{H}^{l} = σ( \\\\widetilde{\\\\mathcal{D}} \\\\mathcal{H}^{l-1} W^{l} + b^{l} ) (4) where W^{l} is the learnable weight matrix, b^{l} is the learnable bias, σ is the activation function, and \\\\widetilde{\\\\mathcal{D}} is the adjacency matrix of the graph. The adjacency matrix is defined as follows: \\\\widetilde{\\\\mathcal{D}} = \\\\mathcal{D} + I, where I is the identity matrix, and \\\\mathcal{D} is the adjacency matrix of the graph. The RAHGCN model is designed to capture the information of the entity type and the relation type.\\nReferences:ID: 1. Title: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\\nID: 2. Title: Neural Message Passing for Quantum Chemistry\\nID: 3. Title: Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\\nID: 4. Title: Knowledge Embedding Based Graph Convolutional Network',\n",
       "  'ground_truth': '\\n\\nGraph Convolutional Networks (GCNs) generalize convolution operations from traditional data (e.g., images or grids) to non-Euclidean data structures (@CITE[4]@). The fundamental idea of graph convolutional networks is to enhance node self-representation by using neighbor information. Therefore, GCNs are typically expressed as a neighborhood aggregation or message-passing scheme (). In the broad application of GCNs, GCN (@CITE[3]@) and GAT (Velickovic et al., 2017) showed the powerful ability to capture neighbor information. Despite this, they performed poorly in KG representation as they ignored relations. To emphasize the essential role of relations in entity representation, R-GCN (Schlichtkrull et al., 2018) used a matrix to represent each relation. However, massive relations in the knowledge graph make it challenging for the relation matrices to be fully learned. Thus, most follow-up works used vectors to represent relations. For example, KB-GAT (@CITE[2]@) concentrated the neighbor triples as information. CompGCN () leveraged the entity-relation composition operations from knowledge embedding methods like TransE () as a message. KE-GCN (@CITE[1]@) passed the gradient of the scoring function to the central node. Nevertheless, none of the above models takes account of the inequality of relations and entities. In contrast, our RHGN is able to make a clear distinction between relations and entities, resulting in more distinct entity representations.\\nReferences:ID: 1. Title: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\\nID: 2. Title: Neural Message Passing for Quantum Chemistry\\nID: 3. Title: Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\\nID: 4. Title: Knowledge Embedding Based Graph Convolutional Network<|eot_id|>'},\n",
       " 5376: {'predict': 'Recently, dense retrieval methods, which use dense representations to encode documents and queries, have achieved remarkable success in various retrieval tasks. They are often pre-trained with a language model and fine-tuned on a downstream task (@CITE[7]@; @CITE[5]@; @CITE[2]@; @CITE[8]@; @CITE[3]@; @CITE[6]@; @CITE[10]@; @CITE[4]@; @CITE[9]@; @CITE[1]@. For example, @CITE[7]@ propose to use BERT (@CITE[8]@ as the encoder to encode documents and queries into dense representations, and then use the inner product between the query and document representations to compute the relevance score. @CITE[5]@ propose to use ERNIE (@CITE[2]@ as the encoder to encode documents and queries into dense representations, and then use a multi-layer perceptron to compute the relevance score. @CITE[2]@ propose to use ERNIE (@CITE[2]@ as the encoder to encode documents and queries into dense representations, and then use a multi-layer perceptron to compute the relevance score. @CITE[3]@ propose to use a language model to encode documents and queries into dense representations, and then use the attention weights between the query and document representations to compute the relevance score. @CITE[6]@ propose to use a language model to encode documents and queries into dense representations, and then use the attention weights between the query and document representations to compute the relevance score. @CITE[4]@ propose to use a language model to encode documents and queries into dense representations, and then use the attention weights between the query and document representations to compute the relevance score. @CITE[9]@ propose to use a language model to encode documents and queries into dense representations, and then use the attention weights between the query and document representations to compute the relevance score. @CITE[1]@ propose to use a language model to encode documents and queries into dense representations, and then use the attention weights between the query and document representations to compute the relevance score. In this paper, we propose a novel dense retrieval method, which uses a language model to encode documents and queries into dense representations, and then uses the attention weights between the query and document representations to compute the relevance score. Our method is different from the previous methods, which use the inner product between the query and document representations to compute the relevance score. In addition, our method can be used in the scenarios where the query and document representations are not available, which is different from the previous methods.\\nReferences:ID: 1. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 2. Title: Training Large-Scale News Recommenders with Pretrained Language Models in the Loop\\nID: 3. Title: PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval\\nID: 4. Title: Sparse, Dense, and Attentional Representations for Text Retrieval\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 7. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 8. Title: ERNIE-Doc: A Retrospective Long-Document Modeling Transformer\\nID: 9. Title: B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval\\nID: 10. Title: AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search',\n",
       "  'ground_truth': '\\n\\nIn the field of dense retrieval models, a bi-encoder architecture is typically employed to separately encode queries and documents, thereby improving search efficiency. This architectural design has found its use in a multitude of applications, such as search engines (, advertising (@CITE[4]@, and recommendation systems (. illustrated that incorporating in-batch negatives during training could substantially boost the performance of dense retrieval models compared to the traditional BM25 model. Consequently, Xiong et al. (2020) introduced ANCE, proposing the utilization of approximate nearest neighbors as negative samples. To refine the selection of negative samples, @CITE[8]@ presented RocketQA, which used a more precise cross-encoder. demonstrated the AR2 model, which concurrently trained the bi-encoder and cross-encoder. Advancements in the quality of dense retrieval are largely attributed to the recent progress in pre-trained language models (; @CITE[5]@. A common approach entails deploying a universally pre-trained model as the bi-encoder. These models are usually pre-trained using one or multiple masked language modeling (MLM) tasks. For instance, BERT (@CITE[1]@ and RoBERTa ( predict masked tokens based on their context. In Ernie (@CITE[7]@ and Spanbert (@CITE[4]@, [DOC] [CLS] Text Token Intra-Block Encoder Intra-Block Encoder Figure 1: Architecture of Longtriever. masked entities and spans are predicted, leading to enhanced performance in entity typing and question answering tasks. However, these generic models, usually pre-trained on token-level tasks, might not effectively cultivate the ability to represent sentences (. To address this limitation, recent studies proposed two primary types of pre-training tasks. The first one is self-contrastive learning (SCL) (@CITE[10]@; ; @CITE[2]@; @CITE[2]@; @CITE[3]@, which trains the language models using relevant sentence pairs from an unlabeled corpus. The second is auto-encoding (AE) (@CITE[3]@; @CITE[3]@; @CITE[6]@; @CITE[9]@, which primarily trains language models to reconstruct the input sentence based on the sentence embedding.\\nReferences:ID: 1. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 2. Title: Training Large-Scale News Recommenders with Pretrained Language Models in the Loop\\nID: 3. Title: PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval\\nID: 4. Title: Sparse, Dense, and Attentional Representations for Text Retrieval\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: ERNIE: Enhanced Representation through Knowledge Integration\\nID: 7. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 8. Title: ERNIE-Doc: A Retrospective Long-Document Modeling Transformer\\nID: 9. Title: B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval\\nID: 10. Title: AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search<|eot_id|>'},\n",
       " 4257: {'predict': 'Poincaré embeddings are a class of hyperbolic embeddings that can be used to represent symbolic data such as text and graphs. @CITE[2]@ propose a method for learning Poincaré embeddings of symbolic data by minimizing the distance between data points and their nearest neighbors. @CITE[3]@ propose a fully hyperbolic neural model for hierarchical multi-class classification. They use a hyperbolic SVM for classification and propose a method for learning hyperbolic embeddings of words in the Poincaré ball. @CITE[4]@ propose a method for learning Poincaré embeddings of text data in an unsupervised manner from text corpora. They use a re-parameterization technique that allows them to learn hyperbolic embeddings of arbitrarily parameterized objects. @CITE[1]@ propose a method for learning hyperbolic embeddings of graphs. They propose a method for learning hyperbolic embeddings of graph nodes by minimizing the distance between graph nodes and their nearest neighbors.\\nReferences:ID: 1. Title: Poincaré Embeddings for Learning Hierarchical Representations\\nID: 2. Title: A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification\\nID: 3. Title: Large-Margin Classification in Hyperbolic Space\\nID: 4. Title: Embedding Text in Hyperbolic Spaces',\n",
       "  'ground_truth': '\\n\\nThe Poincaré ball model (@CITE[1]@ is commonly adopted in hyperbolic neural networks (HNN) and representation learning research due to its differentiable distance function (Dhingra et al., 2018; @CITE[2]@. The Poincaré ball model is a Riemannian manifold that can be formulated as (B, gb x) with a Riemannian metric gb x = λ2 xgE, where λ2 1−∥x∥2 is called the conformal factor, and gE = In is the Euclidean metric tensor. The Riemannian metric defines the geometric properties of a space, such as distances, angles, or curve length. For example, Euclidean space is a manifold with zero curvature, and the distance between two points can be written as d(x, y) = i (xi −yi)2. The Poincaré ball model, on the other hand, is a manifold with a constant negative curvature, where B = {x ∈Rn : ∥x∥< 1} is a unit ball. In natural language processing, researchers have applied such embeddings to tasks as varied as fine-grained entity typing (López and @CITE[3]@, text classification (, and language modeling (@CITE[4]@.\\nReferences:ID: 1. Title: Poincaré Embeddings for Learning Hierarchical Representations\\nID: 2. Title: A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification\\nID: 3. Title: Large-Margin Classification in Hyperbolic Space\\nID: 4. Title: Embedding Text in Hyperbolic Spaces<|eot_id|>'},\n",
       " 3750: {'predict': 'Faithfulness of summarization is another important aspect of the summarization task. Existing approaches to improving faithfulness can be categorized into two main lines: (1) pre-processing and post-processing. Pre-processing methods include using knowledge graphs (@CITE[6]@; @CITE[7]@), fact-aware models (@CITE[1]@; @CITE[3]@; @CITE[4]@; @CITE[5]@), and question answering models (@CITE[2]@). Post-processing methods include fact verification models (@CITE[2]@; @CITE[5]@), correction models (@CITE[1]@; @CITE[7]@), and adversarial training (@CITE[6]@). However, these approaches either require additional knowledge graphs, or manually crafted rules, or additional training data, or retraining, which make them difficult to apply in practice.\\nReferences:ID: 1. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 2. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 3. Title: Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward\\nID: 4. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 5. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 6. Title: Multi-Fact Correction in Abstractive Text Summarization\\nID: 7. Title: Enhancing Factual Consistency of Abstractive Summarization',\n",
       "  'ground_truth': '\\n\\nStudies of faithfulness mainly focus on how to improve the faithfulness of an abstractive summarization model. Though it is challenging, some recent works propose various methods to study this problem, which can be summarized as follows. Some typical methods use pre-extracted information from input the document as additional input (@CITE[6]@, like triplets (@CITE[3]@, keywords (, knowledge graphs (@CITE[1]@; @CITE[2]@ or extractive summaries (@CITE[6]@). These methods encourage the model to copy from the faithful guidance information. Another type of popular method focuses on designing a post-editing module, like a QA model (@CITE[4]@, or a BART-based selection model (@CITE[7]@. But these methods are much less time efficient during inference, thus hard to be used in real-world applications. Some other works apply Reinforcement Learning (RL) based methods, especially policy gradient, which utilize a variety of factual-relevant tasks for calculating rewards, such as information extraction (@CITE[5]@, entailment (@CITE[3]@, QA (@CITE[1]@; @CITE[2]@. This type of methods suffer from the high-variance training of RL.\\nReferences:ID: 1. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 2. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 3. Title: Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward\\nID: 4. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 5. Title: GSum: A General Framework for Guided Neural Abstractive Summarization\\nID: 6. Title: Multi-Fact Correction in Abstractive Text Summarization\\nID: 7. Title: Enhancing Factual Consistency of Abstractive Summarization<|eot_id|>'},\n",
       " 1450: {'predict': 'Automatic evaluation metrics for summarization are widely used in summarization competitions, benchmarking, and research. Early metrics such as ROUGE (@CITE[4]@, METEOR (@CITE[1]@, and CIDEr (@CITE[3]@ are based on n-gram overlap, and later metrics such as BLEU (@CITE[2]@ and BERTScore (Zhang et al., 2020) are based on language modeling. Recent work on automatic evaluation metrics for summarization has mainly focused on developing metrics that better correlate with human judgments. @CITE[2]@ and @CITE[1]@ compare the performance of different metrics on the same benchmark, and show that ROUGE-L is a strong benchmark for summarization evaluation. However, they only focus on the performance of metrics on summarization tasks, and do not explore the interpretability of metrics. In this work, we show that ROUGE-L is not only a strong benchmark but also has strong interpretability.\\nReferences:ID: 1. Title: CIDEr: Consensus-based Image Description Evaluation\\nID: 2. Title: SummEval: Re-evaluating Summarization Evaluation\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: SummEval: Re-evaluating Summarization Evaluation',\n",
       "  'ground_truth': '\\n\\nThe most popular evaluation metric for summarization is ROUGE (@CITE[4]@, which computes the recall of reference n-grams in the evaluated summary. Other n-grams based metrics have been proposed such as CIDEr (@CITE[2]@ and METEOR (, but none of them correlates better with humans according to SummEval, a recent large study conducted by. Recent works have leveraged the success of pre-trained language models. @CITE[1]@ proposed BERTScore, which uses BERT ( to compute a similarity score between the reference and the evaluated text. However, its performance is similar to that of ROUGE (. Several works have explored using natural language inference (NLI) models to evaluate the factual consistency of summaries (Kry´ nski et al., 2019; ; @CITE[3]@, finding mixed results in using NLI models rather than QA models.\\nReferences:ID: 1. Title: CIDEr: Consensus-based Image Description Evaluation\\nID: 2. Title: SummEval: Re-evaluating Summarization Evaluation\\nID: 3. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 4. Title: SummEval: Re-evaluating Summarization Evaluation<|eot_id|>'},\n",
       " 399: {'predict': 'Multi-Task Learning Model. Multi-task learning has been widely used in RE tasks. @CITE[3]@ proposed a multi-task model that jointly extracts entities and relations. @CITE[2]@ proposed a CopyMTL model that can extract multi-token entities. @CITE[3]@ proposed a multi-task model that jointly extracts entities and relations by using a single encoder to encode the input sequence. @CITE[1]@ proposed a multi-task model that jointly extracts entities and relations by using a single encoder to encode the input sequence. @CITE[3]@ proposed a multi-task model that jointly extracts entities and relations by using a single encoder to encode the input sequence. @CITE[1]@ proposed a multi-task model that jointly extracts entities and relations by using a single encoder to encode the input sequence. However, these methods are all based on the sequence encoder, which cannot capture the structural information of the sentence. In contrast, our model can capture the structural information of the sentence and the information of the entity and relation.\\nReferences:ID: 1. Title: End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\\nID: 2. Title: CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning\\nID: 3. Title: End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures',\n",
       "  'ground_truth': '\\n\\nThe third class design a multi-task learning model to extract relational facts (@CITE[3]@; @CITE[2]@; Adel and Sch¨ utze, 2017; ;. Among these works, (@CITE[2]@ consider the dependency structure of the sentence to model the common features between the ER and RC tasks, while (Adel and Sch¨ utze, 2017) employ convolu- tional neural networks (CNN) to encode the common features. ( proposed an MTL model which comprises of an ER model to extract entities with multi-tokens, and a seq2seq model to extract relational facts. Their approach solves the entity extraction problem faced by mod- els which are solely seq2seq based. ( exploits a bidirectional recurrent neural net- work and graph convolutional network to extract the common features. Despite the substantial ef- forts and great successes in the design of these MTL-based methods, these methods follow the conventional MTL approach (@CITE[1]@. Thus, they only capture implicit interactions by means of the shared network of the ER and RC tasks.\\nReferences:ID: 1. Title: End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\\nID: 2. Title: CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning\\nID: 3. Title: End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures<|eot_id|>'},\n",
       " 5827: {'predict': 'In this work, we focus on multi-label text classification. The problem of multi-label classification is to classify a document into a set of labels. A large number of works have been proposed to tackle this problem. In the early stage, many works rely on rule-based methods to solve this problem. @CITE[4]@ proposed a rule-based multi-label classification method to classify documents. @CITE[1]@ proposed a method to combine the predictions of multiple rule-based classifiers. Then, with the development of deep learning, many works have been proposed to solve this problem. @CITE[2]@ proposed a multi-label text classification method based on sequence-to-sequence learning. @CITE[3]@ proposed a method to cast multi-label emotion classification as span-prediction. @CITE[5]@ proposed a method to leverage contrastive learning to enhance distinctiveness for better performance in multi-label image classification. @CITE[1]@ proposed a novel asymmetric loss to balance the positive and negative samples. @CITE[5]@ proposed a multi-label classification method based on graph attention network. @CITE[1]@ proposed a method to leverage the contrastive learning to enhance the model’s performance. @CITE[5]@ proposed a method to leverage the attention mechanism to enhance the model’s performance. However, these methods have not been used to solve the problem of multi-label text classification with label noise.\\nReferences:ID: 1. Title: Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting\\nID: 2. Title: Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification\\nID: 3. Title: SpanEmo: Casting Multi-label Emotion Classification as Span-prediction\\nID: 4. Title: Multi-Label Image Classification with Contrastive Learning\\nID: 5. Title: Asymmetric Loss For Multi-Label Classification',\n",
       "  'ground_truth': '\\n\\nIn the field of text classification, multi-label text classification (MLTC) is always a challenging problem (@CITE[1]@). A sample of multi-label text classification consists of a text and a set of labels. There is a correlation among labels. For this, some research transforms the multi-label classification problem into the seq2seq problem and learns the potential correlation among labels with the sequence generation model (; ; @CITE[5]@). Yang et al. (2019) proposed a reinforcement learning-based seq2set framework, which can capture the correlation among tags and reduce the dependence on tag order. In addition, there is some research introducing label embedding so that the model can simultaneously learn the feature information of text and the co-occurrence information of labels. @CITE[3]@ proposed to learn statistical label co-occurrence via GCN. LELC (Joint Learning from Label Embedding and Label Correlation) simultaneously learned labels attention and label co-occurrence matrix information (@CITE[4]@). ensembled the MLTC and the label co-occurrence task to enhance label correlation feedback. Most dataset of MLTC has the data distribution imbalance problem: imbalance within labels, among labels, and among label-sets. The studies we have discussed above, which use label embedding, have alleviated the impact of label imbalance to some extent while learning label association. Some research solves the problem of data imbalance by resampling. For example, based on the edited nearest neighbor rule, @CITE[2]@ proposed a multi-label undersampling algorithm. They defined a measure of the differential distance between label sets in order to heuristically remove unnecessary samples during resampling. Considering the problem in terms of object functions, proposed an asymmetric loss that dynamically adjusts the asymmetry levels to balance the effect of positive and negative samples in train-.\\nReferences:ID: 1. Title: Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting\\nID: 2. Title: Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification\\nID: 3. Title: SpanEmo: Casting Multi-label Emotion Classification as Span-prediction\\nID: 4. Title: Multi-Label Image Classification with Contrastive Learning\\nID: 5. Title: Asymmetric Loss For Multi-Label Classification<|eot_id|>'},\n",
       " 2793: {'predict': 'Dense retrieval has been widely used in open-domain question answering (@CITE[3]@, passage ranking (@CITE[1]@, and conversational search (@CITE[2]@. For example, @CITE[3]@ proposed a dense retriever for open-domain question answering by encoding questions and passages into dense vectors, and using the inner product to compute the relevance between queries and passages. @CITE[1]@ proposed a sparse-dense hybrid model for efficient and precise retrieval. @CITE[2]@ proposed a compact and effective model for low-latency passage retrieval in conversational search by directly integrating conversational query reformulation into a dense retrieval model. They trained a query encoder to encode conversational queries into dense vectors and used the inner product to compute the relevance between queries and passages. However, in this paper, we use the inner product to compute the relevance between conversational queries and responses, and use the top-k retrieved responses to generate the next conversational query. In addition, we also propose a novel query reformulation method by learning to adjust the L2 norm of query token embeddings, which can improve the retrieval performance and facilitate the query reformulation.\\nReferences:ID: 1. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 2. Title: Sparse, Dense, and Attentional Representations for Text Retrieval\\nID: 3. Title: Contextualized Query Embeddings for Conversational Search',\n",
       "  'ground_truth': '\\n\\nA dense retrieval system usually applies a bi-encoder architecture to encode the query and document into dense vector separately (@CITE[1]@; @CITE[3]@; ). It then computes the query-document relevance with similarity functions (e.g., dot product) and performs retrieval via dense searching algorithms (e.g., approximate nearest neighbor) (Andoni and Indyk, 2008). In conversational search, the query embedding is learned from a set of follow-up queries instead of an ad-hoc query. As labeling ground truth query in conversational search is exhausted, it is difficult to train a dense retrieval model dedicated to the conversational system. Therefore, @CITE[2]@ directly concatenated the follow-up queries into a long sequence and fed them to the dense retrieval model fine-tuned on the CANARD dataset. Through this way, the model can learn a query embedding from the whole query context and then perform dense retrieval. Besides, Yu et al. (2021) proposed a few-shot framework to continue training an ad-hoc dense retrieval model. Mao et al. (2022) utilize a curriculum contrastive method to denoise the conversational context for better representations. Similarly, they then learn a query embedding which is used in the dense retrieval system. In this paper, we first obtain the well-formed query via an Encode-Tag-Modify framework. Then, we use the relevant terms detected in the query context as supervision signals to enhance the robustness of the query encoder.\\nReferences:ID: 1. Title: Dense Passage Retrieval for Open-Domain Question Answering\\nID: 2. Title: Sparse, Dense, and Attentional Representations for Text Retrieval\\nID: 3. Title: Contextualized Query Embeddings for Conversational Search<|eot_id|>'},\n",
       " 4931: {'predict': 'There have been several studies on query prediction, which can be broadly classified into two categories: (1) query prediction for document retrieval, which aims to predict the query given a document and has been widely used in dense retrieval (@CITE[1]@; @CITE[2]@; ; @CITE[4]@; @CITE[3]@). In this paper, we focus on the second category. @CITE[2]@ propose to use a pre-trained language model to predict the query given a passage. @CITE[4]@ propose a multi-task learning framework to predict the query and the passage simultaneously. @CITE[3]@ propose to use a passage-centric similarity relation to improve dense passage retrieval. @CITE[1]@ propose to use a contrastive learning approach to learn the query and passage representations. In contrast to the existing methods, we propose a novel multi-task learning framework to predict the query and the passage simultaneously, and propose to use a query-centric similarity relation to improve dense passage retrieval.\\nReferences:ID: 1. Title: Learning Passage Impacts for Inverted Indexes\\nID: 2. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval\\nID: 3. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\\nID: 4. Title: PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval',\n",
       "  'ground_truth': '\\n\\nQuery Prediction is a technique originally introduced to the IR community to expand passages. It can significantly improve the performance of BM25 by generating additional queries and appending them to passages before building the inverted index (@CITE[3]@). Query prediction has also been used to learn better sparse (@CITE[2]@) or dense (@CITE[4]@) representations for documents. In scenarios where data is scarce, query prediction can be used for domain adaptation by generating synthetic queries on target domains for model training (@CITE[1]@). To reduce noise in the generated data, a cross-encoder can also be used for pseudo-labeling (). The most related work to ours is (@CITE[4]@), which encodes each document with a set of generated pseudo-queries to obtain query-informed document representations. However, (@CITE[4]@) focuses on improving the fine-tuning process for dense retrieval, while we are working on the pre-training process.\\nReferences:ID: 1. Title: Learning Passage Impacts for Inverted Indexes\\nID: 2. Title: Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval\\nID: 3. Title: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\\nID: 4. Title: PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval<|eot_id|>'},\n",
       " 6714: {'predict': 'The development of dialogue systems has witnessed remarkable progress in recent years, with the emergence of large pre-trained language models (PLMs) (@CITE[1]@; @CITE[4]@; @CITE[2]@; @CITE[3]@). However, the existing dialogue systems mainly focus on generating informative responses and neglect the emotional support, which is essential for building human-like dialogue systems. In recent years, several works (@CITE[5]@; @CITE[8]@; @CITE[9]@; @CITE[6]@; @CITE[7]@) have been devoted to building emotional support chatbots. For example, @CITE[5]@ proposed an ESC framework to help the chatbot understand the help-seeker’s emotions and provide emotional support. @CITE[8]@ constructed an emotional support conversation dataset and evaluated the performance of several dialogue models. @CITE[9]@ presented a novel methodology that leverages the capabilities of large language models to curate an extensive emotional support dialogue dataset. However, these works mainly focus on the emotional support of dialogue generation and neglect the emotional support of dialogue understanding. Moreover, most of the existing works are limited to the pre-training or fine-tuning of PLMs, which is not sufficient to fully understand the emotional support of dialogue systems.\\nReferences:ID: 1. Title: A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data\\nID: 2. Title: BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data\\nID: 3. Title: Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization\\nID: 4. Title: COSMIC: COmmonSense knowledge for eMotion Identification in Conversations\\nID: 5. Title: Towards Emotional Support Dialog Systems\\nID: 6. Title: AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation\\nID: 7. Title: Building Emotional Support Chatbots in the Era of LLMs\\nID: 8. Title: Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\\nID: 9. Title: Towards Emotional Support Dialog Systems\\nID: 10. Title: CEM: Commonsense-aware Empathetic Response Generation',\n",
       "  'ground_truth': '\\n\\nMost of the previous work develops personalized (@CITE[9]@; @CITE[2]@; @CITE[3]@; @CITE[4]@, emotional (@CITE[8]@; @CITE[1]@; @CITE[6]@; Deng et al., 2023c; @CITE[7]@, empathetic (@CITE[10]@; @CITE[1]@; @CITE[5]@ dialogue system in isolation, rather than seamlessly blending them all into one cohesive conversational flow (; @CITE[4]@.\\nReferences:ID: 1. Title: A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data\\nID: 2. Title: BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data\\nID: 3. Title: Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization\\nID: 4. Title: COSMIC: COmmonSense knowledge for eMotion Identification in Conversations\\nID: 5. Title: Towards Emotional Support Dialog Systems\\nID: 6. Title: AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation\\nID: 7. Title: Building Emotional Support Chatbots in the Era of LLMs\\nID: 8. Title: Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\\nID: 9. Title: Towards Emotional Support Dialog Systems\\nID: 10. Title: CEM: Commonsense-aware Empathetic Response Generation<|eot_id|>'},\n",
       " 3296: {'predict': 'Language technologies have long been studied for potential intrinsic and extrinsic harms. For example, @CITE[10]@ and @CITE[7]@ showed that word embeddings contain gender stereotypes, while @CITE[5]@ found that large language models contain persistent Muslim-violence bias. @CITE[8]@ and @CITE[4]@ showed that LMs can generate toxic language even from innocuous prompts, and @CITE[9]@ and @CITE[6]@ found that clinical LMs contain biases against marginalized populations. @CITE[2]@ found that NLP models perform worse on African American English, and @CITE[3]@ found that gender bias in coreference resolution can be reduced with data augmentation. Other studies have shown that LMs can encode biases against ethnic groups (@CITE[1]@; @CITE[10]@), and that WEAT scores can predict racial animus (@CITE[10]@). However, these studies have focused on intrinsic harms, such as biases in word embeddings and LMs, rather than extrinsic harms, such as the downstream effects of these biases.\\nReferences:ID: 1. Title: Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\\nID: 2. Title: Persistent Anti-Muslim Bias in Large Language Models\\nID: 3. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 4. Title: Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings\\nID: 5. Title: Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English\\nID: 6. Title: Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\\nID: 7. Title: Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods\\nID: 8. Title: Identifying and Reducing Gender Bias in Word-Level Language Models\\nID: 9. Title: Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency\\nID: 10. Title: Towards Understanding Linear Word Analogies',\n",
       "  'ground_truth': '\\n\\nLanguage technologies have long been studied for potential intrinsic and extrinsic harms. Known intrinsic harms include misrepresentation of gender (@CITE[8]@, race (@CITE[6]@, and ability (Hutchinson et al., 2020) — all types of representational harms (@CITE[7]@; @CITE[7]@; Blodgett et al., 2020). Other intrinsic harms include forms of erasure through under-representation of LGBTQ+ identity terms (; and racial groups (. Extrinsic harms are often found in downstream tasks and include disparities in quality of service among user groups ( such as African-American users (Blodgett and O’@CITE[9]@; @CITE[10]@. However, low statistical power has also made it difficult to make conclusive claims about the presence or absence of bias (@CITE[6]@. Many of these representational harms have been linked to word frequency in static embeddings (@CITE[8]@; Caliskan et al., 2017; ; ; Ethayarajh et al., 2019b; van. Low frequency words also differ geometrically from other words, with smaller inner products (@CITE[5]@ and lower variance (@CITE[1]@. Recent work has also shown how frequency impacts contextual embeddings such as the under-estimation of cosine similarity among high-frequency words (@CITE[7]@ and the discrepancies in representations of personal names (Shwartz et al., 2020; @CITE[8]@. Our work extends these lines of work via the examination of representational harms for country names.\\nReferences:ID: 1. Title: Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\\nID: 2. Title: Persistent Anti-Muslim Bias in Large Language Models\\nID: 3. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 4. Title: Hurtful Words: Quantifying Biases in Clinical Contextual Word Embeddings\\nID: 5. Title: Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English\\nID: 6. Title: Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\\nID: 7. Title: Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods\\nID: 8. Title: Identifying and Reducing Gender Bias in Word-Level Language Models\\nID: 9. Title: Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency\\nID: 10. Title: Towards Understanding Linear Word Analogies<|eot_id|>'},\n",
       " 4414: {'predict': 'Hallucinations in text generation can be a major problem. @CITE[2]@ propose to reduce hallucinations in dialogue systems by grounding the dialogue context with a knowledge graph. @CITE[1]@ propose to improve the faithfulness of knowledge-grounded dialogue systems by using a separate token-level fact critic to identify plausible sources of hallucination. @CITE[3]@ propose to improve the faithfulness of knowledge-grounded dialogue systems by using a stylistic control to encourage the model to generate responses that are faithful to the provided evidence. @CITE[4]@ propose to ground a dialogue context with Wikipedia articles. @CITE[4]@ propose to ground a dialogue context with a Wikipedia article. We use the dataset proposed by @CITE[4]@ in this paper. We use a different approach to reduce hallucinations in text generation by reducing the model’s ability to generate hallucinations. Our approach is orthogonal to previous work and can be used in conjunction with previous work to further reduce hallucinations.\\nReferences:ID: 1. Title: Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\\nID: 2. Title: Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features\\nID: 3. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 4. Title: A Dataset for Document Grounded Conversations',\n",
       "  'ground_truth': '\\n\\nGenerating responses that are unfaithful to the provided knowledge, known as the hallucination, is a tricky problem in knowledge-grounded dialogue systems. Recently, the hallucination problem has attracted increasing attention because the generated text appears smooth and fluent but usually contains false knowledge, which significantly threatens the model’s credibility. Some studies reduce hallucinations by introducing knowledge graph (@CITE[4]@; @CITE[1]@), controllable generation (@CITE[2]@), and contrastive learning (@CITE[4]@). In a recent study, Dziri et al. (2022b) analyze the source of hallucination in detail and find that the most knowledge-grounded conversation datasets (; @CITE[3]@) inherently contain hallucinations, and models trained on such dataset further amplify hallucinations, which demonstrate that the pattern of hallucination responses is more likely to be learned by the model. To address this problem, further propose FaithDial, a new dataset that removes hallucinations in the Wizard of Wikipedia (). Different from these studies about models and datasets, we find that the training method with unexpected learning focus also plays a vital role in the hallucination problem and then present a method to adjust the original focus.\\nReferences:ID: 1. Title: Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding\\nID: 2. Title: Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features\\nID: 3. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 4. Title: A Dataset for Document Grounded Conversations<|eot_id|>'},\n",
       " 5140: {'predict': 'Data augmentation is a widely used technique in NLP to increase the size of the training data and boost the model performance. The most common approach is to apply simple token-level perturbations on the input text. @CITE[10]@ and @CITE[5]@ proposed to insert random punctuation marks into the original text. @CITE[1]@ proposed to replace the original token with its synonyms. @CITE[3]@ proposed to randomly swap the positions of two tokens. @CITE[2]@ proposed to insert random characters into the original text. @CITE[7]@ proposed to perform token-level mixup. @CITE[4]@ proposed to perform span-level mixup. @CITE[8]@ proposed to perform round-trip translation as data augmentation. @CITE[9]@ proposed to perform back-translation as data augmentation. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. @CITE[6]@ proposed to perform counterfactual inference to generate new data. proposed to perform counterfactual inference to generate new data.\\nReferences:ID: 1. Title: An Analysis of Simple Data Augmentation for Named Entity Recognition\\nID: 2. Title: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\\nID: 3. Title: AEDA: An Easier Data Augmentation Technique for Text Classification\\nID: 4. Title: mixup: Beyond Empirical Risk Minimization\\nID: 5. Title: SSMix: Saliency-Based Span Mixup for Text Classification\\nID: 6. Title: Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation\\nID: 7. Title: Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data\\nID: 8. Title: Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues\\nID: 9. Title: An Investigation of the (In)effectiveness of Counterfactually Augmented Data\\nID: 10. Title: Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues',\n",
       "  'ground_truth': '\\n\\nData augmentation is widely recognized in deep learning research as a valuable approach to addressing the scarcity of annotated data. It aids in ensuring that the distribution of the training data maintains robustness even in unseen tests. Over time, several methods have been developed to augment textual content, expanding the diversity and quantity of available training data. @CITE[10]@ proposed a method of word substitution using a synonym thesaurus based on rules, which is followed by augmentation methods utilizing a thesaurus in the studies of (;. @CITE[4]@ suggested an easy data augmentation (EDA) method using word-level replacement, random insertion, random deletion, and random swap. @CITE[5]@ introduced an easier data augmentation (AEDA) technique, improving text classification performance by randomly inserting six pre-defined punctuation marks. Another strategy involves Mixup-based data augmentation (@CITE[1]@, combining two sample data to create new training data. Mixup augmentation was initially applied to image data and extended to text-based deep neural networks (@CITE[8]@. Subsequently, it has also been applied to textual data augmentation using Transformer-based methods, serving as a technique to mitigate model overfitting and enhance generalization capabilities (; ; @CITE[3]@. Sennrich et al. (2016) proposed a back translation method for augmenting data by performing a round-trip translation on data written in the original language, using a neural machine translation system trained on human-annotated data. Data augmentation based on back translation has evolved as one of the crucial techniques since it allows for rewriting the entire sentence, rather than just word-level alterations (@CITE[9]@; ; @CITE[6]@. Recently, research on data augmentation using pre-trained language models has been actively pursued (Kumar et al., 2020; ; Schick and Schütze, 2021; ; Mekala et al., 2022). Among them, the data augmentation technique through counterfactual is particularly noteworthy, demonstrating improvements to the state-of-the-art performance levels across various benchmark datasets (; @CITE[7]@; ; @CITE[2]@. With the advent of LLMs, considerable progress has been made in overcoming augmentation constraints through generative AI (. Thus, we proceed assuming that it is feasible to secure textual content with LLMs. Nevertheless, it is still necessary to expend financial and temporal resources to identify the optimal direction for augmentation. As a result, we focus on determining which augmentation method could benefit model learning from Korean data. Given the diverse derivational and inflectional forms inherent in the agglutinative Korean language, we propose CHEF - an augmentation method that considers these linguistic properties.\\nReferences:ID: 1. Title: An Analysis of Simple Data Augmentation for Named Entity Recognition\\nID: 2. Title: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\\nID: 3. Title: AEDA: An Easier Data Augmentation Technique for Text Classification\\nID: 4. Title: mixup: Beyond Empirical Risk Minimization\\nID: 5. Title: SSMix: Saliency-Based Span Mixup for Text Classification\\nID: 6. Title: Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation\\nID: 7. Title: Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data\\nID: 8. Title: Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues\\nID: 9. Title: An Investigation of the (In)effectiveness of Counterfactually Augmented Data\\nID: 10. Title: Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues<|eot_id|>'},\n",
       " 3812: {'predict': 'Early methods for learning text span representations typically rely on handcrafted features (Collobert et al., 2011; @CITE[4]@; @CITE[11]@) or use a combination of word embeddings and a linear layer (@CITE[6]@; @CITE[8]@; @CITE[12]@). In recent years, pre-trained language models (PLMs) have become the norm for learning representations of text spans. For example, BERT (@CITE[7]@) and its variants ( are pre-trained on large-scale corpora using masked language modeling (MLM) and can be fine-tuned for a variety of downstream tasks. Other PLMs that are specifically designed for learning text span representations include SpanBERT (@CITE[9]@), which masks contiguous spans of text and predicts the entire span, and UCTopic (@CITE[5]@), which uses contrastive learning to learn context-aware phrase representations. In this work, we focus on using PLMs to learn text span representations and do not rely on handcrafted features or additional pre-training objectives.\\nReferences:ID: 1. Title: Natural Language Processing (almost) from Scratch\\nID: 2. Title: Distributed Representations of Words and Phrases and their Compositionality\\nID: 3. Title: Deep contextualized word representations\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Towards Universal Paraphrastic Sentence Embeddings\\nID: 6. Title: Extending Multi-Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications\\nID: 7. Title: LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\\nID: 8. Title: Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration\\nID: 9. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 10. Title: Learning Dense Representations of Phrases at Scale\\nID: 11. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 12. Title: UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining',\n",
       "  'ground_truth': '\\n\\nProducing word-level representations has been extensively explored (@CITE[11]@; Bengio et al., 2000; Turian et al., 2010; ; @CITE[4]@; ; @CITE[3]@; @CITE[7]@, mainly using the surroundings of the word as a signal for its meaning. Representing text spans larger than one word can be achieved by non-trivial combinations of word representations (e.g., @CITE[12]@; @CITE[9]@; Arora et al., 2017; @CITE[1]@). However, these approaches use the phrase context only during the pretraining stage and are consequently less effective for capturing the compositional meaning of the span (@CITE[6]@). Phrase-level embeddings assist in tasks such as paraphrase detection, question-answering (QA), and topic modeling, and are often generated in compliance with the task objective. @CITE[2]@ fine-tune BERT (@CITE[7]@) with synthetically generated paraphrases to detect lexically-differing text similarities. @CITE[10]@ optimize SpanBERT (@CITE[5]@) for QA to link a question to a phrasal answer within a passage. @CITE[8]@ and Zhou and Wakabayashi (2022) apply contrastive learning over LUKE (@CITE[6]@) or a combination of Sentence-BERT () and Phrase-BERT (@CITE[2]@) for the use of clustering together topically related phrases within a corpus. These methods for rendering phrase representations leverage a single sentence-level context of the phrase, only while training. In contrast, our approach takes advantage of many contexts at inference time in order to capture the meaning of the phrase.\\nReferences:ID: 1. Title: Natural Language Processing (almost) from Scratch\\nID: 2. Title: Distributed Representations of Words and Phrases and their Compositionality\\nID: 3. Title: Deep contextualized word representations\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Towards Universal Paraphrastic Sentence Embeddings\\nID: 6. Title: Extending Multi-Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications\\nID: 7. Title: LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\\nID: 8. Title: Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration\\nID: 9. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 10. Title: Learning Dense Representations of Phrases at Scale\\nID: 11. Title: SpanBERT: Improving Pre-training by Representing and Predicting Spans\\nID: 12. Title: UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining<|eot_id|>'},\n",
       " 4059: {'predict': 'Dataset drift is a well-studied problem in machine learning, which has been extensively surveyed in a variety of domains (@CITE[1]@; @CITE[1]@). In the NLP context, the drift is often quantified using semantic similarity metrics. @CITE[2]@ perform a comprehensive analysis of 12 semantic similarity metrics on a human-labeled dataset of 14K sentence pairs. They find that none of the metrics widely used in the literature is close enough to human judgment in semantic similarity tasks. @CITE[3]@ perform a similar analysis, finding that word mover distance is the most reasonable solution to measure semantic similarity in reformulated texts at the moment. However, these works focus on semantic similarity metrics, which are not directly applicable to quantify the drift in the context of dataset drift. In this work, we propose a novel method to quantify dataset drift in the context of NLP, which is based on the notion of semantic similarity. We use the cosine similarity between sentence embeddings to quantify the semantic similarity between sentences, and we use the KL divergence to quantify the drift between the distribution of sentences in two datasets.\\nReferences:ID: 1. Title: Domain Divergences: a Survey and Empirical Analysis\\nID: 2. Title: Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric\\nID: 3. Title: Domain Divergences: a Survey and Empirical Analysis',\n",
       "  'ground_truth': '\\n\\nPast work has quantified the drift between NLP datasets using distances between token frequency distributions or TF-IDF vectors (Bäck, 2019; @CITE[1]@; Sato et al., 2022), language model embedding distances (;, or the ability of domain classifiers to discriminate between datasets (@CITE[3]@; Elsahar and Gallé, 2019; Ruder et al., 2017). Notably, @CITE[1]@ find that these metrics can predict performance degradations when an NLP model is transferred from a training dataset Dtrain to an out-of-domain evaluation dataset Deval. However, existing metrics have generally been designed as holistic measures of linguistic drift, failing to capture specific dimensions that might affect NLP model performance in different ways. Furthermore, the traditional setup for evaluating drift metrics (Elsahar and Gallé, 2019; @CITE[1]@ only allows for dataset-level drift metrics that predict overall model accuracy on out-of-domain datasets. In practice, when real world examples are streamed during test time, it is desirable to predict model performance for individual examples using example-level drift metrics (i.e. drift between an example x and a training dataset Dtrain; @CITE[2]@; Ningenda et al., 2022). In our work, we modify the setup from @CITE[1]@ to predict performance for individual examples (Section 4), using logistic regressions fitted to example-level drift metrics. In contrast to Ramesh Kashyap et al. (2021), we can fit our regressions to predict out-of-domain performance even when only a single in-domain evaluation dataset is available.\\nReferences:ID: 1. Title: Domain Divergences: a Survey and Empirical Analysis\\nID: 2. Title: Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric\\nID: 3. Title: Domain Divergences: a Survey and Empirical Analysis<|eot_id|>'},\n",
       " 3846: {'predict': 'Prompt engineering is a rapidly growing area in NLP research. It aims to improve the performance of pre-trained language models by designing effective prompts to guide the model to generate desired outputs. In the field of summarization, prompt engineering is mainly used to control the summarization style and length. @CITE[5]@ first proposed to use a template-based prompt to control the summarization style. Later, @CITE[2]@ proposed to use a prompt with style and length control to generate summaries. The style control prompt is a template-based prompt that is used to guide the model to generate summaries with a specific style, such as positive, negative, or neutral. The length control prompt is a template-based prompt that is used to guide the model to generate summaries with a specific length. @CITE[1]@ proposed to use a prompt with a specific style and length to generate summaries, and showed that the model can generate summaries with the desired style and length. @CITE[4]@ proposed to use a prompt with a specific length and a specific style to generate summaries, and showed that the model can generate summaries with the desired length and style. @CITE[3]@ proposed to use a prompt with a specific style and length to generate summaries, and showed that the model can generate summaries with the desired style and length. @CITE[3]@ proposed to use a prompt with a specific style and length to generate summaries, and showed that the model can generate summaries with the desired style and length. @CITE[4]@ proposed to use a prompt with a specific style and length to generate summaries, and showed that the model can generate summaries with the desired style and length. In our work, we use a prompt with a specific style and length to generate summaries, and we show that the model can generate summaries with the desired style and length.\\nReferences:ID: 1. Title: Controllable Abstractive Summarization\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 4. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 5. Title: Making Pre-trained Language Models Better Few-shot Learners',\n",
       "  'ground_truth': '\\n\\nPrompts have been a common tool for controllable generation (@CITE[3]@; ; @CITE[4]@; Raffel et al., 2019). Instructions are also constructed as prompts to allow large models to perform new tasks that are unseen in training (@CITE[1]@; ). More recently, prompts, either hand-crafted (Schick and Schütze, 2021; ) or learned (), are found to benefit model learning and improve few-shot performance on downstream tasks. Our textual time-aware prompts extend the year-level prompts in @CITE[5]@ with months and days to incorporate fine-grained temporal information, and we further explore representing timestamps with linear prompts which have been mainly used for length-controlled generation (@CITE[2]@).\\nReferences:ID: 1. Title: Controllable Abstractive Summarization\\nID: 2. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 3. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 4. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 5. Title: Making Pre-trained Language Models Better Few-shot Learners<|eot_id|>'},\n",
       " 3552: {'predict': 'Domain adaptation is a common strategy to improve translation performance on a target domain when only a limited amount of parallel data is available. The parallel data is used to fine-tune a pre-trained model for the target domain. This approach has been widely studied for NMT (@CITE[1]@; @CITE[4]@; @CITE[2]@; ; @CITE[5]@; @CITE[3]@. Most of these studies focus on adapting a single NMT model to a single target domain. However, the adaptation of a single model to multiple domains is a more realistic scenario. @CITE[6]@ propose to train multiple models for multiple domains and use a simple mixture to combine the outputs. @CITE[3]@ propose to train a single model with multiple domain adapters. They show that the combination of domain adapters can improve the performance on the source domains and show that the performance on the target domains is similar to the performance of the best model for each target domain. However, the combination of domain adapters also leads to catastrophic forgetting of the source domains. This is not desirable as the model should be able to translate on all domains. In this paper, we propose a new approach that combines the advantages of both approaches. We train multiple models for each source domain and use a mixture of the outputs. We also train multiple domain adapters for each target domain and use a combination of the adapters. This way, we avoid catastrophic forgetting of the source domains. Our approach also allows us to improve the performance on the target domains. In addition, we show that the combination of multiple models and adapters can also be beneficial for the source domains.\\nReferences:ID: 1. Title: A Survey of Domain Adaptation for Neural Machine Translation\\nID: 2. Title: Dynamic Data Selection for Neural Machine Translation\\nID: 3. Title: Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations\\nID: 4. Title: DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation\\nID: 5. Title: Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation\\nID: 6. Title: Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters',\n",
       "  'ground_truth': '\\n\\nExisting work on domain adaptation for machine translation can be categorized into two types: data-centric and model-centric approaches (@CITE[5]@. The former focus on maximizing the use of in-domain monolingual, synthetic, and parallel data (; Park et al., 2017; van der @CITE[2]@, while the latter design specific training objectives, model architectures or decoding algorithms for domain adaptation (@CITE[1]@; ; @CITE[3]@. In the case of MNMT, adapting to new domains is more challenging because it needs to take into account transfer between languages (@CITE[6]@; @CITE[4]@.)\\nReferences:ID: 1. Title: A Survey of Domain Adaptation for Neural Machine Translation\\nID: 2. Title: Dynamic Data Selection for Neural Machine Translation\\nID: 3. Title: Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations\\nID: 4. Title: DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation\\nID: 5. Title: Multilingual Multi-Domain Adaptation Approaches for Neural Machine Translation\\nID: 6. Title: Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters<|eot_id|>'},\n",
       " 6909: {'predict': 'Continual learning has been widely studied in computer vision (Kirkpatrick et al., 2017; @CITE[3]@; @CITE[4]@). In NLP, @CITE[1]@ proposed a meta-learning framework to learn domain word embeddings, which can be used for lifelong learning. @CITE[2]@ proposed a Boolean operation-based continual learning framework for sentence representations. @CITE[5]@ proposed a lifelong language modeling framework for language learning, which can be applied to various NLP tasks. @CITE[6]@ proposed a Bayesian optimization framework for sentiment classification, which can retain the knowledge learned from past tasks to help future learning.\\nReferences:ID: 1. Title: iCaRL: Incremental Classifier and Representation Learning\\nID: 2. Title: Overcoming catastrophic forgetting in neural networks\\nID: 3. Title: Lifelong Domain Word Embedding via Meta-Learning\\nID: 4. Title: Continual Learning for Sentence Representations Using Conceptors\\nID: 5. Title: LAMOL: LAnguage MOdeling for Lifelong Language Learning\\nID: 6. Title: Lifelong Learning for Sentiment Classification',\n",
       "  'ground_truth': '\\n\\nContinual learning enables models to learn new knowledge and preserve knowledge acquired previously from a data stream with a continuously changing distribution. However, due to the well-known problem of catastrophic forgetting (@CITE[2]@, continual learning is still challenging for current neural models. The same forgetting problem could also appear in curriculum learning because curriculum learning rearranges the examples according to their difficulty levels, which will naturally create a training data stream with continuously changing distribution on the difficulty domain and thus cause the problem of forgetting. Although the catastrophic forgetting problem has been explored in both computer vision (@CITE[1]@; @CITE[3]@; Zenke et al., 2017; @CITE[6]@ and natural language processing (; ; @CITE[4]@; ; ; @CITE[5]@, there are no available studies on curriculum learning mentioning this forgetting problem. Our work is the first attempt to point out the issue of forgetting in curriculum learning and study mechanisms to alleviate it.\\nReferences:ID: 1. Title: iCaRL: Incremental Classifier and Representation Learning\\nID: 2. Title: Overcoming catastrophic forgetting in neural networks\\nID: 3. Title: Lifelong Domain Word Embedding via Meta-Learning\\nID: 4. Title: Continual Learning for Sentence Representations Using Conceptors\\nID: 5. Title: LAMOL: LAnguage MOdeling for Lifelong Language Learning\\nID: 6. Title: Lifelong Learning for Sentiment Classification<|eot_id|>'},\n",
       " 7341: {'predict': 'Existing works on automated reporting focus on the generation of radiology reports (@CITE[3]@; @CITE[2]@, 2022) for assisting radiologists in their time-consuming task of report writing. However, these works lack a mechanism for controlling the report generation, which is crucial for ensuring clinical accuracy. In contrast, our work proposes a controllable automated reporting framework that enables radiologists to control the report generation process. To the best of our knowledge, our work is the first to propose a controllable automated reporting framework. Our framework is also the first to integrate with explainable object detection (@CITE[1]@, 2017) for generating reports that focus on anatomical regions in the image. Our work also extends the idea of interactive radiology report generation (, 2021) to the domain of automated reporting. In our work, radiologists can interact with the model by controlling the report generation process through a user-friendly interface. This is a significant improvement over the previous work of (, 2021), which requires radiologists to manually label the image with bounding boxes. Our work is also the first to use reinforcement learning to ensure clinical accuracy in automated reporting. In our work, we use reinforcement learning to train our model to generate reports that are both clinically accurate and readable.\\nReferences:ID: 1. Title: Clinically Accurate Chest X-Ray Report Generation\\nID: 2. Title: Interactive and Explainable Region-guided Radiology Report Generation\\nID: 3. Title: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "  'ground_truth': '\\n\\nWe define a controllable automated reporting system as one which allows the end users to select what regions in the image they want to report on, giving a level of interpretability. This has partially been tackled using hierarchical approaches (@CITE[1]@, by introducing a multi-head transformer with each head assigned to a specific anatomical region and generating sentences exclusively for that region (@CITE[3]@, and contemporaneously to our work by who (similarly to us) generate sentences based on region-level features extracted through Faster R-CNN (@CITE[2]@. make the assumption that each sentence in the report describes at most one anatomical region. Conversely, we acknowledge that there may be multiple anatomical regions which are relevant to the target text output, e.g., a sentence “No evidence of emphysema.” requires information from both left and right lungs; this requires us to identify valid subsets of anatomical regions in each CXR report for our dropout training strategy (Section 3.4).\\nReferences:ID: 1. Title: Clinically Accurate Chest X-Ray Report Generation\\nID: 2. Title: Interactive and Explainable Region-guided Radiology Report Generation\\nID: 3. Title: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks<|eot_id|>'},\n",
       " 4021: {'predict': 'Multimodal NER on social media has been extensively studied in recent years. Most of the existing work focuses on the fusion of text and image information to improve the performance of NER. Some works (@CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[6]@) adopt the transformer architecture to fuse text and image information, while others (@CITE[5]@; @CITE[4]@) propose to use the attention mechanism to align text and image features. The attention mechanism is widely used in multimodal fusion. However, the attention mechanism has a strong assumption that the text and image features should be aligned. In the real world, the text and image information in social media is not always aligned. Some works (@CITE[6]@; ) use the caption as an auxiliary text to help the model to understand the image. However, the caption is not always available in social media. In this paper, we propose a multimodal NER model that can perform well in both aligned and non-aligned text-image pairs. The model is based on the transformer architecture and uses the object detection results to filter out the irrelevant information from the image. The model can also be used in the scenario where the caption is not available.\\nReferences:ID: 1. Title: RpBERT: A Text-image Relation Propagation-based BERT Model for Multimodal NER\\nID: 2. Title: Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion\\nID: 3. Title: Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion\\nID: 4. Title: MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding\\nID: 5. Title: Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding\\nID: 6. Title: Can images help recognize entities? A study of the role of images for Multimodal NER',\n",
       "  'ground_truth': '\\n\\nWith the rapid growth of multimodal posts on social media, MNER has recently attracted much attention. Most existing MNER methods focus on modeling the text-image interactions by designing various kinds of cross-modal attention mechanism (Moon et al., 2018; ; Zheng et al., 2020). With the recent advancement of deep learning techniques, many studies focus on designing different neural networks for MNER, including Transformer-based methods (; @CITE[4]@; @CITE[4]@;, Graph Neural Network-based methods (@CITE[2]@; @CITE[3]@, Modality Translation-based methods (@CITE[6]@; @CITE[1]@, and Prompt-based models (@CITE[5]@. Despite obtaining promising results, these methods solely utilize the visual clues to better extract entity-type pairs.\\nReferences:ID: 1. Title: RpBERT: A Text-image Relation Propagation-based BERT Model for Multimodal NER\\nID: 2. Title: Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion\\nID: 3. Title: Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion\\nID: 4. Title: MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding\\nID: 5. Title: Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding\\nID: 6. Title: Can images help recognize entities? A study of the role of images for Multimodal NER<|eot_id|>'},\n",
       " 1869: {'predict': 'Story generation and tracking state changes. Previous work has explored tracking state changes in procedural text (e.g., recipe instructions) and in narrative text (e.g., stories). For procedural text, the task is to predict the state changes of entities involved in the process (e.g., what ingredients change and in what way) given a text describing the process. The first dataset for this task was introduced by @CITE[2]@, and has been followed by several others (@CITE[3]@; @CITE[1]@; @CITE[5]@. For narrative text, the task is to track the state changes of characters involved in the story. The first dataset for this task was introduced by, and has been followed by several others (@CITE[4]@; @CITE[5]@. The task has also been explored in the context of story generation (@CITE[2]@; @CITE[4]@.\\nReferences:ID: 1. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 2. Title: Tracking the World State with Recurrent Entity Networks\\nID: 3. Title: Simulating Action Dynamics with Neural Process Networks\\nID: 4. Title: Tracking State Changes in Procedural Text: A Challenge Dataset and Models for Process Paragraph Comprehension\\nID: 5. Title: A Dataset for Tracking Entities in Open Domain Procedural Text',\n",
       "  'ground_truth': '\\n\\nNeural models have been demonstrated to successfully generate stories (Kiddon et al., 2016; ; Zhai et al., 2019; as well as tracking state changes in procedural texts (@CITE[1]@; @CITE[3]@; @CITE[2]@; @CITE[4]@. Our work is related in terms of generating higher-level agenda (or plot) of a story and understanding latent preconditions and effects between events. However, a main difference between these studies and scripts is that story generation and state change tracking explicitly generate and/or predict character’s mental states and entity’s physical attributes (e.g., temperature), whereas scripts focuses on essential core events (@CITE[5]@ in partial order.\\nReferences:ID: 1. Title: PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking\\nID: 2. Title: Tracking the World State with Recurrent Entity Networks\\nID: 3. Title: Simulating Action Dynamics with Neural Process Networks\\nID: 4. Title: Tracking State Changes in Procedural Text: A Challenge Dataset and Models for Process Paragraph Comprehension\\nID: 5. Title: A Dataset for Tracking Entities in Open Domain Procedural Text<|eot_id|>'},\n",
       " 1831: {'predict': 'A common practice in NLP is to use subword vocabularies, which are typically learned from a monolingual corpus. This is done by learning a mapping of characters to subwords using a method such as BPE (@CITE[5]@. Subword vocabularies are commonly used in neural machine translation (NMT) systems (@CITE[1]@; @CITE[3]@; Sennrich et al., 2016) to improve the translation of rare words. Subword vocabularies are also used in multilingual NLP models, such as mBERT (@CITE[4]@, which is trained on a corpus of text from 104 languages. Subword vocabularies are commonly used in transfer learning, where the embeddings of the vocabulary are shared between the source and target languages. However, when transferring between languages with different scripts, subword vocabularies are not effective, as the same character may be represented differently in different languages. For example, the character “a” is represented differently in English and Hindi, and this can lead to errors in translation. This problem can be addressed by romanizing the target language, which is the process of converting text into the Latin script. However, romanization can result in information loss, and the quality of the translation can be affected by the romanization tool used (@CITE[2]@. In this paper, we propose a method for improving the performance of multilingual NLP models when transferring between languages with different scripts. We use a method called script-aware subword segmentation, which is a variant of BPE that is aware of the script of the language. We use this method to learn subword vocabularies for each language, which are then used to initialize the embeddings of the vocabulary of the target language. We evaluate the performance of our method using a variety of multilingual NLP models and show that it is effective for improving the performance of these models.\\nReferences:ID: 1. Title: Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\\nID: 2. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 3. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 4. Title: Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation\\nID: 5. Title: On Romanization for Model Transfer Between Scripts in Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nRecently, multilingual language models have superseded cross-lingual word embeddings, not only because they produce contextualized representations, but also because they can handle the open vocabulary problem through the use of subwords as tokens (Sennrich et al., 2016; @CITE[5]@; ). Multilingual subword vocabularies are simply obtained by learning the subwords on the concatenation of all used languages. Since each subword is assigned to a unique embedding, identical subwords that appear in several languages serve as anchors between languages, providing implicit cross-lingual information (@CITE[1]@; ; @CITE[2]@). Parameter sharing across languages makes subword models particularly suitable for multilingual NLP and machine translation. The number of shared tokens in multilingual vocabularies highly depends on the similarities of script between languages. When this is not the case, transliteration can be applied (@CITE[4]@; Müller et al., 2020; @CITE[3]@). In addition, shared subword vocabularies often produce inconsistent segmentations across languages that can hurt cross-lingual transfer. Regularization techniques that introduce randomness in the tokenization process (; Provilkov et al., 2020) can partially address this problem, or consistency between the different segmentations can be otherwise enforced (Wang et al., 2021). Still, there is no guarantee that shared (sub)words have identical meanings (false positives are not excluded) and, conversely, subwords with identical meanings but different spellings (false negatives) are missed.\\nReferences:ID: 1. Title: Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\\nID: 2. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 3. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 4. Title: Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation\\nID: 5. Title: On Romanization for Model Transfer Between Scripts in Neural Machine Translation<|eot_id|>'},\n",
       " 969: {'predict': 'Action classification is a fundamental task in video analysis. Early methods (@CITE[2]@; @CITE[1]@; @CITE[4]@; ; ) extract features from raw video frames and classify them into predefined action categories. With the development of deep learning, 2D convolutional neural networks (CNNs) are widely used for feature extraction from video frames. However, these methods are not able to capture the temporal information in videos. To address this, 3D CNNs (@CITE[2]@; @CITE[1]@; ) are proposed to extract spatiotemporal features from video frames. Recently, two-stream methods (; @CITE[3]@; ; @CITE[4]@) have achieved state-of-the-art performance in action classification. These methods extract features from both RGB and optical flow videos, and fuse them using a late fusion strategy. Inspired by the success of two-stream methods, we propose a novel three-stream network that extracts features from RGB, optical flow, and motion boundary videos, and fuses them using a late fusion strategy.\\nReferences:ID: 1. Title: Learning Spatiotemporal Features with 3D Convolutional Networks\\nID: 2. Title: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\\nID: 3. Title: A Better Baseline for AVA\\nID: 4. Title: SlowFast Networks for Video Recognition',\n",
       "  'ground_truth': '\\n\\nAction Classiﬁcation is a task of recognizing ac- tions, which are composed of interactions between actors and objects. Therefore, this task has much in common with video QA, in that the model should perform spatio-temporal reasoning. For better spatio-temporal reasoning, @CITE[2]@ introduced C3D, which extends the 2D CNN ﬁlters to the temporal dimension. @CITE[3]@ proposed I3D, which integrates 3D convo- lutions into a state-of-the-art 2D CNN architecture, which now acts as a baseline in action classiﬁcation tasks (Murray et al., 2012; @CITE[4]@. @CITE[1]@ introduced SlowFast, a network which encodes images in two streams with different frame rates and temporal resolutions of convolution. This study based on a two-stream architecture inspired us in terms of assigning dif- ferent inputs to each encoder module. However, our method differs from the former studies in two aspects: (1) we utilize language features as well as vision features, and (2) we expand the two-stream structure to solve more than motion-oriented tasks.\\nReferences:ID: 1. Title: Learning Spatiotemporal Features with 3D Convolutional Networks\\nID: 2. Title: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\\nID: 3. Title: A Better Baseline for AVA\\nID: 4. Title: SlowFast Networks for Video Recognition<|eot_id|>'},\n",
       " 1732: {'predict': 'Text classification is one of the most important applications of word embeddings. For example, word embeddings can be used as input features for machine learning classifiers (@CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[5]@; @CITE[2]@). For example, word embeddings can be used as input features for machine learning classifiers (@CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[5]@; @CITE[2]@).\\nReferences:ID: 1. Title: XGBoost: A Scalable Tree Boosting System\\nID: 2. Title: Enriching Word Vectors with Subword Information\\nID: 3. Title: PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks\\nID: 4. Title: Enriching Word Vectors with Subword Information\\nID: 5. Title: PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks',\n",
       "  'ground_truth': '\\n\\nEarly efforts in ATC focused on improving machine learning algorithms such as Na¨ ıve Bayes, kNN, Logistic Regression and SVM ( using a simple bag of (TFIDF weighted) words representation. Even with such simple document representation, the use of methods such as LinearSVM ( and XGBoost (@CITE[3]@ produced high effectiveness with and efficient convergence for large datasets (. recent strategies, meta- features ( and neural networks (NNs) (;, exploit the training data to build more informative document representations. Particularly, strategies based on metafeatures (; extract information from more basic (bag-of-words) features to enhance the feature space by smartly exploiting a document’s neighborhood. Strategies based on NNs enhance word representations (and thus documents) also exploiting the training data. FastText ( and PTE (, for instance, presented high effectiveness in comparison to (costly) deep learning approaches. Considerable advances on deep learning for ATC were achieved by using pre-trained language models with fine-tuning (, mainly when combined with attention mechanisms (@CITE[4]@; and the parallelization benefits of transformers, better exemplified by BERT (. Following BERT’s success, the recent XLNet network (@CITE[5]@ proposes a new autoregressive formulation to improve the exploitation of contextual information. Though effective, the fine-tuning process of methods such as BERT and XLNet still takes substantial computational time, requiring powerful hardware (GPUs) (Sun et al., 2019).\\nReferences:ID: 1. Title: XGBoost: A Scalable Tree Boosting System\\nID: 2. Title: Enriching Word Vectors with Subword Information\\nID: 3. Title: PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks\\nID: 4. Title: Enriching Word Vectors with Subword Information\\nID: 5. Title: PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks<|eot_id|>'},\n",
       " 7143: {'predict': 'Modeling annotator disagreement has been studied in the context of NLI (@CITE[5]@; @CITE[2]@; @CITE[1]@) and has been shown to be beneficial for a variety of tasks (@CITE[3]@; @CITE[4]@; @CITE[1]@). However, most previous work has focused on modeling the distribution of annotator disagreement and has not considered the role of annotator bias. We address this gap by developing a method that models the annotator bias distribution. Our approach is based on the idea of modeling annotator disagreement as a multi-task learning problem. We assume that each annotator is a unique task, and we train a model to predict the output of each annotator. We then combine the predictions of each annotator to form a prediction. The main difference between our approach and previous work is that we use a bias prior distribution to model the annotator bias. This allows us to model the distribution of annotator bias as a prior distribution over the annotator parameters, which we use to regularize the annotator parameters. This is in contrast to previous work, which has modeled the annotator bias distribution as a prior distribution over the annotator outputs. This approach has several advantages. First, it allows us to model the distribution of annotator bias as a prior distribution over the annotator parameters, which is more interpretable than modeling it as a prior distribution over the annotator outputs. Second, it allows us to model the distribution of annotator bias as a prior distribution over the annotator parameters, which is more flexible than modeling it as a prior distribution over the annotator outputs. Finally, it allows us to model the distribution of annotator bias as a prior distribution over the annotator parameters, which is more general than modeling it as a prior distribution over the annotator outputs.\\nReferences:ID: 1. Title: Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection\\nID: 2. Title: Embracing Ambiguity: Shifting the Training Target of NLI Models\\nID: 3. Title: Learning with Different Amounts of Annotation: From Zero to Many Labels\\nID: 4. Title: Jury Learning: Integrating Dissenting Voices into Machine Learning Models\\nID: 5. Title: Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations',\n",
       "  'ground_truth': '\\n\\nResearchers have proposed various approaches for studying datasets with annotator disagreement. Zhang and de @CITE[4]@ propose Artificial Annotators to simulate the uncertainty in the annotation process. @CITE[5]@ apply additional distribution estimation methods such as Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation to capture human judgment distribution. @CITE[2]@ train models directly on the estimated label distribution of the annotators in the NLI. consider annotator disagreement in a more general setting with a mixture of single-label, multi-label, and unlabeled E[CLS]. Transformer. EText EText Figure 1: Concept diagram of how we apply our methods. We add the weighted annotator and annotation embeddings to the embedding of the “[CLS]” token, and feed the updated embedding sequence to the transformer-based models. examples. introduce jury learning to model every annotator in a dataset with Deep and Cross Network (DCN) (@CITE[3]@. They combine the text and annotator ID together with the predicted annotator’s reaction from DCN for classification. In contrast, we propose to explicitly embed annotator and their labels, and we perform a detailed analysis of these two embeddings. @CITE[1]@ employ a common shared learned representation while having different layers for each annotator. Similar to our work, Koco´ n et al. (2021) also develop trainable embeddings for annotators. In contrast, we propose embedding annotators as well as their labels with learnable matrices associated with each. We test our methods on eight datasets sourced from various domains, while Koco´ n et al. (2021) conduct their experiments on four datasets all sourced from Wikipedia.\\nReferences:ID: 1. Title: Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection\\nID: 2. Title: Embracing Ambiguity: Shifting the Training Target of NLI Models\\nID: 3. Title: Learning with Different Amounts of Annotation: From Zero to Many Labels\\nID: 4. Title: Jury Learning: Integrating Dissenting Voices into Machine Learning Models\\nID: 5. Title: Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations<|eot_id|>'},\n",
       " 5815: {'predict': 'Learning with human feedback is an effective way to improve the performance of NLP models. @CITE[2]@ propose to learn to compare two sentences to evaluate the quality of a natural language generation model. @CITE[2]@ train the model with human preference annotation to better imitate human judgment. @CITE[3]@ train a hierarchical network to summarize meeting transcripts with cross-domain pretraining. @CITE[3]@ train a sequence level training algorithm to optimize the metric used at test time. @CITE[4]@ propose to learn the ranking model to evaluate the quality of a dialogue system. @CITE[1]@ propose to learn to compare a pair of generated sentences by fine-tuning BERT, which has been shown to have good natural language understanding ability. @CITE[3]@ propose to learn to rank a pair of generated sentences by training a hierarchical network. @CITE[1]@ propose to learn to rank a pair of generated sentences by training a sequence level training algorithm. However, the above works are not suitable for the training of a generation model. We propose to learn the ranking model to evaluate the quality of a generation model, and then use the ranking model to optimize the generation model.\\nReferences:ID: 1. Title: Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models\\nID: 2. Title: A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining\\nID: 3. Title: A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining\\nID: 4. Title: Sequence Level Training with Recurrent Neural Networks',\n",
       "  'ground_truth': '\\n\\nRecent research has started to explore incorporating human feedback into the training process to achieve human-preferred systems in different tasks such as dialogue generation (@CITE[1]@; @CITE[1]@; @CITE[1]@), story generation (@CITE[4]@), document summarization (Ziegler et al., 2019; ; B¨ et al., 2019) and etc. Our work is most related to previous work which utilizes human feedback to train document summarization models with Reinforcement Learning (RL) (Ziegler et al., 2019; ; B¨ ohm et al., 2019; ), where human ratings/comparisons over summaries are usually used to learn the reward models to serve as the value networks in RL. Despite the effectiveness, it is challenging to directly apply them to conversation summarization, largely due to the complex structures in conversations, which requires subtle reward design. Inspired by these prior work, we introduce two levels of human feedback to guide the dialogue summarization model to generate more human-favored summaries instead of only collecting pairwise-comparing binary global rating annotations, including the (1) Local Feedback which highlights the important conversation structures to summarize and the (2) Global Feedback which consists of different fine-grained dimensions to provide more comprehensive judgments. Our work is also related to using RL to optimize automatic metrics for summarization, such as ROUGE (@CITE[3]@; @CITE[2]@; ; ), while we are directly optimizing human preferences with RL.\\nReferences:ID: 1. Title: Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models\\nID: 2. Title: A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining\\nID: 3. Title: A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining\\nID: 4. Title: Sequence Level Training with Recurrent Neural Networks<|eot_id|>'},\n",
       " 5910: {'predict': 'Text summarization aims to automatically summarize a long piece of text into a concise and meaningful summary. The most common way is to extract salient sentences from the original text (@CITE[4]@; @CITE[6]@; @CITE[6]@; @CITE[5]@; @CITE[5]@. Recent studies have also proposed to generate abstractive summaries (@CITE[1]@; @CITE[2]@; @CITE[3]@; @CITE[7]@; @CITE[7]@. Most of these methods require a large amount of labeled data, which limits their application in low-resource scenarios. In this paper, we focus on improving the zero-shot performance of abstractive summarization.\\nReferences:ID: 1. Title: SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\\nID: 2. Title: Neural Latent Extractive Document Summarization\\nID: 3. Title: Neural Latent Extractive Document Summarization\\nID: 4. Title: HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\\nID: 5. Title: Extractive Summarization as Text Matching\\nID: 6. Title: AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization\\nID: 7. Title: Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation',\n",
       "  'ground_truth': '\\n\\nExtractive summarization is an important sub-topic for text summarization. Early works (@CITE[1]@; Narayan et al., 2018; @CITE[5]@; @CITE[5]@ formulated it as a sentence binary classification problem and further extend it with different techniques. With the development of the pretrained model, using a transformer-based pretrained model as encoder (Liu and Lapata, 2019; ; @CITE[7]@ leads to a huge improvement in the task. Recently, MATCHSUM (@CITE[4]@ has achieved a state-of-the-art performance by combining contrastive learning with extractive summarization. These models mainly focus on improving the performance on a certain dataset or domain. Research on low-resource text summarization is also increasing. AdaptSum (@CITE[6]@ propose a pre-train and then fine-tune strategy for low-resource domain adaptation for abstractive summarization. Other researchers (@CITE[2]@ present a similar idea but further enhance it with a data augmentation method using the large corpus from Wikipedia. (@CITE[3]@ combines domain words and a prompt-based language model to achieve zero-shot domain adaption in dialogue abstractive summarization. In this work, we aim to explore the low-resource extractive summarization by disentangling context and pattern information.\\nReferences:ID: 1. Title: SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\\nID: 2. Title: Neural Latent Extractive Document Summarization\\nID: 3. Title: Neural Latent Extractive Document Summarization\\nID: 4. Title: HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\\nID: 5. Title: Extractive Summarization as Text Matching\\nID: 6. Title: AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization\\nID: 7. Title: Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation<|eot_id|>'},\n",
       " 303: {'predict': 'Cross-lingual task-oriented dialog systems have been studied in recent years. Some works (@CITE[1]@; @CITE[5]@; @CITE[6]@; @CITE[3]@; @CITE[4]@) focus on building cross-lingual dialogue state trackers by leveraging the source language’s annotated data. They usually use cross-lingual word embeddings to align the representations of the source language and the target language. However, these methods are limited by the quality of cross-lingual word embeddings. Other works (@CITE[2]@; @CITE[6]@; @CITE[3]@) focus on building cross-lingual dialogue systems with only a few parallel data. They use bilingual dictionaries to align the slot names, and use parallel utterances to align the word embeddings. However, these methods are limited by the quality of bilingual dictionaries and parallel utterances. Different from these works, we propose a zero-shot cross-lingual dialogue system with only a few parallel slot names. We use the parallel slot names to align the slot names, and use the parallel slot names to align the word embeddings. Our method is more practical and can be applied to many languages without parallel utterances.\\nReferences:ID: 1. Title: Towards Zero-Shot Frame Semantic Parsing for Domain Scaling\\nID: 2. Title: Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\\nID: 3. Title: XL-NBT: A Cross-lingual Neural Belief Tracking Framework\\nID: 4. Title: Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables\\nID: 5. Title: Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog\\nID: 6. Title: XL-NBT: A Cross-lingual Neural Belief Tracking Framework',\n",
       "  'ground_truth': '\\n\\nDeploying task-oriented dialogue systems in low-resource domains (@CITE[1]@; @CITE[3]@; or languages (@CITE[2]@; @CITE[5]@, where the number of training of samples is limited, is a challenging task. C et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. @CITE[4]@ introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (@CITE[2]@ to align the representations across languages. @CITE[2]@ proposed a teacher-student framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state tracking. Instead of highly relying on extensive bilingual resources, Qin et al. (2020) introduced a data augmentation framework to generate multilingual code-switching data for cross-lingual tasks including the SLU task. @CITE[6]@ leveraged a mixed language training framework for cross-lingual task-oriented dialogue systems. @CITE[5]@ proposed to refine the cross-lingual word embeddings by using very few word pairs, and introduced a latent variable model to improve the robustness of zero-shot cross-lingual SLU. Nevertheless, there still exists improvement space for the cross-lingual alignment. In this paper, we propose to further align the cross-lingual representations so as to boost the performance of cross-lingual SLU systems.\\nReferences:ID: 1. Title: Towards Zero-Shot Frame Semantic Parsing for Domain Scaling\\nID: 2. Title: Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\\nID: 3. Title: XL-NBT: A Cross-lingual Neural Belief Tracking Framework\\nID: 4. Title: Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables\\nID: 5. Title: Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog\\nID: 6. Title: XL-NBT: A Cross-lingual Neural Belief Tracking Framework<|eot_id|>'},\n",
       " 1633: {'predict': 'Recently, neural networks have been widely used for NER. Most of these approaches are based on recurrent neural networks (RNNs), which have been shown to be effective for modeling sequential data (@CITE[3]@; ; @CITE[1]@; @CITE[4]@; @CITE[2]@. In addition, many neural approaches have been proposed to incorporate additional information such as word embeddings (@CITE[3]@; @CITE[2]@, gazetteers (, and external knowledge (@CITE[5]@.\\nReferences:ID: 1. Title: Named Entity Recognition with Bidirectional LSTM-CNNs\\nID: 2. Title: Fast and Accurate Entity Recognition with Iterated Dilated Convolutions\\nID: 3. Title: Neural Architectures for Named Entity Recognition\\nID: 4. Title: Design Challenges and Misconceptions in Neural Sequence Labeling\\nID: 5. Title: GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling',\n",
       "  'ground_truth': '\\n\\nNeural methods have given competitive performance in NER. Some methods (@CITE[5]@; ) treat NER as a local classification problem at each input token, while other methods use CRF (@CITE[5]@) or a sequence-to-sequence framework (; @CITE[5]@). and use a label attention network and Bayesian neural networks, respectively. Yamada et al. (2020) use entity-aware pre-training and obtain state-of-the-art results on NER. These approaches are similar to ours in the sense that parameters can be tuned in supervised learning, but unlike our method, they are designed for prescribed named entity types, which makes their domain adaptation costly with new few-shot entity types. Our work is motivated by distance-based few-shot NER, which aims to minimize domain-adaptation cost.\\nReferences:ID: 1. Title: Named Entity Recognition with Bidirectional LSTM-CNNs\\nID: 2. Title: Fast and Accurate Entity Recognition with Iterated Dilated Convolutions\\nID: 3. Title: Neural Architectures for Named Entity Recognition\\nID: 4. Title: Design Challenges and Misconceptions in Neural Sequence Labeling\\nID: 5. Title: GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling<|eot_id|>'},\n",
       " 2653: {'predict': 'Sentence-level RE aims to classify the relation between two entities in a sentence. Existing methods can be categorized into two classes: (1) extractive methods, which extract the relation spans from the sentence, and (2) non-extractive methods, which predict the relation directly. Extractive methods have been widely used in RE tasks, including SRL (@CITE[3]@), EE (@CITE[1]@; @CITE[1]@) and other tasks. For example, OneEE (@CITE[1]@) and OneER (@CITE[1]@) are designed to extract overlapped and nested entities from sentences. Non-extractive methods are more straightforward and have achieved great success in RE tasks. For example, CR-CNN (@CITE[4]@) uses convolutional neural networks to classify the relation between two entities. BERT (@CITE[2]@) and its variants have also achieved great success in RE tasks. However, existing methods for RE tasks are mostly designed for flat relations, while our task is focused on nested relations.\\nReferences:ID: 1. Title: Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus\\nID: 2. Title: OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction\\nID: 3. Title: OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction\\nID: 4. Title: Classifying Relations by Ranking with Convolutional Neural Networks',\n",
       "  'ground_truth': '\\n\\nRelation Extraction is one of the key tasks of information extraction community (@CITE[4]@; @CITE[3]@, 2021b,a; @CITE[2]@; @CITE[2]@. Sentence-level RE aims at identifying the relationship between two entities in a sentence and many efforts have been devoted to this problem. exploit a convolutional deep neural network to extract lexical and sentence-level features. dos @CITE[1]@ tackle the sentence-level RE task by using a convolutional neural network that performs classification by ranking. present a novel model that strengthen the ability to classifying directions of relationships between entities. @CITE[4]@ propose an extension of graph convolutional networks (, 2020) and applied a novel pruning strategy to incorporate relevant information while removing irrelevant content.\\nReferences:ID: 1. Title: Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus\\nID: 2. Title: OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction\\nID: 3. Title: OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction\\nID: 4. Title: Classifying Relations by Ranking with Convolutional Neural Networks<|eot_id|>'},\n",
       " 7338: {'predict': 'Knowledge-grounded dialog (KGD) is a challenging task in which the dialog system is expected to ground the response to external knowledge. Existing KGD methods can be divided into two categories: retrieval-based methods and generative methods. Retrieval-based methods retrieve relevant knowledge from external knowledge bases (KBs) and then generate a response based on the retrieved knowledge (@CITE[1]@; @CITE[4]@; @CITE[2]@; @CITE[6]@. Generative methods generate knowledge first and then generate a response based on the generated knowledge (@CITE[5]@; @CITE[3]@; @CITE[1]@. Our method is a generative method and different from the existing generative methods. Existing generative methods generate knowledge based on the dialog context and then generate a response based on the generated knowledge. In contrast, our method first generates knowledge based on the given knowledge and then generates a response based on the generated knowledge.\\nReferences:ID: 1. Title: Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition\\nID: 2. Title: Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion\\nID: 3. Title: LaMDA: Language Models for Dialog Applications\\nID: 4. Title: Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition\\nID: 5. Title: Explicit Knowledge Graph Reasoning for Conversational Recommendation\\nID: 6. Title: Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'ground_truth': '\\n\\nKnowledge-grounded response generation in dialogue systems has been explored for years in both task-oriented (Madotto et al., 2018; @CITE[4]@ and open-domain systems (; @CITE[6]@; @CITE[3]@; Shuster et al., 2022b). However, it is important to note that most existing approaches focus on passively responding to user queries rather than proactively conveying the knowledge. A study by @CITE[4]@ proposes a teacher-bot that transmits information in passages to students, aiming to help them acquire new knowledge through conversation. While their work focuses solely on the educational objective, our work combines it with CRS, suggesting suitable products while educating users about the relevant domain. Some CRS have incorporated knowledge graphs (KGs) as complementary resources for analyzing item- and attribute-level user preferences (@CITE[1]@; Zhou et al., 2021;. The primary objective of KGs is to enhance recommendations or aid preference elicitation rather than assisting sellers in answering inquiries or proactively conveying knowledge. Other works (@CITE[2]@; @CITE[5]@ focus on augmenting LLMs with external knowledge to address common pitfalls like hallucinations and lack of up-to-date information. This can greatly benefit our goal; thus, we leverage and augment LLMs with external tools to build conversational agents in this work.\\nReferences:ID: 1. Title: Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition\\nID: 2. Title: Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion\\nID: 3. Title: LaMDA: Language Models for Dialog Applications\\nID: 4. Title: Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition\\nID: 5. Title: Explicit Knowledge Graph Reasoning for Conversational Recommendation\\nID: 6. Title: Toolformer: Language Models Can Teach Themselves to Use Tools<|eot_id|>'},\n",
       " 844: {'predict': 'The most commonly used stance detection approaches are the neural models, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformer-based models. Early approaches were based on RNNs (@CITE[10]@; ; @CITE[5]@; @CITE[11]@; @CITE[8]@; ; ; @CITE[7]@, 2018). Later, CNNs were used for stance detection in tweets (@CITE[1]@; @CITE[4]@). Transformer-based models were used for stance detection in tweets (@CITE[2]@; @CITE[3]@; @CITE[6]@; @CITE[9]@; ; ; ; @CITE[10]@; @CITE[7]@. The transformer-based models were fine-tuned on the stance detection task, and the fine-tuning was performed on the stance detection datasets, such as SemEval-2016 (@CITE[1]@), FNC-1 (@CITE[9]@), and STANCE (@CITE[6]@). The transformer-based models were also fine-tuned on the stance and sentiment detection task, and the fine-tuning was performed on the dataset that contains both stance and sentiment labels, such as SemEval-2016 (@CITE[6]@), FNC-1 (@CITE[3]@), and STANCE (@CITE[11]@). The transformer-based models were also fine-tuned on the stance and emotion detection task, and the fine-tuning was performed on the dataset that contains both stance and emotion labels, such as STANCE (@CITE[11]@), STANCE-2 (, and ProCon (@CITE[7]@). The transformer-based models were also used for stance detection in long discussions of contemporary issues, and the fine-tuning was performed on the dataset that contains stance labels, such as STANCE (@CITE[11]@), STANCE-2 (, and ProCon (@CITE[7]@). The transformer-based models were also used for stance detection in news articles, and the fine-tuning was performed on the dataset that contains stance labels, such as STANCE (@CITE[11]@), STANCE-2 (, and ProCon (.\\nReferences:ID: 1. Title: MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection\\nID: 2. Title: DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs\\nID: 3. Title: Stance Detection with Bidirectional Conditional Encoding\\nID: 4. Title: Attention Is All You Need\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Transfer Learning from Transformers to Fake News Challenge Stance Detection (FNC-1) Task\\nID: 7. Title: Stance Detection in Web and Social Media: A Comparative Study\\nID: 8. Title: Stance Detection in Web and Social Media: A Comparative Study\\nID: 9. Title: Stance and Sentiment in Tweets\\nID: 10. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 11. Title: Stance Prediction for Contemporary Issues: Data and Experiments',\n",
       "  'ground_truth': '\\n\\nIn terms of approaches used for stance detection, strong baseline results based on support vector machines (SVM) with manually engineered features were provided for the SemEval2016 Task 6 by Mohammad et al. (2016, 2017). Deep learning approaches used in SemEval2016 Task 6 included recurrent neural networks (RNNs) (@CITE[3]@) and convolutional neural networks (CNNs) (; @CITE[11]@). Such approaches used the tweets as input but did not use any target-specific information and did not outperform the SVM baselines. Later approaches were provided with both target and tweet representations as input and employed RNNs and/or CNNs, together with the attention mechanism (@CITE[9]@; ; ; ; @CITE[5]@) to improve the performance of the SVM baselines. Given the dominance of transformers (@CITE[4]@), especially bidirectional encoder representations from transformers (BERT) (@CITE[2]@), in NLP tasks, some recent works (; @CITE[1]@; @CITE[10]@) have focused on investigating the use of BERT models for stance detection. For example, @CITE[10]@ explored the reproducibility of approaches for stance detection and compared them to BERT. They found BERT to be the best model overall for stance detection on the SemEval2016 Task 6. @CITE[1]@ also explored BERT-based models with data augmentation and found BERT to be a powerful model for stance detection. Thus, we have selected BERT as a strong baseline for our paper. Several works have shown that auxiliary information, such as sentiment and emotion information, or the subjective/objective nature of a text (provided as additional inputs or presented in the form of auxiliary tasks in a multi-task framework), can help improve the performance obtained from the tweet/target information alone (; @CITE[7]@; @CITE[2]@; ; @CITE[8]@). Other approaches to improve performance, especially when the amount of labeled data for the task of interest is small, include weak supervision (@CITE[2]@) and knowledge distillation (); transfer learning through distant supervision (@CITE[3]@) or pretrained models (Ebner et al., 2019; ); and domain adaptation from a source task to the target task (@CITE[6]@, 2020). In particular, the Dual-view Adaptation Network (DAN) (@CITE[8]@) learns to predict the stance of a tweet by combining the subjective and objective views/representations of the tweet while also learning to adapt them across domains. We use an adaptation of the DAN model as a strong baseline in this work. Most relevant to our work on COVID-19-Stance, compared a supervised in-domain BERT model trained and tested on “lockdown” tweets with cross-domain models and knowledge distillation variants. The results showed significantly improved performance for the knowledge distillation variants and emphasized the importance of having a small amount of data for the task of interest (as a better alternative to zero-shot learning). Similar to Miao et al. (2020), we also use BERT together with knowledge distillation/self-training as a strong baseline.\\nReferences:ID: 1. Title: MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection\\nID: 2. Title: DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs\\nID: 3. Title: Stance Detection with Bidirectional Conditional Encoding\\nID: 4. Title: Attention Is All You Need\\nID: 5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 6. Title: Transfer Learning from Transformers to Fake News Challenge Stance Detection (FNC-1) Task\\nID: 7. Title: Stance Detection in Web and Social Media: A Comparative Study\\nID: 8. Title: Stance Detection in Web and Social Media: A Comparative Study\\nID: 9. Title: Stance and Sentiment in Tweets\\nID: 10. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 11. Title: Stance Prediction for Contemporary Issues: Data and Experiments<|eot_id|>'},\n",
       " 2003: {'predict': 'Recently, there are several studies that try to integrate fine-grained semantic information into sentence representations. For example, @CITE[5]@ use a pre-trained word embedding as a semantic feature to enhance the sentence representation. @CITE[2]@ and @CITE[3]@ use semantic role labeling (SRL) to enrich sentence representation. @CITE[4]@ and @CITE[7]@ use semantic relation between entities to enhance the sentence representation. @CITE[6]@ use dependency parsing to enrich sentence representation. However, these methods are mostly based on hand-crafted features, which are not flexible and are hard to be applied to various tasks. Our work is different from previous studies in that we use a pre-trained model to generate fine-grained semantic information and integrate it into sentence representation.\\nReferences:ID: 1. Title: A large annotated corpus for learning natural language inference\\nID: 2. Title: The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations\\nID: 3. Title: SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation\\nID: 4. Title: Distributed Representations of Sentences and Documents\\nID: 5. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 6. Title: A Structured Self-attentive Sentence Embedding\\nID: 7. Title: Semantics-aware BERT for Language Understanding',\n",
       "  'ground_truth': '\\n\\nDifferent formats of downstream tasks require sentence-level representations, such as natural language inference (@CITE[7]@; @CITE[4]@), semantic textual similarity (@CITE[6]@), and sentiment classification (@CITE[1]@). Besides directly pre-training the representation of coarser granularity (@CITE[2]@; ), a lot of methods have been explored to obtain a task-specific sentence-level representation by integrating fine-grained token-level representations (@CITE[3]@). Kim (2014) shows that by applying a convolutional neural network (CNN) on top of pretrained word vectors, we can get a sentence-level representation that is well-adapted to classification tasks. @CITE[5]@ leverage a self-attentive module over hidden states of a BiLSTM to generate sentence-level representations. use a CNN layer to extract word-level representations form sub-word representations and combine them with word-level semantic role representations. Inspired by these methods, after a series of preliminary attempts, we choose a hierarchical CNN architecture to recombine fine-grained representations to coarse-grained ones.\\nReferences:ID: 1. Title: A large annotated corpus for learning natural language inference\\nID: 2. Title: The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations\\nID: 3. Title: SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation\\nID: 4. Title: Distributed Representations of Sentences and Documents\\nID: 5. Title: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\\nID: 6. Title: A Structured Self-attentive Sentence Embedding\\nID: 7. Title: Semantics-aware BERT for Language Understanding<|eot_id|>'},\n",
       " 5775: {'predict': 'Quantization is a technique that reduces the memory usage of a model by reducing the precision of the model weights and activations. Quantization methods can be roughly classified into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ methods quantize the model weights and activations directly, while QAT methods quantize the model weights and activations during the training process. In PTQ, @CITE[6]@ quantizes the model weights using quantization-aware activation statistics, and @CITE[4]@ proposes a method to quantize the model weights using Hessian information. In QAT, @CITE[3]@ quantizes the model weights and activations during the training process using stochastic rounding, while @CITE[1]@ quantizes the model weights using quantization-aware training with Hessian information. @CITE[2]@ proposes a method to quantize the model weights using quantization-aware training with weight grouping. @CITE[7]@ proposes a method to quantize the model weights and activations during the training process using quantization-aware training with weight binarization. @CITE[5]@ proposes a method to quantize the model weights and activations during the training process using quantization-aware training with weight ternarization. However, these methods do not consider the quantization error of the model activations, which can cause the loss of the model performance. In this paper, we propose a quantization error compensation method to reduce the loss of the model performance caused by the quantization error of the model activations.\\nReferences:ID: 1. Title: A Survey of Quantization Methods for Efficient Neural Network Inference\\nID: 2. Title: GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference\\nID: 3. Title: Extreme Compression for Pre-trained Transformers Made Simple and Efficient\\nID: 4. Title: A Survey of Quantization Methods for Efficient Neural Network Inference\\nID: 5. Title: Deep Learning with Limited Numerical Precision\\nID: 6. Title: Q8BERT: Quantized 8Bit BERT\\nID: 7. Title: Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\\nID: 8. Title: BinaryBERT: Pushing the Limit of BERT Quantization',\n",
       "  'ground_truth': '\\n\\nQuantization, which represents the weights and activations of neural networks with low-bit precision, has been widely studied in computer vision and natural language processing (NLP) communities (@CITE[6]@). Recently, some researchers attempt to compress PLMs to reduce the deployment cost with quantization methods (@CITE[7]@; @CITE[4]@; @CITE[6]@; Bondarenko et al., 2021). Quantization-aware training (QAT) () is a representative approach to quantize a PLM while retaining most of its performance on downstream tasks. Given a downstream task, QAT performs the quantization during the task-specific training (i.e., fine-tuning) process. For example, Q8BERT (@CITE[1]@) and Q-BERT (@CITE[5]@) are typical QAT methods to compress BERT-based models. Unlike QAT, Post-training quantization (PTQ) disentangles the fine-tuning and quantization. The quantization procedure is conducted after the task-specific fine-tuning. In comparison to QAT, PTQ holds the advantages of flexibility and good compatibility. @CITE[2]@ combines PTQ with knowledge distillation to achieve efficient compression for large PLMs. In addition to NLP scenarios, PTQ is also utilized to compress vision transformers (@CITE[3]@). Some very recent researches employ quantization and parameter-efficient fine-tuning jointly. Qadapter (@CITE[8]@) introduces a lightweight module to produce quantization-friendly activations by scaling them channel-wise. AlphaTuning (Kwon et al., 2022) utilizes binary-coding-quantization (BCQ) by only updating scaling factors.\\nReferences:ID: 1. Title: A Survey of Quantization Methods for Efficient Neural Network Inference\\nID: 2. Title: GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference\\nID: 3. Title: Extreme Compression for Pre-trained Transformers Made Simple and Efficient\\nID: 4. Title: A Survey of Quantization Methods for Efficient Neural Network Inference\\nID: 5. Title: Deep Learning with Limited Numerical Precision\\nID: 6. Title: Q8BERT: Quantized 8Bit BERT\\nID: 7. Title: Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\\nID: 8. Title: BinaryBERT: Pushing the Limit of BERT Quantization<|eot_id|>'},\n",
       " 461: {'predict': 'The majority of the existing work on dialog generation can be divided into two categories: (1) task-oriented dialog generation and (2) open-domain dialog generation. The task-oriented dialog generation aims to provide the user with the information they need, such as flight information or restaurant reservation. This kind of dialog is generally restricted to a specific domain, and the user’s request can be classified into a few categories. The open-domain dialog generation, also known as chatbots, aims to have a natural conversation with the user. This kind of dialog is unrestricted to a specific domain and the user’s request is more general. For example, the user may ask the chatbot to recommend movies, or talk about the weather. The majority of the existing work on dialog generation focuses on the task-oriented dialog generation. The task-oriented dialog generation can be divided into two categories: (1) knowledge-grounded dialog generation, and (2) task-oriented dialog generation without knowledge. The knowledge-grounded dialog generation aims to provide the user with the information they need, such as flight information or restaurant reservation. This kind of dialog is generally restricted to a specific domain, and the user’s request can be classified into a few categories. The task-oriented dialog generation without knowledge aims to provide the user with the information they need, such as flight information or restaurant reservation. This kind of dialog is generally unrestricted to a specific domain, and the user’s request is more general. For example, the user may ask the chatbot to recommend movies, or talk about the weather. The majority of the existing work on dialog generation focuses on the task-oriented dialog generation. For example, @CITE[6]@ proposes a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. @CITE[1]@ proposes a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. @CITE[4]@ proposes a Recurrent Neural Network based Encoder-Decoder architecture. @CITE[2]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture. @CITE[3]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus. @CITE[5]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus and the adversarial training. The open-domain dialog generation aims to have a natural conversation with the user. This kind of dialog is unrestricted to a specific domain and the user’s request is more general. The majority of the existing work on open-domain dialog generation focuses on the dialogue generation with the pre-training of the dialogue corpus. For example, @CITE[3]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus. @CITE[5]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus and the adversarial training. @CITE[2]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus and the multi-task learning. @CITE[1]@ proposes a dialogue generation model based on the Transformer encoder-decoder architecture with the pre-training of the dialogue corpus and the reinforcement learning.\\nReferences:ID: 1. Title: Neural Approaches to Conversational AI\\nID: 2. Title: Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking\\nID: 3. Title: Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\\nID: 4. Title: Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\\nID: 5. Title: Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\\nID: 6. Title: Boosting Naturalness of Language in Task-oriented Dialogues via Adversarial Training',\n",
       "  'ground_truth': '\\n\\nVarious dialog systems have been developed (@CITE[1]@, 2020), including task-oriented dialog systems such as Rasa4, Microsoft Bot Framework5, and Conversational Learner6, and chit-chat systems such as XiaoIce (Zhou et al.), DialoGPT (@CITE[4]@, Meena (Adiwardana et al., 2020). In this paper, we focus on task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs (, attention RNNs (Du and Jurcek, 2016), SC-LSTM (@CITE[6]@ and its variants (@CITE[2]@; @CITE[2]@, as well as learning objectives (Zhu et al., 2019; @CITE[5]@; @CITE[3]@. However, they all require large amounts of annotated data to reach satisfactory performance. A more realistic scenario is to require much less labeling and improve the sample efficiency of models.\\nReferences:ID: 1. Title: Neural Approaches to Conversational AI\\nID: 2. Title: Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking\\nID: 3. Title: Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\\nID: 4. Title: Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\\nID: 5. Title: Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks\\nID: 6. Title: Boosting Naturalness of Language in Task-oriented Dialogues via Adversarial Training<|eot_id|>'},\n",
       " 7264: {'predict': 'Multimodal learning has been studied in many areas. For example, @CITE[6]@ proposed an audio-visual language network (AVLnet) that learns audio-visual representations from raw video inputs. @CITE[8]@ proposed a multimodal clustering network that learns a common multimodal embedding space to capture semantic similarities across modalities. @CITE[1]@ proposed a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained vision transformers to audio-visual tasks. @CITE[2]@ proposed a cross-modal attention and language network to learn multi-modal representations from audio-visual data. @CITE[7]@ proposed a QA-cooperative network that transfers knowledge from one agent to another to accomplish video description. @CITE[5]@ proposed a self-supervised approach that learns to perform audio source separation in videos based on natural language queries. @CITE[4]@ proposed a model that represents videos jointly over time -- through a new training objective that learns from audio, subtitles, and video frames. @CITE[9]@ proposed a multi-modal transformer that jointly encodes the different modalities in video, which allows each of them to attend to the others. @CITE[3]@ proposed a multimodal video retrieval network that jointly encodes the different modalities in video and models the temporal information.\\nReferences:ID: 1. Title: AVLnet: Learning Audio-Visual Language Representations from Instructional Videos\\nID: 2. Title: Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos\\nID: 3. Title: Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language\\nID: 4. Title: Vision Transformers are Parameter-Efficient Audio-Visual Learners\\nID: 5. Title: Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language\\nID: 6. Title: Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents\\nID: 7. Title: Language-Guided Audio-Visual Source Separation via Trimodal Consistency\\nID: 8. Title: MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound\\nID: 9. Title: Multi-modal Transformer for Video Retrieval',\n",
       "  'ground_truth': '\\n\\nBy integrating information from multiple modalities, it is expected to explore a sufficient understanding of the scene and reciprocally nurture the development of specific tasks within a single modality. AVLNet (@CITE[1]@) and MCN (@CITE[4]@) utilize audio to enhance text-to-video retrieval. AVCA () proposes to learn multi-modal representations from audio-visual data and exploit textual label embeddings for transferring knowledge from seen classes of videos to unseen classes. Compared to previous works in audio-visual learning, such as sounding object localization (@CITE[8]@; Hu et al., 2020, 2022), and audio-visual event localization (; @CITE[5]@), these works (; @CITE[2]@; @CITE[3]@) have made great progress in integrating the naturally aligned visual and auditory properties of objects and enriching scenes with explicit semantic information by further introducing textual modalities. Besides, there are many works (Akbari et al., 2021; @CITE[6]@; @CITE[7]@) propose to learn multimodal representations from audio, visual and text modalities that can be directly exploited for multiple downstream tasks. Unlike previous works focused on learning single or multi-modal representations, this work delves into the fundamental yet challenging task of spatio-temporal reasoning in scene understanding. Building upon MUSIC-AVQA (@CITE[9]@), our approach leverages textual explicit semantics to integrate audio-visual cues to enhance the study of dynamic and long-term audio-visual scenes.\\nReferences:ID: 1. Title: AVLnet: Learning Audio-Visual Language Representations from Instructional Videos\\nID: 2. Title: Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos\\nID: 3. Title: Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language\\nID: 4. Title: Vision Transformers are Parameter-Efficient Audio-Visual Learners\\nID: 5. Title: Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language\\nID: 6. Title: Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents\\nID: 7. Title: Language-Guided Audio-Visual Source Separation via Trimodal Consistency\\nID: 8. Title: MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound\\nID: 9. Title: Multi-modal Transformer for Video Retrieval<|eot_id|>'},\n",
       " 6439: {'predict': 'Reference-free metrics are proposed to avoid the problems of reference-based metrics. @CITE[5]@ propose a pseudo-reference summary to measure the similarity with the given summary. @CITE[4]@ introduce a centrality-weighted relevance score and a self-referenced redundancy score to evaluate the summary. @CITE[1]@ propose a contrastive learning-based metric which covers both linguistic qualities and semantic informativeness based on BERT. @CITE[6]@ propose a reference-free metric based on question answering. @CITE[3]@ propose a reference-free metric based on the question answering framework (@CITE[2]@). @CITE[1]@ propose a reference-free metric to evaluate the faithfulness of a generated summary given its source document. In this paper, we propose a reference-free metric to evaluate the faithfulness of a generated summary given its source document.\\nReferences:ID: 1. Title: SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization\\nID: 2. Title: A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy\\nID: 3. Title: Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning\\nID: 4. Title: Answers Unite! Unsupervised Metrics for Reinforced Summarization Models\\nID: 5. Title: Product-Aware Answer Generation in E-Commerce Question-Answering\\nID: 6. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization',\n",
       "  'ground_truth': '\\n\\nReference-free metrics aim to evaluate the summary quality without the human-labeled ground truth summary as the reference, and these methods can be categorized into two types: trained model and training-free model. For the training-free methods, SUPERT (@CITE[1]@) first extracts salient sentences from the source document to construct the pseudo reference, then computes the semantic similarity to get the evaluation score. Following SUPERT, @CITE[5]@ propose a centrality-weighted relevance score and a self-referenced redundancy score. While computing the relevance score, the sentences of pseudo reference are weighted by centrality, the importance of each sentence. For the methods which should be trained, LS-Score (@CITE[3]@) is an unsupervised contrastive learning framework consisting of a linguistic quality and a semantic informativeness evaluator. The question-answering paradigm is usually used in evaluating summaries, which evaluates the factual consistency between summary and document with the help of well-trained question-answering models (; @CITE[4]@; @CITE[6]@; @CITE[2]@).\\nReferences:ID: 1. Title: SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization\\nID: 2. Title: A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy\\nID: 3. Title: Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning\\nID: 4. Title: Answers Unite! Unsupervised Metrics for Reinforced Summarization Models\\nID: 5. Title: Product-Aware Answer Generation in E-Commerce Question-Answering\\nID: 6. Title: FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization<|eot_id|>'},\n",
       " 3699: {'predict': 'The SiMT policy is a key component of the SiMT system. The policy is responsible for deciding how to allocate the time for the SiMT model to read the source sentence, which is critical for balancing the translation quality and latency. Existing SiMT policies can be divided into two categories: wait-k policy and adaptive policy. The wait-k policy is to fix the number of words to wait before generating the next output word. @CITE[4]@ proposes a wait-k policy based on the beam search algorithm, and @CITE[2]@ proposes a mixture-of-experts wait-k policy to achieve the best translation quality under arbitrary latency with only one trained model. The adaptive policy is to learn the optimal allocation of the time for the SiMT model to read the source sentence. @CITE[3]@ proposes a reinforcement learning method to learn the adaptive policy, and @CITE[1]@ proposes a MILk attention method to learn the adaptive policy jointly with a neural machine translation model.\\nReferences:ID: 1. Title: Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\nID: 2. Title: Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy\\nID: 3. Title: Learning to Translate in Real-time with Neural Machine Translation\\nID: 4. Title: Monotonic Infinite Lookback Attention for Simultaneous Machine Translation',\n",
       "  'ground_truth': '\\n\\nRecent policies fall into fixed and adaptive. For fixed policy, @CITE[4]@ proposed wait-k policy, which first READ k source tokens and then READ/WRITE one token alternately. Elbayad et al. (2020) proposed an efficient multi-path training for wait-k policy to randomly sample k during training. @CITE[2]@ proposed future-guide training for wait-k policy, which introduces a full-sentence MT to guide training. @CITE[2]@ proposed a char-level wait-k policy. proposed a mixture-of-experts wait-k policy to develop a universal SiMT model. For adaptive policy, @CITE[1]@ trained an agent to decide READ/WRITE via reinforcement learning. @CITE[4]@ proposed MILk, which predicts a Bernoulli variable to determine READ/WRITE. proposed MMA to implement MILk on Transformer. (@CITE[3]@ proposed dual-path SiMT to enhance MMA with dual learning. Zheng et al. (2020) developed adaptive wait-k through a heuristic ensemble of multiple wait-k models. Miao et al. (2021) proposed a generative framework to generate READ/WRITE decisions. proposed Gaussian multi-head attention to decide READ/WRITE based on alignments. Previous policies always treat each token equally when determining READ/WRITE, ignoring the fact that tokens with different amounts of information often play different roles in SiMT policy. Our method aims to develop a more precise SiMT policy by differentiating the importance of various tokens when determining READ/WRITE.\\nReferences:ID: 1. Title: Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\nID: 2. Title: Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy\\nID: 3. Title: Learning to Translate in Real-time with Neural Machine Translation\\nID: 4. Title: Monotonic Infinite Lookback Attention for Simultaneous Machine Translation<|eot_id|>'},\n",
       " 4681: {'predict': 'In recent years, many large-scale speech corpora have been released. Common Voice (@CITE[1]@) is a large-scale multilingual corpus for speech technology research and development. VoxPopuli (@CITE[2]@) is a large-scale multilingual corpus containing 100K hours of unlabelled speech data in 23 languages, 1.8K hours of transcribed speeches in 16 languages, and 5.1K hours of aligned oral interpretations in 5 languages. FLORES-101 (@CITE[3]@) is a multilingual evaluation benchmark consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. To the best of our knowledge, there is no existing large-scale corpus of speech translation, which is the main focus of this paper. Our corpus is based on VoxPopuli, and we provide a detailed description of it in Section 3.\\nReferences:ID: 1. Title: Common Voice: A Massively-Multilingual Speech Corpus\\nID: 2. Title: VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation\\nID: 3. Title: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation',\n",
       "  'ground_truth': '\\n\\nThe Fisher dataset, a collection of approximately 170 hours of telephone conversations in Spanish (@CITE[3]@), is commonly used as training data for Spanish-English S2ST. However, it does not provide parallel English speech. Previous works generate synthesized English speech from English text translations provided by Fisher. Another S2S dataset containing synthesized speech is CVSS, which covers parallel S2ST translations from 21 languages into English. It is derived from Common Voice () and CoVoST 2 (Wang et al., 2021b), and it synthesizes speech from translated texts. The release of the VoxPopuli dataset provides the largest S2S translations in real speech so far (@CITE[1]@). It covers pairwise speech-to-speech translations among 15 languages, and each direction has less than 500 hours of speech. In another initiative named FLEURS, the text-to-text evaluation data of the FLoRes-101 benchmark (@CITE[2]@) was extended to the speech modality. Supporting 102 languages, FLEURS has a larger language coverage than VoxPopuli, but it only contains around 12 hours of speech per language and is intended to be used as N-way parallel test data. In this work, we present SpeechMatrix, a large-scale multilingual speech-to-speech corpus mined from VoxPopuli (@CITE[1]@). It contains speech alignments in 136 language pairs with an average of 1,537-hour source speech per direction.\\nReferences:ID: 1. Title: Common Voice: A Massively-Multilingual Speech Corpus\\nID: 2. Title: VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation\\nID: 3. Title: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation<|eot_id|>'},\n",
       " 4101: {'predict': 'ZS-DR has received increasing attention in recent years. Some works attempt to extend the performance of retrieval models to zero-shot scenarios by using prompt-based methods (@CITE[2]@; ; @CITE[1]@. However, these methods often require a large amount of labeled data, which is not available in many scenarios. Other works focus on the use of pretrained language models (LMs) for ZS-DR, either by fine-tuning LMs on a few examples (@CITE[2]@ or by using LMs as a dense retrieval model (e.g., @CITE[2]@; @CITE[1]@. These approaches are often limited by the performance of the underlying LMs. Our work differs from these approaches as we focus on the use of pretrained LMs as a dense retrieval model and do not rely on any additional data or fine-tuning.\\nReferences:ID: 1. Title: Holistic Evaluation of Language Models\\nID: 2. Title: Holistic Evaluation of Language Models',\n",
       "  'ground_truth': '\\n\\nThe task of zero-shot (dense) retrieval was made empirically prominent to the neural retrieval community by ; their BEIR benchmark encompasses diverse retrieval tasks. The paper and much follow-up research consider the transfer learning setup where the dense retriever is first trained using a diverse and large manually labeled dataset, namely MS MARCO (; @CITE[1]@; @CITE[1]@. However, as stated by @CITE[2]@, such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without any relevance labels. Similar to their work, we also do not assume access to the test corpora during training. This is a more realistic setup and better aligns with emerging zero-shot search needs. By the definition in, our setup is unsupervised. Similar to that work, we also rely on the ability of instruction-following language models to perform search tasks. In the rest of this paper, we do not make a precise distinction between zero-shot and unsupervised, and will use the terms interchangeably to describe our setup: we assume that no test-time query, document or large-scale supervision exists.\\nReferences:ID: 1. Title: Holistic Evaluation of Language Models\\nID: 2. Title: Holistic Evaluation of Language Models<|eot_id|>'},\n",
       " 1655: {'predict': 'Previous work on mortality prediction in critical care has primarily focused on the use of clinical time series data, including physiological signals and lab tests. A number of studies have demonstrated the effectiveness of machine learning models for predicting mortality in critical care, with recent studies demonstrating that LSTMs can outperform traditional machine learning models and hand-crafted features (@CITE[5]@; @CITE[3]@. Additionally, there has been a growing interest in using multimodal data to improve predictive performance, including the use of clinical text and multimodal fusion (@CITE[7]@; @CITE[4]@; @CITE[2]@. However, these studies have primarily focused on the use of clinical text to improve predictive performance, with less work on the use of clinical images. Additionally, these studies have primarily used clinical text and images in isolation, with less work on the use of multimodal data. Our work builds on the use of multimodal data to improve predictive performance, but focuses on the use of multimodal data in combination to improve predictive performance. In particular, we use clinical images and clinical text to improve predictive performance in combination, with the use of clinical images to improve predictive performance in isolation serving as a baseline.\\nReferences:ID: 1. Title: Multitask learning and benchmarking with clinical time series data\\nID: 2. Title: Learning Tasks for Multitask Learning: Heterogenous Patient Populations in the ICU\\nID: 3. Title: Learning to Diagnose with LSTM Recurrent Neural Networks\\nID: 4. Title: Recurrent Neural Networks for Multivariate Time Series with Missing Values\\nID: 5. Title: Multi-Time Attention Networks for Irregularly Sampled Time Series\\nID: 6. Title: Set Functions for Time Series\\nID: 7. Title: BioSentVec: creating sentence embeddings for biomedical texts\\nID: 8. Title: Improving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning',\n",
       "  'ground_truth': '\\n\\nMost of the previous work in mortality prediction in critical care is focused on utilizing patient vital signals as time-series data for making predictions. @CITE[5]@ provided a benchmark and deﬁned 4 tasks based on MIMIC III dataset (@CITE[7]@ for comparing these models. This paper used a recurrent neural network to obtain the predictions. Other models also use recurrent neural networks in different forms to predict outcomes in healthcare data (@CITE[5]@; @CITE[6]@;. Furthermore, others (@CITE[6]@; Shukla and Marlin, 2019; ; @CITE[5]@ used the irregular nature of the data over time in their models. For clinical notes, some models used pretrained embedding models (Zhang et al., 2019; @CITE[8]@, while some use various other machine learning techniques for outcome prediction on data collected from the ICU (; @CITE[7]@;.\\nReferences:ID: 1. Title: Multitask learning and benchmarking with clinical time series data\\nID: 2. Title: Learning Tasks for Multitask Learning: Heterogenous Patient Populations in the ICU\\nID: 3. Title: Learning to Diagnose with LSTM Recurrent Neural Networks\\nID: 4. Title: Recurrent Neural Networks for Multivariate Time Series with Missing Values\\nID: 5. Title: Multi-Time Attention Networks for Irregularly Sampled Time Series\\nID: 6. Title: Set Functions for Time Series\\nID: 7. Title: BioSentVec: creating sentence embeddings for biomedical texts\\nID: 8. Title: Improving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning<|eot_id|>'},\n",
       " 4424: {'predict': 'Recent work on verbalizer design has mainly focused on the prompt-based method. Most of them propose to generate prompts in natural language or learn a verbalizer from a prompt. @CITE[3]@ proposes a prompt-based method that directly generates a prompt from a task name. @CITE[2]@ proposes a pipeline method that automatically generates a prompt from a task name and uses the prompt to guide the model to perform a task. @CITE[5]@ proposes an automated method to create prompts for a diverse set of tasks based on a gradient-guided search. @CITE[1]@ proposes a method to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. @CITE[4]@ proposes to generate prompts using a prompt generator that is trained to generate prompts for a given task. @CITE[1]@ proposes to learn a verbalizer that maps a task name to a prompt. However, these methods are not applicable to the zero-shot setting, as they require training data to learn the verbalizer.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: WARP: Word-level Adversarial ReProgramming',\n",
       "  'ground_truth': '\\n\\nVerbalizers aim to reduce the gap between model outputs and label words, which has always been a critical issue in prompt-based tuning. Most of the current works leverage human written verbalizers (Schick and Schütze, 2021) that prove to be effective to build bridges between them. However, these approaches are highly biased towards lexical semantics of manual verbalizers and require both domain expertise of downstream tasks and understanding of the PLMs’ abilities (@CITE[1]@). @CITE[1]@ and other studies (@CITE[4]@; @CITE[2]@) have designed search-based verbalizers for better verbalizer choices during the training optimization process, intending to reduce the bias caused by personal vocabulary and the cost of intensive human labor. Another line of researches (@CITE[3]@; @CITE[5]@) claims it is hard to find satisfactory label words by searching large vocabulary with few examples and proposes to insert learnable embedding vectors as soft labels/verbalizers optimized during the training process. Nevertheless, the verbalizer design methods for hierarchical labels are less explored in previous works.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: WARP: Word-level Adversarial ReProgramming<|eot_id|>'},\n",
       " 2124: {'predict': \"Question answering (QA) is a fundamental task in natural language processing. Many QA datasets have been introduced to evaluate the performance of different models on this task, such as SQuAD (@CITE[8]@), SQuAD 2.0 (@CITE[8]@), TriviaQA (@CITE[6]@), and Natural Questions (Kwiatkowski et al., 2019). With the development of pre-trained language models, many QA models have been proposed based on these models, such as the pipeline models (Seo et al., 2017; @CITE[7]@), the span-extraction models (Wang et al., 2019), and the generative models (@CITE[5]@; @CITE[4]@; @CITE[3]@; @CITE[2]@). Recently, many works have been done to improve the performance of QA models, including the multi-hop QA models (@CITE[1]@; @CITE[3]@), the multi-document QA models (@CITE[1]@), and the multi-lingual QA models ().\\nReferences:ID: 1. Title: English Machine Reading Comprehension Datasets: A Survey\\nID: 2. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 3. Title: Know What You Don't Know: Unanswerable Questions for SQuAD\\nID: 4. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 5. Title: Simple and Effective Multi-Paragraph Reading Comprehension\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 8. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\",\n",
       "  'ground_truth': \"\\n\\nIf one considers clickbait spoiling as a question answering problem, there are numerous possible solutions. Among the available question-answering benchmarks (@CITE[1]@, we select two to choose appropriate state-of-the-art models for our evaluation: (1) SQuAD ( compiles 107,785 questions and answers based on 536 Wikipedia articles. Although a wide range of questions and answers are included, the vast majority of 93.6% are factual (32% names, 31.8% noun phrases, 19.8% numbers, 5.5% verb phrases, and 3.9% adjective phrases), while the remainder are descriptive (3.7% clauses and 2.7% other). We use SQuAD v1.1, not the v2.0 superset (@CITE[2]@, which contains unanswerable questions, since we do not expect clickbait to be “unspoofable”. (2) TriviaQA (@CITE[3]@ contains 95,000 question–answer pairs, mostly dealing with trivia questions that are supposed to be particularly difficult to answer. These are comparable to clickbait in that many of them address rather trivial things (see Figure 1). The question answering models used in our experiments are ALBERT (@CITE[5]@, AllenAI-Document-QA (@CITE[4]@, BERT (cased/uncased) (@CITE[8]@, Big Bird (Zaheer et al., 2020), DeBERTa (large) (@CITE[7]@, ELECTRA (, Funnel-Transformer (Dai et al., 2020), MPNet (@CITE[8]@, and RoBERTa (base/large) (. Many of them are or were state of the art on the above benchmarks and implement various different architectural paradigms.\\nReferences:ID: 1. Title: English Machine Reading Comprehension Datasets: A Survey\\nID: 2. Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text\\nID: 3. Title: Know What You Don't Know: Unanswerable Questions for SQuAD\\nID: 4. Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\\nID: 5. Title: Simple and Effective Multi-Paragraph Reading Comprehension\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 7. Title: DeBERTa: Decoding-enhanced BERT with Disentangled Attention\\nID: 8. Title: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators<|eot_id|>\"},\n",
       " 6170: {'predict': \"A line of recent work focuses on debiasing NLU models, including debiasing NLI (@CITE[10]@; @CITE[8]@; @CITE[11]@; @CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[5]@; @CITE[6]@; @CITE[9]@; @CITE[7]@. Our work differs from these methods in that we focus on debiasing text classification tasks. We also propose a novel debiasing method that can be applied to any NLU task, which is different from previous methods that require task-specific debiasing strategies. Finally, we introduce a bias-only model to estimate the confidence of the model predictions, which is different from previous methods that only use the predictions of the main model (@CITE[11]@; @CITE[5]@; @CITE[7]@.\\nReferences:ID: 1. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 2. Title: Towards Debiasing Fact Verification Models\\nID: 3. Title: PAWS: Paraphrase Adversaries from Word Scrambling\\nID: 4. Title: Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance\\nID: 5. Title: Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models\\nID: 6. Title: Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis\\nID: 7. Title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\\nID: 8. Title: Feature-Level Debiased Natural Language Understanding\\nID: 9. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them\\nID: 10. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 11. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 12. Title: Uncertainty Calibration for Ensemble-Based Debiasing Methods\",\n",
       "  'ground_truth': \"\\n\\nUnintended dataset biases hinder the generalizability and reliability of NLU models (@CITE[1]@; ; @CITE[5]@. While a wide range of methods have been proposed to tackle this problem, such as knowledge distillation (@CITE[11]@; @CITE[6]@, neural network pruning (Meissner et al., 2022; @CITE[7]@, and counterfactual inference (@CITE[2]@, ensemble-based methods (Clark et al., 2019a; @CITE[4]@; @CITE[9]@ stand out for their impressive empirical success. Recent works extend ensemble-based methods, such as PoE, to mitigate unknown biases by training a weak model to proactively capture the underlying data bias, then learn the residue between the captured biases and original task observations for debiasing (@CITE[10]@; @CITE[12]@; @CITE[8]@. @CITE[3]@ further improves the performance of these methods using a biased model with uncertainty calibration. Nevertheless, most prior works only mitigate unintended biases from top-level logits, ignoring biases in low-level attention.\\nReferences:ID: 1. Title: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\nID: 2. Title: Towards Debiasing Fact Verification Models\\nID: 3. Title: PAWS: Paraphrase Adversaries from Word Scrambling\\nID: 4. Title: Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance\\nID: 5. Title: Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models\\nID: 6. Title: Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis\\nID: 7. Title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\\nID: 8. Title: Feature-Level Debiased Natural Language Understanding\\nID: 9. Title: Learning from others' mistakes: Avoiding dataset biases without modeling them\\nID: 10. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 11. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 12. Title: Uncertainty Calibration for Ensemble-Based Debiasing Methods<|eot_id|>\"},\n",
       " 3438: {'predict': 'The naturalness of translations has been a long-standing issue in MT research. In recent years, the interest in the naturalness of MT has been fueled by the emergence of the post-editing paradigm (@CITE[1]@, 2019), which has been shown to produce translations that are more natural than those of traditional MT systems. In this regard, @CITE[3]@ have found that post-edited translations are more fluent and natural than human translations, while @CITE[2]@ have found that they are less fluent and natural than human translations. The naturalness of MT has also been linked to the lexical and morphological richness of translations (@CITE[3]@;, as well as the degree of interference from the source language (@CITE[1]@. Furthermore, the naturalness of MT has been shown to be related to the performance of the MT system, with systems that produce more natural translations also performing better on human evaluation tasks (@CITE[3]@. In this work, we investigate the naturalness of translations in the context of neural MT, with a focus on the degree of naturalness of the translations produced by different MT systems. In particular, we examine the degree of naturalness of translations produced by different NMT systems, and we investigate the relationship between the degree of naturalness and the performance of the MT system.\\nReferences:ID: 1. Title: Post-editese: an Exacerbated Translationese\\nID: 2. Title: Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation\\nID: 3. Title: APE at Scale and its Implications on MT Evaluation Biases',\n",
       "  'ground_truth': '\\n\\nEvaluation of MT usually focuses on accuracy and/or fluency (@CITE[1]@; Läubli et al., 2020). Recent work has started to look at the richness and complexity of MT output. Vanmassenhove et al. (2019, 2021) address the effects of statistical bias on language generation. They assess lexical diversity and sophistication, and conclude that the translations produced by MT systems are consistently less diverse than the original training data, containing a higher number of frequent patterns while reducing the infrequent ones when compared to original texts. @CITE[3]@ compared MT output with human-generated translations and found that there is a measurable difference between the two. In this work, we use the diversity metrics introduced by to demonstrate that we can build an MT system with lexical diversity similar to human translations (HT). We also incorporate the findings of @CITE[2]@, and show how to reliably evaluate more natural translations on target-original test sets while allowing the model to decrease BLEU scores on source-original test sets.\\nReferences:ID: 1. Title: Post-editese: an Exacerbated Translationese\\nID: 2. Title: Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation\\nID: 3. Title: APE at Scale and its Implications on MT Evaluation Biases<|eot_id|>'},\n",
       " 5862: {'predict': 'LCNMT (, 2019) is a constrained decoding framework that uses a finite state automaton to filter the output tokens. It is based on the monotonic neural machine translation (MNMT) (Liu et al., 2016) framework, which is a type of NMT that generates the output tokens in a left-to-right manner. LCNMT has been shown to be effective in constrained machine translation. @CITE[3]@ proposed Grid Beam Search (GBS), which is a constrained decoding algorithm that uses a grid to store all possible paths of the output tokens. The grid is then updated by a beam search algorithm to find the most likely path. GBS has been shown to be effective in interactive machine translation. @CITE[1]@ proposed a constrained decoding algorithm that uses a lattice to store all possible paths of the output tokens. The lattice is then updated by a beam search algorithm to find the most likely path. @CITE[2]@ proposed a constrained decoding algorithm that uses a finite state automaton to filter the output tokens. The finite state automaton is updated by a beam search algorithm to find the most likely path. In this work, we propose a constrained decoding algorithm that uses a finite state automaton to filter the output tokens. The finite state automaton is updated by a beam search algorithm to find the most likely path. Our algorithm is different from LCNMT in that we use a different algorithm to update the finite state automaton. Our algorithm is also different from GBS in that we use a different data structure to store all possible paths of the output tokens. Our algorithm is also different from in that we use a different algorithm to update the lattice.\\nReferences:ID: 1. Title: Neural Machine Translation Decoding with Terminology Constraints\\nID: 2. Title: Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search\\nID: 3. Title: A Call for Clarity in Reporting BLEU Scores',\n",
       "  'ground_truth': '\\n\\nLCNMT controls the translation output of an NMT model to satisfy some pre-specified lexical constraints. The lexical constraints are usually provided by users or deposit dictionaries covering a wide range of topics and domains, showing great values in practical applications. One line of LCNMT studies focuses on designing constrained decoding algorithm (@CITE[3]@). For example, @CITE[1]@ firstly proposed grid beam search (GBS), which added an additional dimension of the number of constrained lexicons at each decoding step. @CITE[2]@ proposed a dynamically beam allocating (DBA) strategy for constrained decoding, which fixed the beam size and made it unaffected by the number of constrained lexicons. Then, Hu et al. (2019) extended it into vectorized dynamic beam allocation (VDBA) that supports batched decoding. Although these constrained beam search methods have high control over the target constraints, they significantly slow down the decoding speed and tend to reduce the fluency of translation (@CITE[3]@).\\nReferences:ID: 1. Title: Neural Machine Translation Decoding with Terminology Constraints\\nID: 2. Title: Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search\\nID: 3. Title: A Call for Clarity in Reporting BLEU Scores<|eot_id|>'},\n",
       " 6207: {'predict': 'Previous studies have explored document IDs based on T5, which are usually generated by a T5-based retriever (@CITE[2]@; @CITE[4]@; @CITE[2]@. @CITE[2]@ and @CITE[4]@ propose to generate document IDs directly from queries, which are not only easy to be indexed but also can be used as semantic labels to enhance the model’s ability to retrieve relevant documents. @CITE[2]@ propose a three-stage training workflow to capture more knowledge contained in the corpus and associations between queries and document IDs. @CITE[1]@ propose to generate document IDs based on the text of documents, which are easier to be indexed. @CITE[3]@ propose to generate document IDs based on the knowledge of documents, which can be used to retrieve documents from the knowledge base. In this paper, we propose a novel approach to generate document IDs based on the context of documents, which is easier to be indexed and can be used to retrieve documents from the corpus.\\nReferences:ID: 1. Title: A Neural Corpus Indexer for Document Retrieval\\nID: 2. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 3. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 4. Title: A Neural Corpus Indexer for Document Retrieval',\n",
       "  'ground_truth': '\\n\\nThis idea has been further developed in models such as DSI-QG (Zhuang et al., 2022), NCI (@CITE[4]@, and Ultron (@CITE[2]@, which have all effectively improved the retrieval performance of generative retrieval models through the use of pseudo queries. More specifically, DSI- QG (Zhuang et al., 2022) adapted the model for multilingual retrieval tasks, Ultron (@CITE[2]@ attempted to incorporate hyperlink information into document IDs, and NCI (@CITE[4]@ employed richer data, more fine-grained semantic ID mapping, and a novel decoder structure to obtain the best retrieval results. Differentiable indexing models have also been applied to a wide range of tasks such as knowledge-intensive language tasks (, long sequence retrieval (@CITE[3]@ and QA tasks (@CITE[4]@. Additionally, efforts have been made to improve the memory capability ( and decoding capability (@CITE[1]@ of these models. This paper aims to gain a deeper understanding of differentiable indexing models as a retrieval method and proposes a multi-task distillation approach to improve their performance.\\nReferences:ID: 1. Title: A Neural Corpus Indexer for Document Retrieval\\nID: 2. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 3. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer\\nID: 4. Title: A Neural Corpus Indexer for Document Retrieval<|eot_id|>'},\n",
       " 7227: {'predict': 'Self-supervised approaches have gained popularity in recent years, especially for pre-trained language models (@CITE[4]@; @CITE[6]@; @CITE[6]@. They are often used to improve the semantic representation of language models. For example, @CITE[6]@ propose SimCSE, which is a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. @CITE[5]@ find that whitening the sentence representations can enhance the isotropy of the sentence distribution and achieve competitive results. @CITE[3]@ propose a method to generate adversarial examples by perturbing the word embeddings of the model and perform contrastive learning on clean and adversarial examples to teach the model to learn noise-invariant representations. @CITE[5]@ create a word-level adversarial attack generating hard positives on-the-fly as adversarial examples during contrastive learning. @CITE[2]@ propose two novel adversarial training approaches, CARL and RAR, which can effectively improve the robustness of the model. @CITE[1]@ propose an adversarial training framework to generate text-form adversarial samples. However, these methods are not directly applicable to our task because they are designed for text classification and do not consider the problem of overfitting to the adversarial samples.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: Whitening Sentence Representations for Better Semantics and Faster Retrieval\\nID: 3. Title: Improved Text Classification via Contrastive Adversarial Training\\nID: 4. Title: Whitening Sentence Representations for Better Semantics and Faster Retrieval\\nID: 5. Title: Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks\\nID: 6. Title: Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder',\n",
       "  'ground_truth': '\\n\\nMore recently, self-supervised approaches have emerged, employing contrastive objectives to learn effective and robust text representations: SimCSE (@CITE[2]@) introduced a minimal augmentation strategy to predict the input sentence by applying two different dropout masks. The ConSERT model (@CITE[3]@) utilized four distinct data augmentation techniques to generate diverse views for the purpose of executing a contrastive objective: adversarial attacks, token shuffling, cut-off, and dropout. Qiu et al. (2021) introduced two adversarial training methods, CARL and RAR, to strengthen the ML model’s defense against gradient-based adversarial attacks. CARL aims to acquire a resilient representation at the sentence level, whereas RAR focuses on enhancing the robustness of individual word representations. Rima et al. (2022) proposed adversarial training with contrastive learning for training natural language processing models. It involves applying linear perturbations to input embeddings and leveraging contrastive learning to minimize the distance between original and perturbed representations. @CITE[6]@ presents a straightforward approach for regularizing transformer-based encoders during the fine-tuning step. The model achieves noise-invariant representations by generating adversarial examples perturbing word embeddings and leveraging contrastive learning. In comparison to several existing contrastive adversarial learning approaches in the text representation area (@CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[5]@; Rima et al., 2022; @CITE[6]@), our framework stands out by generating more efficient high-risk iterative perturbations in the embedding space. Furthermore, our framework leverages a more powerful contrastive objective approach, leading to high-quality text representations that demonstrate enhanced generalization and robustness properties. Empirical results substantiate the superiority of our approach across various generalization and robustness benchmarks.\\nReferences:ID: 1. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings\\nID: 2. Title: Whitening Sentence Representations for Better Semantics and Faster Retrieval\\nID: 3. Title: Improved Text Classification via Contrastive Adversarial Training\\nID: 4. Title: Whitening Sentence Representations for Better Semantics and Faster Retrieval\\nID: 5. Title: Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks\\nID: 6. Title: Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder<|eot_id|>'},\n",
       " 483: {'predict': 'Knowledge distillation is a popular technique to transfer knowledge from a large teacher model to a smaller student model. In this work, we follow the conventional knowledge distillation framework (@CITE[2]@; @CITE[1]@; @CITE[4]@; @CITE[3]@; @CITE[3]@; ; ), which aims to minimize the difference between the predictions of teacher and student models. The loss function can be written as: L = (1 − α)T (x) - S(x)2 + αLs, where T(x) and S(x) are the predictions of teacher and student models, respectively, and α is the distillation coefficient. The first term is the mean squared error between the predictions of teacher and student models. The second term is the loss function of student model. α is used to balance the two terms. In our work, we use the mean squared error as the loss function of student model. The teacher model is a pre-trained BERT model, and the student model is a BERT-based model with a smaller number of parameters. The teacher and student models share the same architecture, but the student model has a smaller number of parameters. The teacher model is used to train the student model, and the teacher model is fixed during the training process of the student model. The training process of the student model is the same as the training process of the teacher model, except that the student model has a smaller number of parameters.\\nReferences:ID: 1. Title: BAM! Born-Again Multi-Task Networks for Natural Language Understanding\\nID: 2. Title: Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\\nID: 3. Title: Patient Knowledge Distillation for BERT Model Compression\\nID: 4. Title: Scalable Syntax-Aware Language Models Using Knowledge Distillation',\n",
       "  'ground_truth': '\\n\\nOur work is related to knowledge distillation techniques. It has been shown that KD is very effective and scalable for knowledge ensembling (Hinton et al., 2015; Furlanello et al., 2018), and existing methods are divided into two categories: 1) output distillation, which makes a teacher model output logits as a student model training objective (; @CITE[3]@; @CITE[2]@, 2) feature distillation, which allows a student to learn from a teacher’s intermediate feature representations (@CITE[4]@;. In this paper, we enhance the distillation of heterogeneous structures via both output and feature distillations by employing a sequential LSTM as the student. Our work is also closely related to @CITE[1]@, who distill syntactic structure knowledge to a student LSTM model. The difference lies in that they focus on transferring tree knowledge from syntax-aware language model for achieving scalable unsupervised syntax induction, while we aim at integrating heterogeneous syntax for improving downstream tasks.\\nReferences:ID: 1. Title: BAM! Born-Again Multi-Task Networks for Natural Language Understanding\\nID: 2. Title: Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\\nID: 3. Title: Patient Knowledge Distillation for BERT Model Compression\\nID: 4. Title: Scalable Syntax-Aware Language Models Using Knowledge Distillation<|eot_id|>'},\n",
       " 6783: {'predict': 'Our work is related to task decomposition and contrastive learning. For example, @CITE[4]@ decompose NER into two sub-tasks, i.e., span detection and entity typing, and propose a meta-learning framework to learn a good initialization of the span detector and a good embedding space for the entity typ- ing. @CITE[1]@ decompose NER into two sub-tasks, i.e., entity detection and entity classification, and propose a contrastive learning framework to learn a good embedding space for the entity classification. Our work is also related to contrastive learning, e.g., SimCLR (@CITE[3]@, which learns visual representations by contrasting positive and negative pairs. SupCon (@CITE[2]@ is an extension of SimCLR to the supervised setting, which is used in our work to learn the representation of the entity classifier. However, our work is different from these works in that we propose a task decomposition framework to learn a good initialization of the entity detector and a good embedding space for the entity classifier, and we use the learned representations to learn a good initialization of the entity classifier.\\nReferences:ID: 1. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 2. Title: Decomposed Meta-Learning for Few-Shot Named Entity Recognition\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning',\n",
       "  'ground_truth': '\\n\\nRecently, decomposed-based methods have emerged as effective solutions for the NER problem (@CITE[4]@; @CITE[2]@; ; @CITE[1]@;. These methods can learn entity boundary information well in data-limited scenarios and often get better results. However, the widely used prototypical networks in these methods may encounter inaccurate and unstable prototypes given limited support samples at the type classification stage. Besides, they may face the problem of over-detected false spans produced at the span detection stage. Our method can address these two issues via the proposed type-aware contrastive learning and type-aware span filtering strategies. Our method is also inspired by contrastive learning (@CITE[3]@;. Due to its good generalization performance, two recent methods (; @CITE[1]@ borrow this idea for few-shot NER, which construct contrastive loss between tokens or between the token and the prompt. However, they are both the end-to-end approach and thus have the inherent drawback that cannot learn good entity boundary information. In contrast, our method is a decomposed one and our contrastive loss is constructed between tokens with additional type name information, which can find accurate boundary and learn a type-aware feature space.\\nReferences:ID: 1. Title: CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\\nID: 2. Title: Decomposed Meta-Learning for Few-Shot Named Entity Recognition\\nID: 3. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 4. Title: Supervised Contrastive Learning<|eot_id|>'},\n",
       " 4127: {'predict': 'Continual Learning (CL) is a fundamental problem in lifelong learning, which enables models to learn new knowledge without forgetting the previously learned knowledge. CL has been studied in various fields, including computer vision (@CITE[4]@; @CITE[2]@, natural language processing (, and reinforcement learning (@CITE[3]@; @CITE[1]@. The CL problem is closely related to catastrophic forgetting, which occurs when a model forgets previously learned knowledge during the training process. To alleviate catastrophic forgetting, a number of approaches have been proposed, including rehearsal-based methods (, regularization-based methods (@CITE[2]@; @CITE[4]@, and parameter-efficient methods (@CITE[1]@. In the context of lifelong relation extraction, previous studies have attempted to alleviate catastrophic forgetting by utilizing rehearsal-based methods (@CITE[1]@, regularization-based methods (@CITE[2]@, and parameter-efficient methods (@CITE[3]@.\\nReferences:ID: 1. Title: Overcoming catastrophic forgetting in neural networks\\nID: 2. Title: Supermasks in Superposition\\nID: 3. Title: Experience Replay for Continual Learning\\nID: 4. Title: Sentence Embedding Alignment for Lifelong Relation Extraction',\n",
       "  'ground_truth': '\\n\\nContinual Learning, also known as lifelong learning and incremental learning, is the process of training models to perform well on task streams with different data distributions. Its main methods can be divided into three categories: (i) regularization-based methods (@CITE[1]@; Zenke et al., 2017); (ii) architecture-based methods (Fernando et al., 2017; ; and (iii) memory-based methods (;. Memory-based methods, also called rehearsal-based methods, have been shown to be particularly suitable for natural language processing (NLP), especially for CRE (@CITE[4]@; ; @CITE[2]@; Zhao et al., 2022). These methods selectively store old samples in limited memory, training them with new samples to prevent catastrophic forgetting. For example, EMAR ( utilizes relation prototypes for memory reconsolidation exercises. RP-CRE (@CITE[2]@ uses prototypes of all observed relations to refine subsequent sample embeddings. CRECL (@CITE[3]@ builds a classification network and a prototypical contrastive network. However, these methods only compute temporary prototypes before training a new task and store memory samples, which can lead to significant distortion and forgetting as the number of tasks increases. In contrast, our approach stores prototypes in memory without modification.\\nReferences:ID: 1. Title: Overcoming catastrophic forgetting in neural networks\\nID: 2. Title: Supermasks in Superposition\\nID: 3. Title: Experience Replay for Continual Learning\\nID: 4. Title: Sentence Embedding Alignment for Lifelong Relation Extraction<|eot_id|>'},\n",
       " 2600: {'predict': 'The language model has been the backbone of many NLP tasks. @CITE[3]@ presents the first transformer-based language model, which has been widely adopted in many NLP tasks. @CITE[2]@ presents the first text-to-text transformer model, which can be pre-trained in a unified way. The pre-trained language model can be fine-tuned for downstream tasks. @CITE[1]@ introduces the prefix-tuning method, which optimizes a small continuous vector to steer the generation of the language model. @CITE[4]@ proposes a survey of prompt-based learning, which is a new paradigm in NLP. @CITE[5]@ proposes a method to train the prefixes for controllable generation. In this work, we propose a prompt-based method to steer the generation of the language model to generate clinical trials.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision\\nID: 3. Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation\\nID: 4. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 5. Title: Controllable Natural Language Generation with Contrastive Prefixes',\n",
       "  'ground_truth': '\\n\\nLMs are often used for text generation tasks attributed to their auto-regressive nature, e.g., T5 (@CITE[5]@ and BART (@CITE[1]@. Nonetheless, they cannot be directly applied to EHRs generation since EHRs consist of not only plain clinical notes but also longitudinal sequences of events. Although there were works on encoding and generating medical texts by LMs (; ; Kagawa et al., 2021;, none has been done for synthetic EHRs generation. Prompt learning was used to control the topic of text generation (@CITE[3]@; @CITE[2]@; @CITE[4]@. However, they only consider one-hot encoded topics as prefix. In this work, we leverage prompt learning for EHRs generation conditioned on patient baseline features, which include both categorical and numerical values.\\nReferences:ID: 1. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 2. Title: Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision\\nID: 3. Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation\\nID: 4. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 5. Title: Controllable Natural Language Generation with Contrastive Prefixes<|eot_id|>'},\n",
       " 2245: {'predict': 'Data-to-text generation is a task that generates natural language text based on structured data, such as tables (@CITE[6]@; @CITE[2]@; @CITE[4]@; @CITE[3]@, knowledge graphs (@CITE[5]@; @CITE[1]@, and trees (@CITE[7]@. Most existing works use the encoder-decoder framework to model the data-to-text generation task, which can be divided into two categories: 1) the encoder-decoder framework with the pre-defined templates (@CITE[2]@; @CITE[4]@; @CITE[3]@; @CITE[5]@; @CITE[7]@, and 2) the encoder-decoder framework without the pre-defined templates (Bao et al., 2019; @CITE[1]@. In this paper, we focus on the second category. The second category is further divided into two categories: 1) the encoder-decoder framework without the pre-defined templates but with the pre-defined slots (@CITE[5]@; @CITE[7]@, and 2) the encoder-decoder framework without the pre-defined templates and slots (@CITE[1]@; Bao et al., 2019). The second category is more challenging than the first category, as it does not use the pre-defined slots and templates, which makes it difficult to determine the order of generation. However, the second category is more flexible and can be applied to the data-to-text generation tasks with the complex structures, such as tables and knowledge graphs.\\nReferences:ID: 1. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 2. Title: Table-to-Text: Describing Table Region with Natural Language\\nID: 3. Title: DART: Open-Domain Structured Data Record to Text Generation\\nID: 4. Title: Data-to-Text Generation with Content Selection and Planning\\nID: 5. Title: A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation\\nID: 6. Title: Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence\\nID: 7. Title: Deep Graph Convolutional Encoders for Structured Data to Text Generation',\n",
       "  'ground_truth': '\\n\\nData-to-text aims to generate natural language descriptions from the input structured data such as sports commentaries (@CITE[7]@. The structured data is usually represented as tables (@CITE[7]@; Thomson et al., 2020;, sets of table cells (;, semantic representations (Novikova et al., 2017), or sets of relation triples (;. The task requires the model to select the salient information from the data, organize it in a logical order, and generate an accurate and fluent natural language description (@CITE[7]@. Data-to-text models usually adopt the encoder-decoder architecture. The encoders are specifically designed to model the input data, such as multi-layer perceptron (@CITE[1]@,b), recurrent neural network (@CITE[4]@; @CITE[6]@; @CITE[2]@, graph neural network (@CITE[5]@; @CITE[5]@, or Transformer (.\\nReferences:ID: 1. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 2. Title: Table-to-Text: Describing Table Region with Natural Language\\nID: 3. Title: DART: Open-Domain Structured Data Record to Text Generation\\nID: 4. Title: Data-to-Text Generation with Content Selection and Planning\\nID: 5. Title: A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation\\nID: 6. Title: Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence\\nID: 7. Title: Deep Graph Convolutional Encoders for Structured Data to Text Generation<|eot_id|>'},\n",
       " 1937: {'predict': 'Data augmentation is a technique that is commonly used in NLP to artificially increase the size of a dataset. The idea is to create new training examples by applying random transformations to the original examples. The goal is to create new examples that are similar to the original examples, so that the model learns to generalize better. There are two main types of data augmentation: rule-based and model-based. Rule-based augmentation uses hand-coded rules to create new examples, such as replacing words with synonyms or flipping the polarity of sentiment analysis examples. Model-based augmentation, on the other hand, uses a language model to generate new examples. The language model is trained on the original dataset and is used to generate new examples by sampling from the language model’s output distribution. In the past, rule-based augmentation was more popular. However, with the rise of large pre-trained language models, model-based augmentation has become more popular. For example, @CITE[5]@ propose to replace words in a sentence with synonyms generated by a language model. @CITE[2]@ propose to replace words in a sentence with words predicted by a language model. @CITE[4]@ propose to replace words in a sentence with words predicted by a language model, conditioned on the sentence’s context. @CITE[3]@ propose to replace words in a sentence with words predicted by a language model, conditioned on the sentence’s context, and propose to use a discriminator to ensure that the generated words are similar to the original words. @CITE[1]@ propose to replace words in a sentence with words predicted by a language model, conditioned on the sentence’s context, and propose to use a discriminator to ensure that the generated words are similar to the original words, and propose to use a reinforcement learning objective to improve the quality of the generated words. @CITE[4]@ propose to replace words in a sentence with words predicted by a language model, conditioned on the sentence’s context, and propose to use a discriminator to ensure that the generated words are similar to the original words, and propose to use a reinforcement learning objective to improve the quality of the generated words, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence. In this paper, we propose to replace words in a sentence with words predicted by a language model, conditioned on the sentence’s context, and propose to use a discriminator to ensure that the generated words are similar to the original words, and propose to use a reinforcement learning objective to improve the quality of the generated words, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence. However, we propose to use a reinforcement learning objective to improve the quality of the generated words, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence. However, we propose to use a reinforcement learning objective to improve the quality of the generated words, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence, and propose to use a language model to generate the next word in a sentence, conditioned on the previous words in the sentence.\\nReferences:ID: 1. Title: Good-Enough Compositional Data Augmentation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Attention Is All You Need',\n",
       "  'ground_truth': '\\n\\nAlthough the textual content is always increasing, data augmentation is still a highly active area of research since for machine learning applications, especially the new ones, the initial annotated data are usually small. As a result, researchers are constantly coming up with innovative ideas to create new data from the available content. Some have experimented at the input sequence level performing operations on words. For example, to improve machine translation quality, Fadaee et al. (2017) utilize substitution of common words with rare ones, thus providing more context for the rare words, while Sennrich et al. (2016) use back-translation where automatically translated data along with the original human-translated data are employed to train a neural machine translation system. @CITE[5]@ replaces words with their synonyms for classifying tweets. Similarly, @CITE[4]@ replace sentence fragments from common categories with each other in order to produce new sentences. Others have opted for using pre-trained language models such as BERT (@CITE[3]@. @CITE[1]@ utilizes contextual augmentation, replacing the words with the prediction of a bidirectional language model at a desired position in the sentence. @CITE[3]@ and @CITE[2]@ utilize reinforcement learning with a conditional language model which is carried out by attaching the correct label to the input sequence when training (. Working with Transformer model (, propose Mix-Transformer where two input sentences and their corresponding labels are linearly interpolated to create new samples. make use of data noising which can be considered similar to our work with the difference that they replace words choosing from the unigram frequency distribution or insert the underscore character as a placeholder, whereas we insert punctuation characters which usually occur in sentences. The related works mostly use some auxiliary data or a complicated language model to produce augmented data. Conversely, our method is extremely simple to implement and does not need any extra data. In addition, it shows superior performance to EDA in both simple models such as RNNs and CNNs and deep models such as BERT.\\nReferences:ID: 1. Title: Good-Enough Compositional Data Augmentation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations\\nID: 4. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 5. Title: Attention Is All You Need<|eot_id|>'},\n",
       " 1643: {'predict': 'Neural network modules are used to decompose the reasoning process into smaller sub-problems that can be solved by individual modules. NMNs (@CITE[8]@) and N2NMNs (@CITE[2]@) learn to reason over visual scenes by composing modules to answer questions. NMNs for text (@CITE[6]@; @CITE[4]@; @CITE[5]@; @CITE[5]@; @CITE[5]@; @CITE[3]@) compose modules to perform reasoning over natural language. These modules are typically composed of a neural network that is trained on a dataset of questions and answers. The modules are then used to reason over the input data to produce an answer. Neural modules are also used in other areas such as dialogue (@CITE[9]@), vision-language grounding (@CITE[1]@), and reinforcement learning (@CITE[7]@).\\nReferences:ID: 1. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 2. Title: Neural Module Networks\\nID: 3. Title: Learning to Reason: End-to-End Module Networks for Visual Question Answering\\nID: 4. Title: Neural Compositional Denotational Semantics for Question Answering\\nID: 5. Title: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning\\nID: 6. Title: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning\\nID: 7. Title: Neural Module Networks for Reasoning over Text\\nID: 8. Title: Towards Interpretable Reasoning over Paragraph Effects in Situation\\nID: 9. Title: Multi-Step Inference for Reasoning Over Paragraphs',\n",
       "  'ground_truth': '\\n\\nThese modules were first adopted in the Visual Question Answering (VQA) domain (@CITE[5]@; @CITE[2]@; @CITE[1]@. Due to the interpretable, modular, and inherently compositional nature of neural network modules, they were further extended to natural language (@CITE[8]@; @CITE[9]@; @CITE[9]@. @CITE[4]@ extend neural module networks to answer compositional questions. @CITE[7]@ and @CITE[3]@ further introduce neural network modules on one complex reasoning task ROPES (@CITE[6]@. In this work, we explore the effectiveness of neural network modules on a qualitative reasoning task.\\nReferences:ID: 1. Title: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\\nID: 2. Title: Neural Module Networks\\nID: 3. Title: Learning to Reason: End-to-End Module Networks for Visual Question Answering\\nID: 4. Title: Neural Compositional Denotational Semantics for Question Answering\\nID: 5. Title: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning\\nID: 6. Title: Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning\\nID: 7. Title: Neural Module Networks for Reasoning over Text\\nID: 8. Title: Towards Interpretable Reasoning over Paragraph Effects in Situation\\nID: 9. Title: Multi-Step Inference for Reasoning Over Paragraphs<|eot_id|>'},\n",
       " 6746: {'predict': 'Incorporating external knowledge into PLMs has been widely explored. For instance, ERNIE (@CITE[4]@) and K-BERT (@CITE[4]@) are trained on both textual corpora and knowledge graphs (KGs) to enhance the semantic representation of PLMs. ERNIE and K-BERT are both trained with a masked language modeling objective, and the knowledge from KGs is injected into the PLMs by concatenating the entity representations from KGs into the input sequence. @CITE[5]@ further investigates the factual consistency of abstractive summarization and proposes a method to improve the factual consistency of the generated summaries by maximizing the factual consistency metric. Recently, @CITE[1]@ and @CITE[3]@ propose to inject knowledge into PLMs by updating the knowledge neurons in the feed-forward network (FFN) of PLMs. @CITE[2]@ propose to inject knowledge into PLMs by using the attention mechanism to connect the knowledge embeddings with the input representations of PLMs. Different from these works, we propose to inject knowledge into PLMs by using the self-attention mechanism to connect the knowledge embeddings with the hidden states of PLMs.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 3. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 4. Title: Knowledge Neurons in Pretrained Transformers\\nID: 5. Title: Kformer: Knowledge Injection in Transformer Feed-Forward Layers',\n",
       "  'ground_truth': '\\n\\nPrevious works have explored ways to incorporate external knowledge into pre-trained language models (PLMs). ERNIE (@CITE[5]@ and Know-BERT (@CITE[5]@; Peters et al., 2019) enhance the word representations by incorporating external knowledge graphs. K-ADAPTER introduces two adapters to inject factual and linguistic knowledge into PLM respectively (@CITE[4]@. Inspired by @CITE[2]@, recent works focused to add extended FFNs in Transformer-like K-former (@CITE[3]@ or Neural Knowledge Bank (NKB) (@CITE[1]@ to inject and update extra knowledge while keeping the original model parameters frozen. @CITE[2]@ investigated to detect the incorrect knowledge stored in PLMs and proposed CALINET for knowledge calibration.\\nReferences:ID: 1. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 2. Title: ERNIE: Enhanced Language Representation with Informative Entities\\nID: 3. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 4. Title: Knowledge Neurons in Pretrained Transformers\\nID: 5. Title: Kformer: Knowledge Injection in Transformer Feed-Forward Layers<|eot_id|>'},\n",
       " 7373: {'predict': 'CTG is a rapidly developing area in NLP. @CITE[6]@ proposed a conditional transformer language model to control the generation of text. @CITE[1]@ proposed a reinforcement learning method to fine-tune a pre-trained language model. @CITE[4]@ proposed a Plug-and-Play Language Model to control the generation of text. @CITE[2]@ proposed a generative discriminator guided sequence generation method to control the generation of text. @CITE[5]@ proposed a future discriminator to control the generation of text. @CITE[3]@ proposed a latent attribute model to control the generation of text. @CITE[6]@ proposed a prompt-based method to control the generation of text. However, these methods are not designed for CTG to avoid toxicity. In this paper, we propose a new prompt-based method to control the generation of text to avoid toxicity.\\nReferences:ID: 1. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 2. Title: Training language models to follow instructions with human feedback\\nID: 3. Title: Training language models to follow instructions with human feedback\\nID: 4. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 5. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 6. Title: FUDGE: Controlled Text Generation With Future Discriminators',\n",
       "  'ground_truth': '\\n\\nVarious CTG-based toxicity avoidance methods without additional inference costs have been proposed. For nontoxic text generation, model retraining with auxiliary information is used, including control codes as in CTRL (@CITE[6]@) or reinforcement learning from human feedback - InstructGPT (@CITE[2]@). To reduce the cost of retraining, prompt tuning methods have been proposed. DisCup (@CITE[2]@) is a prompt tuning method using unlikelihood training assisted by a discriminator to generate nontoxic tokens without updating the LM. However, these methods still require inference by a language model to learn. As the size of the LM increases, additional training becomes more expensive; therefore, a guided decoding method is proposed. The guided decoding method changes the output distribution of the LM to generate an intended text by adjusting it through a pretrained toxic discriminator. Discriminator training is much less expensive and easier to apply than retraining or prompt tuning. PPLM (@CITE[3]@) calculates the toxicity of the token generated by the model by attaching a discriminator head to the last hidden state of the language model. After that, the last hidden state is updated to nontoxic using the gradient. However, this process is too slow. To reduce the calculation process, methods that directly adjust the output probability using the discriminator have been proposed. These methods are much faster in making inferences and do not require a language model in the discriminator training phase. GeDi (@CITE[5]@) learns the Class-Conditional Language Model (CC-LM) and calculates the nontoxic token probability using the Bayes rule to guide the LM. FUDGE (@CITE[1]@) evaluates the LM’s candidate tokens and induces the selection of appropriate tokens. DExperts (@CITE[4]@) subtracts the output of an Anti-expert and adds the output of an Expert to the output distribution of the LM. In this paper, we propose gated toxicity avoidance, a model-agnostic method for CTG that can be utilized without tuning the LLM.\\nReferences:ID: 1. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 2. Title: Training language models to follow instructions with human feedback\\nID: 3. Title: Training language models to follow instructions with human feedback\\nID: 4. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 5. Title: GeDi: Generative Discriminator Guided Sequence Generation\\nID: 6. Title: FUDGE: Controlled Text Generation With Future Discriminators<|eot_id|>'},\n",
       " 3779: {'predict': 'Conversational infilling has been explored in various forms, such as abductive commonsense reasoning (@CITE[1]@; @CITE[4]@), counterfactual reasoning (@CITE[1]@), and topic transition (@CITE[6]@; @CITE[7]@). Existing methods for conversational infilling are often based on large language models. Some studies have explored ways to control the generation of large language models, such as using control codes (@CITE[2]@), future discriminators (, or energy-based models (@CITE[3]@). However, these methods are limited to controlling the generation of a single turn. In contrast, we focus on generating a bridging utterance to connect two topics in a cooperative and coherent manner. To the best of our knowledge, there is no existing method that can generate bridging utterances in a single turn. Our work is also related to recent work in human-AI collaboration (HAC) (@CITE[5]@; @CITE[7]@), where humans and AI agents collaborate to generate a story. However, existing work in HAC focuses on generating a story from scratch, whereas our work focuses on generating a bridging utterance to connect two topics.\\nReferences:ID: 1. Title: Enabling Language Models to Fill in the Blanks\\nID: 2. Title: Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning\\nID: 3. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 4. Title: FUDGE: Controlled Text Generation With Future Discriminators\\nID: 5. Title: Mix and Match: Learning-free Controllable Text Generation using Energy Language Models\\nID: 6. Title: OTTers: One-turn Topic Transitions for Open-Domain Dialogue\\nID: 7. Title: Wordcraft: a Human-AI Collaborative Editor for Story Writing',\n",
       "  'ground_truth': '\\n\\nWhile, to the best of our knowledge, we are the first to explore conversation infilling, many have explored the related tasks of text infilling (Zhu et al., 2019; @CITE[6]@; @CITE[4]@) and controllable text generation (@CITE[3]@; @CITE[1]@; @CITE[2]@). Closer to conversation infilling, Tang et al. (2019) propose a method to guide conversations towards a target keyword. @CITE[5]@ explore the task of proactive conversation where a dialogue agent leads a conversation by planning over a knowledge graph. @CITE[7]@ and Gupta et al. (2022) explore the task of one-turn topic transitions: given a source utterance ua and a partial utterance ub, generate text u′ b such that the concatenation of u′ b and ub is a sensible response to ua. Conversation infilling, in contrast, requires the generation of an entire conversation that bridges two utterances on behalf of both speakers. Moreover, their proposed methods require fine-tuning and external knowledge bases, while HeLo is a learning-free decoding method. propose NeuroLogic A*esque (NL), a decoding method that also employs a lookahead phase. The main differences between our methods lie in the heuristic score computation and the tasks explored. NL sets the heuristic score as a) the likelihood of the lookahead continuation itself or b) whether some constraint is satisfied in the lookahead, such as whether specific words appear or not. In HeLo, the lookahead completes a partial utterance to produce a wellformed potential conversation. We then set the heuristic as the likelihood of ytgt given this potential conversation. The likelihood of the lookahead itself or whether it satisfies certain lexical constraints does not affect our heuristic score. Moreover, do not explore the task of conversation infilling. Instead, they examine constrained forms of machine translation and commonsense, table-to-text, question, and story generation.\\nReferences:ID: 1. Title: Enabling Language Models to Fill in the Blanks\\nID: 2. Title: Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning\\nID: 3. Title: CTRL: A Conditional Transformer Language Model for Controllable Generation\\nID: 4. Title: FUDGE: Controlled Text Generation With Future Discriminators\\nID: 5. Title: Mix and Match: Learning-free Controllable Text Generation using Energy Language Models\\nID: 6. Title: OTTers: One-turn Topic Transitions for Open-Domain Dialogue\\nID: 7. Title: Wordcraft: a Human-AI Collaborative Editor for Story Writing<|eot_id|>'},\n",
       " 4603: {'predict': 'Prompt-based Learning (PL) has been shown to be effective in leveraging the knowledge learned from pre-trained language models (PLMs) in few-shot learning. PLM can be fine-tuned on a small number of annotated examples, or used as a frozen model to perform zero-shot learning. In the zero-shot setting, a natural language prompt is used to guide the model to generate a sequence of tokens. A number of works have been proposed to improve the performance of PLM in few-shot learning (@CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[7]@; @CITE[3]@; @CITE[3]@; @CITE[2]@; @CITE[8]@; @CITE[6]@; @CITE[4]@. Prompt tuning (@CITE[7]@; @CITE[3]@; @CITE[6]@; @CITE[8]@; @CITE[4]@ is one of the most effective methods to improve the performance of PLM in few-shot learning. In prompt tuning, PLM is fine-tuned on a small number of annotated examples, and a set of trainable parameters (i.e., prompt embeddings) are used to learn task-specific representations. In this work, we propose a novel prompt tuning method called prompt alignment tuning (PAT), which uses an additional loss to align the prompt embeddings of different tasks.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 5. Title: GPT Understands, Too\\nID: 6. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 7. Title: LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5\\nID: 8. Title: Calibrate Before Use: Improving Few-Shot Performance of Language Models',\n",
       "  'ground_truth': '\\n\\nPL is a new paradigm which prepends a task-specific template or prompt to the input for learning new tasks (. Initial PL methods mainly focus on designing, searching or optimizing discrete prompts (@CITE[4]@; ; @CITE[8]@. However, discrete prompts are hard to optimize. To solve this, recent PL methods attempt to optimize prompts in a continuous space, i.e., learn soft prompts (@CITE[8]@; @CITE[6]@;, showing impressive FSL performance (@CITE[1]@. In addition to prompt design, several recent studies have explored the applications (Zhu et al., 2022; @CITE[3]@; @CITE[5]@; and analysis (@CITE[2]@; @CITE[7]@ of PL.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 5. Title: GPT Understands, Too\\nID: 6. Title: The Power of Scale for Parameter-Efficient Prompt Tuning\\nID: 7. Title: LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5\\nID: 8. Title: Calibrate Before Use: Improving Few-Shot Performance of Language Models<|eot_id|>'},\n",
       " 939: {'predict': 'Event argument extraction (EAE) is an essential task in natural language processing (NLP) and has attracted much attention. Previous works on EAE can be roughly divided into two categories: pipeline and joint approaches. Pipeline approaches usually rely on entity recognition (ER) as a preprocessing step, which can suffer from error propagation (@CITE[2]@. Joint approaches, on the other hand, simultaneously extract event triggers and their arguments. Most of them are based on the sequence labeling framework, which are not effective in modeling the long-range dependencies between event triggers and arguments. Recently, there is a growing interest in exploring graph neural networks (GNNs) to model the event graph (@CITE[1]@;. However, these approaches are usually time-consuming to train and are not suitable for real-time applications. Recently, we proposed a novel pipeline EAE framework that formulates EAE as a question answering (QA) task, which can avoid the error propagation and achieve state-of-the-art results on EAE. We also proposed a distillation method to reduce the computational cost of the QA model. However, the QA model is still time-consuming to train and requires a large-scale dataset. In this paper, we propose a lightweight EAE framework that can achieve high accuracy with low computational cost.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 3. Title: TinyBERT: Distilling BERT for Natural Language Understanding',\n",
       "  'ground_truth': '\\n\\nEAE seeks to extract entities with specific roles in an event. Methods that learn direct relation between arguments and triggers have achieved significant progress in this field (; @CITE[3]@; ; @CITE[3]@). Recently, there is a trend to formulate EAE as a Question Answering (QA) problem, and several MRC models report performing well (; @CITE[1]@; ). These methods leverage role-specific questions to extract boundaries of the expected arguments. Implicit Event Argument Extraction (IEAE) is a less studied problem where arguments could span multiple sentences and appear in an implicit way. There have been only a few works for IEAE. Ebner et al. (2020); Zhang et al. (2020b) formulate IEAE as a semantic role labeling task and extract arguments by classifying phrase pairs. These methods only explicitly consider direct relations between triggers and arguments. also consider the relation among arguments, however, their method could only deal with an argument linking task that identifies the role of a given argument span, which is not available in a realistic situation.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 3. Title: TinyBERT: Distilling BERT for Natural Language Understanding<|eot_id|>'},\n",
       " 5587: {'predict': 'Recent advances in LLMs have made it possible to reason with them in a few-shot manner, which has shown remarkable performance on a variety of reasoning tasks (; @CITE[2]@; @CITE[3]@; @CITE[5]@; @CITE[1]@. The Chain-of-Thought (CoT) prompting (@CITE[4]@ is one of the most effective methods to elicit complex multi-step reasoning from LLMs. CoT prompting encourages LLMs to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Recent studies have shown that LLMs can be effective zero-shot reasoners by simply adding \"Let\\'s think step by step\" before each answer (@CITE[1]@; @CITE[5]@. In this paper, we also explore the zero-shot capability of LLMs on a set of reasoning tasks. However, the LLMs used in these studies are not available for public use. In contrast, we focus on the publicly available LLMs and explore their zero-shot reasoning capability.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Training language models to follow instructions with human feedback\\nID: 3. Title: Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters\\nID: 4. Title: Large Language Models are Zero-Shot Reasoners\\nID: 5. Title: PaLM: Scaling Language Modeling with Pathways',\n",
       "  'ground_truth': '\\n\\nLLM has shown remarkable zero-shot reasoning ability, in the way of explicitly encouraging the LLM to generate intermediate rational for solving a problem. On the one hand, recent works, in both the few-shot (@CITE[1]@; @CITE[4]@; @CITE[2]@ and zero-shot (@CITE[5]@ setting, elicit chain-of-thought (CoT) from LLM and modify the answer by step-by-step. On the other hand, the problem decomposition, like least-to-most prompting (@CITE[1]@, re- duces complex problems to the sub-problems, and then solves these sub-problems sequentially; the SC strategy (@CITE[3]@ generates a di- verse set of answers by sampling from LLM, and then marginalizes out the sampled answers to determine the optimal answer. In this work, we focus on investigating the zero-shot reasoning ability of LLM on the NER task.\\nReferences:ID: 1. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 2. Title: Training language models to follow instructions with human feedback\\nID: 3. Title: Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters\\nID: 4. Title: Large Language Models are Zero-Shot Reasoners\\nID: 5. Title: PaLM: Scaling Language Modeling with Pathways<|eot_id|>'},\n",
       " 3965: {'predict': 'Model-based retrieval (MBR) is a paradigm that retrieves documents directly by generating document identifiers from a text-to-text model, without the need for an index. The first work that adopts this paradigm is the Differentiable Search Index (DSI) (@CITE[5]@, which is a text-to-text model that maps a query to the document identifiers. The model is trained with a softmax loss function that is computed by the model itself, which is a differentiable function. The model is optimized with the gradient of the loss function. The model can be used in zero-shot retrieval, which is the setting where the model is trained on one dataset and tested on another dataset. DSI is shown to be competitive with BM25 in zero-shot retrieval. GENRE ( is the first system that retrieves entities by generating their unique names in an autoregressive fashion. It is used for entity retrieval and achieves state-of-the-art results in entity linking and document retrieval tasks. DynamicRetriever (@CITE[3]@ is the first pre-training model-based IR system that directly generates document identifiers from a text-to-text model. It is trained with a pre-training objective that is a combination of masked language modeling and document retrieval. It is shown to be effective in document retrieval. Recently, there are several studies that focus on the zero-shot capability of MBR models. @CITE[2]@ is the first work that systematically studies the zero-shot capability of MBR models. They show that MBR models are sensitive to the source training set and the target dataset. They also show that MBR models are biased towards the source training set and the target dataset. @CITE[4]@ is the first work that proposes a method to improve the zero-shot capability of MBR models. They propose a method to improve the source training set by data augmentation. They also propose a method to improve the target dataset by data augmentation. They show that the proposed methods can improve the zero-shot capability of MBR models. Ultron (@CITE[1]@ is a model-based indexer for document retrieval. It encodes the knowledge of all documents into the model and aims to directly retrieve relevant documents end-to-end. It is trained with a three-stage training workflow that captures more knowledge contained in the corpus and associations between queries and docids. It is shown to be effective in document retrieval.\\nReferences:ID: 1. Title: Transformer Memory as a Differentiable Search Index\\nID: 2. Title: Autoregressive Entity Retrieval\\nID: 3. Title: DynamicRetriever: A Pre-training Model-based IR System with Neither Sparse nor Dense Index\\nID: 4. Title: A Thorough Examination on Zero-shot Dense Retrieval\\nID: 5. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer',\n",
       "  'ground_truth': '\\n\\nBoth sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021; @CITE[5]@). These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (@CITE[4]@), which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequence-to-sequence encoder-decoder architecture for document retrieval (; Bevilacqua et al., 2022; Zhuang et al., 2022; @CITE[1]@; @CITE[2]@; @CITE[3]@; ). As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to deal with these issues with a two-stage generation architecture with URL identifiers.\\nReferences:ID: 1. Title: Transformer Memory as a Differentiable Search Index\\nID: 2. Title: Autoregressive Entity Retrieval\\nID: 3. Title: DynamicRetriever: A Pre-training Model-based IR System with Neither Sparse nor Dense Index\\nID: 4. Title: A Thorough Examination on Zero-shot Dense Retrieval\\nID: 5. Title: Ultron: An Ultimate Retriever on Corpus with a Model-based Indexer<|eot_id|>'},\n",
       " 1665: {'predict': 'Emotional Text Generation is an important task that aims to generate texts with specific emotions. Most of the existing methods use RNN-based language models to generate texts with emotions (@CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@; ; @CITE[5]@. However, the performance of RNN-based language models is limited by the complexity of RNN structures. Recently, pre-trained language models (PLMs) such as BERT (@CITE[6]@ have achieved great success in many natural language processing tasks. To utilize the strength of PLMs for emotional text generation, some works propose to fine-tune PLMs on the target task and generate texts with emotions based on the outputs of the fine-tuned PLMs (@CITE[5]@; ; @CITE[1]@. However, these methods require a large amount of labeled data and may not work well in the low-resource scenario. In this paper, we propose to use the emotional guidance to enhance the emotional expression of texts generated by PLMs, which can be applied to both high- and low-resource scenarios.\\nReferences:ID: 1. Title: A Hierarchical Latent Structure for Variational Conversation Modeling\\nID: 2. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 3. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 4. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 5. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'ground_truth': '\\n\\nVAE-based models (@CITE[5]@; Shen et al., 2017; Zhao et al., 2017; @CITE[6]@, adversarial networks (@CITE[5]@; ; Yu et al., 2017) and reinforcement learning systems (@CITE[5]@, 2016b) have dominated sentiment-aware dialogue models. Other methods have been developed using LSTM (@CITE[5]@ and GRU (; @CITE[6]@. All these methods, however, are built on relatively coarse emotion types, partially due to the limited modeling ability of RNNs. Our model outperforms current state-of- the-art R-CVAE (@CITE[6]@ in the same 64-emoji settings.\\nReferences:ID: 1. Title: A Hierarchical Latent Structure for Variational Conversation Modeling\\nID: 2. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 3. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 4. Title: Reading Wikipedia to Answer Open-Domain Questions\\nID: 5. Title: Simple, Scalable Adaptation for Neural Machine Translation\\nID: 6. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<|eot_id|>'},\n",
       " 3154: {'predict': \"The importance of OOD generalization has been recognized in recent years. Existing debiasing methods are mainly designed for NLU tasks, which can be roughly categorized into two types: 1) dataset bias detection and 2) self-debiasing. Dataset bias detection methods (@CITE[4]@; @CITE[2]@; @CITE[5]@; @CITE[6]@; @CITE[1]@; @CITE[3]@) focus on identifying biased instances and down-weighting them during training. Self-debiasing methods (@CITE[4]@; @CITE[5]@; @CITE[1]@) are based on a biased model and a main model. The biased model is trained to mimic the behavior of the main model. Then, the main model is trained with the biased model’s predictions as the additional training signal. These methods have been proven to be effective in improving OOD generalization. However, they are mainly designed for NLU tasks. For VQA, the main challenge of OOD generalization is the lack of data. In this work, we propose a novel debiasing method, which is based on a bias model and a main model. The bias model is trained with the biased examples, and the main model is trained with the biased model’s predictions as the additional training signal. Our method is different from previous self-debiasing methods in that we use a bias model to detect biased examples, and we use a main model to generate the additional training signal. Besides, we design a novel debiasing objective, which is based on the discrepancy between the predictions of the biased model and the main model. This objective is different from the previous debiasing objectives in that it is based on the discrepancy between the predictions of the biased model and the main model. We show that our method outperforms previous methods in improving OOD generalization for VQA.\\nReferences:ID: 1. Title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\\nID: 2. Title: Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance\\nID: 3. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 4. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 5. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 6. Title: Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques\",\n",
       "  'ground_truth': \"\\n\\nOOD settings have been studied in recent years in NLU. To tackle dataset bias, most existing work relies on instance reweighting with a bias model for debiasing. Specifically, these methods (@CITE[5]@ first design a bias model and then train a target debiased model fused with the bias model. Training instances predicted correctly by the bias model will be down-weighted in the training of the debiased model. Early work mainly revolves around different fusion methods (@CITE[2]@; @CITE[5]@; ; with known bias. Then researchers started looking into unknown bias by designing the bias model with heuristics, e.g., a model trained with a very small amount of data (@CITE[1]@ or a model with only the bottom layers of the language model (@CITE[3]@. However, instance reweighting-based methods rely on either prior knowledge of bias or heuristic design of the bias model. Furthermore, it is pointed out that such bias models may not be able to predict the main model’s reaction to biased samples, and reweighting may waste data (@CITE[6]@. In contrast, our method is derived from causal inference (@CITE[4]@, which is not related to any form of reweighting. Meanwhile, our method does not adopt any bias model (which requires careful design or prior knowledge of bias).\\nReferences:ID: 1. Title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\\nID: 2. Title: Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance\\nID: 3. Title: End-to-End Bias Mitigation by Modelling Biases in Corpora\\nID: 4. Title: Towards Debiasing NLU Models from Unknown Biases\\nID: 5. Title: End-to-End Self-Debiasing Framework for Robust NLU Training\\nID: 6. Title: Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques<|eot_id|>\"},\n",
       " 3572: {'predict': 'There is a long line of research on deﬁnition modeling, which is mainly divided into two categories: 1) deﬁnition modeling for word sense disambiguation (WSD) and 2) deﬁnition modeling for event deﬁnition. For the ﬁrst category, deﬁnition modeling is used to capture the semantic meaning of word senses and to improve the performance of WSD. Early works (@CITE[2]@; @CITE[4]@) mainly focus on word sense disambiguation in a knowledge-based approach. Recently, neural networks are widely used for WSD (@CITE[3]@; @CITE[1]@; ; ; ). For the second category, deﬁnition modeling is used to capture the semantic meaning of event deﬁnitions and to improve the performance of event extraction. @CITE[3]@; @CITE[1]@; ;  propose to use pre-trained language models to encode the deﬁnition of an event type. However, they only use the deﬁnition of an event type to improve the performance of event extraction. To the best of our knowledge, we are the ﬁrst to use deﬁnitions of multiple event types to improve the performance of event extraction.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge\\nID: 3. Title: Moving Down the Long Tail of Word Sense Disambiguation with Gloss-Informed Biencoders\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings',\n",
       "  'ground_truth': '\\n\\nHumans are capable of understanding new con- cepts by reading their glosses or deﬁnitions. Thus, how to leverage the deﬁnitions and explanations from dictionaries to help understand human lan- guage is a long-standing question in the NLP com- munity. Most of the previous efforts in this direc- tion are working on the word sense disambigua- tion task (; @CITE[3]@; @CITE[2]@; ; @CITE[1]@; ; @CITE[4]@,b). These models learn to map a token into the correct pre-deﬁned synset by either jointly or disjointly encoding the tokens and deﬁ- nitions. Even though the setting of our model and these WSD models are similar, identifying event mentions that satisfy an arbitrary event type deﬁni- tion is a more challenging task (Senel et al., 2022). WSD aims to learn to distinguish the correct synset versus several (typically less than 10) other pre- deﬁned synsets, while our goal is to align an event mention and the corresponding deﬁnition, where all other arbitrary deﬁnitions are considered to be the negative candidates. To address the engineering limitation that negative candidates exceed the GPU memory limitation, we propose a coarse-to-ﬁne negative sampling strategy to help models learn the minor differences between similar deﬁnitions without forgetting the big picture.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge\\nID: 3. Title: Moving Down the Long Tail of Word Sense Disambiguation with Gloss-Informed Biencoders\\nID: 4. Title: SimCSE: Simple Contrastive Learning of Sentence Embeddings<|eot_id|>'},\n",
       " 4012: {'predict': \"Contrastive learning has been widely adopted to learn generalizable and robust representations for various tasks (@CITE[6]@; @CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[3]@; @CITE[5]@. The core idea is to pull together the representations of semantically similar samples and push apart those of dissimilar samples. In this paper, we adopt the InfoNCE loss () to implement the contrastive learning objective. The InfoNCE loss is a probabilistic version of the InfoMax loss and is widely adopted in various contrastive learning tasks. It is defined as follows: $$L_{con} = -\\\\mathbb{E}_{x\\\\sim P(x)}\\\\log \\\\frac{\\\\exp(sim(x,z))}{\\\\exp(sim(x,z)) + \\\\sum_{x' \\\\sim P(x')} \\\\exp(sim(x,x'))$$ where x and x' are the augmented versions of x, sim(x, z) is the cosine similarity between the representation of x and z, and P(x) is the data distribution. z is the anchor representation of x. In this paper, we set the anchor representation as the representation of x. The InfoNCE loss is widely used to learn contrastive representations for various tasks, including image classification (, graph classification (@CITE[4]@, and cross-lingual NLP (@CITE[5]@.\\nReferences:ID: 1. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 2. Title: Debiased Contrastive Learning\\nID: 3. Title: Supervised Contrastive Learning\\nID: 4. Title: Graph Contrastive Learning with Augmentations\\nID: 5. Title: SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning\\nID: 6. Title: Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings\",\n",
       "  'ground_truth': '\\n\\nSelf-supervised contrastive learning has been widely used to generate representations for various tasks (; @CITE[3]@; @CITE[5]@; @CITE[2]@; @CITE[6]@; @CITE[1]@; @CITE[4]@. In a nutshell, contrastive learning pulls positive pairs closer while pushing negative pairs apart. Supervised contrastive learning (@CITE[5]@ further constructs positive pairs with labeled samples of the same class, which ensures the validity of positive pairs. Das et al. (2022) leverages contrastive learning for name entity recognition, but they work on monolingual few-shot settings while we focus on cross-lingual NER self-training.\\nReferences:ID: 1. Title: A Simple Framework for Contrastive Learning of Visual Representations\\nID: 2. Title: Debiased Contrastive Learning\\nID: 3. Title: Supervised Contrastive Learning\\nID: 4. Title: Graph Contrastive Learning with Augmentations\\nID: 5. Title: SANCL: Multimodal Review Helpfulness Prediction with Selective Attention and Natural Contrastive Learning\\nID: 6. Title: Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings<|eot_id|>'},\n",
       " 5355: {'predict': 'CRS has gained increasing attention in recent years, which enables users to interact with the system in natural language and obtain personalized recommendations. In CRS, a question-based user preference elicitation strategy is widely used to obtain user preferences towards items and attributes. For example, CRM (@CITE[2]@) and EAR (@CITE[3]@) are two typical CRS frameworks that ask questions about attributes and then recommend items. To improve the recommendation performance, a few studies (@CITE[1]@; @CITE[3]@; ) have explored the reinforcement learning (RL) method to learn the recommendation policy in CRS. However, these methods usually treat the recommendation policy as a separate component from the conversation policy, which can lead to a suboptimal policy and poor performance. To address this issue, we propose a unified policy learning framework for CRS, which jointly learns the conversation and recommendation policies in a unified framework. In our framework, the conversation and recommendation policies are formulated as two sub-policies, and we use a dynamic weighted graph to model the interaction between them.\\nReferences:ID: 1. Title: Advances and Challenges in Conversational Recommender Systems: A Survey\\nID: 2. Title: Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems\\nID: 3. Title: Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning',\n",
       "  'ground_truth': '\\n\\nCRS is a recommendation system that can elicit the dynamic preferences of users and take actions based on their current needs through real-time multi-turn interactions(@CITE[1]@). In this work, we focus on attribute-based CRS(; @CITE[2]@,b; ; @CITE[3]@) which ask users attributes to efficiently acquire user preferences and quickly narrow down the recommendation candidates. The problem settings of attribute-based CRS can be mainly divided into two categories: single-round and multi-round. In a single-round scenario(; ), the CRS only makes recommendations once, ending the session regardless of the results. In a multi-round scenario, the CRS can interact with the user by asking attributes and recommending items multiple times until the task succeeds or the user terminates the session. Due to the wide acknowledgment of Multi-round Conversational Recommendation (MCR) as the most realistic setting(@CITE[2]@) for CRS, this work is conducted based on the multi-round scenario. EAR(@CITE[2]@) extends the single-round setting into a multi-round setting where the system can recommend multiple times. SCPR(Lei et al., 2020b) models the MCR as an interactive path reasoning problem on a graph, which is able to prune off many irrelevant candidate attributes. FPAN() utilizes a gating mechanism to aggregate online feedback from users. UNICORN() proposes to formulate three decision-making problems in CRS as a unified policy learning task based on dynamic weighted graph. MIMCPL(@CITE[3]@) extends MCR to a more realistic scenario setting named MIMCR where the user may accept multiple attribute instances with the same type and multiple items with partially overlapped attributes. However, the existing MCR assumptions about user interest still deviate from real-world scenarios. Hence, we propose a novel framework called HutCRS to tackle these issues.\\nReferences:ID: 1. Title: Advances and Challenges in Conversational Recommender Systems: A Survey\\nID: 2. Title: Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems\\nID: 3. Title: Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning<|eot_id|>'},\n",
       " 6669: {'predict': 'Adversarial training (@CITE[1]@) is a widely used method to improve the robustness of neural networks against adversarial attacks. It is first proposed in computer vision and later applied to NLP (@CITE[2]@; @CITE[5]@; @CITE[4]@; @CITE[6]@). In NLP, the most common adversarial attack is word substitution attack, which can be categorized into two types: synonym substitution and random word substitution. Synonym substitution attacks (@CITE[7]@; @CITE[3]@; @CITE[5]@; @CITE[4]@) replace the words in the input sentence with their synonyms, while random word substitution attacks (@CITE[2]@; @CITE[6]@; ) randomly replace the words in the input sentence with other words. Adversarial training can be performed in different ways, including training on adversarial examples generated by different attacks (@CITE[2]@; @CITE[6]@), training on adversarial examples generated by the same attack with different perturbation sizes (@CITE[3]@; @CITE[4]@), and training on adversarial examples generated by different attacks and perturbation sizes (@CITE[7]@). In this paper, we propose to train on adversarial examples generated by different attacks and perturbation sizes to improve the robustness of NLP models.\\nReferences:ID: 1. Title: Adversarial Training Methods for Semi-Supervised Text Classification\\nID: 2. Title: Towards Robustness Against Natural Language Word Substitutions\\nID: 3. Title: Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation\\nID: 4. Title: Towards Robustness Against Natural Language Word Substitutions\\nID: 5. Title: TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP\\nID: 6. Title: On Evaluating Adversarial Robustness\\nID: 7. Title: Towards Improving Adversarial Training of NLP Models',\n",
       "  'ground_truth': '\\n\\nBased on the proximity assumption, researchers have explored methods to enhance the robustness and effectiveness of trained models by introducing disturbances to the original training samples, commonly known as adversarial training (Goodfellow et al., 2014b). Miyato et al. (@CITE[2]@) first proposed a NLP-oriented method that adds noise to the word embeddings of the input sample for adversarial training in text classification task. Similar approaches have been applied to adversarial training for pretrained language models (Liu et al., 2020) and other NLP tasks (@CITE[3]@), showing improvements in model performance. Despite their effectiveness, disturbance in the continuous spaces lack clear meaning in actual language, making them less interpretable. Recent works in adversarial training for NLP tasks consider modifications at the lexical level to make the perturbed samples more consistent with real samples. Early works use synonym substitution to explore proximate samples in the sample space. Huang et al. (@CITE[6]@) and Dong et al. (@CITE[3]@) used different sample space exploration methods to find samples that effectively enhance model robustness in adversarial training. The SSMBA method proposed by Ng et al. (@CITE[1]@) generates perturbed samples by using pretrained language models to complete corrupted texts. Recently, researchers consider using adversarial sample generation methods for adversarial training. Previous work (@CITE[4]@) has shown that using samples generated by a specific adversarial sample generation method for augmented training generally only improves the robustness against that method. Yoo et al. designed the A2T method (@CITE[5]@), which introduced a fast lexical substitution adversarial sample generation method into the model training process, achieving improved robustness against multiple model attacking methods. Current adversarial training methods for NLP models are mostly based on constrained searching in the sample space, and lack the ability to self-learn and adapt to models or datasets. Moreover, the lexical substitution based generation methods have potential problems regarding semantic soundness and syntactic correctness. (@CITE[7]@)\\nReferences:ID: 1. Title: Adversarial Training Methods for Semi-Supervised Text Classification\\nID: 2. Title: Towards Robustness Against Natural Language Word Substitutions\\nID: 3. Title: Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation\\nID: 4. Title: Towards Robustness Against Natural Language Word Substitutions\\nID: 5. Title: TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP\\nID: 6. Title: On Evaluating Adversarial Robustness\\nID: 7. Title: Towards Improving Adversarial Training of NLP Models<|eot_id|>'},\n",
       " 3605: {'predict': 'The goal of empathetic response generation is to generate a response that is empathetic towards the user. The empathetic response generation task has been explored by many previous studies. @CITE[2]@ propose to generate empathetic response by identifying the emotions expressed in the input text. @CITE[3]@ propose a multi-resolution model that considers both the coarse-grained dialogue-level and fine-grained token-level emotions. @CITE[1]@ propose a model that utilizes commonsense to draw more information about the user’s situation. @CITE[4]@ propose to generate empathetic response by using emotion and sentiment information of the input text. @CITE[1]@ propose a model that considers both the emotion and sentiment of the input text. However, these models are not capable of generating empathetic response for users who do not express their emotions in the input text. In this work, we propose a model that can generate empathetic response for users who do not express their emotions in the input text by identifying the user’s emotions and sentiments. We also propose a model that can generate empathetic response by considering the user’s emotions and sentiments.\\nReferences:ID: 1. Title: A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support\\nID: 2. Title: MIME: MIMicking Emotions for Empathetic Response Generation\\nID: 3. Title: EmpDG: Multiresolution Interactive Empathetic Dialogue Generation\\nID: 4. Title: CEM: Commonsense-aware Empathetic Response Generation',\n",
       "  'ground_truth': '\\n\\nSince empathy is a critical character for social chatting systems (; Pérez-@CITE[2]@, many studies have contributed to empathetic response generation. Earlier work mainly focuses on the affective aspect of empathy. MoEL ( adopts a mixture of experts architecture to combine outputs from different decoders, each of which represents one emotion. Based on the idea of emotion mixture, MIME (@CITE[1]@ takes emotion polarity (positive or negative) into account. Moreover, it uses emotion stochastic sampling and emotion mimicry to generate empathetic responses. propose to capture nuances of emotion at the token-level for decoding. Moreover, an adversarial learning framework is leveraged to involve user feedback. Having realized that ignorance of cognition impedes empathy in conversation, some recent methods involve both affection and cognition by incorporating emotion causes and effects. Wang et al. (2021) incorporate emotion causes into empathetic response generation by multi-hop reasoning from emotion causes to emotion states. @CITE[3]@ identify emotion causes from dialogue context, and use gates at the decoder to control the involvement of these emotion causes in the response generation. Kim et al. (2021) emphasize emotion causes in dialogue context by a rational speech act framework. These three methods identify emotion causes via a classifier, which detects whether there is a causal relationship between a conversation fragment and an emotion statement or word each time. CEM (@CITE[4]@ uses COMET, an if-then commonsense generator, to generate causes and effects of user experiences, and refines dialogue context with them for response generation. However, all these methods obtain causalities independently.\\nReferences:ID: 1. Title: A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support\\nID: 2. Title: MIME: MIMicking Emotions for Empathetic Response Generation\\nID: 3. Title: EmpDG: Multiresolution Interactive Empathetic Dialogue Generation\\nID: 4. Title: CEM: Commonsense-aware Empathetic Response Generation<|eot_id|>'},\n",
       " 6114: {'predict': 'The first work to explore adversarial attack in NLP is @CITE[1]@, which shows that the DNN-based text classifier is also prone to adversarial sample attack. The adversarial samples are generated by inserting, modifying, and removing the important words. Later, @CITE[2]@ proposes TextFooler, which is a strong baseline for generating natural adversarial text. It uses the gradient information of the target model to find the important words and then performs the attack by modifying, inserting, and removing the words. @CITE[3]@ proposes DeepWordBug, which is a black-box attack to generate adversarial text by identifying the important tokens. It uses the scoring strategies to find the critical tokens and applies the simple character-level transformations to the tokens to minimize the edit distance of the perturbation. However, these attacks are not applicable to the pre-trained models. @CITE[4]@ proposes A2T, which is a simple and efficient word substitution attack. It is based on the gradient information of the target model and uses the gradient information to find the important tokens. However, it still needs to train the model in the adversarial training framework. To solve this problem, @CITE[5]@ proposes BERT-Attack, which uses BERT to generate adversarial samples. It first masks a portion of the text and then uses BERT to generate the alternatives for the masked tokens. However, it is still a rule-based attack, which may not be effective in some cases. Recently, @CITE[6]@ proposes BAE, which is a black-box attack to generate adversarial samples. It first masks a portion of the text and then uses BERT-MLM to generate the alternatives for the masked tokens. However, it still needs to train the model in the adversarial training framework. @CITE[7]@ proposes GNA, which is a gradient-based attack to generate adversarial samples. It first calculates the gradient information of the target model and then uses the gradient information to find the important tokens. However, it still needs to train the model in the adversarial training framework. To solve this problem, @CITE[8]@ proposes GPT-Attack, which is a gradient-based attack to generate adversarial samples. It first calculates the gradient information of the target model and then uses the gradient information to find the important tokens. It is a black-box attack, which does not need to train the model in the adversarial training framework. It can be applied to the pre-trained models. However, it still needs to train the model in the adversarial training framework. To solve this problem, @CITE[9]@ proposes GPT-Attack++, which is a gradient-based attack to generate adversarial samples. It first calculates the gradient information of the target model and then uses the gradient information to find the important tokens. It is a black-box attack, which does not need to train the model in the adversarial training framework. It can be applied to the pre-trained models. However, it still needs to train the model in the adversarial training framework. Recently, @CITE[10]@ proposes BERT-Attack++, which is a black-box attack to generate adversarial samples. It first masks a portion of the text and then uses BERT to generate the alternatives for the masked tokens. It is a black-box attack, which does not need to train the model in the adversarial training framework. It can be applied to the pre-trained models. However, it still needs to train the model in the adversarial training framework. To solve this problem, we propose a new black-box attack, which does not need to train the model in the adversarial training framework. It can be applied to the pre-trained models. The key idea of our attack is to generate the adversarial samples by masking a portion of the text and then using BERT-MLM to generate the alternatives for the masked tokens. However, it is different from the previous works. It is a population-based optimization algorithm, which can find the optimal solution in the massive space of possible combinations of replacements. It is more effective and efficient than the previous works. In addition, it can be applied to the pre-trained models. We conduct the experiments on the IMDB dataset to demonstrate the effectiveness and efficiency of our attack.\\nReferences:ID: 1. Title: Intriguing properties of neural networks\\nID: 2. Title: Universal Sentence Encoder\\nID: 3. Title: Adversarial Attacks and Defences Competition\\nID: 4. Title: Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\\nID: 5. Title: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\\nID: 6. Title: Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs\\nID: 7. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 8. Title: Deep Text Classification Can be Fooled\\nID: 9. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 10. Title: Interpretable Adversarial Perturbation in Input Embedding Space for Text\\nID: 11. Title: Towards Improving Adversarial Training of NLP Models\\nID: 12. Title: Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\\nID: 13. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 14. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 15. Title: BAE: BERT-based Adversarial Examples for Text Classification\\nID: 16. Title: Generating Natural Language Adversarial Examples',\n",
       "  'ground_truth': '\\n\\nAdversarial attack has been well-studied in image and speech domains (; @CITE[1]@; ; @CITE[6]@. However, due to the discrete nature of language, the adversarial attack against pre-trained language models is much more difficult. Earlier works mainly focus on designing heuristic rules to generate adversaries, including swapping words (), transforming syntactic structure (@CITE[2]@), and paraphrasing by back-translation (@CITE[13]@; @CITE[7]@). However, these rule-based methods are label-intensive and difficult to scale. Recently, adversarial attack in NLP is framed as a combinatorial optimization problem. Mainstream studies design a series of search algorithms with two detached stages. In the first stage, they iteratively search for modification positions, including saliency-based ranking (; @CITE[5]@; @CITE[15]@; Garg and Ramakrishnan, 2020), gradient-based descent algorithm (@CITE[9]@; @CITE[10]@), and temporal-based searcher (@CITE[16]@). In the second stage, a series of studies design different substitution strategies, including dictionary method (@CITE[5]@), word embeddings (@CITE[3]@; @CITE[15]@), or language models (@CITE[14]@; @CITE[12]@; @CITE[4]@). In this paper, we formally propose to define the adversarial attack task as a sequential decision-making problem, further considering that scores in the next step are influenced by the editing results in the current step. The other line of recent studies is sampling-based methods. @CITE[8]@ and @CITE[11]@ apply genetic-based algorithm, Zang et al. (2019) propose a particle swarm optimization-based method, and generate adversaries via distribution approximate sampling. However, their execution time is much more expensive due to the properties of sampling, so it is unlikely to generate large-scale adversarial samples. In addition, conducts reinforcement learning on attacking the neural machine translation task, but their search path is fixed from left to right. In this paper, SDM-ATTACK can determine any search order to find the appropriate attack path.\\nReferences:ID: 1. Title: Intriguing properties of neural networks\\nID: 2. Title: Universal Sentence Encoder\\nID: 3. Title: Adversarial Attacks and Defences Competition\\nID: 4. Title: Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\\nID: 5. Title: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\\nID: 6. Title: Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs\\nID: 7. Title: Unsupervised Data Augmentation for Consistency Training\\nID: 8. Title: Deep Text Classification Can be Fooled\\nID: 9. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 10. Title: Interpretable Adversarial Perturbation in Input Embedding Space for Text\\nID: 11. Title: Towards Improving Adversarial Training of NLP Models\\nID: 12. Title: Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\\nID: 13. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 14. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 15. Title: BAE: BERT-based Adversarial Examples for Text Classification\\nID: 16. Title: Generating Natural Language Adversarial Examples<|eot_id|>'},\n",
       " 1368: {'predict': 'The efficient Transformer architectures are based on the idea of approximating the softmax attention matrix with a linear or recurrent layer, which reduces the quadratic complexity of the Transformer. The Performer (@CITE[1]@) approximates the softmax attention with a linear layer. The linear attention is approximated with a learned feature map and the dot-product attention is computed with a linear layer. The recurrent attention (@CITE[2]@; @CITE[3]@) approximates the softmax attention with a recurrent layer. The recurrent attention is computed with a recurrent layer and the dot-product attention is computed with a linear layer. The Linear Transformer (@CITE[3]@) approximates the softmax attention with a linear layer. The linear attention is computed with a linear layer. The Reformer (@CITE[3]@) approximates the softmax attention with a combination of linear and recurrent layers. The linear and recurrent attention are computed with a linear and recurrent layer, respectively. The Longformer (@CITE[3]@) approximates the softmax attention with a combination of linear and recurrent layers. The linear and recurrent attention are computed with a linear and recurrent layer, respectively. The BigBird (@CITE[3]@) approximates the softmax attention with a combination of linear and recurrent layers. The linear and recurrent attention are computed with a linear and recurrent layer, respectively. The Linear Transformer (@CITE[3]@) approximates the softmax attention with a linear layer. The linear attention is computed with a linear layer. The Linformer (@CITE[3]@) approximates the softmax attention with a linear layer. The linear attention is computed with a linear layer. The Fastformer (@CITE[3]@) approximates the softmax attention with a combination of linear and recurrent layers. The linear and recurrent attention are computed with a linear and recurrent layer, respectively.\\nReferences:ID: 1. Title: Rethinking Attention with Performers\\nID: 2. Title: Finetuning Pretrained Transformers into RNNs\\nID: 3. Title: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention',\n",
       "  'ground_truth': '\\n\\nTransformers suffer from complexity quadratic to the sequence length. Various methods have been proposed to improve the efficiency of Transformers. We classify them into three categories. The first category of efficient Transformer omits the calculation of part of the attention matrix, exploiting the sparsity of the attention matrix. Kitaev et al. (2020) groups queries into buckets by local sensitive hash and computes intra-bucket attention weights only. Zaheer et al. (2020); @CITE[3]@ limit attention matrix to specific sparse shapes. The second kind of efficient Transformers lowers matrix rank to reduce computation. @CITE[1]@ projects keys and values to constant length independent of sequence lengths. generates attention weights without keys. The third category of efficient Transformers, named Performer in this paper, leverages kernel methods to speed models up. ; view attention weights as a kernel function of queries and keys, so they can be approximated by random features. @CITE[2]@ relaxes the approximation requirement and finds that the model still works. ; Kasai et al. (2021) implement the unidirectional Performer as RNN so that their memory footprint is constant.\\nReferences:ID: 1. Title: Rethinking Attention with Performers\\nID: 2. Title: Finetuning Pretrained Transformers into RNNs\\nID: 3. Title: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention<|eot_id|>'},\n",
       " 6595: {'predict': 'Implicit aggregation methods do not explicitly aggregate the retrieved documents. Instead, they train a single model to directly generate the answer based on the concatenation of the original question and the retrieved documents. For example, @CITE[1]@; @CITE[2]@ train a generative model on the concatenation of the original question and the retrieved documents. They show that the model can effectively aggregate the retrieved documents by increasing the number of retrieved documents. Similarly, @CITE[3]@ train a generative model on the concatenation of the original question and the retrieved documents, and show that the model can effectively aggregate the retrieved documents. @CITE[3]@ also propose a retrieval-augmented language model, which is trained on the concatenation of the original question and the retrieved documents, and achieves a new state-of-the-art perplexity of 15.79 on the Wikitext-103 dataset. In addition, @CITE[3]@ propose a kNN-LM, which extends a pre-trained neural language model by linearly interpolating it with a k-NN model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Our work is also related to the retrieval-augmented language model proposed by @CITE[3]@, but we train the model with a new training objective and with a different pre-trained model.\\nReferences:ID: 1. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 2. Title: End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\\nID: 3. Title: Generalization through Memorization: Nearest Neighbor Language Models',\n",
       "  'ground_truth': '\\n\\nFiD (@CITE[2]@ uses a DPR to retrieve candidate documents, and then splices and encodes the candidate documents with the source text, and inputs them into the generator in the form of vectors. EMDR2 (@CITE[1]@ is similar to FiD, and it provides an end-to-end framework to train both the retriever and the generator. However, the query, key and value parts of FiD and EMDR2 do not contain future information, which will cause the value part to be similar to the query part and not conducive to the generation of future tokens. RETRO (Borgeaud et al., 2022) and KNN-LM (@CITE[3]@ set key and value parts as a piece of text, and added the continuation and the next token of this text in value part, respectively. However, they only calculate the similarity between the query and key while ignoring future information in value part, resulting in high similarity documents containing future information that may not necessarily help the generator. Our model sets both key and value parts as latent variables of a piece of text and its future continuation, and the query encoded by the VAE encoder also contains future information, so future information is also taken into account when calculating the similarity between query and key, making up for the shortcomings of previous stud-.\\nReferences:ID: 1. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\\nID: 2. Title: End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\\nID: 3. Title: Generalization through Memorization: Nearest Neighbor Language Models<|eot_id|>'},\n",
       " 7358: {'predict': 'Event extraction is a fundamental task in information extraction, which aims to extract specific event triggers and corresponding arguments from unstructured text. Existing methods can be roughly categorized into two types: 1) pipeline methods (@CITE[1]@; ; @CITE[4]@; @CITE[2]@; ; ; @CITE[6]@; @CITE[3]@; @CITE[5]@; @CITE[7]@; @CITE[7]@; @CITE[5]@, and 2) joint methods (@CITE[1]@; @CITE[7]@; @CITE[6]@; @CITE[5]@; @CITE[7]@. The pipeline methods decompose the event extraction task into trigger detection and argument extraction, which can be solved by classification and span extraction separately. The joint methods treat event extraction as a unified task, and solve the task with a single model. Most existing methods adopt the classification-based methods, which first predict the trigger and then extract the arguments. However, the trigger detection and argument extraction are closely related, and the classification-based methods usually ignore the interactions between them. Recently, the generation-based methods have attracted increasing attention, which treat the event extraction as a sequence generation task and generate the trigger and arguments jointly. However, the generation-based methods usually suffer from the data sparsity problem, which makes them difficult to learn the event extraction knowledge. In this paper, we propose a novel method, which leverages the pre-trained language model and generates the trigger and arguments in a multi-step manner. The trigger detection and argument extraction are solved in a joint manner, which can effectively capture the interactions between them.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Entity, Relation, and Event Extraction with Contextualized Span Representations\\nID: 4. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 5. Title: Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction\\nID: 6. Title: Dynamic Prefix-Tuning for Generative Template-based Event Extraction\\nID: 7. Title: Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction',\n",
       "  'ground_truth': '\\n\\nSEE extracts the event trigger and its arguments from a single sentence. It has achieved great progress in previous studies. employ various hand-designed features to extract events. With the popularity of deep learning, researchers use various neural networks to extract events, such as convolutional neural network (CNN)-based models (; @CITE[1]@), recurrent neural network (RNN)-based models (@CITE[6]@; Sha et al., 2018), and graph neural networks (GNN)-based models (@CITE[2]@). As pre-trained language models (PLM) have been proven to be powerful in language understanding and generation (@CITE[3]@, ; ; @CITE[5]@; Wang et al. (2021); ; @CITE[7]@; @CITE[4]@; use PLM-based methods to extract events.\\nReferences:ID: 1. Title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: Entity, Relation, and Event Extraction with Contextualized Span Representations\\nID: 4. Title: Event Extraction by Answering (Almost) Natural Questions\\nID: 5. Title: Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction\\nID: 6. Title: Dynamic Prefix-Tuning for Generative Template-based Event Extraction\\nID: 7. Title: Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction<|eot_id|>'},\n",
       " 5831: {'predict': 'Multilingual datasets are becoming increasingly important in NLP research. Recent datasets for summarization include MLSUM (@CITE[2]@, XL-Sum (@CITE[1]@, and IndicNLG (@CITE[3]@. These datasets provide a large number of examples, but are still limited by the availability of multilingual data. Our approach focuses on generating summaries from a single language, but provides a method to generate summaries for multiple languages. In contrast, previous work has focused on multilingual generation, but does not provide a method to generate summaries for multiple languages. In this work, we focus on English data, but the approach we present can be used for other languages as well. We also note that the datasets mentioned above are not available in the public domain, and require a license agreement to access.\\nReferences:ID: 1. Title: MLSUM: The Multilingual Summarization Corpus\\nID: 2. Title: XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages\\nID: 3. Title: IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages',\n",
       "  'ground_truth': '\\n\\nThe Columbia Newsblaster (@CITE[1]@) was one of the first datasets to include multilingual data for summarization. It includes around 500 news articles in English, Russian, and Japanese, with annotations for articles, texts, titles, images, and captions. In recent years, larger datasets with a greater variety of languages have been curated. MLSum (@CITE[3]@) comprises 1.5 million pairs of articles and summaries in French, German, Spanish, Russian, and Turkish. XLSum (@CITE[2]@) follows the format of XSUM and crawls the BBC websites to obtain 1 million pairs in 44 languages. This dataset also includes the headlines of the articles, along with their summaries. propose a headline generation dataset specifically for Indic languages, which includes 1.3 million data points in 11 Indic languages.\\nReferences:ID: 1. Title: MLSUM: The Multilingual Summarization Corpus\\nID: 2. Title: XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages\\nID: 3. Title: IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages<|eot_id|>'},\n",
       " 4133: {'predict': 'The multimodal representation learning is a critical step in many multimodal applications. Some works (@CITE[1]@; @CITE[4]@; @CITE[2]@) try to learn modality-invariant features to reduce the distributional modality gap and modality-specific features to capture the unique characteristics of each modality. MISA (@CITE[3]@) proposes a novel framework, which projects each modality to two distinct subspaces. The first subspace is modality-invariant, where the representations across modalities learn their commonalities and reduce the modality gap. The second subspace is modality-specific, which is private to each modality and captures their characteristic features. These representations provide a holistic view of the multimodal data, which is used for fusion that leads to task predictions. However, the modality-invariant features are not necessarily the most discriminative features, which may lose the discriminative information of the multimodal data. In this paper, we propose a novel framework to learn both modality-invariant and modality-specific features, and further use the modality-specific features to generate the discriminative features.\\nReferences:ID: 1. Title: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis\\nID: 2. Title: Discriminative Latent Semantic Graph for Video Captioning\\nID: 3. Title: Exploring Modality-shared Appearance Features and Modality-invariant Relation Features for Cross-modality Person Re-Identification\\nID: 4. Title: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis',\n",
       "  'ground_truth': '\\n\\nRecent studies in many multimodal tasks suggest that the model benefits from both shared and individual modality representations, including multimodal sentiment analysis (@CITE[4]@; @CITE[2]@; @CITE[3]@), person re-identification (Wei et al., 2021; ), cross-modal retrieval (@CITE[1]@), and image-sentence matching (, etc. MISA (@CITE[4]@) maps the multimodal features into two spaces as modality-invariant and -specific representations, and then fuses them for downstream classification. MCLNet (Hao et al., 2021) learns modality-invariant representations by minimizing inter-modal discrepancy and maximizing cross-modal similarity. VI-REID () builds an individual network for each modality, with a shared identity loss to learn modality-invariant representations. However, these methods map utterance-level representations to modality-invariant or -specific spaces via similarity cost functions, while AVSR is a sequence-to-sequence task that requires contextual semantic information. To this end, we propose an adversarial network with mutual information maximization to refine frame-level modality-invariant representations that subject to temporal contextual dependencies.\\nReferences:ID: 1. Title: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis\\nID: 2. Title: Discriminative Latent Semantic Graph for Video Captioning\\nID: 3. Title: Exploring Modality-shared Appearance Features and Modality-invariant Relation Features for Cross-modality Person Re-Identification\\nID: 4. Title: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis<|eot_id|>'},\n",
       " 4551: {'predict': 'Perturbation-based methods generate a perturbed version of the input to measure the importance of each word. The perturbed version is often generated by randomly replacing words with random words ( or by generating a new word based on the word to be perturbed ( or by removing the word (@CITE[4]@. These methods can be divided into two categories: gradient-based and non-gradient-based. Gradient-based methods use the gradient of the model output with respect to the input to compute the importance of each word. For example, @CITE[1]@ uses the gradient of the model output to compute the importance of each word. Non-gradient-based methods do not rely on the gradient of the model output to compute the importance of each word. For example, @CITE[2]@ generates a perturbed version of the input by replacing each word with a random word, and then measures the importance of each word by comparing the model output with the perturbed version. More recently, attention-based methods have been proposed to measure the importance of each word by analyzing the attention weights. For example, @CITE[3]@ measures the importance of each word by analyzing the attention weights of the Transformer model. @CITE[3]@ measures the importance of each word by analyzing the attention weights and the value of the Transformer model. However, these methods are not designed to measure the importance of each word in the context of the input. In contrast, our method is designed to measure the importance of each word in the context of the input, and it can be used to measure the importance of each word in any input.\\nReferences:ID: 1. Title: Model-Agnostic Interpretability of Machine Learning\\nID: 2. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 3. Title: Measuring the Mixing of Contextual Information in the Transformer\\nID: 4. Title: Quantifying Context Mixing in Transformers',\n",
       "  'ground_truth': '\\n\\nAnother set of interpretability methods, broadly classified as perturbation-based methods, encompasses widely recognized approaches such as LIME (@CITE[2]@) and SHAP (@CITE[3]@). However, these were excluded from our choice of comparison techniques, primarily due to their documented inefficiencies and reliability issues as highlighted by. We follow recent work (@CITE[4]@; @CITE[1]@) and mainly compare against gradient-based methods which have consistently proven to be more faithful than perturbation-based methods. @CITE[1]@ recently presented a method called Value zeroing to measure the extent of context mixing in encoder layers. Their approach involves setting the value representation of each token to zero in each layer and then calculating attribution scores by comparing the cosine distances with the original representations. Although they focused on local-level faithfulness, their global experiment has clear drawbacks due to its reliance on rollout aggregation and naive evaluation metric (cf. A.3).\\nReferences:ID: 1. Title: Model-Agnostic Interpretability of Machine Learning\\nID: 2. Title: A Diagnostic Study of Explainability Techniques for Text Classification\\nID: 3. Title: Measuring the Mixing of Contextual Information in the Transformer\\nID: 4. Title: Quantifying Context Mixing in Transformers<|eot_id|>'},\n",
       " 5608: {'predict': 'A number of recent works have probed the language abilities of pre-trained multimodal models. These works have shown that these models struggle to distinguish between similar image captions (@CITE[4]@; @CITE[2]@), to understand the meaning of verbs (@CITE[5]@), and to perform compositional reasoning (@CITE[6]@; @CITE[7]@; @CITE[1]@; @CITE[3]@). In this work, we also evaluate the language abilities of these models, but we focus on their ability to recover the referent of pronouns.\\nReferences:ID: 1. Title: FOIL it! Find One mismatch between Image and Language caption\\nID: 2. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 3. Title: Probing Image-Language Transformers for Verb Understanding\\nID: 4. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 5. Title: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality\\nID: 6. Title: Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality\\nID: 7. Title: Dealing with Semantic Underspecification in Multimodal NLP',\n",
       "  'ground_truth': '\\n\\nMotivated by the impressive performance of pre-trained multimodal Transformer models, a recent line of research investigated whether, and to what extent, this corresponds to a genuine understanding of visually-grounded language. Using FOIL (@CITE[6]@, a benchmark of minimally wrong image descriptions where all previous-generation models have proven to fail, some work (; @CITE[4]@ showed that Transformer-based models can almost perfectly distinguish between correct and foil sentences in a zero-shot setting. This indicated that models are good at grounding nouns in vision, likely due to their ITM pre-training objective. Consistently, an intrinsic evaluation of the embeddings learned by these models showed that they are better at representing highly concrete—rather than abstract—words (Pezzelle et al., 2021). Leveraging a FOIL-like paradigm, subsequent studies revealed that Transformer-based models struggle when dealing with verb arguments (SVO-Probes; @CITE[5]@, negation (@CITE[7]@, numbers, spatial relations (VALSE; @CITE[4]@, and expressions requiring compositional abilities (WinoGround; ; @CITE[1]@ or embedding semantically underspecified language (@CITE[2]@. Crucially, all this previous work focused on phenomena and tasks that require more than a basic language understanding to be properly mastered. As recently pointed out by @CITE[3]@, indeed, performing well on each of these benchmarks requires models to handle different skills, ranging from mathematics to pragmatics and reasoning abilities. Inspired by the work discussed above, we take a novel perspective and assess language-and-vision models on their genuine lexical and grammatical competence. We consider three linguistic constructions—active-passive voice, coordination, and relative clauses—that have been shown to be mastered even by preschool children. In this paper, we refer to them as Basic Language Abilities.\\nReferences:ID: 1. Title: FOIL it! Find One mismatch between Image and Language caption\\nID: 2. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 3. Title: Probing Image-Language Transformers for Verb Understanding\\nID: 4. Title: MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning\\nID: 5. Title: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality\\nID: 6. Title: Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality\\nID: 7. Title: Dealing with Semantic Underspecification in Multimodal NLP<|eot_id|>'},\n",
       " 202: {'predict': 'Diversified dialogue generation is a challenging task due to the inherent complexity of human conversations. Early works on diversified dialogue generation mainly focus on the design of neural architectures. @CITE[3]@ proposed a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. @CITE[5]@ proposed a hierarchical latent variable encoder-decoder model for generating dialogues. @CITE[4]@ proposed a conditional Wasserstein autoencoder specially designed for dialogue modeling. Recently, many works focus on improving the generation diversity. @CITE[2]@ proposed a SpaceFusion model to jointly optimize diversity and relevance. @CITE[1]@ proposed a diversity-promoting objective function for neural conversation models. @CITE[6]@ proposed a posterior generative adversarial network to improve the informativeness and coherence of generated responses. @CITE[7]@ proposed a topic-aware neural response generation model to generate informative and interesting responses for chatbots. Different from the above works, we focus on generating more personalized responses. We design a personalized dialogue generation model with a personalized module to generate personalized responses.\\nReferences:ID: 1. Title: Hierarchical Recurrent Attention Network for Response Generation\\nID: 2. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 3. Title: DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\\nID: 4. Title: Jointly Optimizing Diversity and Relevance in Neural Response Generation\\nID: 5. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 6. Title: Posterior-GAN: Towards Informative and Coherent Response Generation with Posterior Generative Adversarial Network\\nID: 7. Title: Topic Aware Neural Response Generation',\n",
       "  'ground_truth': '\\n\\nRecently, various researches have focused on neural dialogue models to generate diverse, informative, and relevant responses. One line of research attempts to extract relevant contexts from redundant dialogue history accurately (@CITE[6]@; @CITE[3]@; ). Another line of research tries to explicitly incorporate a latent variable to inject the variability of response in the decoding process (; Zhao et al., 2017). @CITE[3]@; @CITE[2]@; @CITE[5]@ further enriched the latent variable approach. Also, some works redesigned the objective function or automatically learned it by adversarial learning (@CITE[7]@, 2017a; ; @CITE[1]@), which improves diversity but brings a fragile training process. Finally, some researchers have adapted external knowledge, such as topic information (@CITE[4]@), persona (), knowledge base (Ghazvininejad et al., 2018). Unlike the above models to predict responses given a dialogue history, our method combines the future conversation with the dialogue history as the implicit conversation scenario, which contains comprehensive background information to guide the response generation.\\nReferences:ID: 1. Title: Hierarchical Recurrent Attention Network for Response Generation\\nID: 2. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 3. Title: DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\\nID: 4. Title: Jointly Optimizing Diversity and Relevance in Neural Response Generation\\nID: 5. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 6. Title: Posterior-GAN: Towards Informative and Coherent Response Generation with Posterior Generative Adversarial Network\\nID: 7. Title: Topic Aware Neural Response Generation<|eot_id|>'},\n",
       " 4712: {'predict': 'Query generation in text retrieval has been widely studied. The early works usually employed a template-based approach to generate queries from the original queries. For example, @CITE[3]@ used a template-based method to generate questions from natural language queries. Recently, many works have been done to generate queries from the original queries with the help of neural networks. @CITE[4]@ proposed a neural query generation method to generate queries for re-ranking. @CITE[1]@ proposed a few-shot query generation method to generate queries for dense retrieval. @CITE[2]@ proposed a neural corpus indexer that generates relevant document identifiers directly for a designated query. @CITE[2]@ proposed a prefix-aware weight-adaptive decoder architecture to optimize the recall performance of the neural corpus indexer. In this work, we propose a few-shot query generation method to generate queries for dense retrieval, and we use the prefix-aware weight-adaptive decoder architecture to optimize the recall performance of the neural corpus indexer.\\nReferences:ID: 1. Title: Passage Re-ranking with BERT\\nID: 2. Title: Learning Passage Impacts for Inverted Indexes\\nID: 3. Title: Promptagator: Few-shot Dense Retrieval From 8 Examples\\nID: 4. Title: A Neural Corpus Indexer for Document Retrieval',\n",
       "  'ground_truth': '\\n\\nQuery generation is originally introduced to the IR community to improve the traditional term-based methods. @CITE[2]@ showed that appending the T5-generated queries to the document \"Who sings does he love you?\" before building the inverted index can bring substantial improvements over BM25. More recently, @CITE[3]@ used generated queries as term expansion to learn better sparse representations for documents. In the context of dense retrieval, the generated pseudo-queries were used as extra data to improve the training process of dense retrieval. For example, @CITE[1]@ aimed to generate synthetic queries on the target domain for model training. @CITE[4]@ achieved excellent performance in few-shot retrieval with prompt enhanced query generation. In generative retrieval, also explored the use of pseudo-queries as extra data to train DSI. In this paper, we are the first to use pseudo-queries as one view of identifiers for generative retrieval.\\nReferences:ID: 1. Title: Passage Re-ranking with BERT\\nID: 2. Title: Learning Passage Impacts for Inverted Indexes\\nID: 3. Title: Promptagator: Few-shot Dense Retrieval From 8 Examples\\nID: 4. Title: A Neural Corpus Indexer for Document Retrieval<|eot_id|>'},\n",
       " 4452: {'predict': 'To overcome the domain shift problem, recent work has attempted to fine-tune the pre-trained model on the downstream data, either by fine-tuning the entire model (@CITE[4]@; @CITE[1]@) or by pre-training the model on the downstream data (@CITE[3]@; @CITE[2]@). However, these methods are not applicable to our setting since the downstream data is not available before the inference. Another line of work has attempted to pre-train the model on the downstream data without any labeled data (@CITE[2]@; @CITE[3]@). These methods can be used for zero-shot domain adaptation, but they are not suitable for the multi-domain setting since they require the model to learn to predict the domain label. In this paper, we propose to pre-train the model on the downstream data without any domain label. This allows the model to learn the domain-invariant features, which is beneficial for the multi-domain setting.\\nReferences:ID: 1. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 2. Title: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling\\nID: 3. Title: IMHO Fine-Tuning Improves Claim Detection\\nID: 4. Title: NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework',\n",
       "  'ground_truth': '\\n\\nAdaptive PreTraining (TAPT (@CITE[3]@) consists of taking off-the-shelf pretrained models like BERT and RoBERTa and engaging in further pretraining on the downstream datasets before finetuning them to the task at hand. TAPT has been shown to improve performance of off-the-shelf models in a variety of works (Logeswaran et al., 2019; @CITE[1]@; @CITE[4]@). Another way in which downstream data has been used is for retrieval to create a small pretraining corpus for efficient pretraining (@CITE[2]@). By contrast, our work pretrains models only on the downstream dataset, enabling a head-to-head comparison between the performance of off-the-shelf and self-pretrained models, and (in some situations) challenging the necessity of upstream corpora altogether.\\nReferences:ID: 1. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 2. Title: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling\\nID: 3. Title: IMHO Fine-Tuning Improves Claim Detection\\nID: 4. Title: NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework<|eot_id|>'},\n",
       " 5795: {'predict': 'KE methods have been applied to medical domain in entity and relation extraction. @CITE[3]@ proposes a fine-tuned BERT model with a dynamic attention mechanism to extract medical entities and relations. @CITE[1]@ presents a knowledge-enhanced collective inference model to extract biomedical entities and relations. @CITE[2]@ and @CITE[2]@ propose to inject knowledge base information into end-to-end joint entity and relation extraction. @CITE[3]@ and @CITE[3]@ propose to extract medical entities and relations with external knowledge in a multi-task setting. @CITE[3]@ proposes a knowledge-enhanced entity and relation extraction model for medical text. However, the above methods focus on medical domain in general, and the domain of COVID-19 is not considered. Moreover, the above methods mainly focus on the extraction of relations between entities, but not the extraction of events and their relations. The above methods also do not consider the extraction of events and their relations in the context of COVID-19, which is a critical task in the fight against COVID-19.\\nReferences:ID: 1. Title: Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text\\nID: 2. Title: Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference\\nID: 3. Title: Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution',\n",
       "  'ground_truth': '\\n\\nDespite that these works extract entity and relation in a joint way, they are not designed for medical text. While some previous works are designed for medical text, they ignore the help of medical domain knowledge when modeling (e.g., @CITE[3]@; ; ; @CITE[2]@). Only a few works incorporate medical domain knowledge to enhance the contexts of the medical text for joint entity and relation extraction (; @CITE[1]@). Among them, extract entity and relation jointly with knowledge-enhanced collective inference. @CITE[1]@ inject knowledge base information into entity and relation extraction and coreference resolution simultaneously. However, both and @CITE[1]@ fail to explicitly incorporate complicated relation contexts for their involved entities. Thus the interaction between entity and relation extraction will not be captured. Our method overcomes this drawback by providing relation contexts when modeling deep representations of entities. Most of the current work does not use domain knowledge, or only uses the entity context in domain knowledge, and does not include the relation context. While we explicitly model the entity context and relation context as the important context information for entity span, which improves the results of joint extraction.\\nReferences:ID: 1. Title: Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text\\nID: 2. Title: Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference\\nID: 3. Title: Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution<|eot_id|>'},\n",
       " 4613: {'predict': 'Reference-only caption evaluation methods are widely used in the image captioning community to evaluate the performance of image captioning models. The most widely used reference-only caption evaluation metric is CIDEr (@CITE[3]@, which measures the similarity between the generated captions and the reference captions. However, CIDEr has a limitation in that it is sensitive to the word order of the generated captions. To overcome this limitation, some recent works (@CITE[1]@; @CITE[2]@; propose to use BERT (@CITE[1]@ as a word embedding layer in CIDEr. In this work, we also use BERT as a word embedding layer in CIDEr. However, our method is different from the previous works in that we use BERT as a word embedding layer in CIDEr for the first time, and we also propose a new metric named CIDEr-BERT to evaluate the performance of image captioning models. Our method can measure the semantic similarity between the generated captions and the reference captions.\\nReferences:ID: 1. Title: CIDEr: Consensus-based Image Description Evaluation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: X-Linear Attention Networks for Image Captioning',\n",
       "  'ground_truth': '\\n\\nThis type of evaluation only employs human-written captions as references and measures text similarity as the evaluation score. Most widely used metrics such as BLEU-4 (, ROUGE-L (@CITE[1]@, METEOR (, CIDEr (@CITE[3]@ and SPICE (Anderson et al., 2016) all fall into this category. BLEU-4 calculates the precision of n-gram matches; ROUGE-L measures the recall of the longest common subsequence; METEOR utilizes wordnet-based synonym matching to relieve the shortage of exact word matching; CIDEr introduces tf-idf to re-weight the importance of different n-grams; SPICE converts captions into scene graphs for similarity comparison. One major limitation of the above metrics is that they cannot properly count synonym matches. To overcome this deficiency, BERT-S (Zhang et al., 2020) leverages learned embeddings from a pre-trained language model BERT ( to better measure semantic similarities. BERT-S++ (@CITE[2]@ further improves BERT-S by taking into account the variance of multiple references.\\nReferences:ID: 1. Title: CIDEr: Consensus-based Image Description Evaluation\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: X-Linear Attention Networks for Image Captioning<|eot_id|>'},\n",
       " 2731: {'predict': \"Since the emergence of BERT, many studies have been conducted to investigate the attention mechanism in transformers. Some studies focus on the attention patterns of pre-trained models. @CITE[4]@ analyze the attention patterns of BERT and show that certain attention heads correspond well to linguistic notions of syntax and coreference. @CITE[5]@ find that the attention patterns of BERT exhibit overparametrization, and disabling certain attention heads can improve the performance of downstream tasks. @CITE[3]@ find that the attention patterns of BERT exhibit bias and are sensitive to the input sequence length. Some studies focus on the attention patterns of fine-tuned models. @CITE[2]@ find that the attention patterns of BERT exhibit a high degree of variability across different tasks and that certain attention heads are more important than others for the performance of downstream tasks. @CITE[1]@ analyze the attention patterns of BART and find that the attention patterns of BART exhibit a higher degree of variability across different tasks than BERT. @CITE[1]@ analyze the attention patterns of BART and find that the attention patterns of BART exhibit a higher degree of variability across different tasks than BERT. @CITE[1]@ find that the attention patterns of BERT and BART exhibit a high degree of variability across different downstream tasks, and that certain attention heads are more important than others for the performance of downstream tasks.\\nReferences:ID: 1. Title: What Does BERT Look At? An Analysis of BERT's Attention\\nID: 2. Title: Revealing the Dark Secrets of BERT\\nID: 3. Title: A Multiscale Visualization of Attention in the Transformer Model\\nID: 4. Title: Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models\\nID: 5. Title: A Multiscale Visualization of Attention in the Transformer Model\",\n",
       "  'ground_truth': \"\\n\\nVarious works have investigated the attention head matrices in transformers (@CITE[4]@; ; ; @CITE[1]@; @CITE[2]@, often with the aid of visualization tools (@CITE[3]@; @CITE[5]@;. For examples, @CITE[3]@ visually explore attention patterns in BERT and GPT-2, analyzing their alignment with syntax. While Voita et al. (2019) characterize the functions of the attention heads in Machine Translation (MT) models (positional, syntactic, and rare words), and evaluate the importance of those head functions. More recently, Bian et al. (2021) find the redundancy in BERT’s attention patterns to be both phase-independent (pretrained and fine-tuned) and task-agnostic. Lastly, infer discourse structures from the attention patterns of language models (BERT and BART), and find discourse information to be consistently captured in the same heads even when fine-tuned for different tasks. In this paper, we also aim to find task-specific important attention patterns, but in contrast to previous work that identifies and categorizes attention patterns, we propose a pipeline to leverage these patterns in improving models’ performance and interpretability.\\nReferences:ID: 1. Title: What Does BERT Look At? An Analysis of BERT's Attention\\nID: 2. Title: Revealing the Dark Secrets of BERT\\nID: 3. Title: A Multiscale Visualization of Attention in the Transformer Model\\nID: 4. Title: Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models\\nID: 5. Title: A Multiscale Visualization of Attention in the Transformer Model<|eot_id|>\"},\n",
       " 6298: {'predict': 'AMR is a formal representation of semantics, which has been widely used in various NLP tasks. @CITE[2]@ first proposed to use AMR for dialogue modeling. @CITE[4]@ proposed a method to construct AMR graphs for dialogue, which has been used as the baseline of our work. @CITE[3]@ used AMR for dialogue summarization. @CITE[1]@ used AMR for dialogue segmentation. These works all use the graph structure to capture the semantic relations between utterances. However, they do not consider the relations between utterances and the dialogue context, which is important for dialogue segmentation. In this work, we propose to construct AMR for dialogue topic segmentation, which captures both the semantic relations between utterances and the relations between utterances and the dialogue context.\\nReferences:ID: 1. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 2. Title: Advances in domain independent linear text segmentation\\nID: 3. Title: Semantic Representation for Dialogue Modeling\\nID: 4. Title: AMR Parsing via Graph-Sequence Iterative Inference',\n",
       "  'ground_truth': '\\n\\nA topic segment in a long dialogue is a consecutive sequence of topically-coherent lines. Topic segmentation can provide valuable high-level understanding of complex long dialogue structures, thus contributing important insights for summarization. Within a topic segment, speakers may respond to each other, request and perform actions, and refer to entities mentioned by others. To make sure an AMR graph captures coherent semantics, we first perform topic segmentation on long dialogue. We adopt an existing strategy (@CITE[2]@) that combines the classic topic segment algorithm C99 (@CITE[4]@) with SentenceBERT (, to segment long dialogues into topic segments with reasonable lengths. To construct topic segment AMRs after segmentation, we adapt the steps in @CITE[3]@ to build dialogue AMRs and we additionally introduce speaker nodes, utterance nodes, and a new procedure of node merging leveraging coreference relations. Given a topic segment consisting of multiple utterances, we use the AMR parser by @CITE[1]@ to obtain an AMR graph for each utterance. And then as illustrated by Figure 1, we construct the topic segment AMR by connecting utterance AMRs with utterance nodes, speaker nodes, and a topic segment node (the root node) with appropriate edges, and perform node merging based on coreferences. For topic segment si, AMR_algo (si) = {Vi, Ei}. Compared with @CITE[3]@, we want our proposed speaker nodes and utterance nodes to encode the fine-grained hierarchical information from different levels of the topic segment AMR (synthesizing multiple utterances by one speaker, multiple sentences within one utterance, etc.). This is made possible by our graph encoder, which exploits speaker and utterance nodes’ abundant and unique interactions with other AMR concepts (see Section 4.1). We perform node merging based on coreference relations to obtain the final topic segment AMR.\\nReferences:ID: 1. Title: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization\\nID: 2. Title: Advances in domain independent linear text segmentation\\nID: 3. Title: Semantic Representation for Dialogue Modeling\\nID: 4. Title: AMR Parsing via Graph-Sequence Iterative Inference<|eot_id|>'},\n",
       " 1257: {'predict': 'In contrast to our task, previous work on compositional generalization has focused on natural language tasks. For example, @CITE[4]@ proposed a benchmark for compositional generalization in natural language questions, where the test set has unseen combinations of sub-words. @CITE[3]@ proposed to decompose questions into simpler ones, which can be answered by off-the-shelf question answering models. @CITE[1]@ and @CITE[2]@ proposed to answer complex questions on semi-structured tables using question-answer pairs as supervision. Spider (@CITE[2]@ proposed a large-scale dataset for complex and cross-domain semantic parsing and text-to-SQL tasks, where different complex SQL queries and databases appear in the train and test sets. However, in their task, the test set still contains the same database schema as the train set. In contrast, our task requires the model to generalize to both new SQL queries and new database schemas. Moreover, the test set in our task has new database schema that has not been seen in the training set, which makes the task much more challenging.\\nReferences:ID: 1. Title: Measuring Compositional Generalization: A Comprehensive Method on Realistic Data\\nID: 2. Title: Unsupervised Question Decomposition for Question Answering\\nID: 3. Title: Compositional Semantic Parsing on Semi-Structured Tables\\nID: 4. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task',\n",
       "  'ground_truth': '\\n\\nThere has been work in program synthesis that generates SQL programs for unseen database schemas (@CITE[2]@; @CITE[4]@). However, these work operate on web or Wikipedia tables with small schemas. For example, in WikiTable-Questions (@CITE[3]@) the average number of columns in a table is 5.8 and in Spider dataset (@CITE[1]@), the average number of columns is 28.1. On the other hand, our model has to consider all possible Freebase relations (in thousands). Previous work perform schema-aware encoding which is not possible in our case because of the large number of relations. The retrieve step of CBR-KBQA can be seen as a pruning step which narrows the number of candidate relations by retrieving relevant questions and their logical forms.\\nReferences:ID: 1. Title: Measuring Compositional Generalization: A Comprehensive Method on Realistic Data\\nID: 2. Title: Unsupervised Question Decomposition for Question Answering\\nID: 3. Title: Compositional Semantic Parsing on Semi-Structured Tables\\nID: 4. Title: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task<|eot_id|>'},\n",
       " 6161: {'predict': 'Federated learning (@CITE[2]@ has been widely applied to various NLP tasks, including text classification, sentiment analysis, and machine translation (@CITE[1]@. However, few works have explored federated learning for NER. @CITE[3]@ proposed a federated learning framework for NER, which uses a multi-task learning framework to train the NER model and the entity linking model. However, this framework requires a large number of labeled data to train the linking model, which is not practical in low-resource scenarios. @CITE[3]@ proposed a lightweight tuning paradigm for low-resource NER via pluggable prompting. However, this method requires to pre-train a base model on a large-scale corpus, which is not suitable for NER tasks that require domain-specific knowledge. In this paper, we propose a federated learning framework for NER that does not require a large number of labeled data and can be applied to any domain.\\nReferences:ID: 1. Title: A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting',\n",
       "  'ground_truth': '\\n\\n@CITE[2]@ presents a pilot study of federated learning for clin- ical NER. introduces adversarial training to solve the adversarial attack problem for federated NER. One of the major problems is that these approaches require sharing or com- municating the whole NER model (or its encoder) between the server and platforms of federated learn- ing. This will induce huge communication cost in training with the recent Pretrained Langauge Mod- els (PLMs) (@CITE[3]@; @CITE[1]@, i.e., containing hundreds of millions of parameters. In this work, we study using a prompt- based pretrained NER model ( for our federated learning, thus only communicat- ing prompt-related parameters. This significantly reduces the communication cost compared to fine tuning all the pretrained parameters. Further, dif- ferent from @CITE[2]@;, we focus on federated domain adaptation that ef- ficiently transfer knowledge among platforms of different domains. ( investigates knowledge distillation in federated learning with NER, but is not targeting the federated domain adaptation problem as in our setting.\\nReferences:ID: 1. Title: A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection\\nID: 2. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 3. Title: LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting<|eot_id|>'},\n",
       " 224: {'predict': 'Cross-lingual NLU has been studied in several areas, including machine translation (@CITE[5]@;, parsing (@CITE[2]@;, question answering (@CITE[6]@, textual entailment (@CITE[3]@;, and relation extraction (@CITE[7]@; ; @CITE[4]@;. Our work is related to the multilingual RE models. However, we focus on zero-shot RE and propose a novel approach that does not rely on a pivot language. Existing zero-shot RE models are based on multilingual embeddings (@CITE[1]@;, projection (@CITE[7]@, or cross-lingual encoders (@CITE[4]@;. In particular, @CITE[4]@ used a pivot language to translate Wikipedia sentences and then trained a multilingual BERT model. @CITE[7]@ proposed a multilingual projection model to map Wikipedia sentences from different languages into a shared space. However, these models are not applicable to our setting, where we have no pivot language. @CITE[1]@ proposed a cross-lingual encoder to map sentences into a shared space, but the model requires a large amount of multilingual data to train. In contrast, we propose a simple yet effective approach to perform zero-shot RE without any multilingual data.\\nReferences:ID: 1. Title: One Model to Rule them all: Multitask and Multilingual Modelling for Lexical Analysis\\nID: 2. Title: Parameter sharing between dependency parsers for related languages\\nID: 3. Title: Baselines and test data for cross-lingual inference\\nID: 4. Title: Inducing Language-Agnostic Multilingual Representations\\nID: 5. Title: Multilingual Open Relation Extraction Using Cross-lingual Projection\\nID: 6. Title: Multilingual Relation Extraction using Compositional Universal Schema\\nID: 7. Title: X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension',\n",
       "  'ground_truth': '\\n\\nCross-lingual learning has a fairly short history in NLP and has mainly been restricted to traditional NLP tasks, such as PoS tagging, morphological inflection, and parsing. In contrast to these tasks, which have seen much cross-lingual attention (Plank et al., 2016; @CITE[4]@; @CITE[7]@; de @CITE[1]@), there has been relatively little work on cross-lingual NLU, partly due to a lack of benchmark datasets. Existing work has mainly been focused on NLI (; ; @CITE[3]@), and to a lesser degree on RE (@CITE[2]@; @CITE[6]@) and QA (; @CITE[5]@).\\nReferences:ID: 1. Title: One Model to Rule them all: Multitask and Multilingual Modelling for Lexical Analysis\\nID: 2. Title: Parameter sharing between dependency parsers for related languages\\nID: 3. Title: Baselines and test data for cross-lingual inference\\nID: 4. Title: Inducing Language-Agnostic Multilingual Representations\\nID: 5. Title: Multilingual Open Relation Extraction Using Cross-lingual Projection\\nID: 6. Title: Multilingual Relation Extraction using Compositional Universal Schema\\nID: 7. Title: X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension<|eot_id|>'},\n",
       " 3002: {'predict': 'ROUGE (Lin, 2004) is a widely used metric for evaluating summarization systems. However, it is biased towards surface lexical similarities, making it unsuitable for the evaluation of abstractive summarization. To overcome this, several recent studies have proposed to use word embeddings (e.g., BERT (Devlin et al., 2019)) for semantic similarity evaluation (@CITE[3]@; @CITE[1]@; @CITE[5]@). Besides, a line of work has been proposed to use textual entailment (TE) (Dagan et al., 2006) for evaluating factual consistency. @CITE[4]@ and @CITE[7]@ propose to use sentence-level TE for evaluating factual consistency. @CITE[6]@ and @CITE[2]@ propose to use dependency arc TE for evaluating factual consistency. @CITE[8]@ propose to use counterfactual estimation for evaluating factual consistency. @CITE[3]@ propose to use semantic graph representations for evaluating factual consistency. @CITE[3]@ propose to use question answering for evaluating factual consistency. However, these approaches are limited to specific datasets or tasks, and cannot be directly applied to other tasks. In this paper, we propose a new approach to evaluate factual consistency based on semantic matching. Our approach is different from existing works in that we propose a new semantic matching method based on semantic matching.\\nReferences:ID: 1. Title: Better Summarization Evaluation with Word Embeddings for ROUGE\\nID: 2. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 3. Title: Evaluating Factuality in Generation with Dependency-level Entailment\\nID: 4. Title: Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\\nID: 5. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 6. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 7. Title: Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation\\nID: 8. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations',\n",
       "  'ground_truth': '\\n\\nGiven the limitations of the ROUGE metric (Cha- ganty et al., 2018; Kry´ nski et al., 2019), new metrics are proposed to better measure two funda- mental qualities of summary: relevance and factual consistency. Relevance metrics such as ROUGE variants (@CITE[1]@; @CITE[4]@; and BERTScore ( measure whether a summary contains the main ideas of the source. A factual consis- tency metric assesses whether a summary is factu- ally consistent with the source (@CITE[6]@; @CITE[8]@. Due to the high rate of factual errors in the summaries generated by short-document models (@CITE[3]@; @CITE[5]@, there have been substantial efforts in developing effective metrics which can measure the factuality of a summary (@CITE[2]@; @CITE[7]@;.\\nReferences:ID: 1. Title: Better Summarization Evaluation with Word Embeddings for ROUGE\\nID: 2. Title: BERTScore: Evaluating Text Generation with BERT\\nID: 3. Title: Evaluating Factuality in Generation with Dependency-level Entailment\\nID: 4. Title: Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\\nID: 5. Title: Faithful to the Original: Fact Aware Neural Abstractive Summarization\\nID: 6. Title: On Faithfulness and Factuality in Abstractive Summarization\\nID: 7. Title: Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation\\nID: 8. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations<|eot_id|>'},\n",
       " 5336: {'predict': 'The safety issues of LMs have been extensively studied in recent years. Toxicity (@CITE[5]@; @CITE[6]@; @CITE[5]@; @CITE[1]@ is a common problem of LMs, which may lead to offensive and harmful responses. Other safety issues include bias (@CITE[4]@; @CITE[3]@, information leakage (@CITE[2]@, and hallucination (@CITE[6]@; @CITE[1]@. Our work is related to the bias and hallucination problems, but we focus on the problem of hallucination in the context of commonsense reasoning, which has not been well studied.\\nReferences:ID: 1. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 2. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 3. Title: Persistent Anti-Muslim Bias in Large Language Models\\nID: 4. Title: Extracting Training Data from Large Language Models\\nID: 5. Title: SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\\nID: 6. Title: Red Teaming Language Models with Language Models',\n",
       "  'ground_truth': '\\n\\nLanguage models have been shown to exhibit various safety issues, such as generating offensive contents (@CITE[1]@, reinforcing unfairness/discrimination (@CITE[1]@; @CITE[5]@, leaking privacy information (@CITE[4]@;, and promoting illegal activities (. Recently, new safety issues that emerge with LLMs attract increasing attention since the remarkable capabilities of LLMs can lead to a significant threat (@CITE[6]@; @CITE[2]@. Different from prior works on probing explicit toxic outputs from LLMs that can be easily detected with existing toxicity classifiers, we investigate whether LLMs can generate undetectable implicit toxic outputs. The most similar work to ours is TOXIGEN (@CITE[3]@, which proposes an adversarial classifier-in-the-loop decoding method to generate implicit toxic outputs with GPT-3 via few-shot prompting. However, in contrast to TOXIGEN, which solely focuses on generating toxic statements targeting minority groups, we investigate how to generate toxic responses encompassing diverse toxic types and linguistic features. Additionally, we go beyond simple prompting and propose to further induce implicit toxicity in LLMs via reinforcement learning, achieving significantly higher attack success rates.\\nReferences:ID: 1. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 2. Title: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\\nID: 3. Title: Persistent Anti-Muslim Bias in Large Language Models\\nID: 4. Title: Extracting Training Data from Large Language Models\\nID: 5. Title: SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions\\nID: 6. Title: Red Teaming Language Models with Language Models<|eot_id|>'},\n",
       " 568: {'predict': 'Recent years have seen a surge in interest in developing open-domain conversational models (@CITE[11]@; @CITE[7]@; @CITE[8]@; @CITE[6]@; @CITE[12]@; @CITE[13]@; @CITE[3]@; @CITE[14]@; @CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[9]@; @CITE[10]@; @CITE[16]@; @CITE[17]@; @CITE[18]@; @CITE[15]@; @CITE[19]@; @CITE[20]@; @CITE[19]@; @CITE[16]@; @CITE[21]@; @CITE[13]@; @CITE[22]@; @CITE[18]@; @CITE[19]@; @CITE[23]@; @CITE[24]@; @CITE[25]@; @CITE[26]@. However, most of these models are evaluated on a single metric, usually perplexity (@CITE[11]@; @CITE[7]@; @CITE[8]@; @CITE[6]@; @CITE[12]@; @CITE[13]@; @CITE[3]@; @CITE[14]@; @CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[9]@; @CITE[10]@; @CITE[16]@; @CITE[17]@; @CITE[15]@; @CITE[19]@; @CITE[20]@; @CITE[19]@; @CITE[16]@; @CITE[21]@; @CITE[13]@; @CITE[22]@; @CITE[18]@; @CITE[19]@; @CITE[23]@; @CITE[24]@; @CITE[25]@; @CITE[26]@, or BLEU (@CITE[11]@; @CITE[7]@; @CITE[8]@; @CITE[6]@; @CITE[12]@; @CITE[13]@; @CITE[3]@; @CITE[14]@; @CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[5]@; @CITE[9]@; @CITE[10]@; @CITE[16]@; @CITE[17]@; @CITE[15]@; @CITE[19]@; @CITE[20]@; @CITE[19]@; @CITE[16]@; @CITE[21]@; @CITE[13]@; @CITE[22]@; @CITE[18]@; @CITE[19]@; @CITE[23]@; @CITE[24]@; @CITE[25]@; @CITE[26]@, which are not always aligned with human judgments. To better understand the limitations of these models, we need to incorporate human evaluations into the evaluation process.\\nReferences:ID: 1. Title: A Persona-Based Neural Conversation Model\\nID: 2. Title: The Second Conversational Intelligence Challenge (ConvAI2)\\nID: 3. Title: TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\\nID: 4. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 5. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 6. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 7. Title: Towards Exploiting Background Knowledge for Building Conversation Systems\\nID: 8. Title: A Controllable Model of Grounded Response Generation\\nID: 9. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 10. Title: Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\\nID: 11. Title: Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory\\nID: 12. Title: Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution\\nID: 13. Title: Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution\\nID: 14. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 15. Title: Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems\\nID: 16. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 17. Title: Neural Approaches to Conversational AI\\nID: 18. Title: What makes a good conversation? How controllable attributes affect human judgments\\nID: 19. Title: Controlling Output Length in Neural Encoder-Decoders\\nID: 20. Title: A Study on Evaluation Metrics for Open-Domain Dialogue Systems\\nID: 21. Title: A Study on Evaluation Metrics for Open-Domain Dialogue Systems\\nID: 22. Title: How to Evaluate Dialogue Systems?\\nID: 23. Title: On the Evaluation of Dialogue Systems\\nID: 24. Title: A Study on Evaluation Metrics for Open-Domain Dialogue Systems\\nID: 25. Title: A Study on Evaluation Metrics for Open-Domain Dialogue Systems\\nID: 26. Title: How to Evaluate Dialogue Systems?',\n",
       "  'ground_truth': '\\n\\nGenerating human-like responses involves overcoming a variety of challenges such as personalization (@CITE[13]@; @CITE[6]@; @CITE[12]@; @CITE[10]@; @CITE[2]@), knowledge grounding (@CITE[18]@; Gopalakrishnan et al., 2019; @CITE[8]@; @CITE[3]@; @CITE[1]@), emotions (@CITE[17]@; ; @CITE[19]@; @CITE[5]@; @CITE[5]@), diversity (@CITE[7]@; @CITE[9]@; @CITE[14]@; @CITE[4]@) and so on. In terms of controlled dialogue generation, @CITE[15]@ studied conditional generative models (@CITE[16]@) and weighted decoding (@CITE[11]@) in controlling models trained on persona-. @CITE[15]@ concluded that controlling specificity, relatedness, and repetition increase human engagement, motivating the extension of controllability to styles and topics.\\nReferences:ID: 1. Title: A Persona-Based Neural Conversation Model\\nID: 2. Title: The Second Conversational Intelligence Challenge (ConvAI2)\\nID: 3. Title: TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\\nID: 4. Title: Plug and Play Language Models: A Simple Approach to Controlled Text Generation\\nID: 5. Title: Wizard of Wikipedia: Knowledge-Powered Conversational agents\\nID: 6. Title: A Knowledge-Grounded Neural Conversation Model\\nID: 7. Title: Towards Exploiting Background Knowledge for Building Conversation Systems\\nID: 8. Title: A Controllable Model of Grounded Response Generation\\nID: 9. Title: DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset\\nID: 10. Title: Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\\nID: 11. Title: Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory\\nID: 12. Title: Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution\\nID: 13. Title: Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution\\nID: 14. Title: A Diversity-Promoting Objective Function for Neural Conversation Models\\nID: 15. Title: Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems\\nID: 16. Title: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\\nID: 17. Title: Neural Approaches to Conversational AI\\nID: 18. Title: What makes a good conversation? How controllable attributes affect human judgments\\nID: 19. Title: Controlling Output Length in Neural Encoder-Decoders<|eot_id|>'},\n",
       " 4734: {'predict': 'GLUE (@CITE[1]@) and SuperGLUE (@CITE[2]@) are widely used for evaluating the performance of NLU models. These benchmarks are based on English datasets and are limited to 8 languages, including English. XTREME (@CITE[3]@) is a multilingual benchmark, covering 40 languages and 10 tasks. XTREME-R (@CITE[4]@) is an extension of XTREME, with a broader range of tasks and languages. Both XTREME and XTREME-R are limited to NLU tasks. These benchmarks are not representative of the diversity of languages and tasks, and therefore, do not provide a comprehensive picture of the performance of models on a wide range of languages and tasks. The proposed benchmark, X-Bench, addresses these limitations by covering a diverse range of tasks and languages.\\nReferences:ID: 1. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 2. Title: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\nID: 3. Title: The State and Fate of Linguistic Diversity and Inclusion in the NLP World\\nID: 4. Title: XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation',\n",
       "  'ground_truth': '\\n\\nBenchmarks like GLUE () and SuperGLUE (@CITE[4]@) have driven research on multitask models for English and IndicGULE (@CITE[3]@) has been created to benchmark performance on Indic languages. Similarly, there have been multiple efforts to drive research on crosslingual, multitask models. Important among them are XGLUE (), XTREME (), and XTREME-R (@CITE[1]@). In order to accommodate a diverse set of languages, these benchmarks have a limited representation of Indic languages. Also, most evaluation sets are automatically translated or generated which is known to have problems (@CITE[2]@). In this work, we aim to fill this gap by presenting an Indic family-specific evaluation benchmark consisting of 9 tasks with human-created or human-translated test sets.\\nReferences:ID: 1. Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\\nID: 2. Title: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\nID: 3. Title: The State and Fate of Linguistic Diversity and Inclusion in the NLP World\\nID: 4. Title: XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation<|eot_id|>'},\n",
       " 826: {'predict': 'The existing methods for entity alignment can be roughly divided into three categories: embedding-based methods, graph neural network-based methods, and graph convolutional network-based methods. Embedding-based methods (@CITE[10]@; @CITE[7]@; @CITE[3]@; @CITE[9]@; @CITE[8]@; @CITE[11]@; @CITE[5]@; @CITE[1]@; @CITE[2]@) align entities based on their semantic embeddings. Graph neural network-based methods (@CITE[4]@; ; @CITE[6]@; @CITE[4]@; @CITE[1]@; @CITE[11]@) align entities based on their graph structures. Graph convolutional network-based methods (@CITE[5]@; @CITE[12]@) align entities based on their graph convolutional network structures. In addition, some methods (@CITE[8]@; @CITE[1]@; @CITE[6]@; @CITE[11]@) integrate multiple methods to achieve better performance. However, most of the existing methods are based on the assumption that entities in different KGs have the same semantics, which may not always hold in practice. In this paper, we propose a novel method for entity alignment based on the assumption that entities in different KGs have the same semantics, but may have different names. Our method aligns entities based on the similarities of their semantic embeddings.\\nReferences:ID: 1. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 2. Title: Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs\\nID: 3. Title: Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs\\nID: 4. Title: Multi-Channel Graph Neural Network for Entity Alignment\\nID: 5. Title: Multi-Channel Graph Neural Network for Entity Alignment\\nID: 6. Title: Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation\\nID: 7. Title: Deep Graph Matching Consensus\\nID: 8. Title: Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment\\nID: 9. Title: Jointly Learning Entity and Relation Representations for Entity Alignment\\nID: 10. Title: Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment\\nID: 11. Title: Visual Pivoting for (Unsupervised) Entity Alignment',\n",
       "  'ground_truth': '\\n\\nEmbedding-based entity alignment is first attempted in MTransE (, which jointly learns a translational embedding model and a transform-based alignment model for two KGs. Later studies generally follow three lines of improvement. (i) The first line improves the embedding technique to better suit the alignment task, including contextual translation techniques (), long-term dependency techniques (), and neighborhood aggregation (or GNN-based) ones (; ; ;,a;. (ii) The second line focuses on effective alignment learning with limited supervision. Some leverage semi-supervised learning techniques to resolve the training data insufficiency issue, including self-learning (; ) and co-training (@CITE[9]@). (iii) Another line of research seeks to retrieve auxiliary or indirect supervision signals from profile information or side features of entities, such as entity attributes (Sun et al., 2017; @CITE[11]@; @CITE[5]@; @CITE[5]@, literals (@CITE[11]@, 2020b; ), free text (@CITE[8]@), pre-trained language models (; ), or visual modalities (@CITE[8]@). Due to the large body of recent advances, we refer readers to a more comprehensive summarization in the survey (@CITE[7]@).\\nReferences:ID: 1. Title: Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\\nID: 2. Title: Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs\\nID: 3. Title: Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs\\nID: 4. Title: Multi-Channel Graph Neural Network for Entity Alignment\\nID: 5. Title: Multi-Channel Graph Neural Network for Entity Alignment\\nID: 6. Title: Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood Aggregation\\nID: 7. Title: Deep Graph Matching Consensus\\nID: 8. Title: Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment\\nID: 9. Title: Jointly Learning Entity and Relation Representations for Entity Alignment\\nID: 10. Title: Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment\\nID: 11. Title: Visual Pivoting for (Unsupervised) Entity Alignment<|eot_id|>'},\n",
       " 4635: {'predict': 'The recent success of multilingual PLMs (@CITE[5]@; @CITE[3]@; @CITE[1]@ has sparked a growing interest in zero-shot cross-lingual transfer. This line of research aims to leverage multilingual PLMs as a universal encoder for zero-shot transfer across different languages. @CITE[4]@ investigate the factors that contribute to the cross-lingual ability of multilingual BERT and find that the depth of the network is an integral part of it. @CITE[2]@ propose a fine-tuning method to align word embeddings of multilingual PLMs on parallel corpora, which can be used to improve the performance of zero-shot cross-lingual transfer. @CITE[2]@ further propose a method to adapt multilingual PLMs to a target language by fine-tuning on in-domain parallel corpora. The method is shown to be effective for zero-shot cross-lingual transfer in the downstream tasks of NER and POS tagging. In this work, we focus on zero-shot cross-lingual transfer in the downstream task of NLI. We show that the performance of zero-shot cross-lingual transfer in NLI is significantly improved by fine-tuning on in-domain parallel corpora, which is consistent with the findings of @CITE[2]@. However, our work differs from theirs in that we focus on zero-shot cross-lingual transfer in NLI, and we propose a method that dynamically adjusts the weights of training examples based on the difficulty of the examples, which can further improve the performance of zero-shot cross-lingual transfer.\\nReferences:ID: 1. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 2. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 3. Title: Cross-Lingual Ability of Multilingual BERT: An Empirical Study\\nID: 4. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora\\nID: 5. Title: Dice Loss for Data-imbalanced NLP Tasks',\n",
       "  'ground_truth': '\\n\\nThis is a popular method for low-resource languages that relies on shared multilingual representations to help low-resource languages by transferring information from high-resource language NER models. Particularly, NER models finetuned on pre-trained language models like mBERT (Devlin et al., 2019), XLM-RoBERTa (@CITE[3]@ for high resource languages are used to tag low-resource language sentences (zero-shot NER). @CITE[4]@ demonstrate that multilingual models perform well for zero-shot NER transfer on related languages. However, zero-shot performance is limited for distant languages, particularly when there are structural/word order differences between the two languages (@CITE[2]@. Unlike many other NLP tasks, zero-shot cross-lingual NER has seen only limited benefit from recent advances in cross-lingual representation learning (@CITE[5]@. To overcome this limitation, a knowledge distillation approach has been proposed to create synthetic in-language training data (@CITE[1]@. Here, the source language teacher NER model is used to create distillation data in the target language via zero-shot cross-lingual transfer, which is used to train a target language model. We find that the projection-based approaches outperform zero-shot transfer and knowledge distillation approaches.\\nReferences:ID: 1. Title: Unsupervised Cross-lingual Representation Learning at Scale\\nID: 2. Title: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\\nID: 3. Title: Cross-Lingual Ability of Multilingual BERT: An Empirical Study\\nID: 4. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora\\nID: 5. Title: Dice Loss for Data-imbalanced NLP Tasks<|eot_id|>'},\n",
       " 1021: {'predict': 'Dialogue emotion detection aims to predict the emotion of a speaker given a dialogue history. Previous works have explored various approaches for this task, including recurrent neural network (RNN) based methods (@CITE[3]@; @CITE[2]@), graph neural network (GNN) based methods (@CITE[1]@; @CITE[4]@), and transformer based methods (Zhou et al., 2021). These methods typically utilize the textual content of the dialogue and often also incorporate multimodal information, such as audio and video, to improve performance. However, these methods are limited by the lack of commonsense knowledge, which is crucial for understanding the context of a dialogue. Recently, there has been growing interest in incorporating commonsense knowledge into the dialogue emotion detection task. For example, @CITE[3]@ utilized the ATOMIC knowledge base to predict the emotions of speakers in a dialogue. @CITE[1]@ proposed a graph neural network to model the commonsense knowledge and dialogue context. @CITE[4]@ proposed a hierarchical memory network to incorporate commonsense knowledge into the dialogue emotion detection task. However, these methods are limited by the lack of attention mechanisms, which are important for highlighting the relevant commonsense knowledge in a dialogue. In this work, we propose a novel approach to incorporate commonsense knowledge into the dialogue emotion detection task using a multi-task learning framework with an attention mechanism.\\nReferences:ID: 1. Title: HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition\\nID: 2. Title: DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\\nID: 3. Title: Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network\\nID: 4. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning',\n",
       "  'ground_truth': '\\n\\nMajumder et al. (2019) recognized the importance of dialogue context in dialogue emotion detection. They used a Gated Recurrent Unit (GRU) to capture the global context which is updated by the speaker ad-hoc GRUs. At the same time, @CITE[4]@ presented a hierarchical neural network model that comprises two GRUs for the modelling of tokens and utterances respectively. @CITE[4]@ explicitly modelled the emotional dependencies on context and speakers using a Graph Convolutional Network (GCN). Meanwhile, Ghosal et al. (2019) extended the prior work ( by taking into account the intra-speaker dependency and relative position of the target and context within dialogues. Memory networks have been explored in ( to allow bidirectional influence between utterances. A similar idea has been explored by Li et al. (2020b). While the majority of works have been focusing on textual conversations, Zhong et al. (2019) enriched utterances with concept representations extracted from the ConceptNet (@CITE[4]@. developed COSMIC which exploited ATOMIC ( for the acquisition of commonsense knowledge. Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection.\\nReferences:ID: 1. Title: HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition\\nID: 2. Title: DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\\nID: 3. Title: Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network\\nID: 4. Title: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning<|eot_id|>'},\n",
       " 1733: {'predict': 'Utterance rewrite aims to generate a new utterance to complete the missing information of the original utterance. There are two main categories of methods. One is to use a sequence-to-sequence model to rewrite the utterance (@CITE[4]@; @CITE[5]@; @CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[1]@. Another is to use a semantic segmentation model to predict the edit operations to complete the utterance (@CITE[1]@; @CITE[2]@. Our work is similar to the second category, but we use a different task to guide the utterance rewrite, and we use a different model structure.\\nReferences:ID: 1. Title: GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue\\nID: 2. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation\\nID: 3. Title: Semantic Parsing with Dual Learning\\nID: 4. Title: Incomplete Utterance Rewriting as Semantic Segmentation\\nID: 5. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation',\n",
       "  'ground_truth': '\\n\\nRecently, the utterance rewrite has raised large attention. Some works use the sequence-to-sequence architecture with copy mechanism (; ; @CITE[5]@ to solve the incomplete question problem. @CITE[5]@ decompose the utterance rewrite model as two-phase subtasks: split and recombine. The split and recombine models are both learned from the well-designed reward function by the policy gradient method. Borrowing the idea from image segmentation, formulate the utterance rewrite as the semantic segmentation task, where the rewrite model is implemented with UNet (Ronneberger et al., 2015). For the downstream task, the utterance rewrite has been successfully used in dialogue state tracking (DST) tasks (@CITE[5]@; @CITE[5]@. propose a rule-based and self-supervised learning method to generate weakly-supervised rewrite data, which are used to fine-tune GPT-2. Different from the previous works, we directly use the pretrained BART, which is a denoising autoencoder, as the utterance rewrite model.\\nReferences:ID: 1. Title: GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue\\nID: 2. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation\\nID: 3. Title: Semantic Parsing with Dual Learning\\nID: 4. Title: Incomplete Utterance Rewriting as Semantic Segmentation\\nID: 5. Title: Scaling Multi-Domain Dialogue State Tracking via Query Reformulation<|eot_id|>'},\n",
       " 7144: {'predict': 'Cross-lingual NLU is a crucial component of multilingual NLP. Recent studies have explored various methods for aligning monolingual word embeddings (@CITE[11]@; @CITE[1]@; @CITE[14]@; @CITE[7]@; @CITE[4]@; @CITE[5]@; @CITE[6]@; @CITE[15]@; @CITE[13]@. Multilingual pre-trained language models (mPLMs) like mBERT (@CITE[10]@ and XLM (@CITE[2]@ have also been shown to be effective for cross-lingual transfer. However, these models rely on large amounts of monolingual data and do not directly address the problem of cross-lingual NLU. To address this problem, recent studies have proposed various approaches to align contextualized embeddings (@CITE[12]@; @CITE[9]@; @CITE[3]@; @CITE[8]@; @CITE[3]@. However, these methods either require parallel corpora or do not consider the structure of the data. In contrast, we propose a novel approach to align the data structure across languages and improve cross-lingual NLU performance.\\nReferences:ID: 1. Title: Analyzing the Limitations of Cross-lingual Word Embedding Mappings\\nID: 2. Title: A Survey Of Cross-lingual Word Embedding Models\\nID: 3. Title: Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models\\nID: 4. Title: Unsupervised Cross-lingual Transfer of Word Embedding Spaces\\nID: 5. Title: Multilingual Alignment of Contextual Word Representations\\nID: 6. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora\\nID: 7. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 9. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 10. Title: Syntax-augmented Multilingual BERT for Cross-lingual Transfer\\nID: 11. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 12. Title: Cross-lingual Text Classification with Heterogeneous Graph Neural Network\\nID: 13. Title: Multilingual Alignment of Contextual Word Representations\\nID: 14. Title: Multilingual BERT Post-Pretraining Alignment\\nID: 15. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora',\n",
       "  'ground_truth': '\\n\\nCross-lingual NLU tasks has been studied from two major perspectives: 1) multilingual word embedding (@CITE[14]@; ; ; Shi et al., 2015; Søgaard et al., 2019; @CITE[5]@; @CITE[8]@; @CITE[1]@; @CITE[12]@, and 2) pre-trained multilingual language models, such as multilingual BERT (@CITE[7]@, and XLM-RoBERTa (@CITE[11]@. Some studies (@CITE[7]@; @CITE[4]@ have shown pre-trained multilingual language models (MLM) have much better performance than traditional word embedding methods for multiple NLU tasks. Therefore, recent works (Hu et al., 2020; @CITE[13]@ usually fine-tune MLMs for downstream NLU tasks. Specifically, works based on MLMs can be roughly categorised as data augmentation-based and language alignment-based methods. For the data augmentation-based methods, they are to solve the key challenge of cross-lingual NLU: lack of low-resource language data. For example, Code-switch (Qin et al., 2020) proposes to randomly replace the phrase of high-resource languages to corresponding low-resource language phrase; @CITE[9]@ augments the data via reordering; proposes the augmentation method based on syntax; and Bari et al. Multilingual BERT Cross entropy 𝐶𝐸(𝒕𝑖, 𝑦𝑖) Coding Rate Reduction 𝑅𝑿, 𝜖−\\u0de0 𝑅𝑐(𝑿, 𝜖|𝜫) Class 1 Class 2 Class 3 language 1 language 2 language 3 Language 1 Corpus Language 2 Corpus Language 3 Corpus Figure 1: The framework of proposed Macedon. (2020) generates augmented data directly from MLMs based on the vicinity distribution of high-resource and low-resource language data samples. They are effective to improve cross-lingual NLU performance. However, they usually need to draw on additional resources such as bilingual dictionary. Another category is to align different languages. Some works attempt to align the contextual word embedding of different languages, such as learning projection transformations (@CITE[15]@; @CITE[4]@ and forcing model into having similar predictions for parallel sentences (@CITE[2]@; ; @CITE[1]@; @CITE[10]@; @CITE[12]@. And there are also a lot of works exploring adversarial training to align different languages (@CITE[3]@, 2018b,a; ; ; @CITE[6]@; Zou et al., 2018). Their motivation is to learn language-agnostic representation to make model focus more on understand text semantics and generalize better on low-resource languages. However, this line of work usually needs parallel corpus or large amount low-resource language data, which are expensive to obtain and may be not always available.\\nReferences:ID: 1. Title: Analyzing the Limitations of Cross-lingual Word Embedding Mappings\\nID: 2. Title: A Survey Of Cross-lingual Word Embedding Models\\nID: 3. Title: Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models\\nID: 4. Title: Unsupervised Cross-lingual Transfer of Word Embedding Spaces\\nID: 5. Title: Multilingual Alignment of Contextual Word Representations\\nID: 6. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora\\nID: 7. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 8. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 9. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 10. Title: Syntax-augmented Multilingual BERT for Cross-lingual Transfer\\nID: 11. Title: Emerging Cross-lingual Structure in Pretrained Language Models\\nID: 12. Title: Cross-lingual Text Classification with Heterogeneous Graph Neural Network\\nID: 13. Title: Multilingual Alignment of Contextual Word Representations\\nID: 14. Title: Multilingual BERT Post-Pretraining Alignment\\nID: 15. Title: Word Alignment by Fine-tuning Embeddings on Parallel Corpora<|eot_id|>'},\n",
       " 5816: {'predict': 'Active learning aims to reduce the need for human annotation by selecting the most informative samples to label. In the early days of deep learning, active learning has been mainly used for image classification tasks, and the most popular approach is uncertainty-based active learning, which selects samples with high uncertainty for annotation (@CITE[1]@). Recently, with the development of self-supervised learning, researchers have proposed to use self-supervised pre-training loss as a proxy for classification uncertainty (@CITE[3]@; @CITE[3]@). The main idea is to treat the self-supervised pre-training loss as the uncertainty of the model, and select samples with high self-supervised loss for annotation. However, this method is not suitable for our task because the self-supervised pre-training loss cannot reflect the uncertainty of the model in a multi-task setting. Therefore, we propose to use the gradient-based uncertainty to select samples for annotation, which is more suitable for the multi-task setting.\\nReferences:ID: 1. Title: Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision\\nID: 2. Title: Cold-start Active Learning through Self-supervised Language Modeling\\nID: 3. Title: Cold-start Active Learning through Self-supervised Language Modeling',\n",
       "  'ground_truth': '\\n\\nActive learning is a well-studied problem in machine learning with numerous scenarios and query strategies (Section 3). Though active learning has shown promise in many tasks, susceptibility to bias, particularly for neural networks, is a concern raised by several works. There are existing works that aim to mitigate this bias. Farquhar et al. (2021) proposes using corrective weights to mitigate bias. @CITE[3]@ propose self-supervised Fischer-Kernel for active learning on biased datasets. These approaches, however, often require a deep understanding of active learning and neural networks. Our approach is tailored for the NLP community and can easily be deployed. In recent years, there has been a renewed interest in active learning within the NLP community. Some recent works have applied active learning with BERT models for specific tasks such as intent classification, sentence matching, parts-of-speech tagging or named entity recognition. Margatina et al. (2022) propose continued pretraining on unlabeled data for active learning. adapt active learning to multi-task scenarios for transformer models. Ein-Dor et al. (2020) perform a large-scale empirical study of existing active learning strategies on binary classification tasks. In comparison, we target a diverse range of binary and multi-class classification tasks. Some other works in the NLP domain have adapted advanced active learning approaches. @CITE[2]@ adapt the BADGE framework for active learning with BERT. While BADGE computes gradient embedding using the output layer of a neural network and then clusters the gradient space, @CITE[2]@ computes surprisal embeddings by using Masked Language Model loss. @CITE[1]@ use acquisition functions to obtain contrastive samples for BERT. Our algorithm is comparatively straightforward, not requiring in-depth understanding of surprisal embeddings or acquisition functions. Our algorithm is also model-agnostic and can be applied to neural networks such as BERT, and traditional models such as SVMs. In addition, our clustering step relies on feature representation independent from the learner’s representation which may induce bias during the learning process. While some of the aforementioned works compute diversity in selected samples, our work is the first to analyze and address bias in active learning from a socio-cultural perspective.\\nReferences:ID: 1. Title: Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision\\nID: 2. Title: Cold-start Active Learning through Self-supervised Language Modeling\\nID: 3. Title: Cold-start Active Learning through Self-supervised Language Modeling<|eot_id|>'},\n",
       " 5556: {'predict': 'While LLMs are not typically trained on visual data, it has been shown that they can be used to generate multimodal content (; @CITE[4]@; @CITE[6]@. This is often done by prompting the LLM with a multimodal instruction, such as a text-image pair, and generating text conditioned on the instruction. The generated text can then be used as a caption for the image, or as a response to a user query. @CITE[5]@ showed that LLMs can be used to generate multimodal content without any visual data, by generating text-conditioned image embeddings and then sampling from the resulting distribution. @CITE[3]@ showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction. They also showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction. @CITE[1]@ showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction. They also showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction. @CITE[2]@ showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction. They also showed that LLMs can be used to generate multimodal content by prompting the LLM with a multimodal instruction, and generating text conditioned on the instruction.\\nReferences:ID: 1. Title: Scaling Language Models: Methods, Analysis & Insights from Training Gopher\\nID: 2. Title: LLaMA: Open and Efficient Foundation Language Models\\nID: 3. Title: Training language models to follow instructions with human feedback\\nID: 4. Title: Multimodal Few-Shot Learning with Frozen Language Models\\nID: 5. Title: Visual Instruction Tuning\\nID: 6. Title: mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality',\n",
       "  'ground_truth': '\\n\\nLarge language models (LLMs) have demonstrated outstanding performance across various applications (@CITE[1]@; Brown et al., 2020; Chowdhery et al., 2022; @CITE[4]@; @CITE[3]@), demonstrating their ability to generate responses in accordance with user intent (; ). Researchers have explored the use of LLMs in tasks related to visual understanding (; ; @CITE[2]@; @CITE[5]@; @CITE[6]@); however, their application in visually-situated natural language understanding (NLU) tasks, such as question answering on text-rich document images, remains limited. Previous approaches have attempted to integrate visual embeddings or OCR tokens into LLMs to address this gap (@CITE[2]@; ; ). However, these methods suffer from inefficiency, as they require numerous LLM tokens and entail considerable computational overhead. For instance, documents in DocVQA () consume an average of 400 and a maximum of nearly 3K OCR tokens. To overcome these obstacles, we propose the integration of Cream, which imparts language-image understanding capabilities to LLMs using soft visual prompts with a fixed size. This integration has the potential to enhance the efficiency and accuracy of LLMs in visually-situated NLU tasks.\\nReferences:ID: 1. Title: Scaling Language Models: Methods, Analysis & Insights from Training Gopher\\nID: 2. Title: LLaMA: Open and Efficient Foundation Language Models\\nID: 3. Title: Training language models to follow instructions with human feedback\\nID: 4. Title: Multimodal Few-Shot Learning with Frozen Language Models\\nID: 5. Title: Visual Instruction Tuning\\nID: 6. Title: mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality<|eot_id|>'},\n",
       " 6187: {'predict': 'The first generation of adversarial attacks for NLP, e.g., TextFooler (@CITE[5]@, TextBugger (Jia et al., 2019), WordAttack (@CITE[8]@, and TextAttack (@CITE[1]@, are based on rule-based methods. These methods rely on heuristics to replace words, characters, or tokens with their synonyms, antonyms, or semantically similar words, or insert/replace tokens with random words. However, these methods are not effective in generating adversarial examples for transformer-based models such as BERT (@CITE[2]@. To address this issue, more recent studies adopt contextualized perturbation methods. For example, BERT-Attack (@CITE[6]@, BAE (@CITE[4]@, and CLARE (@CITE[9]@ use a pre-trained masked language model to generate contextualized perturbations. Moreover, some studies leverage gradient-based methods (@CITE[10]@; @CITE[11]@; @CITE[3]@ to generate adversarial examples. However, these methods are not effective in generating adversarial examples for NLP tasks, which are usually more complex than image classification. In addition, these methods may not be effective in generating adversarial examples for large-scale NLP tasks, such as sentiment analysis, because they require to compute gradients of the target model, which is time-consuming and may not be feasible for large-scale NLP tasks. In contrast, our method is based on a hybrid method, which combines the advantages of rule-based and contextualized perturbation methods, and is effective in generating adversarial examples for large-scale NLP tasks.\\nReferences:ID: 1. Title: TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP\\nID: 2. Title: Generating Natural Language Adversarial Examples\\nID: 3. Title: Certified Robustness to Adversarial Word Substitutions\\nID: 4. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 5. Title: Towards Improving Adversarial Training of NLP Models\\nID: 6. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 7. Title: BAE: BERT-based Adversarial Examples for Text Classification\\nID: 8. Title: Contextualized Perturbation for Textual Adversarial Attack\\nID: 9. Title: Beyond Accuracy: Behavioral Testing of NLP models with CheckList\\nID: 10. Title: Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\\nID: 11. Title: Combating Adversarial Misspellings with Robust Word Recognition',\n",
       "  'ground_truth': '\\n\\nMany adversarial attacks have emerged since 2018 and have been supported by TextAttack (@CITE[1]@. We categorize all seventeen attacks from TextAttack related to text classification by their levels. In word level, Alzantot (@CITE[6]@, Faster Alzantot (, and an improved generic algorithm (IGA)(Wang et al., 2021) ran a generic algorithm to generate adversarial text. PWWS (Ren et al., 2019) and TextFooler (@CITE[10]@ transformed a text with synonyms from a fixed lexical database (WordNet) and word embedding, respectively. @CITE[5]@ applied particle swarm optimization (PSO) to synonyms from a big database (HowNet). Kuleshov (@CITE[7]@ and A2T (@CITE[2]@ measured the similarity with GPT-2 and DistillBERT, respectively. BERT-Attack (@CITE[4]@ extracted the synonyms from a masked language model. BAE ( used both word insertion and replacement with the masked model. CLARE ( extended this model by merging two consecutive words. Checklist (@CITE[11]@ verified the model consistency by changing an input word into a neutral entity (e.g., location, name, or number). Input-Reduction (@CITE[8]@ removes the word of lowest importance until the target model changes the prediction. character hybrid levels, (@CITE[3]@ accessed gradient of a target model to manipulate the loss change. DeepWordBug ( transformed words using four-character operations including swapping, substitution, deletion, and insertion. Pruthi (@CITE[9]@ added a QWERTY keyboard operator. TextBugger ( combined character and word operators.\\nReferences:ID: 1. Title: TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP\\nID: 2. Title: Generating Natural Language Adversarial Examples\\nID: 3. Title: Certified Robustness to Adversarial Word Substitutions\\nID: 4. Title: Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\\nID: 5. Title: Towards Improving Adversarial Training of NLP Models\\nID: 6. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 7. Title: BAE: BERT-based Adversarial Examples for Text Classification\\nID: 8. Title: Contextualized Perturbation for Textual Adversarial Attack\\nID: 9. Title: Beyond Accuracy: Behavioral Testing of NLP models with CheckList\\nID: 10. Title: Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\\nID: 11. Title: Combating Adversarial Misspellings with Robust Word Recognition<|eot_id|>'},\n",
       " 6899: {'predict': 'Domain adaptation is a long-standing problem in machine learning that has been extensively studied in the context of image classification (, 2018; @CITE[3]@; @CITE[4]@; @CITE[1]@, 2019; @CITE[6]@; @CITE[5]@, 2021; @CITE[2]@, 2021). In this paper, we focus on unsupervised domain adaptation, where the target domain is unknown at training time. Most existing methods in this line of work focus on learning domain-invariant representations. These methods can be categorized into two groups: (1) feature-level methods, which aim to learn domain-invariant features, such as MMDA (@CITE[3]@), CORAL (@CITE[8]@), DANN (@CITE[7]@), and DRCN (@CITE[1]@), and (2) classifier-level methods, which aim to learn domain-invariant classifiers, such as MADA (@CITE[4]@), IRM (, and MoE (@CITE[2]@). In this paper, we focus on the classifier-level methods. The main idea of these methods is to learn a domain-invariant classifier by training the classifier to be invariant to the domain information. Our method differs from these methods in that we aim to learn domain-invariant classifiers that can be easily extended to new domains by simply updating the domain-invariant classifiers.\\nReferences:ID: 1. Title: Unsupervised Domain Adaptation by Backpropagation\\nID: 2. Title: Neural Structural Correspondence Learning for Domain Adaptation\\nID: 3. Title: Deep Contextualized Self-training for Low Resource Dependency Parsing\\nID: 4. Title: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling\\nID: 5. Title: DILBERT: Customized Pre-Training for Domain Adaptation withCategory Shift, with an Application to Aspect Extraction\\nID: 6. Title: PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains\\nID: 7. Title: Invariant Risk Minimization\\nID: 8. Title: Transformer Based Multi-Source Domain Adaptation',\n",
       "  'ground_truth': '\\n\\nMost recent DA research addresses UDA (Blitzer et al., 2006; ; Glorot et al., 2011). Since the rise of DNNs, the main focus of UDA research shifted to representation learning methods (@CITE[4]@; Glorot et al., 2011; @CITE[2]@; @CITE[8]@, 2018, 2019; @CITE[3]@; @CITE[5]@; Ben-David et al., 2020;. Our paper considers a recent DA setup, presum- ing no training-time knowledge of the target do- main, termed as any-domain adaptation (ADA) by @CITE[1]@). As discussed in §1, some papers that addressed this setup follow the domain robustness path (@CITE[6]@, while others learn a mixture of domain experts (@CITE[7]@. Ben-David et al. (2022) presented PADA, an algorithm trained on data from multiple domains and adapted to test ex- amples from unknown domains through prompting. PADA leverages domain related features (DRFs) to implement an example-based prompting mech- anism. The DRFs provide semantic signatures for the source domains, representing the similarities among them and their unique properties. PADA is trained in a multitask fashion on source domain examples. For each example, it is trained to either generate a DRF signature or classify the example, using the signature as a prompt. Infer- ence involves PADA generating a DRF for its input and classifying it using this signature. The addi- tion of an architecture component, HNs, necessi- tates a two-phase training process, as outlined in §3. Unlike previous DA work in NLP (and specifi- cally PADA), we perform adaptation through HNs which are trained to generate the weights of the task classifier in a domain-based or example-based man- ner. This framework allows us to explicitly model domain-invariant and domain-specific aspects of the data and perform example-based adaptation.\\nReferences:ID: 1. Title: Unsupervised Domain Adaptation by Backpropagation\\nID: 2. Title: Neural Structural Correspondence Learning for Domain Adaptation\\nID: 3. Title: Deep Contextualized Self-training for Low Resource Dependency Parsing\\nID: 4. Title: Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling\\nID: 5. Title: DILBERT: Customized Pre-Training for Domain Adaptation withCategory Shift, with an Application to Aspect Extraction\\nID: 6. Title: PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains\\nID: 7. Title: Invariant Risk Minimization\\nID: 8. Title: Transformer Based Multi-Source Domain Adaptation<|eot_id|>'},\n",
       " 1279: {'predict': 'Neuro-symbolic methods are a family of approaches that aim to combine the strengths of neural networks and symbolic representations to achieve better interpretability and explainability. These methods are based on the idea that symbolic representations can be used to guide the learning process of neural networks, and can also be used to explain the reasoning process of neural networks. Neural Module Networks (@CITE[3]@; @CITE[1]@; @CITE[2]@) are a family of methods that use symbolic representations to guide the learning process of neural networks. These methods first parse the input question into a symbolic representation, and then use the symbolic representation to construct a neural network. At test time, the symbolic representation is used to guide the execution of the neural network. Neural Module Networks are widely used in visual question answering (@CITE[3]@; @CITE[1]@; @CITE[2]@; @CITE[4]@), image captioning (@CITE[5]@), and knowledge base question answering (@CITE[6]@). Text Modular Networks (@CITE[7]@) are a family of methods that use symbolic representations to decompose complex tasks into simpler subtasks. These methods first use a neural network to parse the input into a symbolic representation, and then use the symbolic representation to decompose the task into simpler subtasks. At test time, the symbolic representation is used to guide the execution of the subtasks. Text Modular Networks are widely used in question answering (@CITE[7]@), natural language inference (@CITE[8]@), and machine translation (@CITE[8]@). Our method is different from these methods in that we use symbolic representations to guide the learning process of neural networks, and we use symbolic representations to explain the reasoning process of neural networks. In addition, our method is different from these methods in that we use symbolic representations to guide the learning process of neural networks in a hierarchical way. For example, we first use symbolic representations to guide the learning process of the attention module, and then use symbolic representations to guide the learning process of the encoder module. In addition, our method is different from these methods in that we use symbolic representations to guide the learning process of neural networks in a hierarchical way, and we use symbolic representations to guide the learning process of neural networks in a continuous way. For example, we first use symbolic representations to guide the learning process of the attention module, and then use symbolic representations to guide the learning process of the encoder module. In addition, we first use symbolic representations to guide the learning process of the attention module, and then use symbolic representations to guide the learning process of the encoder module.\\nReferences:ID: 1. Title: Neural Module Networks\\nID: 2. Title: Using Syntax to Ground Referring Expressions in Natural Images\\nID: 3. Title: Learning to Collocate Neural Modules for Image Captioning\\nID: 4. Title: Learning to Compose Neural Networks for Question Answering\\nID: 5. Title: Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models\\nID: 6. Title: Neural Discrete Representation Learning\\nID: 7. Title: Learning to Describe Differences Between Pairs of Similar Images\\nID: 8. Title: Neuro-Symbolic Visual Reasoning: Disentangling \"Visual\" from \"Reasoning\"',\n",
       "  'ground_truth': '\\n\\nNeuro-Symbolic Methods: @CITE[3]@ proposed to use neural modular networks for visual question answering. Since then, similar approaches have been used for several other tasks such as referring expression comprehension (@CITE[6]@, image captioning (@CITE[4]@, and text ques-tion answering (@CITE[8]@; @CITE[5]@. Compared to such past efforts, we induce the latent numerical and temporal detection operations, pick a high scoring program, and condition only on a program encoding to generate the out-put description. In this respect, our work is also related to prior work on neural discrete representation learning (van den @CITE[7]@; Zhao et al., 2018), though none of these past works explore uti-lizing such techniques for data to text problems. Our proposed model abstracts the numerical pat-tern detection from text generation. Related ideas have been explored in the past in other domains and tasks (Gehrmann et al., 2018; @CITE[1]@; @CITE[2]@.\\nReferences:ID: 1. Title: Neural Module Networks\\nID: 2. Title: Using Syntax to Ground Referring Expressions in Natural Images\\nID: 3. Title: Learning to Collocate Neural Modules for Image Captioning\\nID: 4. Title: Learning to Compose Neural Networks for Question Answering\\nID: 5. Title: Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models\\nID: 6. Title: Neural Discrete Representation Learning\\nID: 7. Title: Learning to Describe Differences Between Pairs of Similar Images\\nID: 8. Title: Neuro-Symbolic Visual Reasoning: Disentangling \"Visual\" from \"Reasoning\"<|eot_id|>'},\n",
       " 7206: {'predict': 'With the advent of large-scale pre-trained language models (LMs), many studies have been conducted to utilize the pre-trained LM for machine translation. @CITE[2]@ first introduced the fine-tuning paradigm to leverage the pre-trained LM for machine translation. The fine-tuning paradigm is based on the assumption that the pre-trained LM has acquired a general knowledge of language, and the knowledge can be transferred to the downstream task. However, fine-tuning the pre-trained LM on the target task is not sufficient to utilize the pre-trained LM. This is because the training objective of the pre-trained LM and the downstream task is different. To address this issue, many studies have been conducted to utilize the pre-trained LM for machine translation. For example, @CITE[4]@ proposed a dynamic fusion mechanism to fuse the pre-trained LM with the NMT model. @CITE[1]@ proposed a concerted training framework to integrate the pre-trained LM into NMT. @CITE[3]@ proposed a BERT-fused model to fuse the pre-trained LM with the NMT model. @CITE[1]@ proposed a pre-training aided NMT to leverage the pre-trained LM for machine translation. @CITE[3]@ proposed a pre-training aided NMT to leverage the pre-trained LM for machine translation. In this paper, we propose a new method to utilize the pre-trained LM for machine translation.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Acquiring Knowledge from Pre-trained Model to Neural Machine Translation\\nID: 3. Title: Towards Making the Most of BERT in Neural Machine Translation\\nID: 4. Title: Incorporating BERT into Neural Machine Translation',\n",
       "  'ground_truth': '\\n\\nSince the successful utilization of Transformer-based LMs pre-trained on large text corpora for downstream NLP tasks (; @CITE[4]@;, the utilization of these models for MT has become a significant research area. Several studies have demonstrated the effectiveness of incorporating encoder-only LMs into NMT models. ; combine the LM representations with the encoder’s representation using a gating mechanism. propose attention between BERT and both the encoder and decoder. leverage mBERT as an encoder and introduce a decoder that attends to grouped representations of the encoder output. Another research direction focuses on developing LMs with the encoder-decoder architecture designed for NMT as the target downstream task (@CITE[1]@; @CITE[1]@. These models show improvements particularly for low-resource language pairs. To enhance their adaptation for MT, various methods have been proposed, including fine-tuning specific parts of the LMs (Cooper Stickland et al., 2021), reducing domain mismatch and overestimation (@CITE[3]@ and mitigating the copying behavior (@CITE[2]@. The integration of pre-trained LMs into SiMT remains an underexplored area of research. To date, Indurthi et al. (2022) is the only related study we are aware of. It improves MMA by integrating the LM’s prediction of the next target token, which is encoded using their model’s vocabulary before being inputted into the model. However, this approach sacrifices the semantic coherence of the original tokens due to token fragmentation. Additionally, their approach falls under target-side LM integration, overlooking the potential advantages of source-side LM integration. In this paper, we demonstrate an effective way of integrating a source-side LM into SiMT systems, offering a more versatile solution that can be integrated into most existing neural SiMT models. Building upon previous research conducted in offline MT (, we introduce essential modifications, with a particular focus on word-level policies as a pivotal component. The effective management of vocabulary mismatches between the LM and the SiMT model is contingent upon the successful implementation of a word-level SiMT policy, a key aspect that we address in our study.\\nReferences:ID: 1. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\nID: 2. Title: Acquiring Knowledge from Pre-trained Model to Neural Machine Translation\\nID: 3. Title: Towards Making the Most of BERT in Neural Machine Translation\\nID: 4. Title: Incorporating BERT into Neural Machine Translation<|eot_id|>'},\n",
       " 2261: {'predict': 'Vision-and-Language Navigation (VLN) is a task that requires an embodied agent to navigate in a visual environment according to a natural language instruction (@CITE[7]@. Most existing methods (@CITE[5]@; @CITE[8]@; @CITE[6]@; @CITE[3]@; @CITE[4]@; @CITE[1]@; @CITE[2]@; @CITE[6]@; @CITE[2]@; @CITE[1]@ focus on improving the agent’s navigation ability by learning the semantic alignment between the visual observation and the instruction. However, the VLN task is more challenging due to the lack of training data. To alleviate this problem, some works (@CITE[6]@; @CITE[2]@; @CITE[1]@; @CITE[7]@; @CITE[6]@; @CITE[2]@; @CITE[7]@; @CITE[6]@ propose to use the pre-trained language model (PLM) to generate the navigation instructions. Different from these works, we focus on improving the agent’s navigation ability by using the pre-trained visual model to generate the navigation instructions. In addition, the instruction generation task is more challenging than the VLN task, because the agent needs to generate a long instruction that is consistent with the visual observation. Therefore, we use the pre-trained visual model to generate the navigation instructions to help the agent learn the semantic alignment between the visual observation and the instruction.\\nReferences:ID: 1. Title: On Evaluation of Embodied Navigation Agents\\nID: 2. Title: Matterport3D: Learning from RGB-D Data in Indoor Environments\\nID: 3. Title: Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation\\nID: 4. Title: Vision-Language Navigation with Random Environmental Mixup\\nID: 5. Title: Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\\nID: 6. Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation\\nID: 7. Title: Transferable Representation Learning in Vision-and-Language Navigation\\nID: 8. Title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training',\n",
       "  'ground_truth': '\\n\\nAnderson et al. ( proposed the first Vision- Language Navigation (VLN) benchmark combining real imagery (@CITE[6]@ and natural language navigation instructions. To solve this task, Wang et al. (@CITE[5]@ proposed a novel SERL model to learn reward functions from the expert distribution. And combining imitation learning and reinforcement learning (@CITE[5]@ has been proved to be beneficial for VLN. Since the VLN dataset is relatively small-scale, some works propose augmentation approaches (@CITE[6]@; ; to improve robustness. Auxiliary losses (; Zhu et al., 2020; @CITE[7]@ is used to take advantage of the additional training signals derived from the semantic information. Some pretraining methods (; @CITE[5]@ have been proposed to learn generic cross-modal representations. This is further extended to a recurrent model that significantly improves sequential action prediction (@CITE[8]@. However, the limited number of environments in pretraining constrain the generalization ability to unseen scenarios. Most related to this work, VLN- BERT ( transfers knowledge from abundant, but out-of-domain image-text data to improve path-instruction matching. In contrast, we not only propose an effective method to build an in-domain dataset by sampling trajectory and generating instructions with templates, but also present a prompt-based pretraining strategy to improve VLN.\\nReferences:ID: 1. Title: On Evaluation of Embodied Navigation Agents\\nID: 2. Title: Matterport3D: Learning from RGB-D Data in Indoor Environments\\nID: 3. Title: Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation\\nID: 4. Title: Vision-Language Navigation with Random Environmental Mixup\\nID: 5. Title: Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\\nID: 6. Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation\\nID: 7. Title: Transferable Representation Learning in Vision-and-Language Navigation\\nID: 8. Title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training<|eot_id|>'},\n",
       " 5324: {'predict': 'Table reasoning has been studied in various forms, including QA (@CITE[6]@; @CITE[5]@; @CITE[4]@; @CITE[3]@; @CITE[3]@; @CITE[3]@; @CITE[3]@), NLG (@CITE[3]@; @CITE[3]@; @CITE[8]@), table-to-logic form generation (@CITE[9]@; @CITE[1]@; @CITE[7]@), and table-to-text generation (@CITE[2]@; @CITE[2]@). These works mainly focus on reasoning over flat tables, and few of them (@CITE[3]@; @CITE[3]@) consider hierarchical tables. Our work is also related to table pretraining, which aims to pretrain language models on large-scale tabular data. Existing table pretraining methods (@CITE[7]@; @CITE[8]@; @CITE[4]@; @CITE[3]@; @CITE[9]@) mainly focus on flat tables. FORTAP (@CITE[2]@) is the first method that considers numerical reasoning over semi-structured tables, but it does not focus on hierarchical tables. In contrast, our work is the first to consider both hierarchical and flat tables in table pretraining. We design a new pretraining task that is specifically tailored to the hierarchical table structure, and we also extend the pretraining task to consider numerical reasoning over tables. We conduct experiments on WikiTableQuestions (@CITE[6]@) and HiTab (@CITE[3]@) to demonstrate the effectiveness of our pretraining method.\\nReferences:ID: 1. Title: Compositional Semantic Parsing on Semi-Structured Tables\\nID: 2. Title: RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations\\nID: 3. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 4. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 5. Title: TAPEX: Table Pre-training via Learning a Neural SQL Executor\\nID: 6. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 7. Title: PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation\\nID: 8. Title: OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering\\nID: 9. Title: FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining',\n",
       "  'ground_truth': '\\n\\nEnhancing the table reasoning capabilities of models is essential for a variety of tasks related to tables, such as table question answering (@CITE[2]@; @CITE[6]@; Zhong et al., 2018; @CITE[7]@), table fact verification (@CITE[9]@), and table-to-text generation (@CITE[1]@; ). One prevalent approach is pre-training models with table-text joint reasoning data (Herzig et al., 2020; @CITE[5]@; ; @CITE[8]@; ; @CITE[3]@; @CITE[3]@; Xie et al., 2022). Nevertheless, these models generate text in an end-to-end manner, resulting in reduced explainability and difficulties in handling more complex reasoning, such as arithmetic calculation. Therefore, we propose REFACTOR, which can retrieve and generate query-relevant facts from tables as intermediate results for model input (@CITE[3]@; @CITE[4]@), mitigating the implicit reasoning processes of text generation models.\\nReferences:ID: 1. Title: Compositional Semantic Parsing on Semi-Structured Tables\\nID: 2. Title: RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations\\nID: 3. Title: Logical Natural Language Generation from Open-Domain Tables\\nID: 4. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 5. Title: TAPEX: Table Pre-training via Learning a Neural SQL Executor\\nID: 6. Title: HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nID: 7. Title: PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation\\nID: 8. Title: OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering\\nID: 9. Title: FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining<|eot_id|>'},\n",
       " 5742: {'predict': 'Prompt tuning with LLMs has been widely used in recent years. @CITE[4]@ proposed to use a prompt to guide the LLMs to perform the task. They demonstrated that LLMs can be fine-tuned in a few-shot setting using a prompt. @CITE[2]@ proposed a novel pipeline for automatically generating a prompt. @CITE[3]@ proposed a method to automatically select label mappings for few-shot text classification with prompting. @CITE[1]@ proposed to use a template to guide the LLMs to generate the prompt. @CITE[4]@ proposed a method to automatically select label mappings for few-shot text classification with prompting. @CITE[2]@ proposed to use a template to guide the LLMs to generate the prompt. These methods are all based on the idea that the LLMs can be fine-tuned in a few-shot setting using a prompt. However, these methods need to manually design the prompt. In contrast, our method can automatically generate a prompt from a few-shot examples.\\nReferences:ID: 1. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 2. Title: Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification',\n",
       "  'ground_truth': '\\n\\nPrompt tuning may have a better generalization on various tasks due to the aligned nature of language descriptions and answer semantics, e.g., classification problems (@CITE[3]@; @CITE[1]@). Compared with fine-tuning, prompt tuning may have a better generalization on various tasks due to the aligned nature of language descriptions and answer semantics, e.g., classification problems (@CITE[3]@; @CITE[1]@). At the same time, there are some efforts to leverage prompts with structural inputs for knowledge customization (@CITE[4]@). Injecting hierarchy information into prompts is also promising. For example, using top-level predictions to refine prompts of bottom levels can surpass soft prompts and hard prompts (@CITE[2]@). Nevertheless, how to employ LLMs to better involve hierarchy knowledge is still under investigation.\\nReferences:ID: 1. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 2. Title: Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification\\nID: 3. Title: Making Pre-trained Language Models Better Few-shot Learners\\nID: 4. Title: Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification<|eot_id|>'},\n",
       " 2716: {'predict': 'The most widely used LID tools are the ones based on statistical models, which have been widely used in the past. The most popular ones are the ones based on n-gram models, such as the CLD2 tool (@CITE[5]@, which uses a set of n-gram models to identify languages. Other statistical models include the ones based on Hidden Markov Models (HMM) ( and Support Vector Machines (SVM) (Sakaki et al., 2011). However, these models are not able to generalize well to new languages and domains, and they do not perform well in low-resource languages. In recent years, there has been a growing interest in developing deep learning-based LID tools. The most widely used deep learning-based LID tool is the one based on the fastText model (@CITE[4]@, which is a shallow neural network that uses a combination of character n-grams and word n-grams to identify languages. Other deep learning-based LID tools include the ones based on Convolutional Neural Networks (CNN) (@CITE[2]@ and Recurrent Neural Networks (RNN) (, which use word embeddings and character-level features to identify languages. There are also LID tools based on pre-trained language models, such as the ones based on BERT (@CITE[6]@ and T5 (@CITE[3]@, which use language-specific tokenization and language-specific pre-trained weights to identify languages.\\nReferences:ID: 1. Title: Mapping Languages: The Corpus of Global Language Use\\nID: 2. Title: Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets\\nID: 3. Title: Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus\\nID: 4. Title: Improved Text Language Identification for the South African Languages\\nID: 5. Title: A Portuguese Native Language Identification Dataset\\nID: 6. Title: AraT5: Text-to-Text Transformers for Arabic Language Generation',\n",
       "  'ground_truth': '\\n\\nLID tools are often used to select data to pre-train language models ( and, more generally, develop multilingual corpora (Buck et al., 2014b; ; @CITE[3]@; Ortiz Suárez et al., 2019). For many languages, including African languages, LID tools are either not avail- able or perform poorly (; @CITE[2]@. A few works, however, have already focused on African language identifica- tion. For example, cover Yorùbá, Hausa, and Igbo. Similarly, @CITE[6]@; @CITE[5]@ treat 10 Indigenous South African official languages. In ad- dition, a handful of other African languages are covered in LID tools such as CLD2 (McCand- less, 2010), CLD3 (, Equi- lid (@CITE[4]@, FastText, Franc, LangDe- tect (@CITE[1]@ and Langid.py (Lui and Bald- win, 2012) and works such as Abdul-Mageed et al. (2020, 2021) and.\\nReferences:ID: 1. Title: Mapping Languages: The Corpus of Global Language Use\\nID: 2. Title: Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets\\nID: 3. Title: Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus\\nID: 4. Title: Improved Text Language Identification for the South African Languages\\nID: 5. Title: A Portuguese Native Language Identification Dataset\\nID: 6. Title: AraT5: Text-to-Text Transformers for Arabic Language Generation<|eot_id|>'},\n",
       " 7107: {'predict': 'The integration of template information into NMT models has been explored in many ways. @CITE[4]@ introduced a translation memory module into NMT. The memory module contains a set of pre-defined translation templates and the model can retrieve the most similar template to the source sentence and integrate the retrieved template into the encoder. @CITE[2]@ proposed a phrase memory module into NMT. The memory module contains a set of pre-defined translation templates and the model can retrieve the most similar template to the source sentence and integrate the retrieved template into the encoder. @CITE[3]@ proposed to use posterior regularization to integrate prior knowledge into NMT. The prior knowledge can be represented as a log-linear model, which guides the learning process of the neural translation model. @CITE[1]@ proposed a cross-lingual memory retriever into NMT. The retriever can retrieve the most similar sentence to the source sentence from the monolingual corpus and integrate the retrieved sentence into the encoder. @CITE[1]@ proposed a cross-lingual memory retriever into NMT. The retriever can retrieve the most similar sentence to the source sentence from the monolingual corpus and integrate the retrieved sentence into the encoder. In contrast, our model integrates the retrieved template into the decoder, which can further improve the translation quality.\\nReferences:ID: 1. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 2. Title: Neural Machine Translation with External Phrase Memory\\nID: 3. Title: Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora\\nID: 4. Title: Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization',\n",
       "  'ground_truth': '\\n\\nFigure 1: Knowledge integration framework with prompting and an example representation. ( uses an auxiliary network to fuse information from the source sentence and the retrieved value from a TM and then integrate it into the NMT model architecture. (@CITE[2]@ proposes to pack a TM into a compact graph corresponding to multiple words for different sentences in a TM, and then encode the packed graph into a deep representation during the decoding phase. ( utilizes data augmentation to train an NMT model whose training instances are bilingual sentences augmented with the translation retrieved from the TM. For input sentences that are not very similar to their TMs, the translation performance of these methods suffers significantly. (@CITE[4]@ introduces Example Layer consisting of multi-head attention and cross-attention to translate any input sentences whether they are similar to their TM or not. ( extends the TM from the bilingual setting to the monolingual setting through learnable memory retrieval in a cross-lingual manner. The key idea of TM is to integrate the retrieved sentence pairs from a TM into the NMT architecture for accurate translations. Most of the works integrate the TM knowledge via model modification and the models need to be retrained when loading another TM in new domains. The general knowledge integration to NMT is an ongoing work (@CITE[3]@; @CITE[1]@;. ( proposes to use extracted templates from tree structures as soft target templates to incorporate the template information into the encoder-decoder framework. (Shang et al., 2021) introduce to a template-based machine translation (TBMT) model to integrate the syntactic knowledge of the retrieved target template into the NMT decoder. ( represents prior knowledge sources as features in a log-linear model to guide the learning process of NMT models. These approaches have demonstrated the clear benefits by incorporating different single types of knowledge into NMT models. Our approach is designed to integrate multiple types of knowledge into NMT models through a unified framework.\\nReferences:ID: 1. Title: Neural Machine Translation with Monolingual Translation Memory\\nID: 2. Title: Neural Machine Translation with External Phrase Memory\\nID: 3. Title: Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora\\nID: 4. Title: Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization<|eot_id|>'},\n",
       " 1560: {'predict': 'Video captioning aims to generate natural language descriptions for a given video. Recent studies mainly focus on building better visual encoders (@CITE[5]@; @CITE[6]@; @CITE[7]@; @CITE[8]@; @CITE[3]@; @CITE[1]@; @CITE[2]@), or designing more effective language models (@CITE[4]@; @CITE[8]@; @CITE[7]@; @CITE[7]@; @CITE[6]@; @CITE[2]@). However, these methods usually treat video captioning as a sequence-to-sequence translation task, and ignore the fact that the video content is more like a sequence of image sequences, which can be modeled by a sequence of sequence-to-sequence translation tasks. Inspired by this, we propose a novel video captioning model based on the sequence-to-sequence translation framework, which can capture the temporal dynamics of the video and generate captions with high quality.\\nReferences:ID: 1. Title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks\\nID: 2. Title: Describing Videos by Exploiting Temporal Structure\\nID: 3. Title: Jointly Modeling Embedding and Translation to Bridge Video and Language\\nID: 4. Title: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\\nID: 5. Title: Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation\\nID: 6. Title: Video Description: A Survey of Methods, Datasets and Evaluation Metrics\\nID: 7. Title: Object Relational Graph with Teacher-Recommended Learning for Video Captioning\\nID: 8. Title: Neural Machine Translation by Jointly Learning to Align and Translate',\n",
       "  'ground_truth': '\\n\\nRecently, a large number of encoder-decoder based neural models have been proposed for video captioning (@CITE[7]@; ; @CITE[6]@,a; Xu et al., 2017; @CITE[7]@; ; @CITE[5]@; @CITE[6]@; Yang et al., 2021; Perez-Martin et al., 2021). These methods mainly introduce a convolutional neural network (CNN) (@CITE[7]@ to encode the video and employ a LSTM ( or a Transformer (Zhou et al., 2018) to generate the coherent captions with the attention mechanism (@CITE[8]@; @CITE[6]@. However, these methods lack controllability, i.e., their behaviors can hardly be influenced. Our model allows an easy way to control the contents of video captions rather than merely syntactic variations in existing studies.\\nReferences:ID: 1. Title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks\\nID: 2. Title: Describing Videos by Exploiting Temporal Structure\\nID: 3. Title: Jointly Modeling Embedding and Translation to Bridge Video and Language\\nID: 4. Title: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\\nID: 5. Title: Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation\\nID: 6. Title: Video Description: A Survey of Methods, Datasets and Evaluation Metrics\\nID: 7. Title: Object Relational Graph with Teacher-Recommended Learning for Video Captioning\\nID: 8. Title: Neural Machine Translation by Jointly Learning to Align and Translate<|eot_id|>'},\n",
       " 3637: {'predict': 'Prompt-tuning is a popular method to enhance PLMs’ performance on downstream tasks. Recent studies have explored the use of prompt-tuning for relation extraction (@CITE[4]@; @CITE[5]@; @CITE[6]@). @CITE[4]@ propose to learn the prompt template from a few labeled data with the help of knowledge graph. @CITE[5]@ propose to generate prompts with rules for many-class classification. @CITE[6]@ propose to learn the prompt template from a few labeled data with the help of knowledge graph. @CITE[1]@ propose to use a prompt template generator to generate prompts. @CITE[3]@ propose to learn the prompt template from a few labeled data with the help of knowledge graph. @CITE[2]@ propose to learn the prompt template from a few labeled data with the help of knowledge graph. However, these methods are limited to few-shot settings and cannot be used in zero-shot settings. In this paper, we propose to use prompt-tuning to enhance PLMs’ performance in zero-shot relation extraction.\\nReferences:ID: 1. Title: KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction\\nID: 2. Title: PTR: Prompt Tuning with Rules for Text Classification\\nID: 3. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: PTR: Prompt Tuning with Rules for Text Classification\\nID: 6. Title: PTR: Prompt Tuning with Rules for Text Classification',\n",
       "  'ground_truth': '\\n\\nThere is an emerging interest in using prompts to extract knowledge from large language models (@CITE[3]@; ; @CITE[6]@; Ye et al., 2022; ). Prompt-tuning is a significant research in the past years. The GPT-3 (@CITE[1]@) fueled the development of prompt-based learning, which relies on handcraft prompts and achieves impressive performance. To further construct automatic prompt, AutoPrompt (@CITE[4]@) and LM-BFF () propose automatic prompt construction through generating discrete tokens. Recently, Prefix-tuning (@CITE[5]@), P-tuningV2(@CITE[6]@), PTR (@CITE[6]@), and soft-prompts (@CITE[2]@) propose continuous prompt construction, which introduce differential parameters to solve the issue. Unlike previous work (), our prompt-based framework mainly focuses on SDT, which is a novel exploration of this challenging task in a low resource setting.\\nReferences:ID: 1. Title: KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction\\nID: 2. Title: PTR: Prompt Tuning with Rules for Text Classification\\nID: 3. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 4. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\\nID: 5. Title: PTR: Prompt Tuning with Rules for Text Classification\\nID: 6. Title: PTR: Prompt Tuning with Rules for Text Classification<|eot_id|>'},\n",
       " 7521: {'predict': 'Vision-and-language navigation (VLN) aims to learn a language-guided agent to navigate through a 3D environment (; ; @CITE[4]@; @CITE[1]@; @CITE[10]@; ; ; @CITE[5]@; @CITE[11]@; ; @CITE[2]@; @CITE[7]@; @CITE[3]@; @CITE[8]@; @CITE[13]@; @CITE[12]@; @CITE[16]@; @CITE[9]@; @CITE[15]@; @CITE[14]@; @CITE[6]@; @CITE[17]@. The agent is trained with a sequence of instructions and the corresponding action sequences, and then follows the instructions to navigate to a target location. Existing works on VLN mainly focus on the learning of the navigation agent and the generation of instructions. In this paper, we focus on the generation of instructions for VLN. Existing works on instruction generation mainly focus on the generation of text-only instructions (@CITE[18]@; @CITE[19]@; ; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@. For VLN, existing works mainly focus on the generation of text-only instructions (@CITE[16]@; @CITE[12]@; @CITE[9]@; @CITE[13]@. Recent works (@CITE[15]@; @CITE[14]@; @CITE[17]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[3]@; @CITE[2]@; @CITE[1]@; @CITE[4]@; @CITE[11]@; @CITE[10]@; @CITE[5]@; @CITE[17]@ generate instructions with visual guidance. However, existing works on VLN instruction generation mainly focus on the generation of instructions with a single language, while we consider the generation of instructions in multiple languages. Furthermore, existing works on VLN instruction generation mainly focus on the generation of instructions with a single language, while we consider the generation of instructions in multiple languages. Existing works on multilingual instruction generation mainly focus on the generation of instructions for text-only instructions (@CITE[18]@; @CITE[19]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@; @CITE[20]@. For VLN, existing works on multilingual instruction generation mainly focus on the generation of instructions with visual guidance (@CITE[15]@; @CITE[14]@; @CITE[17]@; @CITE[8]@; @CITE[6]@; @CITE[7]@; @CITE[3]@; @CITE[2]@; @CITE[1]@; @CITE[4]@; @CITE[11]@; @CITE[10]@; @CITE[5]@; @CITE[17]@. However, existing works on multilingual instruction generation for VLN mainly focus on the generation of instructions with a single modality, while we consider the generation of instructions with both text and visual modalities.\\nReferences:ID: 1. Title: Unified Pragmatic Models for Generating and Following Instructions\\nID: 2. Title: Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation\\nID: 3. Title: Robust Navigation with Language Pretraining and Stochastic Sampling\\nID: 4. Title: Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes\\' Rule\\nID: 5. Title: AI2-THOR: An Interactive 3D Environment for Visual AI\\nID: 6. Title: On Evaluation of Embodied Navigation Agents\\nID: 7. Title: Habitat: A Platform for Embodied AI Research\\nID: 8. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nID: 9. Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments\\nID: 10. Title: Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding\\nID: 11. Title: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks\\nID: 12. Title: TEACh: Task-driven Embodied Agents that Chat\\nID: 13. Title: Asynchronous Methods for Deep Reinforcement Learning\\nID: 14. Title: Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation\\nID: 15. Title: History Aware Multimodal Transformer for Vision-and-Language Navigation\\nID: 16. Title: Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout\\nID: 17. Title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training\\nID: 18. Title: FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation\\nID: 19. Title: Less is More: Generating Grounded Navigation Instructions from Landmarks\\nID: 20. Title: On the Role of Visual Context in Vision-and-Dialogue Navigation\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Dialogue Navigation\\nReferences:ID: 2. Title: AI2THOR: An Interactive 3D Environment for Visual AI Research\\nReferences:ID: 3. Title: On the Role of Visual Context in Vision-and-Dialogue Navigation\\nReferences:ID: 4. Title: Vision-and-Dialogue Navigation: Interpreting visually-grounded natural language instructions in real environments\\nReferences:ID: 5. Title: On the Role of Visual Context in Vision-and-Dialogue Navigation\\nReferences:ID: 6. Title: Habitat: A Platform for Embodied AI Research\\nReferences:ID: 7. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 8. Title: On the Role of Visual Context in Vision-and-Dialogue Navigation\\nReferences:ID: 9. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 10. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 11. Title: Vision-and-Dialogue Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 12. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 13. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 14. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 15. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 16. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 17. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 2. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 3. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 8. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: 1. Title: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: On the Role of Visual Context in Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nReferences:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of Visual Context in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision-and-Language Navigation:ID: On the Role of this work in Vision: On the Role of this work in Vision-and-language Navigation:ID: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the key results: On the Role of this work in a method: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the following work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the key. On the key. On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the Role of this work in Vision: On the keyword. On the Role of this work in Vision: On the Role of this work in Vision: On the keyword. On the role of this work in Vision: On the keyword. On the role of this work in Vision: On the role of this work in Vision: On the keyword. On the Role of this work in Vision: On the role of this work in Vision: On the keyword. On the keyword. On the Role of this work in Vision: On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the role of this work in Vision: On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the model pre 1. On the best of vision. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the high- assistant 2015. On the keyword. On the keyword. On the related to the keyword. On the most 1. On the title the keyword. On the title of vision. On the keyword. On the keyword. On the keyword. On the keyword. On the keyword. On the R: on the keyword. On the keyword. On the keyword. On the keyword: On the method.\\nOn the keyword. On the method the method. On the keyword. On the 1. On the following. On the keyword. On the research the heterogeneous. On the keyword. On the key. On the method the following technology: On the key to follow the new vision. On the key. On the key to the following the method of the same as the keyword. On the following vision. On the following: On the keyword. On the new vision. We have a novel. the key of vision. On the keyword. to follow the keyword. On the following vision: On the keywords. On the high-levels to follow the vision. On the vision 1. On the following: On the high- -- -- -- --, \"Vision -- -- --... the key to get 3d: On the R, a set the language navigation. On the following: On the keyword. On the following techniques on the key of vision 3d of the vision. On the key. On the high-levels of vision to follow the R of the R the bests of training. On the high-levels of vision 3d model, On the language n the vision with a general vision: On the vision with vision as a general vision key (vision models of vision (2 different vision at vision of vision of vision with vision with vision, On the key to vision. On the topic: On the related to vision with the vision in vision of vision with method of vision of vision of vision: On the key and key (research on related to vision of vision of vision. On the related to which is a knowledge of vision: On the related to the paper of the related to the related to what property of vision to the related to the related to the related to which keyword \"vision to do vision of vision of the related to the related to the following title of the related to the following 1 vision of vision of vision of vision',\n",
       "  'ground_truth': \"\\n\\nBuilding vision-and-language navigation models received increasing attention in recent years (Anderson et al., 2018b; @CITE[18]@; @CITE[1]@; @CITE[14]@; Zhu et al., 2020b; @CITE[6]@ and various benchmarks have been proposed to evaluate the ability of embodied agents to follow instructions and accomplish specified tasks (@CITE[10]@; @CITE[2]@; @CITE[16]@; @CITE[8]@; @CITE[15]@; @CITE[5]@; @CITE[17]@; @CITE[11]@. In this line of research, representative works include @CITE[12]@ who propose panoramic action space and use a speaker follow to synthesize additional training data. In addition, Tan et al. (2019) propose to mix imitation learning and A3C (@CITE[20]@ and increase the diversity of the synthesized data by adding noise into the environments during data generation. utilize additional training signals, @CITE[3]@ propose the self-monitoring agent that improves vision-language alignment with a co-grounding module and progress monitor; propose four self-supervised auxiliary tasks that are beneficial for the task of VLN. There are also works on designing better architectures (@CITE[13]@, 2022) for VLN. In terms of data augmentation, in addition to methods (@CITE[12]@; @CITE[4]@; @CITE[9]@; that synthetic language instructions are generated from randomly sampled paths, there are also works on synthesizing both the environments and the instructions (@CITE[7]@; @CITE[19]@. However, most of these works require external models to generate imperfect data, which can potentially harm the performance of VLN models.\\nReferences:ID: 1. Title: Unified Pragmatic Models for Generating and Following Instructions\\nID: 2. Title: Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation\\nID: 3. Title: Robust Navigation with Language Pretraining and Stochastic Sampling\\nID: 4. Title: Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule\\nID: 5. Title: AI2-THOR: An Interactive 3D Environment for Visual AI\\nID: 6. Title: On Evaluation of Embodied Navigation Agents\\nID: 7. Title: Habitat: A Platform for Embodied AI Research\\nID: 8. Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\\nID: 9. Title: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments\\nID: 10. Title: Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding\\nID: 11. Title: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks\\nID: 12. Title: TEACh: Task-driven Embodied Agents that Chat\\nID: 13. Title: Asynchronous Methods for Deep Reinforcement Learning\\nID: 14. Title: Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation\\nID: 15. Title: Vision-Language Navigation with Self-Supervised Auxiliary Reasoning Tasks\\nID: 16. Title: History Aware Multimodal Transformer for Vision-and-Language Navigation\\nID: 17. Title: Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout\\nID: 18. Title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training\\nID: 19. Title: FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation\\nID: 20. Title: Less is More: Generating Grounded Navigation Instructions from Landmarks<|eot_id|>\"},\n",
       " 5126: {'predict': 'A different approach to formal verification is to use a generative model to generate the proof, and then verify it (@CITE[3]@; @CITE[4]@; @CITE[5]@; @CITE[2]@). For example, can use a generative QA model to generate a proof, and then verify it using an ATP solver. Similarly, can use a generative model to generate a proof sketch, and then verify it using an ATP solver. This approach has the advantage that the model can generate a proof for a theorem that the ATP solver cannot prove. However, this approach also has the disadvantage that it is hard to verify the correctness of the generated proof.\\nReferences:ID: 1. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System\\nID: 2. Title: UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark\\nID: 3. Title: General-Purpose Question-Answering with Macaw\\nID: 4. Title: Scaling Instruction-Finetuned Language Models\\nID: 5. Title: Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs',\n",
       "  'ground_truth': '\\n\\nWith the rapid progress in generative LMs, researchers have been largely building general-purpose problem-solving methods with a generative approach (, 2022; @CITE[5]@; @CITE[4]@; @CITE[1]@). However, current generative LMs are still prone to hallucination errors and lack an intrinsic mechanism to express confidence level on their outputs. Verification, on the other hand, shows promise to complement these shortcomings and has been adopted to improve the outcome of generation (@CITE[3]@; @CITE[2]@). In this work, we take a pure verification approach and build a general-purpose verifier for commonsense statements, which to our best knowledge is the first of its kind.\\nReferences:ID: 1. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System\\nID: 2. Title: UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark\\nID: 3. Title: General-Purpose Question-Answering with Macaw\\nID: 4. Title: Scaling Instruction-Finetuned Language Models\\nID: 5. Title: Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs<|eot_id|>'},\n",
       " 2775: {'predict': 'Prompting is a recent technique that has shown great success in various NLP tasks (@CITE[8]@; @CITE[1]@; @CITE[5]@; @CITE[4]@; @CITE[2]@; @CITE[6]@; @CITE[3]@. The basic idea is to use a prompt to guide the model to generate the desired response. In the vision-language domain, prompting has been shown to be useful for both zero-shot and few-shot learning (@CITE[7]@; @CITE[2]@; @CITE[4]@; @CITE[6]@; @CITE[3]@. For example, @CITE[2]@ propose to use a prompt to retrieve knowledge from a knowledge base and then use it to answer a question. @CITE[7]@ propose to use a prompt to guide the model to generate a visual description for an image. @CITE[6]@ propose to use a prompt to guide the model to generate a text description for a video. In this paper, we use a prompt to guide the model to generate a visual description for an image and a text description for a video. The main difference between our work and previous work is that we use a prompt to guide the model to generate a description for both images and videos, and we use a prompt to guide the model to generate a description for both visual and textual modalities.\\nReferences:ID: 1. Title: VisualBERT: A Simple and Performant Baseline for Vision and Language\\nID: 2. Title: oLMpics -- On what Language Model Pre-training Captures\\nID: 3. Title: The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task\\nID: 4. Title: KAT: A Knowledge Augmented Transformer for Vision-and-Language\\nID: 5. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 6. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 7. Title: Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language\\nID: 8. Title: KAT: A Knowledge Augmented Transformer for Vision-and-Language',\n",
       "  'ground_truth': '\\n\\nThe increasing popularity of large-scale PLMs such as GPT (@CITE[8]@; Brown et al., 2020), BERT (Devlin et al., 2019), RoBERTa (@CITE[5]@) has also popularized prompt-based learning. 1Code: https://gitlab.com/bottle_shop/safe/prompthate. Existing prompt-based learning studies have explored using PLMs as implicit and unstructured knowledge bases (@CITE[7]@; Davison et al., 2019; ). Recent studies have also prompted PLMs for various NLP tasks such as natural language inference and sentiment classification, and yield good performance in few-shot settings (@CITE[1]@; Schick and Schütze, 2021a,b). There are also recent works that prompt visual-language models for computer vision tasks (@CITE[3]@, 2021; @CITE[2]@). Nevertheless, most of the existing prompt-based learning studies are limited to unimodal tasks, and there are fewer works on prompting PLM for multimodal tasks (; @CITE[4]@; @CITE[1]@). @CITE[6]@ has explored prompting GPT-3 model (Brown et al., 2020) for the visual question & answering task. However, there are limitations: large models such as GPT-3 are expensive to tune. Furthermore, the constraint on the input length limits the number of training instances. In this paper, we adopt a different approach and propose a novel framework, PromptHate, which prompts RoBERTa (@CITE[5]@) for the multimodal hateful meme classification. PromptHate is a much smaller model compared to GPT-3, and it can be fine-tuned with training instances.\\nReferences:ID: 1. Title: VisualBERT: A Simple and Performant Baseline for Vision and Language\\nID: 2. Title: oLMpics -- On what Language Model Pre-training Captures\\nID: 3. Title: The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task\\nID: 4. Title: KAT: A Knowledge Augmented Transformer for Vision-and-Language\\nID: 5. Title: Conditional Prompt Learning for Vision-Language Models\\nID: 6. Title: Learning Transferable Visual Models From Natural Language Supervision\\nID: 7. Title: Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language\\nID: 8. Title: KAT: A Knowledge Augmented Transformer for Vision-and-Language<|eot_id|>'},\n",
       " 7290: {'predict': 'Query generation is a key component of many conversational systems, such as conversational search (@CITE[6]@; @CITE[1]@; @CITE[3]@; @CITE[5]@) and conversational question answering (@CITE[4]@; @CITE[2]@). Most of these works generate queries by rewriting the original query in a conversational context. In this work, we focus on the task of generating a query from the context of a conversation. We propose a generative model that is able to generate queries that are consistent with the conversation context. Our model is designed to be general and can be used for a variety of conversational systems, such as conversational search and conversational question answering.\\nReferences:ID: 1. Title: Few-Shot Generative Conversational Query Rewriting\\nID: 2. Title: Open-Domain Question Answering Goes Conversational via Question Rewriting\\nID: 3. Title: Query Resolution for Conversational Search with Limited Supervision\\nID: 4. Title: CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning\\nID: 5. Title: Reinforced Question Rewriting for Conversational Question Answering\\nID: 6. Title: Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling',\n",
       "  'ground_truth': '\\n\\nWhen an off-the-shelf retriever is used for knowledge retrieval, how to make an input text query is important for obtaining appropriate documents. Especially, in knowledge-grounded dialogue, a self-contained query should be generated from the multi-turn dialogue context. A number of prior works have tried to train a supervised query rewriting model by human rewrites (@CITE[5]@; Lin et al., 2020; @CITE[6]@; @CITE[3]@. Similar to the knowledge annotation, the limitations of manual query annotation have come up with unsupervised learning of the query generator. A number of works have proposed novel rewards for reinforcement learning of a query rewriter (@CITE[2]@; @CITE[1]@. QKConv (@CITE[4]@ has proposed an unsupervised query enhanced method for knowledge-grounded conversation. This work is similar to ours in that it consists of a query generator, an off-the-shelf knowledge retriever, and a response generator, and unsupervised joint training is applied to the query generator and the response generator. However, their training objective is based on the approximated marginal likelihood over candidate queries, which is different from our objective that includes the posterior distillation based on the ELBO loss.\\nReferences:ID: 1. Title: Few-Shot Generative Conversational Query Rewriting\\nID: 2. Title: Open-Domain Question Answering Goes Conversational via Question Rewriting\\nID: 3. Title: Query Resolution for Conversational Search with Limited Supervision\\nID: 4. Title: CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning\\nID: 5. Title: Reinforced Question Rewriting for Conversational Question Answering\\nID: 6. Title: Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling<|eot_id|>'},\n",
       " 3956: {'predict': 'Faithfulness has been studied in the context of summarization and text generation. For summarization, @CITE[11]@ introduced a weakly supervised framework to evaluate factual consistency. They generated synthetic data by applying a series of rule-based transformations to the sentences of the source documents. @CITE[4]@ proposed FactGraph, which decomposed the document and the summary into structured meaning representations (MR) and encoded them using a graph encoder and a text encoder. @CITE[6]@ introduced a fine-grained inconsistency detection model, FineGrainFact, which represented facts in the documents and summaries with semantic frames extracted by semantic role labeling. @CITE[5]@ proposed a model called SummaCConv that enabled NLI models to be successfully used for inconsistency detection by segmenting documents into sentence units and aggregating scores between pairs of sentences. @CITE[3]@ proposed a novel learning algorithm that maximized the proposed metric during model training. @CITE[9]@ presented a contrastive learning formulation that leveraged both reference summaries and automatically generated erroneous summaries to train summarization systems that are better at distinguishing between them. @CITE[7]@ proposed a training strategy that improved the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called ConFiT. @CITE[8]@ proposed a novel framework for generating training examples that are informed by the known styles and strategies of human-authored propaganda. @CITE[10]@ introduced a new approach to improve reference quality while retaining all data: to selectively re-write unsupported reference sentences to better reflect source data. @CITE[1]@ proposed a collaborative language model that is trained to imitate the entire writing process itself. @CITE[2]@ presented a method that leverages the knowledge of the model to revise the summaries and improve the faithfulness of the generated summaries.\\nReferences:ID: 1. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 2. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\\nID: 3. Title: Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization\\nID: 4. Title: SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\\nID: 5. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 6. Title: CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization\\nID: 7. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 8. Title: Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation\\nID: 9. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 10. Title: Learning to Revise References for Faithful Summarization\\nID: 11. Title: PEER: A Collaborative Language Model',\n",
       "  'ground_truth': '\\n\\nPrevious studies addressing faithfulness are mostly in the summarization field and can be roughly divided into two categories, evaluation and enhancement. Within faithfulness evaluation, one line of work developed entailment-based metrics by training document-sentence entailment models on synthetic data (@CITE[7]@; Yin et al., 2021) or human-annotated data (@CITE[8]@; @CITE[11]@, or applying conventional NLI models at the sentence level (@CITE[6]@. Another line of work evaluates faithfulness by comparing information units extracted from summaries and input sources using QA (; Deutsch et al., 2021). There is a recent study that integrates QA into entailment by feeding QA outputs as features to an entailment model (@CITE[9]@. We combine QA and entailment by using entailment to score the correction candidates produced by QA. Within faithfulness enhancement, some work improves factual consistency by incorporating auxiliary losses into the training process (@CITE[1]@; @CITE[4]@; @CITE[5]@;. Some other work devises factuality-aware pre-training and fine-tuning objectives to reduce hallucinations (@CITE[5]@. The most similar to our work are studies that utilize a separate rewriting model to fix hallucinations in summaries. For example, Cao et al. (2020) present a post-hoc corrector trained on synthetic data, where negative samples are created via perturbations. @CITE[10]@ fix factually inconsistent information in the reference summaries 6We are not able to compare with PEER (@CITE[2]@ as its checkpoints have not been released by the time we ran the experiments. to prevent the summarization from learning hallucinating examples. @CITE[3]@ propose a compression-based post-editor to correct extrinsic errors in the generated summaries. By contrast, we leverage the power of QA and entailment together to address faithfulness.\\nReferences:ID: 1. Title: Evaluating the Factual Consistency of Abstractive Text Summarization\\nID: 2. Title: FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations\\nID: 3. Title: Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization\\nID: 4. Title: SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization\\nID: 5. Title: Improving Factual Consistency of Abstractive Summarization via Question Answering\\nID: 6. Title: CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization\\nID: 7. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 8. Title: Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation\\nID: 9. Title: CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning\\nID: 10. Title: Learning to Revise References for Faithful Summarization\\nID: 11. Title: PEER: A Collaborative Language Model<|eot_id|>'},\n",
       " 5151: {'predict': 'Our work is related to a line of research that uses sequence modeling to make decisions. For example, @CITE[3]@, @CITE[4]@, and @CITE[5]@ frame decision making as a sequence modeling problem, where the goal is to predict a sequence of actions that leads to a sequence of rewards. Other work uses sequence modeling to generate action sequences, but in a more general sense (@CITE[1]@; @CITE[6]@; @CITE[2]@;. In particular, @CITE[1]@, @CITE[6]@, and @CITE[2]@ use a Transformer model to generate action sequences from a sequence of states and rewards. @CITE[6]@ and @CITE[2]@ use a masked Transformer to predict actions conditioned on a sequence of states, and @CITE[1]@ uses a causal masked Transformer to generate actions conditioned on a sequence of states and rewards. Our method is also related to the idea of imagination-based RL (@CITE[5]@;, which aims to use a model of the world to imagine future states and rewards, and then use these imagined states and rewards to train an RL agent. However, in contrast to these methods, we use a model of the world to imagine future states and rewards, and then use these imagined states and rewards to train a sequence model. This allows us to leverage the power of sequence models to generate action sequences that are longer than what can be generated by a model of the world, and to use the model of the world to generate longer action sequences than what can be generated by the agent. This allows us to use the model of the world to imagine longer action sequences than what can be generated by the agent, and to use these imagined action sequences to train a sequence model.\\nReferences:ID: 1. Title: Offline Reinforcement Learning as One Big Sequence Modeling Problem\\nID: 2. Title: Decision Transformer: Reinforcement Learning via Sequence Modeling\\nID: 3. Title: Solving Quantitative Reasoning Problems with Language Models\\nID: 4. Title: Masked Trajectory Models for Prediction, Representation, and Control\\nID: 5. Title: Representation Matters: Offline Pretraining for Sequential Decision Making\\nID: 6. Title: Dream to Control: Learning Behaviors by Latent Imagination',\n",
       "  'ground_truth': '\\n\\nInspired by the success of sequence modeling in language models, some recent work (@CITE[1]@; @CITE[4]@; @CITE[2]@; Zeng et al., 2023; @CITE[6]@) formulates decision making problems using sequence modeling by treating actions as tokens. Although decision making via sequence modeling has shown impressive performance on downstream tasks, sequence modeling objective does not explicitly drive the models to learn abstract and updatable representations of environments, which have been shown to be essential for solving decision making problems (@CITE[3]@; @CITE[5]@). Thus, in order to further understand why decision making via sequence modeling is successful despite minimal inductive biases, in this work we examine whether the sequence modeling objective leads to the emergence of such representations of environments.\\nReferences:ID: 1. Title: Offline Reinforcement Learning as One Big Sequence Modeling Problem\\nID: 2. Title: Decision Transformer: Reinforcement Learning via Sequence Modeling\\nID: 3. Title: Solving Quantitative Reasoning Problems with Language Models\\nID: 4. Title: Masked Trajectory Models for Prediction, Representation, and Control\\nID: 5. Title: Representation Matters: Offline Pretraining for Sequential Decision Making\\nID: 6. Title: Dream to Control: Learning Behaviors by Latent Imagination<|eot_id|>'},\n",
       " 4066: {'predict': 'Zero-shot learning and reasoning have been studied in the context of LLMs. Zero-shot learning typically involves prompting LLMs with a few examples of the task and asking them to complete the task without further training (@CITE[5]@; @CITE[4]@; @CITE[2]@; @CITE[3]@; @CITE[1]@; @CITE[3]@. Zero-shot reasoning involves using LLMs to reason about a question or problem without explicit training on the specific task. Recent work has shown that LLMs can be used for zero-shot reasoning (@CITE[3]@; @CITE[1]@; @CITE[4]@; @CITE[5]@; @CITE[2]@; ; @CITE[1]@. In this work, we use LLMs to generate explanations for the output of other LLMs, and we show that these explanations can be used to improve the performance of the LLMs on downstream tasks.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 3. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 4. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 5. Title: Training language models to follow instructions with human feedback',\n",
       "  'ground_truth': '\\n\\nLarge- scale unsupervised pre-trained LMs like GPT- 3 (@CITE[4]@, GPT-4 (@CITE[5]@, and PaLM (@CITE[2]@ are able to perform zero-shot learning on many downstream tasks with a task description provided at inference time. Instruction-finetuned LMs (@CITE[3]@; @CITE[2]@;, which are pre-trained on multiple supervised tasks using human instructions, also also exhibit robust zero- shot learning capabilities. pro- pose a new scheme of zero-shot reasoning, which first prompts large LMs to generate relevant docu- ments and then perform reading comprehension on the generated contents. Recently, there has been a growing trend of utilizing plug-and-play knowledge injection to enhance the zero-shot per- formance of LMs, which is achieved through map- ping network (@CITE[1]@ or document encoding (@CITE[1]@. Our work improves the zero-shot generalization of LMs by utilizing the retrieved information. We demonstrate that identi- fying LMs’ preferences to train the retriever can in turn bring additional evidence texts for LMs.\\nReferences:ID: 1. Title: GPT-4 Technical Report\\nID: 2. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 3. Title: Multitask Prompted Training Enables Zero-Shot Task Generalization\\nID: 4. Title: PaLM: Scaling Language Modeling with Pathways\\nID: 5. Title: Training language models to follow instructions with human feedback<|eot_id|>'},\n",
       " 2738: {'predict': 'Contract Review is a challenging task that has attracted significant research attention in recent years. Most existing methods for contract review are based on sequence labeling, and they focus on identifying the important clauses and elements of contracts, such as payment terms and confidentiality clauses, for further review. For example, @CITE[2]@ proposed a multi-task learning framework that jointly predicts the types of contract clauses and identifies the clauses of interest. @CITE[3]@ proposed a graph neural network-based framework that models the relationships between clauses and elements. @CITE[1]@ proposed a task-specific dataset, CUAD, and a baseline model for contract review. However, these methods focus on identifying the clauses and elements of contracts, rather than understanding the meaning of the clauses and elements. In contrast, our method focuses on identifying the clauses and elements of contracts, and understanding the meaning of the clauses and elements. In addition, our method can be used to identify the clauses and elements of contracts in a multi-document setting, which is not considered in previous work.\\nReferences:ID: 1. Title: Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines\\nID: 2. Title: Cross-Domain Contract Element Extraction with a Bi-directional Feedback Clause-Element Relation Network\\nID: 3. Title: CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review',\n",
       "  'ground_truth': '\\n\\nEarlier works start from classifying lines of contracts into predefined labels, where handcrafted rules and simple machine learning methods are adopted (@CITE[3]@). Then, some works take further steps to analyze contracts in a fine granularity, where a small set of contract elements are supposed to be extracted, including named entities (@CITE[1]@), parties’ rights and obligations (Funaki et al., 2020), and red-flag sentences (Leivaditi et al., 2020). They release corpora for automatic contract review, allowing neural models to get surprising performance (Chalkidis and Androutsopoulos, 2017; ). Recently, studies grow increasing attention on CCE to extract clauses, which are complete units in contracts, and carefully select a large number of clause types worth human attention (; ; @CITE[2]@). Due to the repetition of contract language that new contracts usually follow the template of old contracts (Simonson et al., 2019), existing methods tend to incorporate structure information to tackle CCE. For example, @CITE[1]@ assign a fixed extraction zone for each clause type and limit the clauses to be extracted from corresponding extraction zones. leverage visual cues such as document layout and placement as additional features to better understand contracts.\\nReferences:ID: 1. Title: Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines\\nID: 2. Title: Cross-Domain Contract Element Extraction with a Bi-directional Feedback Clause-Element Relation Network\\nID: 3. Title: CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review<|eot_id|>'},\n",
       " 511: {'predict': 'Continual learning aims to learn from data streams that are partially labeled and continuously updated. Catastrophic forgetting (@CITE[1]@; @CITE[3]@) is a common problem in continual learning, which means that the model’s performance on previously seen tasks degrades after learning new tasks. To alleviate this problem, several approaches have been proposed, including regularization-based methods (@CITE[6]@), parameter isolation-based methods (@CITE[4]@), and knowledge distillation-based methods (@CITE[2]@; @CITE[5]@). Recently, some works (@CITE[6]@; @CITE[5]@) have attempted to apply continual learning in the context of multi-modal learning. However, the problem of catastrophic forgetting has not been fully addressed, especially in the case of multi-modal continual learning.\\nReferences:ID: 1. Title: An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\\nID: 2. Title: Learning without Forgetting\\nID: 3. Title: Overcoming catastrophic forgetting in neural networks\\nID: 4. Title: Continual Lifelong Learning with Neural Networks: A Review\\nID: 5. Title: Distilling the Knowledge in a Neural Network\\nID: 6. Title: Learning without Memorizing',\n",
       "  'ground_truth': '\\n\\nIt is similar to lifelong learning, but its main goal is to overcome catastrophic forgetting to ensure learning of a new task will not forget the models learned for previous tasks (; @CITE[6]@. For example, LWF ( uses knowledge distillation loss to ensure that after learning a new task, it can still approximate the performance of the old tasks. EWC (@CITE[1]@ introduces constraints to control parameter changes when learning a new task. HAT (Serre et al., 2018) masks units that are important to previous tasks by a hard attention. PGMA (@CITE[3]@ generates a subset of parameters. Two reviews of continual learning can be found in (@CITE[2]@; @CITE[4]@. Our lifelong learning setting focuses on transferring the past knowledge to the current task. We don’t ensure that the models learned in the past still work well after learning a new task. Although Progressive Networks (Rusu et al., 2016) also tries to help future learning through knowledge transfer, but it is not scalable as its network size scales quadratically in the number of tasks. Knowledge Distillation Loss was proposed in ( for transferring knowledge in a large model to a smaller one. LWF uses knowledge distillation to help deal with forgetting. @CITE[5]@ proposed an information preserving penalty, attention distillation loss, to preserve the information about existing classes. This setting is different from ours as it incrementally learns more classes. Each of our tasks is an independent sentiment classification problem with multiple classes.\\nReferences:ID: 1. Title: An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\\nID: 2. Title: Learning without Forgetting\\nID: 3. Title: Overcoming catastrophic forgetting in neural networks\\nID: 4. Title: Continual Lifelong Learning with Neural Networks: A Review\\nID: 5. Title: Distilling the Knowledge in a Neural Network\\nID: 6. Title: Learning without Memorizing<|eot_id|>'},\n",
       " 2763: {'predict': 'Prompt learning in NLP has been studied extensively in recent years (@CITE[1]@). The goal of prompt learning is to use a template (i.e., prompt) to elicit the knowledge stored in the pretrained language models (PLMs) to perform downstream tasks. Recent studies have explored various prompt learning techniques, such as prompt tuning (@CITE[2]@), soft prompt tuning (@CITE[8]@), and delta tuning (@CITE[5]@). Some works have also studied the relationship between prompts and model parameters (@CITE[7]@; @CITE[6]@; @CITE[3]@). However, there is limited work that explores the relationship between prompts and the model architecture. @CITE[4]@ proposed a prompt-based model architecture, where a prompt generator is trained to generate prompts for downstream tasks. However, this work is orthogonal to our work, as it focuses on the prompt generator and does not explore the impact of the model architecture on the prompt. Our work fills this gap by studying the impact of the model architecture on the prompt.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models\\nID: 3. Title: Sparse Teachers Can Be Dense with Knowledge\\nID: 4. Title: The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models\\nID: 5. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System\\nID: 6. Title: ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\\nID: 7. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 8. Title: The Power of Scale for Parameter-Efficient Prompt Tuning',\n",
       "  'ground_truth': '\\n\\nWith the development of GPT-3 (Brown et al., 2020), prompt learning has drawn much attention in the NLP community (@CITE[7]@; @CITE[3]@; @CITE[6]@, which enables efficient learning by adding a number of prompt tokens to the input. Prompt learning has been proven to be effective in various downstream tasks (Davison et al., 2019; ; @CITE[4]@; @CITE[8]@; @CITE[5]@. Recently, prompt has been extended from discrete tokens (tokens in the vocabularies) to continuous tokens (trainable embeddings), i.e., soft prompt (; @CITE[1]@; @CITE[2]@. For example ( proposes a parameter-efficient prompt tuning approach by only tuning soft prompts and fixing the entire parameters in PLM. Prompt tuning achieves great success and shows that it can reach the performance of fine-tuning with large PLM.\\nReferences:ID: 1. Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\\nID: 2. Title: Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models\\nID: 3. Title: Sparse Teachers Can Be Dense with Knowledge\\nID: 4. Title: The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models\\nID: 5. Title: UnifiedQA: Crossing Format Boundaries With a Single QA System\\nID: 6. Title: ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning\\nID: 7. Title: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts\\nID: 8. Title: The Power of Scale for Parameter-Efficient Prompt Tuning<|eot_id|>'},\n",
       " 3203: {'predict': 'PLMs are widely used for many downstream tasks. However, the long-tailed distribution of these tasks is a common issue. Existing methods for addressing the problem can be divided into three categories: loss reweighting, data re-sampling, and transfer learning. @CITE[3]@ proposed a focal loss to reduce the impact of easy examples. @CITE[1]@ proposed a dynamic meta-embedding to integrate the knowledge from head and tail classes. @CITE[4]@ proposed to decouple representation and classifier to solve the problem. @CITE[2]@ proposed to use PLMs to solve the problem. @CITE[5]@ proposed to prompt PLMs to solve the problem. However, the problem of long-tailed distribution is still not solved. In this paper, we propose to use a novel prompt to solve the problem of long-tailed distribution. The prompt is generated based on the information from the head classes. The prompt is used to prompt PLMs to solve the problem of long-tailed distribution.\\nReferences:ID: 1. Title: Focal Loss for Dense Object Detection\\nID: 2. Title: Large-Scale Long-Tailed Recognition in an Open World\\nID: 3. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 4. Title: Decoupling Representation and Classifier for Long-Tailed Recognition\\nID: 5. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts',\n",
       "  'ground_truth': '\\n\\nIn contrast, little work has been investigated to make PLMs good long-tailed learners. Intuitively, the tail classes are essentially few-shot ones. Thus, we speculate that Prompt-tuning is also a promising choice to make PLMs good long-tailed learners via transferring knowledge of head classes. As long-tailed classification is a long-standing problem in the general area of machine learning (@CITE[5]@; @CITE[2]@; @CITE[1]@; @CITE[3]@; @CITE[4]@) and the long-tailed phenomenon also exists in the domain of natural language processing, we believe that a systematic exploration on whether and why Prompt-tuning can make PLMs good long-tailed learners will facilitate further advances in the related areas.\\nReferences:ID: 1. Title: Focal Loss for Dense Object Detection\\nID: 2. Title: Large-Scale Long-Tailed Recognition in an Open World\\nID: 3. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 4. Title: Decoupling Representation and Classifier for Long-Tailed Recognition\\nID: 5. Title: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts<|eot_id|>'},\n",
       " 4252: {'predict': 'Analogy is a long-standing challenge in AI research. @CITE[4]@ discuss the advantages and limitations of various approaches to construct AI systems with analogy-making abilities. Recently, there has been a growing interest in developing models with visual analogy-making abilities, such as RAVEN (@CITE[2]@) and RPM (@CITE[3]@). To learn analogies, researchers have proposed various methods, such as rule-aware network (@CITE[1]@), meta-analogical contrastive learning (@CITE[6]@), and probabilistic abductive reasoning (@CITE[7]@). These methods have achieved promising results in solving visual analogy-making tasks. However, these methods are mainly designed for solving RPM-style tasks, which is a specific type of analogy-making task. The analogy-making task in our work is more complex and requires models to learn the analogies from a single example. In addition, our task is not limited to a specific type of analogy, and the analogies are more abstract and implicit.\\nReferences:ID: 1. Title: Abstraction and Analogy-Making in Artificial Intelligence\\nID: 2. Title: RAVEN: A Dataset for Relational and Analogical Visual rEasoNing\\nID: 3. Title: Stratified Rule-Aware Network for Abstract Visual Reasoning\\nID: 4. Title: Evaluating Understanding on Conceptual Abstraction Benchmarks\\nID: 5. Title: Learning to Make Analogies by Contrasting Abstract Relational Structure\\nID: 6. Title: Few-shot Visual Reasoning with Meta-analogical Contrastive Learning\\nID: 7. Title: Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution',\n",
       "  'ground_truth': '\\n\\nPast work has studied analogy in AI across various domains. @CITE[1]@ provides a comprehensive overview of these efforts, especially those applied in idealized symbolic domains. Here, symbolic and probabilistic methods have traditionally been applied (@CITE[3]@; ; @CITE[6]@). However, these approaches typically require hard-coding domain-specific concepts, and require substantial search through domain knowledge to operate on their target problems, thus making them unscalable. The creation of large-scale image datasets for analogy tasks here (@CITE[2]@; @CITE[4]@; ) have enabled further research with deep learning and neuro-symbolic methods (; @CITE[5]@; ; @CITE[7]@), which bring the advantage of requiring less ad-hoc encoding of domain knowledge, but require thousands of training examples to learn the tasks, still limiting their generalization capability.\\nReferences:ID: 1. Title: Abstraction and Analogy-Making in Artificial Intelligence\\nID: 2. Title: RAVEN: A Dataset for Relational and Analogical Visual rEasoNing\\nID: 3. Title: Stratified Rule-Aware Network for Abstract Visual Reasoning\\nID: 4. Title: Evaluating Understanding on Conceptual Abstraction Benchmarks\\nID: 5. Title: Learning to Make Analogies by Contrasting Abstract Relational Structure\\nID: 6. Title: Few-shot Visual Reasoning with Meta-analogical Contrastive Learning\\nID: 7. Title: Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution<|eot_id|>'},\n",
       " 7410: {'predict': 'Pre-trained models have shown impressive performance in many NLP tasks, and their pre-training methods can be divided into masked language modeling (MLM) and sequence-to-sequence modeling (@CITE[1]@; @CITE[2]@; @CITE[4]@; @CITE[6]@; @CITE[3]@; @CITE[5]@; @CITE[7]@; @CITE[5]@; @CITE[7]@). In the field of code generation, pre-trained models are widely used in many tasks, including code summarization (@CITE[3]@), code translation (@CITE[5]@), code generation (@CITE[6]@), and code repair (@CITE[1]@). However, due to the lack of large-scale code data, code pre-trained models usually focus on the MLM task, which only predicts a single token at a time. In contrast, our model is trained with sequence-to-sequence tasks and can generate a sequence of tokens. In addition, our model is also trained with a variety of tasks, including code generation, code summarization, and code translation, which enables it to learn more comprehensive semantic representations for code.\\nReferences:ID: 1. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 2. Title: CodeSearchNet Challenge: Evaluating the State of Semantic Code Search\\nID: 3. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 4. Title: Unified Pre-training for Program Understanding and Generation\\nID: 5. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 6. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 7. Title: Evaluating Large Language Models Trained on Code',\n",
       "  'ground_truth': '\\n\\nFollowing the success of pre-trained language models (@CITE[1]@ in NLP, code pre-trained models (CodePTMs) have recently demonstrated remarkable success in a wide range of downstream tasks in the domain of neural code intelligence. By pretraining on massive code-based data (e.g. GitHub repositories), these models can learn rich contextual representations that can be transferred to code-related downstream tasks. @CITE[5]@ use bimodal data from CodeSearchNet (@CITE[3]@ to train CodeBERT that shares the same model architecture as RoBERTa (@CITE[7]@. Then, GraphCodeBERT also uses the same architecture while additionally considering the inherent structure of code, specifically, the data flow graph. There are also models with encoder-decoder architectures, such as CodeT5 (Wang et al., 2021) and PLBART (@CITE[4]@, which inherit the multi-task training strategies of T5 ( and BART (. UniXcoder (@CITE[2]@ harness the UniLM (@CITE[7]@ architecture and is pre-trained on cross-modal data to support both code understanding and generation. Moreover, decoder-only CodePTMs are crafted to generate high-quality code sequences, which can be utilized for program synthesis ( and even excel in programming competitions (@CITE[6]@.\\nReferences:ID: 1. Title: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\\nID: 2. Title: CodeSearchNet Challenge: Evaluating the State of Semantic Code Search\\nID: 3. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 4. Title: Unified Pre-training for Program Understanding and Generation\\nID: 5. Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\nID: 6. Title: Unified Language Model Pre-training for Natural Language Understanding and Generation\\nID: 7. Title: Evaluating Large Language Models Trained on Code<|eot_id|>'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/xzou428/miniconda3/envs/yuh/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score:  {'bleu': 0.6322909254594729, 'precisions': [0.6549036365967267, 0.6326281529698943, 0.624260761340715, 0.6179820222786117], 'brevity_penalty': 1.0, 'length_ratio': 1.3565167992209057, 'translation_length': 234010, 'reference_length': 172508}\n",
      "rouge_score:  {'rouge1': 0.8680185422157359, 'rouge2': 0.837546809347565, 'rougeL': 0.8446929613673848, 'rougeLsum': 0.8498592602069218}\n",
      "bert_score:  0.9612765333652497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# calculate the BLEU score with HUGGINGFACE datasets\n",
    "import evaluate\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "bleu_score_metric = evaluate.load(\"bleu\")\n",
    "for id, res in res_dict.items():\n",
    "    bleu_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "bleu_score = bleu_score_metric.compute()\n",
    "print(\"bleu_score: \", bleu_score)\n",
    "\n",
    "rouge_score_metric = evaluate.load(\"rouge\")\n",
    "for id, res in res_dict.items():\n",
    "    rouge_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "rouge_score = rouge_score_metric.compute()\n",
    "print(\"rouge_score: \", rouge_score)\n",
    "\n",
    "bert_score_metric = evaluate.load(\"bertscore\")\n",
    "for id, res in res_dict.items():\n",
    "    bert_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "bert_score = bert_score_metric.compute(model_type=\"distilbert-base-uncased\")\n",
    "print(\"bert_score: \", np.mean(bert_score['precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_score:  {'bleu': 0.08927876998456319, 'precisions': [0.1245171334234342, 0.08852481468166333, 0.0789748685179792, 0.07298098846279122], 'brevity_penalty': 1.0, 'length_ratio': 4.362359508634715, 'translation_length': 292265, 'reference_length': 66997}\n",
      "rouge_score:  {'rouge1': 0.35857489742799686, 'rouge2': 0.23754719828266235, 'rougeL': 0.27778690266208955, 'rougeLsum': 0.2972290725568194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/xzou428/miniconda3/envs/yuh/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_score:  0.8776230165362358\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# calculate the BLEU score with HUGGINGFACE datasets\n",
    "import evaluate\n",
    "\n",
    "bleu_score_metric = evaluate.load(\"bleu\")\n",
    "for id, res in res_dict.items():\n",
    "    bleu_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "bleu_score = bleu_score_metric.compute()\n",
    "print(\"bleu_score: \", bleu_score)\n",
    "\n",
    "rouge_score_metric = evaluate.load(\"rouge\")\n",
    "for id, res in res_dict.items():\n",
    "    rouge_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "rouge_score = rouge_score_metric.compute()\n",
    "print(\"rouge_score: \", rouge_score)\n",
    "\n",
    "bert_score_metric = evaluate.load(\"bertscore\")\n",
    "for id, res in res_dict.items():\n",
    "    bert_score_metric.add(prediction=res[\"predict\"], reference=res[\"ground_truth\"])\n",
    "bert_score = bert_score_metric.compute(model_type=\"distilbert-base-uncased\")\n",
    "print(\"bert_score: \", np.mean(bert_score['precision']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('yuh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92619f85c62eb73280c07ca2268c8e47b90999a589aa097a1a08a504bd2fb2c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
